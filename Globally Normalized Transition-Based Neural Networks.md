# Globally Normalized Transition-Based Neural Networks

## 핵심 주장 및 주요 기여  
**1. 전역 정규화(Global Normalization)의 도입**  
- 전통적 전이 기반 파서의 국소 정규화(local normalization) 방식이 갖는 *label bias* 문제를 지적.  
- 전역 정규화(CRF 스타일)를 뉴럴 전이 모델에 결합하여, 빔 서치 상에서 전체 결정 시퀀스의 점수를 일관되게 비교할 수 있도록 함.  

**2. 순환구조 없이도 최첨단 성능 달성**  
- LSTM 등 순환 신경망 없이 단순 feed-forward 네트워크와 전역 정규화만으로 품사 태깅, 의존 구문 분석, 문장 압축에서 SOTA 성능 달성.  
- 전역 정규화를 통해 검색 과정 중 이전 결정의 수정 능력(revision capability)을 극대화.  

**3. 효율성 및 공개 구현**  
- TensorFlow 기반의 오픈소스 ‘SyntaxNet’ 및 영어 의존 구문 파서 “Parsey McParseface” 공개.  
- 비재귀(feed-forward) 구조 사용으로 LSTM 대비 처리 속도 수십 배 향상.  

***

## 1. 해결하고자 하는 문제  
국소 정규화 모델(pL)은 각 결정 $$d_j$$에 대해  

$$
p_L(d_j \mid d_{1:j-1}, x) = \frac{\exp \rho(d_{1:j-1}, d_j)}{\sum_{d'} \exp \rho(d_{1:j-1}, d')}  
$$  

를 적용하여 빔 서치를 수행하지만,  
- 이전 결정이 잘못되었을 때 이후 정보로 수정하기 어려운 *label bias* 문제가 발생.  
- 이로 인해 전역 최적화가 필요한 구조화 예측 과제에 한계 존재.

***

## 2. 제안하는 방법  

### 2.1 모델 구조  
- **Transition System**: 전이 기반 파싱 시스템(Nivre, 2006)  
- **Feature Embedding**: 위치별 단어·클러스터·문자 n-그램 임베딩  
- **Feed-Forward Network**: 1~2개 은닉층, 비재귀 구조  
- **빔 서치**: 빔 크기 $$B$$를 늘려 다양한 가설 유지  

### 2.2 전역 정규화(Global CRF Objective)  
전체 결정 시퀀스 $$d_{1:n}$$에 대해  

```math
p_G(d_{1:n}\mid x) = \frac{\exp \sum_{j=1}^n \rho(d_{1:j-1}, d_j;\theta)}{Z_G(\theta)},  
\quad Z_G(\theta) = \sum_{d'_{1:n}} \exp \sum_{j=1}^n \rho(d'_{1:j-1}, d'_j;\theta).
```

- **근사 학습**: 빔 서치 내에 골드 시퀀스가 사라지면 조기 업데이트(early update) 적용  
- **2단계 학습**: 국소 정규화 비용으로 초기 학습 후, 전역 정규화 비용으로 미세 조정  

***

## 3. 성능 향상 및 한계  

| 과제          | 로컬(B=1) | 로컬(B>1) | 전역(B>1) | LSTM 기반 비교 |
|---------------|-----------|-----------|-----------|----------------|
| 품사 태깅     | 97.4%     | 97.4%     | **97.5%** | 97.4%          |
| 의존 구문 파싱 | 92.9%     | 93.6%     | **94.6%** | 92.7%          |
| 문장 압축     | 78.7%     | 75.7%     | **81.4%** | 82.8%          |

- **Label Bias 해소**: 빔 서치 중 공백 압축 생성 등의 심각한 오류 없이 안정적 예측  
- **효율성**: 모든 실험에서 LSTM 대비 학습·추론 속도 10×–100× 빠름  
- **한계**:  
  - 전역 분할 함수 $$Z_G$$ 계산이 근사적(빔)이며, 완전 최적화 보장은 어려움  
  - 복잡한 트랜지션 시스템에 적용 시 근사 오차 증가 가능성

***

## 4. 모델의 일반화 성능 향상 관련 고찰  
- **표현력 증대**: 전체 시퀀스 점수를 전역적으로 최적화하여, 국소 결정이 장기문맥에 미치는 왜곡 완화  
- **Early Update**: 훈련 시 골드 경로가 사라지는 즉시 경사하강을 적용해, 희귀 구조에도 강건  
- **비재귀 구조**: 순환 구조의 과적합 위험 감소, 적은 파라미터로 더 넓은 데이터 분포 일반화  

***

## 향후 연구에 미치는 영향 및 고려 사항  
- **영향**: 전역 정규화 전이 기반 패러다임을 널리 확산시키며, 다양한 구조화 예측 과제(번역·개체명 인식 등)에 응용 가능성  
- **고려점**:  
  1. **근사화 오차 감소**: 빔 외의 전역 파라미터 근사화 기법 연구  
  2. **정규화 결합**: 전역+국소 하이브리드 모델 및 더 복합적인 특성 기반 함수 설계  
  3. **대규모 비식별화**: 비재귀 모델의 일반화 한계를 극복하기 위해 확률적 정규화와 대규모 언어모델 결합 검토

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b859604f-44c2-4aeb-b9e1-a405d50c03c9/1603.06042v2.pdf)
