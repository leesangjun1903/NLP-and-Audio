# Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning

### 1. 핵심 주장 및 주요 기여

**Agent0**는 인간이 제작한 데이터에 의존하지 않고 완전히 자율적으로 고성능 에이전트를 진화시키는 혁신적인 프레임워크입니다. 기존의 자기진화 방식이 모델의 내재적 능력에 의해 제한되고 단일 턴 상호작용만 가능한 문제를 해결하기 위해, Agent0는 **도구 통합**과 **다단계 공진화(co-evolution)**를 결합했습니다.[1]

**핵심 주장:**
- LLM 에이전트의 학습은 인간 주석 데이터에 의존하는 강화학습(RL/RLHF)에 제한되어 있으며, 이는 확장성 병목과 인간 지식의 한계를 초래합니다[1]
- 기존 자기진화 프레임워크는 모델의 고유 능력으로 제한되어 복잡한 도구 사용이나 다단계 추론을 포함한 커리큘럼을 발전시킬 수 없습니다[1]
- 외부 도구의 통합과 두 에이전트 간의 공진화를 통해 이러한 한계를 극복할 수 있습니다[1]

**주요 기여:**
- 완전 자율적 공진화 프레임워크로 Qwen3-8B-Base 모델을 수학적 추론에서 18%, 일반 추론에서 24% 향상시켰습니다[1]
- 커리큘럼 에이전트와 실행 에이전트 간의 "공생적 경쟁"을 통한 자기강화 순환 구조 구축[1]
- 다단계 도구 통합 상호작용을 지원하는 프레임워크 확장으로 현실의 복잡한 문제 해결 능력 향상[1]

***

### 2. 해결하는 문제, 제안 방법, 모델 구조

#### 2.1 핵심 문제

Agent0가 해결하는 주요 문제들은 다음과 같습니다:[1]

1. **인간 데이터 의존성의 확장성 병목**: 대규모 고품질 인간 주석 데이터 수집의 시간 소비적, 비용 집약적 특성
2. **자기진화 프레임워크의 능력 한계**: 생성되는 과제가 모델의 현재 복잡도를 거의 넘지 못해 학습 정체 발생
3. **단일 턴 상호작용의 제약**: 실제 문제 해결에 필요한 다단계 추론과 동적 맥락 처리 불가능
4. **도구 사용의 미흡한 활용**: 외부 도구의 능력을 활용한 체계적인 커리큘럼 생성 부재

#### 2.2 제안 방법론

Agent0의 방법론은 두 에이전트의 반복적 공진화에 기반합니다. 각 반복 t에서 다음 두 단계가 진행됩니다:[1]

**단계 1: 커리큘럼 에이전트 훈련(Curriculum Evolution)**

커리큘럼 에이전트 $$\pi_\theta$$는 현재 실행 에이전트 $$\pi_{\phi}^{(t-1)}$$를 도전하는 과제 $$x$$를 생성하도록 RL로 훈련됩니다. 복합 보상 신호 $$R_C$$는 세 가지 요소로 구성됩니다:[1]

**자기 일관성 기반 불확실성 보상(Uncertainty Reward):**

$$R_{unc}(x; \pi_\phi) = 1 - 2|\hat{p}(x; \pi_\phi) - 0.5|$$

여기서 $$\hat{p}(x; \pi_\phi)$$는 실행 에이전트가 생성한 k개 응답 중 다수의 답변과 일치하는 응답의 비율입니다. 이 함수는 $$\hat{p} = 0.5$$일 때 최대화되어 에이전트가 혼동하는 과제를 선호합니다.[1]

**도구 사용 보상(Tool Use Reward):**

$$R_{tool}(x; \pi_\phi) = \gamma \cdot \min(N_{tool}(y), C)$$

여기서 $$N_{tool}(y)$$는 완전한 예측 y에서 도구 호출(```output```

**반복 페널티(Repetition Penalty):**

$$R_{rep}(x_i) = \lambda_{rep} \frac{|C_k|}{B}$$

BLEU 유사성을 이용해 작업들을 클러스터 $$C = \{C_1, ..., C_K\}$$로 그룹화하고($$d_{ij} < \tau_{BLEU}$$), 클러스터 크기에 비례하여 페널티를 적용합니다.[1]

**복합 보상 함수:**

$$R_C(x_i) = R_{format}(x_i) \cdot \max(0, (\lambda_{unc}R_{unc} + \lambda_{tool}R_{tool}) - R_{rep}(x_i))$$

커리큘럼 에이전트는 Group Relative Policy Optimization(GRPO) 알고리즘으로 업데이트됩니다.[1]

**단계 2: 실행 에이전트 훈련(Executor Evolution)**

실행 에이전트는 다음 두 개의 혁신적인 메커니즘을 통해 훈련됩니다:[1]

**도전적 데이터셋 구성과 자기 일관성 기반 필터링:**

동결된 커리큘럼 에이전트 $$\pi_\theta^{(t)}$$에서 생성된 과제 풀 중, 자기 일관성 점수가 정보 대역 내에 있는 과제들만 선별합니다:[1]

$$D^{(t)} = \{x \in X_{pool} \mid |\hat{p}(x; \pi_\phi^{(t-1)}) - 0.5| \leq \delta\}$$

여기서 δ는 난이도 조절 임계값(보통 0.25, 자기 일관성 0.3-0.8 범위)입니다.[1]

**다단계 도구 통합 롤아웃:**

표준 단일 턴 생성 대신 상호작용적 다단계 프로세스를 사용합니다. 각 k개 궤적은:[1]
1. 정책 $$\pi_\phi^{(t-1)}$$가 텍스트 추론 $$t_1$$을 생성
2. 도구 호출 트리거(```python...```
3. 코드 $$c_1$$을 샌드박스에서 실행하여 결과 $$f_1$$ 반환
4. 피드백을 ```output...```
5. 정책이 최종 답변(```boxed{}```

이 동적 인터리빙 피드백 메커니즘은 자기 수정의 "아하" 순간을 모방합니다.[1]

#### 2.3 Ambiguity-Dynamic Policy Optimization (ADPO)

표준 GRPO는 모든 샘플을 동등하게 취급하므로, 다수결 투표에서 파생된 의사 레이블의 신뢰성 차이를 고려하지 않습니다. ADPO는 이를 해결하기 위해 두 가지 수정을 제안합니다:[1]

**자기 일관성 인식 이점 스케일링:**

의사 레이블의 노이즈가 높은 작업(낮은 $$\hat{p}(x)$$)에 대해 훈련 신호를 축소합니다:[1]

$$\tilde{A}_i(x) = \hat{A}_i \cdot s(x) = \hat{A}_i \cdot f(\hat{p}(x))$$

여기서 $$s(x) = f(\hat{p}(x))$$는 자기 일관성의 증가 함수로, 신뢰도가 낮은 샘플의 영향을 감소시킵니다.[1]

**자기 일관성 조정 신뢰 영역:**

표준 PPO의 고정 클리핑 ε은 불확실한 작업에서 새로운 추론 경로의 출현을 억제합니다. ADPO는 동적 상단 클리핑 경계를 도입합니다:[1]

$$L_{ADPO}(\theta) = \mathbb{E}_{x \sim D^{(t)}} \left[-\frac{1}{G}\sum_{i=1}^{G}\min\left(r_i(\theta)\tilde{A}_i(x), \text{clip}(r_i(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}(x))\tilde{A}_i(x)\right)\right]$$

여기서 $$\epsilon_{high}(x)$$는 $$\hat{p}(x)$$의 감소 함수로, 높은 불확실성(낮은 $$\hat{p}(x)$$) 작업에 대해 더 큰 정책 업데이트를 허용하여, 현재 정책 분포의 낮은 확률 영역에서 올바른 솔루션이 나타날 수 있게 합니다.[1]

#### 2.4 GRPO 알고리즘

양쪽 에이전트 모두 Group Relative Policy Optimization을 사용합니다:[1]

$$L_{GRPO}(\theta) = -\frac{1}{G}\sum_{i=1}^{G}\min\left(\frac{\pi_\theta(x_i)}{\pi_{\theta_{old}}(x_i)}\hat{A}_i, \text{clip}\left(\frac{\pi_\theta(x_i)}{\pi_{\theta_{old}}(x_i)}, 1-\epsilon, 1+\epsilon\right)\hat{A}_i\right) + \beta\text{KL}(\pi_\theta \| \pi_{\theta_{old}})$$

정규화된 이점은 z-점수 정규화로 계산됩니다:[1]

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^{G})}{\text{std}(\{r_j\}_{j=1}^{G}) + \epsilon_{norm}}$$

***

### 3. 성능 향상 결과

#### 3.1 수학적 추론 벤치마크

Qwen3-8B-Base 모델에 대한 Agent0의 성능 향상 결과는 다음과 같습니다:[1]

| 벤치마크 | 기본 모델 | 도구 포함 | +Socratic-Zero | +Agent0 | 향상도 |
|--------|---------|---------|----------------|--------|--------|
| 평균 | 49.2% | 53.2% | 56.1% | **58.2%** | +18% |
| AMC | 52.0% | 60.3% | 63.7% | **62.4%** | +10.4% |
| MATH | 50.0% | 54.9% | 52.4% | **61.3%** | +11.3% |
| GSM8K | 78.0% | 79.2% | 81.2% | **82.4%** | +4.4% |
| Olympiad | 89.1% | 90.7% | 87.3% | **94.5%** | +5.4% |
| AIME24 | 13.9% | 20.9% | 28.4% | **28.0%** | +14.1% |

#### 3.2 일반 도메인 추론 벤치마크

일반화 성능 평가 결과:[1]

| 벤치마크 | 기본 모델 | +R-Zero | +Socratic-Zero | +Agent0 |
|--------|---------|---------|----------------|--------|
| 전체 평균 | 34.5% | 38.7% | 39.2% | **42.1%** |
| SuperGPQA | 28.3% | 31.4% | 30.1% | **33.0%** |
| MMLU-Pro | 51.8% | 58.2% | 60.9% | **63.4%** |
| BBEH | 8.6% | 10.6% | 9.5% | **13.7%** |

#### 3.3 반복별 일관된 개선

Figure 4에 따르면 3회 반복을 통해 안정적인 성능 향상이 이루어집니다:[1]
- Iteration 1: 수학 55.1%, 일반 40.7%
- Iteration 2: 수학 56.5%, 일반 41.3%
- Iteration 3: 수학 58.2%, 일반 42.1%
- **반복당 평균 약 2% 개선**

#### 3.4 과제 난이도 및 도구 사용의 진화

Table 5에 따르면 커리큘럼 에이전트가 점진적으로 더 어려운 과제를 생성합니다:[1]

| 데이터셋 | Iteration 1 통과율 | 평균 도구 호출 수 |
|--------|-----------------|-----------------|
| D^(Iter 1) | 64.0% | 1.65 |
| D^(Iter 2) | 58.5% | 2.10 |
| D^(Iter 3) | 51.0% | 2.60 |

고정된 실행 에이전트(Iteration 1)의 통과율이 감소하고 도구 호출이 증가하는 것은 커리큘럼이 더 복잡한 도구 기반 과제로 진화함을 증명합니다.[1]

#### 3.5 컴포넌트별 기여도 분석

Table 3의 절제 연구 결과:[1]

| 방법 | 일반 추론 | 수학 추론 | 감소량 |
|-----|---------|---------|--------|
| 완전 Agent0 | 36.7% | 58.2% | - |
| w/o 커리큘럼 훈련 | 29.5% | 46.8% | **-9.3%** |
| w/o 도구 보상 | 31.8% | 48.7% | **-7.2%** |
| w/o 반복 페널티 | 31.3% | 47.9% | **-7.1%** |
| w/o ADPO | 34.9% | 56.2% | **-1.9%** |
| w/o 다단계 상호작용 | 35.3% | 55.9% | **-1.8%** |

***

### 4. 모델 일반화 성능 향상 가능성

#### 4.1 크로스 도메인 일반화

Agent0의 가장 주목할 만한 특성은 **수학 도메인에서 학습한 능력이 일반 도메인 작업으로 효과적으로 전이**되는 것입니다. Table 2에 따르면:[1]

- 수학 추론으로만 훈련했음에도 불구하고, 일반 추론(SuperGPQA, MMLU-Pro, BBEH)에서 다른 모든 데이터 무료 방법을 능가합니다
- Qwen3-8B에서 일반 평균: 42.1% (최고 수행 성과)
- **특히 MMLU-Pro에서 63.4%로 Socratic-Zero의 60.9%를 초과**

이는 복잡한 다단계 추론 능력이 도메인 독립적 기술임을 시사합니다.[1]

#### 4.2 일반화 메커니즘

최신 연구에 따르면 도구 통합 강화학습의 일반화는 여러 핵심 요소에 의해 가능합니다:[2]

1. **도메인 독립적 도구 인터페이스**: 표준화된 도구 인터페이스와 명시적 종료를 통해 전이 가능한 호출 패턴 학습
2. **분해된 보상 시스템**: 도메인 불변 행동(도구 효율성, 추론 추상화)을 장려하는 다중 구성 요소 보상
3. **모듈형 프롬프트 구조**: XML 기반 템플릿으로 사고, 도구 호출, 응답 분리

#### 4.3 다단계 상호작용의 역할

Table 9의 다단계 상호작용 턴 수 분석에 따르면:[1]

| 턴 수 | 전체 평균 | 수학 평균 | 일반 평균 |
|-----|---------|---------|---------|
| 1 | 35.5% | 50.4% | 30.8% |
| 2 | 35.8% | 50.7% | 31.1% |
| 3 | 36.1% | 51.2% | 31.3% |
| 4 | **36.7%** | **51.9%** | **31.6%** |

4턴 설정에서 단일 턴 기준 대비 **전체 3.4%, 수학 3%, 일반 2.6% 개선**. 이는 맥락 의존성이 있는 긴 부분에 걸친 논리적 일관성을 유지하는 능력 향상을 의미합니다.[1]

#### 4.4 소형 모델의 일반화

Qwen3-4B-Base에 대한 결과도 유사한 패턴을 보입니다:[1]
- 수학 평균: 42.6% → 52.5% (+23.3%)
- 일반 평균: 27.1% → 37.6% (+38.8%)

소형 모델도 효과적인 일반화를 달성하여, Agent0의 스케일 불변성을 시사합니다.[1]

***

### 5. 주요 한계

Agent0는 혁신적이지만 다음과 같은 한계가 있습니다:[1]

1. **도구 의존성**: 코드 인터프리터와 같은 검증 도구가 필수적이므로, 검증 불가능한 도메인(예: 창의적 작성)에 적용 불가능
2. **계산 비용**: 다중 에이전트 공진화와 샌드박스 기반 도구 실행에 상당한 계산 자원 필요
3. **초기 성능 의존성**: 기본 모델의 초기 능력이 최종 성능의 상한선을 설정
4. **다양성 제약**: 반복 페널티를 통한 다양성 장려가 모든 도메인에서 동등하게 효과적이지 않을 수 있음
5. **수렴 속도**: 3회 반복에서 수렴이 둔화되는 경향, 추가 반복의 한계 수익 체감

***

### 6. 앞으로의 연구에 미치는 영향과 고려 사항

#### 6.1 연구 영향

Agent0는 여러 주요 방향에서 후속 연구에 영향을 미칠 것으로 예상됩니다:[3][4][5][1]

**1. 자기진화 커리큘럼 학습의 정준화**

최신 연구인 Self-Evolving Curriculum (SEC)과 Multi-Agent Evolve (MAE)는 Agent0의 개념을 다양한 형태로 발전시킵니다. SEC는 커리큘럼 선택을 비정상 다중 팔 강도 문제로 모델링하여 더 정교한 자기진화 메커니즘을 제공합니다.[4][5][3]

**2. 도구 통합 강화학습의 도메인 일반화**

Chen et al. (2025)의 연구에 따르면, 도구 통합 RL이 학습 도메인을 넘어 효과적으로 일반화될 수 있음이 실증적으로 증명되었습니다. 표준화된 도구 인터페이스와 분해된 보상이 핵심입니다.[2]

**3. 멀티 에이전트 공진화 프레임워크의 확산**

Multi-Agent Evolve는 제안자(Proposer), 해결자(Solver), 판사(Judge)의 세 에이전트 구조로 확장되어, 검증 가능한 보상이 없는 도메인에서도 자가 개선이 가능함을 보여줍니다. 평균 4.54% 개선으로 일반 추론 능력 향상.[6][4]

**4. 웹 에이전트 및 실세계 응용**

WEBRL 프레임워크는 Agent0의 원리를 웹 기반 작업에 적용하여, 오픈소스 LLM 기반 웹 에이전트를 획기적으로 향상시켰습니다. Llama-3.1-8B가 4.8%에서 42.4%로 개선.[7]

#### 6.2 앞으로 연구 시 고려할 점

**1. 다양한 도구 생태계 통합**

현재 Agent0는 코드 인터프리터에 중점을 두지만, 향후 연구는 다양한 도구(검색, 데이터베이스 쿼리, 상징적 추론)를 통합해야 합니다. 도구 선택의 자동화와 도메인 특화 도구 개발이 중요합니다.[8][1]

**2. 보상 신호의 정교화**

자기 일관성만으로는 부족하며, 다양한 자가 감독 신호(신뢰도, 논리적 일관성, 도메인 특화 메트릭)의 조합이 필요합니다.[5][1]

**3. 안정성과 수렴성 보장**

공진화 시스템은 모드 붕괴와 학습 정체의 위험이 있습니다. 이론적 분석과 더 강력한 정규화 기법이 필요합니다.[3][1]

**4. 비정상 환경에서의 적응**

동적으로 변화하는 환경이나 장기간 학습 설정에서 Agent0의 성능을 이해하는 것이 중요합니다.[5]

**5. 효율성 최적화**

다중 에이전트 훈련의 계산 비용을 감소시키기 위해 지식 증류, 정책 통합, 효율적인 샌드박싱 기법이 필요합니다.[1]

**6. 도메인 적응 메커니즘**

모델의 일반화 능력은 주목할 만하지만, 저리소스 도메인이나 새로운 작업 유형으로의 적응 전략이 필요합니다.[2]

#### 6.3 최신 관련 연구 동향

**자기진화 에이전트의 확산 (2024-2025)**
- **METEOR (2024)**: 약한-강한 데이터 증류, 반복 훈련, 자기진화 전략의 3단계 진화 프로세스 제시[9]
- **Self-Evolving Curriculum (2025)**: 비정상 다중 팔 강도 접근으로 더 정교한 커리큘럼 학습 구현[3][5]
- **Tool-Integrated Reasoning의 일반화 (2025)**: 수학 도메인 외 다양한 도메인으로의 도구 사용 기술 전이 실증[2]

**응용 확대**
- **WebRL (2025)**: 웹 환경에서의 자기진화 에이전트 구현[7]
- **Multi-Agent Evolve (2025)**: 검증 가능한 보상 없는 일반 도메인에서의 자가 개선[4][6]
- **Code Evolution (2025)**: 전체 리포지토리 규모의 코드 진화, NP-완전 문제 해결[10]

***

### 결론

Agent0는 **인간 데이터 의존성을 완전히 제거하면서도 LLM 에이전트의 추론 능력을 획기적으로 향상**시키는 혁신적 프레임워크입니다. 커리큘럼 에이전트와 실행 에이전트의 공생적 경쟁, 도구 통합, 그리고 다단계 상호작용의 결합을 통해 "자기강화 순환"을 구현했습니다.[1]

특히 주목할 점은 **수학 도메인에서 학습한 복잡한 추론 능력이 일반 도메인으로 효과적으로 일반화**된다는 것입니다. 이는 도구 통합 추론이 도메인 독립적 메타 기술임을 시사하며, 향후 자기진화 에이전트 연구의 새로운 방향을 제시합니다.[2][1]

향후 연구는 도구 생태계 확장, 보상 신호 정교화, 효율성 최적화, 그리고 새로운 도메인으로의 적응 메커니즘에 중점을 두어야 할 것으로 예상됩니다.[5][7][1]

***

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/5e642a03-832f-407d-8a93-c06077ee407b/2511.16043v1.pdf)
[2](https://arxiv.org/html/2510.11184v1)
[3](https://openreview.net/pdf/0ef27194231b60f42775c4446a04fb05679fc2d5.pdf)
[4](https://arxiv.org/abs/2510.23595)
[5](https://arxiv.org/html/2505.14970v3)
[6](https://www.themoonlight.io/en/review/multi-agent-evolve-llm-self-improve-through-co-evolution)
[7](https://proceedings.iclr.cc/paper_files/paper/2025/file/c66e1fcc9691aae706250638f36f681b-Paper-Conference.pdf)
[8](https://www.emergentmind.com/topics/tool-integrated-reasoning-tir)
[9](http://arxiv.org/pdf/2411.11933.pdf)
[10](https://arxiv.org/abs/2509.07367)
[11](https://www.semanticscholar.org/paper/a647788b47b1bac9c137ab192316f72de52471d4)
[12](https://ijsshr.in/v8i8/101.php)
[13](https://arxiv.org/abs/2504.16084)
[14](https://diabetesjournals.org/diabetes/article/74/Supplement_1/1083-P/159316/1083-P-Interim-Success-of-an-Online-CME-Curriculum)
[15](https://ejournal.uin-suka.ac.id/tarbiyah/Ijber/article/view/11619)
[16](https://www.ewadirect.com/proceedings/ace/article/view/25643)
[17](https://elementaria.my.id/index.php/e/article/view/90)
[18](https://journal.iaisambas.ac.id/index.php/IJGIE/article/view/3816)
[19](https://arxiv.org/abs/2508.13382)
[20](http://arxiv.org/pdf/2310.00533v4.pdf)
[21](http://arxiv.org/pdf/2406.00606.pdf)
[22](https://arxiv.org/html/2502.07709v2)
[23](http://arxiv.org/pdf/2406.06326.pdf)
[24](http://arxiv.org/pdf/2405.07490.pdf)
[25](http://arxiv.org/pdf/2407.08937.pdf)
[26](http://arxiv.org/pdf/2407.11773.pdf)
[27](https://www.emergentmind.com/topics/self-evolving-reasoning-llm)
[28](https://openreview.net/forum?id=Ep0TtjVoap)
[29](https://ieeexplore.ieee.org/document/10600472/)
[30](https://relevanceai.com/prompt-engineering/learn-to-use-tool-integrated-reasoning-agents-for-problem-solving)
[31](https://arxiv.org/abs/2508.03864)
[32](https://www.puppyagent.com/ko/blog/ai-self-evolution)
