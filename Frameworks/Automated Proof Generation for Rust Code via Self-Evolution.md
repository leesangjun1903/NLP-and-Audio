# Automated Proof Generation for Rust Code via Self-Evolution

### 1. 핵심 주장과 주요 기여 (간결 요약)

**SAFE**는 **Rust 코드의 자동 증명 생성**이라는 도전적 문제를 해결하기 위한 혁신적 프레임워크입니다. 이 논문의 핵심 주장은 **데이터 부족을 극복하기 위해 자기진화(self-evolution) 메커니즘**을 통해 증명 생성 능력을 단계적으로 향상시킬 수 있다는 것입니다.[1]

**주요 기여:**

- **데이터 합성 - 검증 피드백 루프**: GPT-4o의 초기 부트스트래핑과 Verus 검증기의 신뢰성을 결합하여 인간 라벨링 없이 고품질 데이터를 생성합니다.[1]

- **자기디버깅(Self-debugging) 메커니즘**: 생성 과정에서 나온 오류 증명들을 학습 데이터로 재활용하여 모델이 오류 메시지를 통해 증명을 수정할 수 있도록 훈련합니다.[1]

- **성능 향상**: DeepSeekCoder 모델을 초기 형식 검증 미경험 상태에서 **VerusBench에서 43.17% → 79.14% (자기디버깅 적용 시)**로 개선했으며, GPT-4o의 11.51%를 크게 앞서갑니다.[1]

***

### 2. 문제 정의, 방법론 및 모델 구조

#### 2.1 문제의 다층적 구조

Rust 코드의 형식 검증을 위한 **Verus 증명 생성**에서 직면한 세 가지 데이터 부족 문제:[1]

1. **증명 친화적 프로그램 부족**: Verus는 모든 Rust 특성을 지원하지 않으며, GitHub에 500개 미만의 검증된 파일만 존재합니다.

2. **형식 명세 부족**: 함수의 전제조건(precondition)과 후제조건(postcondition)을 명시하는 Verus 스타일의 명세가 필요한데, 이는 존재하는 검증 파일에서만 얻을 수 있습니다.

3. **증명 및 주석 부족**: 루프 불변식(loop invariants)과 같은 증명 주석을 수작업으로 작성하는 것은 매우 어렵고 시간이 소요됩니다.

#### 2.2 SAFE 프레임워크의 3단계 방법론

**단계 1: Verus 호환 코드 생성**

GPT-4o를 사용하여 코드를 번역하고, Verus 컴파일러로 컴파일 가능한 코드만 필터링합니다.[1]

**단계 2: 자기진화 명세 합성**

명세의 품질을 정량적으로 측정하기 위해 다음 메트릭을 사용합니다:[1]

$$\text{Correctness}_S = \frac{|\{(i, o) | S(i, o) \text{ and } (i, o) \in T\}|}{|T|}$$

$$\text{Completeness}_S = \frac{|\{(i, o') | \neg S(i, o') \text{ and } (i, o') \in T'\}|}{|T'|}$$

여기서 $S(i, o)$는 명세 $S$가 입출력 쌍 $(i, o)$를 만족하는지를 나타내는 부울값입니다. **임계값**: 정확성 ≥80%, 완전성 ≥60%의 명세만 유지합니다.[1]

각 함수당 최대 3개의 명세를 보존하여 다음 단계의 증명 합성을 위한 다양성을 확보합니다.

**단계 3: 자기진화 증명 합성 - 핵심 학습 목표**

두 가지 작업을 결합한 공동 훈련:[1]

**작업 #1: 증명 생성 (Proof Generation)**

$$L_{Gen}(\theta) = -\sum_{i=0}^{N} \log P_\theta(y_i | S, y_{ < i})$$

여기서 $S$는 명세가 있는 코드 스니펫, $Y = (y_0, \ldots, y_N)$는 생성해야 할 증명입니다.[1]

**작업 #2: 자기디버깅 (Self-Debugging)**

$$L_{Debug}(\theta) = -\sum_{i=0}^{N} \log P_\theta(y_i | Y^X, \text{Error}_{Y^X}, y_{<i})$$

여기서 $Y^X$는 오류 증명, $\text{Error}_{Y^X}$는 Verus가 보고한 검증 오류, $Y^\checkmark$는 최종 올바른 증명입니다.[1]

#### 2.3 모델 구조 및 자기진화 메커니즘

**Architecture 개요** (알고리즘 1 참조):[1]

```
입력: model₀ (open-source LLM), rust_programs
출력: modelᵣ (자기진화된 LLM)

명세 생성:
  spec-data₀ ← GPT-4o.generate(programs).filter(Tests)
  for r in [1, ..., R₁]:
    modelᵣ ← modelᵣ₋₁.fine-tune(∪ᵢ₌₁ʳ⁻¹ spec-dataᵢ)
    spec-dataᵣ ← modelᵣ.generate(programs).filter(Tests)

증명 생성:
  proof-data₀, debug-data₀ ← GPT-4o.generate(programs).filter(Verus Verifier)
  for r in [1, ..., R₂]:
    modelᵣ ← modelᵣ₋₁.fine-tune(∪ᵢ₌₁ʳ⁻¹ (proof-dataᵢ + debug-dataᵢ))
    proof-dataᵣ ← modelᵣ.generate(programs).filter(Verus Verifier)
    debug-dataᵣ ← modelᵣ.debug(programs).filter(Verus Verifier)
```

**자기디버깅 데이터 생성 로직**:[1]

특정 프로그램에 대해 최종 올바른 증명 $Y^\checkmark = (y_0, \ldots, y_N)$이 여러 시도 끝에 생성되면, 그 전에 생성된 오류 증명들 $[Y_1, \ldots, Y_m]$에 대해:

$$\{Y^X, \text{Error}_{Y^X}, Y^\checkmark\} \rightarrow \text{self-debugging training data}$$

이러한 삼중 데이터는 모델이 Verus의 오류 피드백을 바탕으로 증명을 반복적으로 수정하는 능력을 학습하게 합니다.

***

### 3. 성능 향상 분석

#### 3.1 벤치마크 성능 비교

**VerusBench** (인간 작성 벤치마크):[1]

| 모델 | Accuracy@1 | Accuracy@10 | Accuracy@100 |
|------|-----------|-----------|-----------|
| GPT-4o (Raw) | 11.51% | 24.46% | 43.88% |
| SAFE (DeepSeekCoder) | 43.17% | 53.96% | 59.71% |
| SAFE+ (디버깅) | 49.64% | - | 70.50% |

**CodeNet-Test** (합성 벤치마크):[1]

| 모델 | Accuracy@1 | Accuracy@2 |
|------|-----------|-----------|
| GPT-4o (Raw) | 0.28% | 0.70% |
| SAFE (DeepSeekCoder) | 43.83% | 44.74% |
| SAFE+ (디버깅) | - | 48.43% |

#### 3.2 자기진화 단계별 성능 개선

**Round별 진화** (VerusBench):[1]

| 라운드 | Accuracy@1 | Accuracy@10 | 자기디버깅 적용 시 |
|------|-----------|-----------|----------|
| Round 1 | 32.37% | 41.73% | 33.81% |
| Round 2 | 42.45% | 49.64% | 51.08% |
| Round 3 | **43.17%** | **53.96%** | **70.50%** |

- **Round 1 → Round 2**: 유의미한 성능 향상 (특히 MBPP 및 Tutorial 부분집합)
- **Round 2 → Round 3**: 개선 폭 감소, 3라운드 종료 결정의 정당성

#### 3.3 자기디버깅 메커니즘의 효과

**디코딩 전략 분석** (Figure 2):[1]

- **Sampling+Greedy** (생성 단계에서 샘플링, 디버깅 단계에서 탐욕 알고리즘): 최고 성능
- **Greedy+Sampling** (생성 단계에서 탐욕, 디버깅 단계에서 샘플링): 더 낮은 성능
- **다중 자기디버깅 라운드의 한계**: 2-3라운드 이후 개선 폭 < 1%

**핵심 통찰**: 초기 생성에서의 다양성(sampling)이 중요하며, 각 오류 증명에 대해 단일 디버깅(greedy)으로도 충분합니다.

#### 3.4 명세 품질의 영향

**명세 품질에 따른 성능 저하** (Table 3):[1]

| 명세 품질 | Accuracy@1 | Accuracy@10 |
|---------|-----------|-----------|
| 고품질 (SAFE) | 43.17% | 53.96% |
| 혼합 품질 | 30.22% | 41.01% |
| 저품질 | 17.27% | 23.02% |

**성능 저하율**: 고품질 vs 저품질 명세 사용 시 최대 **60% 이상** 성능 감소

***

### 4. 모델의 일반화 성능 및 한계

#### 4.1 일반화 성능 분석

**크로스 도메인 성능** (VerusBench 부분집합):[1]

- **SV (C/Java → Rust 변환)**: Round 3에서 78.95% (GPT-4o 39.47% 대비 2배 향상)
- **MBPP (알고리즘)**: Round 3에서 30.77% (GPT-4o 19.23% 대비 1.6배)
- **Tutorial (기초 예제)**: Round 3에서 26.09% (GPT-4o 26.09% 동등)

→ **Tutorial 부분에서는 현저한 개선 없음**: 이미 간단한 작업이므로 자기진화의 이점이 제한적

#### 4.2 작은 모델에 대한 적응성

**DeepSeekCoder-1.3B로의 적용** (Table 8):[1]

- Raw: Accuracy@1 1.44%
- Round 3: Accuracy@1 21.58%
- Round 3 + 디버깅: Accuracy@1 27.34%

→ 자기진화 프레임워크가 **작은 백본 모델(1.3B)에서도 효과적**이며, 모델 크기 제약이 있는 환경에서도 확장 가능

#### 4.3 명시적 한계

**1. 대규모 실제 프로젝트로의 확장 제약**[1]

- 논문의 초점: **단일 함수 수준** 알고리즘 문제
- 문제점: 실제 대규모 Rust 프로젝트의 함수는 다른 함수/명세 함수에 대한 **의존성** 존재
- 해결 과제: 크로스 파일 의존성 해결, 컨텍스트 제공

**2. 세밀한 자기디버깅의 한계**[1]

- 현재 방식: 오류 증명과 올바른 증명의 전체 차이 학습
- 개선 필요: 어느 편집(edit)이 어느 오류를 수정하는지 추적하는 **fine-grained diff 분석**
- 결과: 다중 라운드 디버깅의 한계로 나타남

**3. 도메인 특화 증명 기법 부재**[1]

- 예시: Verus의 `reveal` 기능이 학습 데이터에 없으면 모델이 습득하지 못함
- 영향: 특정 유형의 증명(Listing 12의 홀수/짝수 필터링)에 대해 실패

**4. 합성 데이터의 한계**

- 데이터 합성 과정에서 생성된 코드와 명세가 자연 분포를 완전히 반영하지 못할 가능성
- 결과: CodeNet-Test에서 VerusBench 대비 전반적으로 낮은 성능

***

### 5. 논문의 연구 영향과 향후 고려사항

#### 5.1 기존 연구에 미치는 영향

**형식 검증 자동화 분야의 패러다임 전환**[1]

- **이전 접근**: Prompt engineering 기반 (제한된 일반화 능력) 또는 소수 인간 작성 증명에 의존
- **SAFE의 기여**: 데이터 부족 문제를 "합성+검증+피드백 루프"로 해결하는 새로운 프레임워크 제시

**AI 기반 코드 검증의 실용성 증진**[1]

- DeepSeekCoder 같은 open-source 모델을 **GPT-4o 수준 이상**으로 향상시킴
- 비용 효율성과 배포 용이성 개선

#### 5.2 향후 연구 시 고려할 점 (최신 동향 기반)

**2024-2025년 최신 연구 동향 고려:**[1]

1. **멀티태스크 학습 및 커리큘럼 학습 통합**
   - SAFE의 순차적 라운드를 더 정교한 커리큘럼으로 확장
   - 단순 증명 → 복잡 증명으로 점진적 학습

2. **LLM 아키텍처 개선과의 시너지**
   - 최신 대규모 모델(GPT-4 Turbo, Claude 3 등)과의 비교
   - Multi-modal 모델을 활용한 코드-증명 간 관계 학습

3. **형식 검증 도구의 다양화**
   - Dafny, F*, Lean 외 다른 검증 도구로의 확장
   - 각 도구의 특수성을 반영한 맞춤형 프레임워크 설계

4. **오류 분류 및 대응 전략**
   - Listing 12 사례처럼 실패하는 패턴 분류
   - 각 오류 유형에 대한 전문화된 디버깅 전략

5. **Verus 와일드카드 프로그램 지원**
   - 현재: 알고리즘 코드 (단일 함수 수준)
   - 향후: 실제 시스템 코드(복수 함수, 상태 관리) 지원

6. **검증 가능성 예측 (Provability Prediction)**
   - 주어진 명세에 대해 사전에 증명 가능성 예측
   - 불가능한 명세 조기 필터링으로 효율성 개선

#### 5.3 실제 적용 시나리오

**근기(Near-term) 응용**:
- **시스템 보안 검증**: Rust 기반 보안 소프트웨어의 자동 증명 생성
- **산업 코드 검증**: 금융 소프트웨어, 블록체인 스마트 컨트랙트

**장기(Long-term) 비전**:
- **통합 개발 환경(IDE) 플러그인**: 실시간 증명 제안
- **자동 코드 리팩토링**: 증명 가능한 형태로 자동 변환

***

### 결론

**SAFE**는 **형식 검증 자동화**라는 장기 미해결 문제에 대해 **데이터 합성, 모델 진화, 오류 피드백 루프**의 혁신적 조합을 제시합니다. 43.17% → 79.14%의 성능 향상은 단순한 수치 개선을 넘어, **제한된 자원으로도 고급 형식 검증이 가능함**을 보여줍니다.

다만, 향후 연구는 다음을 중점 추진해야 합니다:
- **규모 확장성**: 단일 함수 → 모듈/시스템 수준
- **일반화 능력**: 합성 데이터 편향 극복
- **도메인 특수성**: Verus 고급 기능의 체계적 학습
- **실무 통합**: 산업 환경에서의 신뢰성 검증

이러한 방향의 진전이 이루어진다면, SAFE는 **안전성 중심의 미래 소프트웨어 개발 기반**을 마련하는 데 중추적 역할을 할 것으로 기대됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/701ff83b-f5a7-4f2d-9eeb-18d465be64e3/2410.15756v2.pdf)
