# Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation

### 1. 논문의 핵심 주장 및 주요 기여 요약

이 논문은 **온라인 대화형 환경에서 작동하는 GUI 자동화 시스템의 근본적인 문제**를 제시합니다. 기존의 다중모달 대규모 언어 모델(MLLM) 기반 GUI 에이전트는 각 단계에서의 오류에 높은 불관용성을 보이며, 특히 불가역적 작업(파일 삭제, 결제 등)의 위험성이 있습니다.[1]

논문의 **핵심 주장**은 다음과 같습니다:

- **사전 예방적 비판 메커니즘의 필요성**: 행동 실행 전에 문제를 진단하고 피드백을 제공해야 한다
- **비효율성 해결**: GUI 자동화에는 다양한 경로가 존재하므로, 사전 비판이 최적 경로 선택을 돕고 단계 수를 줄인다
- **일반화 성능 향상**: 새로운 환경과 응용 프로그램에 견고한 대응 능력의 중요성

**주요 기여**는 다음 세 가지입니다:[1]

1. **Suggestion-aware Group Relative Policy Optimization (S-GRPO)** 전략 도입 - 새로운 제안 보상(suggestion reward)을 포함한 혁신적 강화학습 접근
2. **추론 기반 데이터 수집 파이프라인** - GUI-Critic-Train(6k 고품질 데이터) 및 GUI-Critic-Test 데이터셋 구축
3. **GUI-Critic-R1 모델** - 7B 파라미터의 효율적 사전 비판 모델로 AndroidWorld에서 성공률 22.4%→27.6% 향상 달성[1]

***

### 2. 해결하는 문제, 제안 방법, 모델 구조, 성능 향상 및 한계

#### 2.1 해결하고자 하는 문제

**근본적 도전 과제**:

GUI 자동화는 오프라인 멀티모달 작업(VQA, OCR)과 달리 **온라인 대화형 환경**에서 작동합니다. 이는 다음의 특수한 문제를 야기합니다:[1]

- **누적적 오류**: 한 단계의 오류가 이후 모든 단계에 영향을 미쳐 작업 실패 초래
- **불가역 오류의 위험성**: 파일 삭제, 금전 거래 등 실행 후 복구 불가능한 오류 존재
- **반사 능력의 부족**: 현재 MLLM은 독립적으로 오류를 감지하고 정정하지 못함[1]

예를 들어, "음성 파일 이름 변경" 작업에서 에이전트가 실수로 "삭제" 버튼을 클릭하려 할 때, 이를 실행 후에 발견하면 파일이 영구 삭제되어 복구할 수 없습니다.[1]

#### 2.2 제안하는 방법 및 수식

**사전 비판(Pre-Operative Critic) 메커니즘**:

논문은 다음과 같은 형식적 정의를 제시합니다.[1]

**마르코프 의사결정 과정 정의**:
$$M = (E, A, P, \pi_{agent})$$

여기서:
- $\epsilon \in E$: 환경 상태(사용자 지시, 과거 상호작용, 현재 스크린샷)
- $A$: 행동 공간(click, long press, type, scroll, home, back, done)
- $P(\epsilon'|\epsilon, a)$: 상태 전이 함수
- $\pi_{agent}$: MLLM 기반 에이전트

**비판 모델 정의**:
$$\pi_{critic}(\epsilon, a) \rightarrow (l, c, s)$$

비판 모델은 상태 $\epsilon$와 행동 $a$를 입력받아 다음을 출력합니다:[1]

- $l \in $: 정확성 점수(correctness score)[1]
- $c$: 자연어 비판(critique explaining the rationale)
- $s$: 교정 제안(corrective suggestion if incorrect)

#### 2.3 데이터 수집 파이프라인

**추론 기반 부트스트래핑 전략**:[1]

$$E_{(\epsilon,a,l,s) \sim D_{c\_action}}(\bar{t_i}, \bar{l_i}, \bar{s_i})_{i=1}^{max} = \bar{\pi}(\epsilon, a)$$
$$\text{Select}(\bar{t_i}, \bar{l_i}, \bar{s_i}) | (\bar{l_i} = l) \land (\bar{s_i} = s)$$

이 수식은 우수 MLLM $\bar{\pi}$가 주어진 상태와 행동만으로 여러 번($max$번) 생각과 판단을 생성하고, 실제 주석과 일치하는 경우만 선택하는 프로세스를 나타냅니다.[1]

**점진적 CoT 패러다임**:[1]

생성된 각 출력은 다음 구조를 따릅니다:

```
<thinking>
**Observation**: 스크린샷 상태 분석 (사용자 지시 무시)
**Possible Result**: 행동 실행의 가능한 결과 예측
**Critique**: 행동이 맞는지 틀린지 평가 및 이유 제시
</thinking>
<score>Correct/Incorrect</score>
<suggestion>올바른 경우 요약, 틀린 경우 개선 제안</suggestion>
```

#### 2.4 S-GRPO 및 보상 함수

**제안 보상(Suggestion Reward)**:[1]

$$r_s(o) = I_{similar}(s, s')$$

여기서:
- $o = (l, s, c)$: 비판 모델의 출력
- $I_{similar}(\cdot)$: 대규모 언어 모델을 활용한 유사도 계산
- $s$: 생성된 제안, $s'$: 주석된 제안

**최종 보상 함수**:[1]

$$r(o) = \lambda_f \cdot r_f(o) + \lambda_s \cdot r_s(o) + (1 - \lambda_f - \lambda_s) \cdot r_a(o)$$

여기서:
- $r_f(o)$: 형식 보상(출력 구조 준수)
- $r_a(o)$: 정확도 보상(예측 점수 정확성)
- $\lambda_f, \lambda_s$: 가중치 파라미터

**최적화 목표**:[1]

$$L_{GRPO} = \frac{1}{|G|} \sum_{i=1}^{G} \min\left(\frac{\pi_{critic}(o_i|\epsilon,a)}{\pi_{\theta_{old}}(o_i|\epsilon,a)}A_i, \text{clip}\left(\frac{\pi_{critic}(o_i|\epsilon,a)}{\pi_{\theta_{old}}(o_i|\epsilon,a)}, 1-\varepsilon, 1+\varepsilon\right)A_i\right) - \beta D_{KL}(\pi_{critic}\|\pi_{ref})$$

여기서:
- $D_{KL}(\pi_{critic}\|\pi_{ref}) = \frac{\pi_{ref}(o_i|\epsilon,a)}{\pi_{critic}(o_i|\epsilon,a)} - \log\frac{\pi_{ref}(o_i|\epsilon,a)}{\pi_{critic}(o_i|\epsilon,a)} - 1$: KL 발산
- $A_i = \frac{r(o_i) - \text{mean}(\{r(o_1),...,r(o_G)\})}{\text{std}(\{r(o_1),...,r(o_G)\})}$: 정규화된 상대적 이득[1]

#### 2.5 모델 구조

**RFT 콜드스타트(Reinforced Fine-Tuning Cold-Start)**:[1]

$$L_{rft} = E_{(\epsilon,a,l,s) \sim D_{c\_action}, (\hat{\epsilon},\hat{a},\hat{l},\hat{s},\hat{t}) \sim D_{c\_cot}} -\log(\pi_{critic}(l, s|\epsilon, a)) - \log(\pi_{critic}(\hat{l}, \hat{s}, \hat{t}|\hat{\epsilon}, \hat{a}))$$

초기 모델 학습은 두 부분으로 구성됩니다:
- 인간 주석 데이터에서 GUI 지식 추출
- 기존 MLLM의 추론 과정에서 경험 추출

**백본 모델**: Qwen2.5-VL-7B 기반 7B 파라미터 모델[1]

#### 2.6 성능 향상

**정적 평가 결과**:[1]

| 벤치마크 | 메트릭 | GUI-Critic-R1 | Qwen2.5-VL-7B | 개선도 | GPT-4o 비교 |
|---------|---------|---------|---------|---------|---------|
| GUI-I (명령어 일반화) | 비판 정확도 | 69.20% | 54.88% | +14.32%p | +3.19%p |
| GUI-I | 제안 정확도 | 52.43% | 43.14% | +9.29%p | +11.89%p |
| GUI-S (시나리오 일반화) | 비판 정확도 | 58.77% | 57.02% | +1.75%p | -3.51%p |
| GUI-S | 제안 정확도 | 47.37% | 37.72% | +9.65%p | +14.04%p |

**동적 평가 결과 (AndroidWorld)**:[1]

- 성공률: 22.4% → 27.6% (+5.2%p)
- 효율 이점 비율(EAR): 31.8% (기준: 23.2%)
- GPT-4o 대비 사전 비판 방식이 사후 비판보다 효율성 우위

#### 2.7 한계점

논문의 부록에서 제시된 한계점:[1]

1. **경량 모델 미지원**: 현재 7B 모델이 최소 규모이며, 3B 모델 등 더 작은 모델로의 확장 필요
2. **단일 단계 정보 의존성**: 단일 스크린샷 및 의미론적 작업 이력만 사용하며, 전체 궤적 수준의 비판 미포함
3. **도메인 특성 제한**: 웹 시나리오에서 상대적으로 낮은 성능(63.08% 비판 정확도)
4. **교정 능력 한계**: 정확성은 높지만 제안의 정확성 개선에 추가 연구 필요

***

### 3. 모델의 일반화 성능 향상 가능성 중심 분석

#### 3.1 현재 일반화 성능 평가

**세 가지 일반화 설정**:[1]

1. **GUI-I (명령어 일반화)**: AMEX 데이터셋의 미학습 명령어
   - 가장 높은 성능: 69.20% 비판 정확도
   - 기준 모델 대비 +14.32%p 개선
   - 해석: 새로운 명령어에 잘 적응

2. **GUI-S (시나리오 일반화)**: Odyssey의 미학습 앱
   - 상대적 약한 개선: +1.75%p 비판 정확도
   - 하지만 제안 정확도에서 큰 개선: +9.65%p
   - TikTok, Chaton, ClevCalc 등 완전 신규 앱에서 견고한 대응

3. **GUI-W (웹 시나리오 일반화)**: GUICourse의 웹 환경
   - 도메인 이동으로 가장 도전적
   - 비판 정확도: 63.08%, 제안 정확도: 39.48%
   - 앱 간 액션 공간 차이로 인한 성능 저하

#### 3.2 일반화 향상 메커니즘

**1. 추론 기반 부트스트래핑의 역할**:[1]

추론 기반 데이터 수집이 일반화에 기여하는 방식:

- **분포 정렬**: 실제 오류 분포와의 일치로 수집된 부정적 샘플의 질 향상
- **다양성 증대**: 다양한 오류 패턴의 학습으로 미학습 도메인에서의 강인성 증가
- **CoT 재정의 정제화**: 생각 과정의 명시화를 통해 일반화 가능한 추론 패턴 학습

**2. S-GRPO의 일반화 강화 효과**:[1]

절제 연구(Ablation Study) 결과:

| 구성 | GUI-I 비판 정확도 | GUI-S 비판 정확도 |
|------|---------|---------|
| RFT만 사용 | 63.16% | 55.26% |
| S-GRPO (형식+정확도) | 67.98% | 54.38% |
| S-GRPO (전체) | 69.20% | 58.77% |

제안 보상의 추가로 +1.22%p 향상 (GUI-I) 및 +4.39%p 향상 (GUI-S)

**3. 파라미터 민감도 분석**:[1]

- **제안 보상 가중치 최적값**: $\lambda_s = 0.1$
  - 너무 낮으면 제안 제약 불충분
  - 너무 높으면 정확도 저하
  
- **그룹 크기 최적값**: 그룹 크기 6에서 최적 성능
  - 작은 크기: 상대적 이득 계산 불안정
  - 큰 크기: 계산 효율성 저하

#### 3.3 새로운 환경으로의 전이 가능성 분석

**긍정적 신호**:

1. **웹 도메인 전이**: 모바일에서 훈련된 모델이 웹 시나리오에 어느 정도 적응(39.48% 제안 정확도)
2. **미학습 앱 대응**: Odyssey의 새로운 6개 앱(TikTok, Tripadvisor 등)에서 견고한 성능 유지
3. **명령어 다양성 처리**: GUI-I의 높은 성능으로 새로운 작업 유형 적응 능력 입증

**제약 요인**:

1. **도메인 특성 차이**: 웹 환경의 액션 공간(더블탭, 스와이프 제약) 차이로 인한 성능 저하
2. **복잡 상호작용 패턴**: 교차 앱 네비게이션 같은 복잡한 패턴의 일반화 한계
3. **시각적 특징 변동성**: 서로 다른 디자인의 GUI 요소 인식 편차

#### 3.4 향후 일반화 성능 향상 방향

**논문에서 제시된 방향**:[1]

1. **경량화**: 3B 모델로의 축소를 통한 적응성 향상
2. **궤적 수준 비판**: 단일 단계가 아닌 전체 작업 시퀀스를 고려한 평가

**관련 연구에서의 시사점**:

최신 연구 동향(2024-2025)에 따르면:[2][3][4]

- **계층적 추론**: PC-Agent처럼 명령어-부작업-행동의 다층 구조가 복잡한 작업에서 일반화 개선[5]
- **활성 인식 모듈**: Ponder&Press의 해석자-위치지정자 분리가 GUI 요소 정확 위치 파악을 32.5%p 향상[4]
- **환경 특화 데이터 수집**: GUI-Bee의 자율 탐색이 신규 환경에 대한 적응성을 53.7%로 향상[6]

***

### 4. 논문이 앞으로의 연구에 미치는 영향 및 고려 사항

#### 4.1 학문적 기여와 영향

**1. 사전 예방적 비판 패러다임의 제시**:[1]

기존 GUI 에이전트 연구는 사후 반성(post-hoc reflection)에 초점을 맞췄으나, 본 논문이 제시한 **사전 비판(pre-operative critic)**은:

- 온라인 대화형 환경에서의 실질적 위험 감소
- 불가역 오류 예방의 새로운 패러다임 제공
- 효율성과 안전성의 동시 달성 모델 제시

이는 향후 GUI 에이전트, 로봇 제어, 자율 시스템 설계에 광범위한 영향을 미칠 것으로 예상됩니다.[1]

**2. 강화학습 기반 다중모달 추론 개선**:[1]

S-GRPO의 제안 보상 메커니즘은:

- DeepSeek-R1의 순수 RL 접근을 다중모달 영역으로 확장
- 특수화된 보상 함수 설계의 중요성 강조
- Vision-RFT, R1-VL 같은 최신 다중모달 RL 연구의 직접적 선행 연구[7][8]

#### 4.2 산업적 응용 가능성

**1. 실제 자동화 시스템에서의 적용**:[1]

AndroidWorld 벤치마크에서의 +5.2%p 성공률 향상은:

- 모바일 앱 테스팅 자동화
- 사용자 지원 자동화 시스템
- RPA(Robotic Process Automation) 개선

**2. 안전 중심의 자동화 시스템**:

특히 금융, 의료 등 높은 위험도 환경에서:
- 오류 사전 진단이 시스템 신뢰도 향상
- 감사 추적 강화를 통한 규정 준수 개선

#### 4.3 후속 연구 시 고려할 핵심 사항

**1. 데이터 품질 및 확장성**

현재 GUI-Critic-Train이 6K 고품질 데이터에 불과한 점:[1]

- 더 대규모 데이터셋 구축의 필요성
- 도메인별 데이터 불균형 해결
- 다국어 GUI 데이터 확충

**2. 모델 크기와 효율성의 트레이드오프**

현재 7B 모델의 한계:[1]

- 실시간 시스템에서의 레이턴시 문제
- 에지 디바이스 배포의 어려움
- 경량 모델(3B 이하)에서의 성능 유지 방법 연구 필요

**3. 교차 도메인 일반화 개선**

GUI-W에서의 상대적 약한 성능:[1]

- 웹 환경 특화 데이터의 확충
- 도메인 적응 기법(domain adaptation) 통합
- 비전 인코더의 다양성 강화

**4. 해석 가능성과 설명 가능한 AI**

사용자 신뢰도 향상을 위해:

- 비판 과정의 명시적 설명 강화
- 사용자가 이해 가능한 수준의 제안 생성
- 오류 진단 논리의 투명성 향상

#### 4.4 관련 최신 연구 동향(2024-2025)과의 연결점

**1. 계층적 에이전트 아키텍처의 확산**[9][10][11][5]

- **PC-Agent**: 명령어-부작업-행동의 3단계 분해로 복잡 작업 처리
- **InfiGUIAgent**: 2단계 SFT로 기본 능력과 추론 능력 분리
- **시사점**: GUI-Critic-R1의 단일 단계 비판을 계층적 구조로 확장 가능

**2. 자율 탐색과 환경 적응**[6]

- **GUI-explorer**: 함수 인식 탐색으로 새로운 앱 학습 자동화
- **GUI-Bee**: Q-ICRL을 통한 효율적 환경 특화 데이터 수집
- **시사점**: 초기 데이터 수집의 자동화로 확장성 향상

**3. 시각적 기초 모델의 발전**[12][13][14]

- **V-Zen, Ponder&Press**: 이중 해상도 인코더로 GUI 요소 정확도 32.5%p 향상
- **시사점**: 더 나은 비전 인코더 통합으로 비판 정확도 추가 개선 가능

**4. 강화학습 기반 추론 능력 강화**[8][7]

- **DeepSeek-R1**: 순수 RL을 통한 자발적 추론 능력 개발
- **Vision-R1, R1-V, LMM-R1**: 다중모달 도메인으로의 확장
- **시사점**: S-GRPO의 보상 함수 설계를 다른 다중모달 작업으로 일반화

#### 4.5 연구자가 고려해야 할 실무적 고려사항

**1. 평가 지표의 한계성**

현재 정적 평가(GUI-Critic-Test)와 동적 평가(AndroidWorld)의 차이:

- 정적 평가는 고도로 정제된 데이터셋 기반
- 실제 환경은 훨씬 다양한 오류 패턴 포함
- 더 광범위한 실제 데이터 기반 평가 필요

**2. 계산 자원의 현실성**

훈련 구성:[1]

- 8개 A100 GPU 필요
- 대부분의 연구 기관에서 접근성 제한
- 경량화 및 분산 훈련 방식 개발 필요

**3. 배포 환경에서의 신뢰성**

실제 시스템에서:

- 모델 불확실성 정량화 부재
- 신뢰도 점수의 보정(calibration) 필요
- 사용자 개입 메커니즘의 설계

***

### 5. 2020년 이후 관련 최신 연구 탐색

#### 5.1 GUI 에이전트의 진화 단계

**Phase 1 (2020-2022): 기초 MLLM 에이전트**[15]

- **Mobile-Agent (2024)**: 시각 인식 도구로 GUI 요소 파악
- 기본적인 엔드-투-엔드 멀티모달 에이전트 구현

**Phase 2 (2023-2024): 포괄적 인식 및 조건부 예측**[16][2]

- **CoCo-Agent (2024)**: 포괄적 환경 인식(CEP)과 조건부 행동 예측(CAP)
- 인지능력 강화를 통한 SOTA 성능 달성

**Phase 3 (2024-2025): 계층적 구조 및 반성 메커니즘**[10][11][9][5]

- **PC-Agent (Feb 2025)**: 계층적 다중 에이전트 협력 프레임워크
- 능동 인식 모듈(APM)과 반성 에이전트로 32% 절대 성능 개선
- **InfiGUIAgent (Jan 2025)**: 네이티브 추론 및 반성 능력
- **G-TADS (June 2025)**: 작업 분해 전략으로 다중 시나리오 일반화

#### 5.2 오류 진단 및 비판 관련 연구

**오류 감지 및 정정 연구**

- LLM의 자가 정정 능력에 대한 기본적 한계 제시
- "LLMs cannot find reasoning errors, but can correct them given the error location"

**비판 모델 관련 연구**[17]

- **Critic-V (2024)**: VLM 비판 모델이 멀티모달 추론 오류 포착
- **LLava-Critic (2024)**: 멀티모달 모델 평가를 위한 비판 모델
- **Critic-based LLM evaluation**: 독립적 비판 모델의 효과 입증

#### 5.3 강화학습 기반 다중모달 추론

**DeepSeek-R1 계열 연구**[18][7][8]

- **DeepSeek-R1 (Jan 2025)**: 순수 RL로 추론 능력 70%+ 향상 달성
- **Vision-R1 (March 2025)**: 기하 문제와 객체 개수 세기에 적용
- **R1-V (May 2025)**: 멀티모달 추론에서 R1 접근의 효과 입증
- **LMM-R1 (May 2025)**: 3B LMM에서 RL 기반 강화 시연

**다중모달 RL 일반화**[19][20]

- **Visual-RFT (2024)**: 오픈 어휘 탐지, 퓨샷 분류에 적용
- **MM-EUREKA (May 2025)**: 규칙 기반 RL로 시각적 발견 위한 대규모 탐색
- **R1-OneVision (May 2025)**: 일반화된 멀티모달 추론 패러다임

#### 5.4 GUI 에이전트의 특화 방향

**1. 환경 특화 데이터 수집**[12][10][6]

- **GUI-explorer (May 2025)**: 함수 인식 탐색으로 SPA-Bench 53.7% 달성
- **GUI-Bee (Jan 2025)**: Q-value 기반 인컨텍스트 RL로 신규 환경 적응
- **시사점**: 지속적 학습과 환경 적응의 중요성

**2. 시각적 기초 모델 발전**[21][13][4]

- **Ponder&Press (Dec 2024)**: 해석자-위치지정자 분리로 ScreenSpot 벤치마크 22.5%p 향상
- **TinyClick (Oct 2024)**: 경량 0.27B 모델로 강력한 성능 유지
- **V-Zen**: 이중 해상도 인코더로 효율적 GUI 이해
- **시사점**: 경량화와 특화된 비전 아키텍처의 가능성

**3. 다중 에이전트 협력**[9][5][16]

- **PC-Agent**: 매니저-진행-결정-반성 에이전트의 4계층 구조
- **Mobile-Agent-v2**: 계획, 의사결정, 반성의 분리
- **시사점**: 단순 모노리식 에이전트에서 전문화된 다중 에이전트로의 진화

#### 5.5 통합적 학습 파이프라인

**2단계 이상의 구조화된 훈련**[22][18][9]

- **Stage 1**: 기본 GUI 이해 및 기초 능력
- **Stage 2**: 고급 추론 및 반성 능력
- **RFT + RL 결합**: 감독 학습과 강화학습의 시너지

#### 5.6 종합적 연구 지형

**현재까지의 진화 경로**:

```
2023-2024: 포괄적 인식 (CoCo-Agent)
    ↓
2024-2025: 계층적 구조 + 능동 인식 (PC-Agent, InfiGUIAgent)
    ↓
2025+: 자율 탐색 + 환경 특화 + 다중모달 RL (GUI-explorer, GUI-Bee, Vision-R1)
    ↓
미래: 지속적 학습과 안전성이 통합된 범용 GUI 에이전트
```

***

### 결론

"Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation"은 **온라인 대화형 환경에서의 근본적 안전 문제를 새로운 관점에서 해결**합니다. 기존의 사후 반성에서 벗어나 **사전 예방적 비판**이라는 패러다임을 제시함으로써, GUI 자동화의 신뢰성과 효율성을 동시에 향상시킵니다.[1]

특히 **S-GRPO 및 제안 보상의 설계**는 다중모달 도메인에서 특화된 강화학습 기법의 가능성을 보여주며, **추론 기반 부트스트래핑**은 고품질 데이터 확보의 새로운 방법을 제시합니다. AndroidWorld에서의 +5.2%p 성공률 향상과 GUI-S에서의 +9.65%p 제안 정확도 향상은 실무적 가치를 입증합니다.

2024-2025년의 최신 연구 동향에서 보이는 **계층적 구조, 자율 탐색, 다중모달 RL의 확산**은 이 연구 결과를 토대로 더욱 발전될 것으로 예상됩니다. 향후 연구자들은 경량 모델로의 확장, 교차 도메인 일반화 개선, 그리고 실제 배포 환경에서의 신뢰성 확보에 주력해야 할 것입니다.

이 논문의 **근본적 기여는 단순히 성능 개선을 넘어, GUI 에이전트 개발의 새로운 철학**을 제시했다는 점에 있습니다. "미리 보고 뛰어라(Look Before You Leap)"는 제목에서 엿볼 수 있듯이, 안전성 중심의 자동화 시스템 설계로의 산업 전체의 패러다임 전환을 유도할 것으로 기대됩니다.

***

### 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4348bb01-92c4-4317-ae47-64afe221811c/2506.04614v2.pdf)
[2](https://aclanthology.org/2024.findings-acl.539)
[3](https://arxiv.org/abs/2410.11871)
[4](https://arxiv.org/abs/2412.01268)
[5](https://arxiv.org/abs/2502.14282)
[6](https://arxiv.org/abs/2505.16827)
[7](https://www.nature.com/articles/s41586-025-09422-z)
[8](https://arxiv.org/abs/2501.12948)
[9](https://arxiv.org/abs/2501.04575)
[10](https://arxiv.org/abs/2501.13896)
[11](https://ieeexplore.ieee.org/document/11209200/)
[12](https://arxiv.org/abs/2508.04482)
[13](http://arxiv.org/pdf/2410.05243.pdf)
[14](https://yuqi-zhou.github.io/GUI-Agent-with-Foundation-Models.github.io/assets/files/survey.pdf)
[15](https://arxiv.org/html/2401.16158v1)
[16](https://aclanthology.org/2025.naacl-demo.43)
[17](https://www.nature.com/articles/s41746-025-02047-6)
[18](https://arxiv.org/html/2509.17418v1)
[19](http://arxiv.org/pdf/2408.11824v1.pdf)
[20](https://www.emergentmind.com/topics/llm-brained-gui-agents)
[21](http://arxiv.org/pdf/2405.15341.pdf)
[22](https://arxiv.org/pdf/2501.04575.pdf)
[23](http://arxiv.org/pdf/2410.11871.pdf)
[24](https://arxiv.org/pdf/2410.02958.pdf)
[25](https://arxiv.org/html/2411.04890)
[26](https://aclanthology.org/2024.findings-acl.539/)
[27](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/deepseek-r1/)
[28](https://openreview.net/pdf?id=fO1xnmW8T6)
[29](https://www.themoonlight.io/ko/review/coco-agent-a-comprehensive-cognitive-mllm-agent-for-smartphone-gui-automation)
[30](https://arxiv.org/html/2510.04791v1)
[31](https://github.com/X-PLUG/MobileAgent)
