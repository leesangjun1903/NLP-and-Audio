# Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity

**핵심 주장:**  
몇 개의 샘플만을 이용한 *In-context Learning*에서, 동일한 프롬프트 예시를 제공하더라도 **샘플 순서**에 따라 모델 성능이 **랜덤 추측 수준(≈50%)에서 최첨단 성능(≈90% 이상)**까지 극단적으로 달라진다. 이에, 레이블이 없는 인공 개발 세트(프로빙 셋)를 생성하고 엔트로피 기반 통계치를 활용해 **순서 민감도를 자동으로 완화**할 수 있음을 보인다.[1]

**주요 기여:**  
1. Few-shot 프롬프트에 내재된 **순서 민감도(order sensitivity)** 현상을 체계적으로 분석.  
2. 레이블 없는 **인공 개발 세트(probing set)** 생성 기법 제안: 모델의 생성 능력을 활용해 입력 분포와 유사한 무라벨 데이터 획득.  
3. **엔트로피 기반 프로빙 지표(GlobalE, LocalE)**를 통해 최적 샘플 순서를 자동으로 선별.  
4. GPT 계열 모델(GPT-2/3) 6개 크기, 11개 텍스트 분류 과제에서 평균 **상대 성능 13%** 개선 및 분산 축소 실험적 입증.[1]

***

## 1. 해결하고자 하는 문제  
Few-shot In-context Learning은 몇 개의 예시만으로 텍스트 분류 등을 수행하지만,  
- **샘플 순서(permutation)**에 민감하여 결과가 불안정  
- 순서 좋은 프롬프트를 찾기 위해서는 추가 개발 데이터(라벨) 필요 → *true* few-shot 환경에 어긋남[1]

***

## 2. 제안 방법

### 2.1 인공 개발 세트(probing set) 구성  
1.  n개의 레이블된 예시 $$S=\{(x_i,y_i)\}_{i=1}^n$$를 템플릿 $$T$$로 자연어 문장 $$t_i=T(x_i,y_i)$$으로 변환.  
2.  가능한 순서 전부(예: 4개 샘플 시 $$4!=24$$개)의 연결(concatenation) 프롬프트 $$c_m$$ 생성.  
3.  각 $$c_m$$를 조건으로 언어 모델이 생성한 문장 $$g_m\sim P(\cdot\mid c_m;\theta)$$에서 $$T^{-1}(g_m)$$으로 (x′,y′) 추출 → **무라벨 probing set** $$D$$ 구성.[1]

### 2.2 엔트로피 기반 프로빙 지표  
- **Global Entropy (GlobalE):**  

```math
    p_{v,m} =\frac{1}{|D|}\sum_{(x',y')\in D}1\{\hat y_{i,m}=v\},\quad 
    \mathrm{GlobalE}_m = -\sum_{v\in V}p_{v,m}\log p_{v,m}
```
  
  *균형 잡힌 예측 분포*를 선호.  
- **Local Entropy (LocalE):**  

```math
    p_{v,i,m}=P(v\mid c_m\oplus T(x'_i);\theta),\quad
    \mathrm{LocalE}_m = \frac{1}{|D|}\sum_i\bigl(-\sum_{v\in V}p_{v,i,m}\log p_{v,i,m}\bigr)
```
  
  *개별 예시에 대한 불확실성*을 측정.[1]

프롬프트 $$c_m$$들에 대해 엔트로피 지표를 계산 후 상위 $$K$$개(실험에서 $$K=4$$) 선택하여 최종 평가.

***

## 3. 모델 구조  
- **GPT-2**: 0.1B, 0.3B, 0.8B, 1.5B 파라미터  
- **GPT-3**: 2.7B, 175B 파라미터  
- 샷 수: 1~4 shot  
- 프로빙 생성 시 온도 샘플링($$t=2$$), 최대 생성 길이 128, n-gram 차단기법 적용.[1]

***

## 4. 성능 향상 및 한계  

| 모델 크기    | 평균 상대 개선률 (GlobalE) | 분산 감소 효과 | 한계                                 |
|-------------|--------------------------|---------------|--------------------------------------|
| GPT-2 0.1B  | +8%                      | 중간          | 아주 작은 모델은 여전히 불안정        |
| GPT-2 0.8B  | +13%                     | 크게 감소     | 문장쌍 태스크(RTE, CB) 성능 열세       |
| GPT-3 175B  | +4-5%                    | 소폭 감소     | 이미 높은 성능(≈90%↑)에서 한계적 개선 |

- **분산(표준편차)**가 최대 30%p 줄어들어 안정성 향상.  
- **작은 모델의 문장쌍 과제(RTE, CB)**는 프롬프트 최적화만으로 해결 어려움.[1]

***

## 5. 일반화 성능 향상 가능성  
- **모델 크기 및 과제 유형에 무관하게**(사이즈 0.1B~175B, 11개 데이터셋) 일관된 개선 확인.  
- *레이블 없는 probing set*은 추가 라벨 데이터 없이 **진정한 few-shot** 환경에 적합.  
- 엔트로피 기반 지표는 템플릿 형식에도 **민감하지 않음**(다양한 템플릿 실험에서 유사한 개선).[1]
- 대규모 언어 모델의 **생성 능력**을 활용한 일반화 가능성 높음.

***

## 6. 향후 연구에의 영향 및 고려 사항  
- **프롬프트 디자인** 연구: 샘플 순서 최적화를 포함한 자동화된 프롬프트 탐색 필요.  
- **더 복잡한 과제**(다중 홉 추론, 구조화된 입력)로의 확장 검토.  
- **프로빙 셋 품질**: 샘플링 전략(온도, 길이)과 엔트로피 지표 외 추가 메트릭 개발 가능.  
- **상호작용적 few-shot**: 실사용 환경에서 실시간 순서 재선택 프로토콜 설계.  
- **윤리적 고려**: 생성 프로빙 데이터의 편향 확인 및 완화 전략 마련.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/a2a5d58e-aa1f-41d1-8a22-f4d5c028530d/2104.08786v2.pdf)
