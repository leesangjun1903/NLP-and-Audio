# LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models | Instruction Following, Fine-tuning, Question answering

LongLoRA는 **사전학습된 대형 언어모델(LLM)의 문맥 길이를 효율적으로 확장**하기 위한 파라미터-효율적 미세조정 기법이다[1].  
주요 기여:
- Shifted Sparse Attention(S2-Attn)를 도입해, 긴 문맥 학습 시 전역 어텐션을 국소적 희소 어텐션으로 근사하여 연산량과 메모리 사용량을 최대 75%까지 절감[1].  
- LoRA에 *임베딩* 및 *정규화* 계층의 학습을 추가(LoRA+)함으로써, 긴 문맥 적응 시 LoRA와 완전 미세조정(full-FT) 간에 존재하던 성능 격차를 해소[1].  
- 단일 8×A100 GPU 환경에서 Llama2-7B를 최대 100k 토큰, 70B 모델을 32k 토큰으로 확장 가능함을 실험적으로 입증[1].

# 상세 설명

## 문제 정의  
사전학습된 LLM은 통상 수천 토큰(2k‒4k) 문맥 길이로 한정되며, 더 긴 문서를 처리하려면 전체 모델을 재학습하거나 완전 미세조정 시 연산 및 메모리 비용이 기하급수적으로 증가한다[1].

## 제안 기법  

1. **Shifted Sparse Attention (S2-Attn)**  
   - 문맥 길이 $$N$$을 크기 $$G$$ 의 그룹으로 분할하고,  
   - 전체 헤드 중 절반은 그룹 경계 단위로 토큰을 반절씩 시프트(roll)하여 인접 그룹 간 정보 흐름 보장  
   - 토큰 차원→배치 차원 전치 후 각 그룹 내에서만 어텐션 연산  
   - 계산 복잡도: $$O(N^2)\to O\bigl((N/G)\times G^2\bigr)=O(NG)$$  
   
$$
  \text{qkv} = \mathrm{reshape}\Bigl(\bigl[\mathrm{chunk}_0,\;\mathrm{roll}(\mathrm{chunk}_1,-G/2)\bigr],(B\!N/G,G,3,H,D)\Bigr)\,,
$$  

$$
     \text{out} = \mathrm{self\_attn}(\text{qkv}),\quad
     \text{out} = [\,\mathrm{chunk}_0,\;\mathrm{roll}(\mathrm{chunk}_1,+G/2)\,].
$$  

   두 줄의 코드로 구현 가능하며, **추론 시에는 표준 전역 어텐션**으로 원상 복귀[1].

2. **LoRA+ (저랭크 적응 개선)**  
   - 기존 LoRA는 어텐션 선형층(Wq, Wk, Wv, Wo)만 학습하지만,  
   - 임베딩 및 층정규화 파라미터를 학습 대상으로 포함해 긴 문맥 적응 성능 격차를 완전 미세조정 수준으로 축소[1].

## 모델 구조  
- 기반 모델: Llama2 7B‒70B  
- 미세조정 시: S2-Attn 적용 + LoRA+(랭크 8‒256) + 임베딩/정규화 학습  
- 추론 시: 위치역할 보간(Position Interpolation)된 RoPE + 표준 전역 어텐션

## 성능 향상  
- **메모리**: 32k 문맥에서 S2-Attn 사용 시 전역 어텐션 대비 1.8× 메모리 절감[1].  
- **연산 속도**: 65k 문맥 학습 시 LoRA 대비 1.8× 학습 시간 단축[1].  
- **정확도**: 32k 문맥에서 완전 미세조정과 유사한 perplexity(7B 모델: 2.50 vs. 2.49)[1].  
- **일반화**: 문서 내 임의 위치 키워드 검색(passkey retrieval) 과제에서, 33‒34k 길이까지 90% 이상 정확도 유지[1].

## 한계  
- **추론 속도**: 추론 단계에서 여전히 $$O(N^2)$$ 전역 어텐션을 사용하므로, 극한의 문맥 길이(100k 이상)에서는 비효율적일 수 있음[1].  
- **위치 보간 의존성**: RoPE 보간(Position Interpolation)에 따른 작은 성능 저하(짧은 문맥) 관찰[1].  
- **LoRA 랭크 민감도**: 랭크 값에 따라 학습 안정성 및 최종 성능이 달라짐.

# 일반화 성능 향상 관점  
LoRA+가 임베딩·정규화 레이어까지 학습 대상으로 확장함으로써, 모델이 사전학습 시 분포를 벗어난 *긴 문맥*에 더 잘 적응하며, S2-Attn의 근사 가중치가 전역 어텐션 구조와 모양이 유사해 **사전학습된 패턴을 훼손하지 않고** 추가 문맥 정보를 학습함으로써 일반화 성능을 크게 향상시킨다[1].

# 향후 영향 및 고려 사항  
- **하드웨어 친화적 장기 학습**: S2-Attn과 같은 국소 근사는 하드웨어 구현 최적화를 통해 극단적 문맥 확장을 현실화할 기반이 된다.  
- **추론 단계 효율화**: 전역 어텐션의 대안으로, 학습된 LoRA+ 파라미터를 유지하면서도 예측 단계에서도 희소 어텐션을 적용할 수 있는 연구가 필요하다.  
- **다양한 위치 인코딩**: RoPE 외 다른 위치 인코딩과의 호환성·성능 비교를 통해, 보간에 따른 구조적 손실 최소화를 탐구해야 한다.  
- **응용 도메인 확대**: 문서 요약, 법률·의료 기록 등 초장문 처리 과제에 대한 실제 응용 및 평가를 통해, 세부 하이퍼파라미터(그룹 크기, LoRA 랭크 등) 튜닝 지침을 정립할 필요가 있다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/44b483e2-8f8a-422c-8dab-88a882728c16/2309.12307v3.pdf
