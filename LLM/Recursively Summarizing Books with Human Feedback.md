# Recursively Summarizing Books with Human Feedback

## 1. 핵심 주장 및 주요 기여  
“Recursively Summarizing Books with Human Feedback” 논문은 **책 전체 분량의 추상적 요약을** 가능케 하는 새로운 학습 프레임워크를 제안한다.[1]
- **스케일러블 오버사이트**: 인간 평가자가 직접 긴 문서를 읽지 않고도 모델 평가가 가능하도록, 텍스트를 고정된 크기로 분할한 뒤 단계별로 요약하고 이를 재귀적으로 합성한다.[1]
- **인간 피드백 결합**: 행동 복제(Behavioral Cloning, BC)와 강화학습(Reinforcement Learning, RL)을 번갈아 적용해 요약 정책을 향상시킨다.[1]
- **최첨단 성능**: BookSum 데이터셋에서 비추출적(abstractive) 최강 모델 대비 ROUGE-1 3–4점, BERTScore 대폭 개선을 달성했다.[1]

## 2. 문제 정의 및 제안 방법  
### 2.1 해결하려는 문제  
- **장문 요약의 어려움**: 인간이 장편 소설·논문 등 긴 문서를 온전히 평가·요약하기에는 비용과 시간이 지나치게 큼.[1]
- **기존 추출적 한계**: 중요한 사건을 나열하는 데 그치며, 이야기 구조나 맥락을 포착하기 어려움.  

### 2.2 재귀적 분해 요약(RSR) 방법  
1. **텍스트 분할**: 길이 L 이상의 텍스트를 $$N$$개의 청크로 분할  
2. **하위 요약**: 각 청크를 모델(혹은 인간)이 요약 → 최대 길이 $$k_0$$ 토큰  
3. **합성 요약**: 여러 청크 요약을 연결하여 다시 요약 → 상위 단계에서도 동일 모델 사용  
4. **인간 피드백 통합**  
   - **행동 복제(BC)**: 인간 데모 $$D = \{(x_i, y_i)\}$$로 교차엔트로피 학습  
   - **보상 모델 학습**: 인간 비교 라벨 $$\{(y_i^A, y_i^B, r)\}$$로 보상 함수 $$R_\phi$$ 학습  
   - **RL 최적화**: PPO를 통해 요약 정책 $$\pi_\theta$$를  

```math
       \max_\theta \mathbb{E}_{y\sim\pi_\theta}[R_\phi(y)] - \beta D_{\mathrm{KL}}(\pi_\theta \Vert \pi_{\text{BC}})
```
   
   - $$\beta$$: KL 페널티 계수, $$\pi_{\text{BC}}$$: 행동 복제 초기 정책.[1]

### 2.3 모델 구조  
- **GPT-3 계열 Transformer** (2048 토큰 컨텍스트) 사용.[1]
- 모든 단계에서 동일한 파라미터 공유 모델로 **Leaf**(하위 청크)와 **Compose**(합성 단계) 작업 수행.  
- 재귀 깊이(depth) 최대 3–4, 단계별 요약 길이 제한(128–384 토큰) 설정.  

### 2.4 성능 향상  
| 평가 데이터셋 | 비교 모델      | ROUGE-1 | ROUGE-2 | BERTScore |  
|---------------|----------------|--------|--------|----------|  
| BookSum       | 추출적 BertExt | 36.71  | 6.16   | 0.028    |  
|               | T5 FT          | 39.46  | 7.69   | 0.060    |  
|               | **RSR (175B)** | **43.19** | **10.63** | **0.178** |  

- **BookSum**에서 비추출적 최고 모델 대비 ROUGE-1 +3.7, BERTScore +0.118 개선.[1]
- **Goodreads 40권** 인간 평점 기반 full-book 평가: 175B RL 모델 15% 이상이 인간 수준(5/7 이상) 달성.[1]

### 2.5 제한점  
- **맥락 누락**: 로컬 청크만으로 요약해 중요한 후반부 단서 누락 가능.[1]
- **오류 누적**: 단계별 오류가 상위 단계로 전파되어 최종 요약 품질 저하.[1]
- **고차원 RL 어려움**: 트리 상위 단계(Root)에 대한 RL 학습 불안정 및 과적합 관찰.[1]

## 3. 일반화 성능 향상 가능성  
- **첫 번째 서브트리 학습**만으로도 full-book 작업 일반화 가능성 확인. 전체 트리 학습 대비 유사 성능 달성.[1]
- **자동 유도 분포 이동(ADS)** 감소: 낮은 깊이(task)부터 순차 학습하여 입력 분포 차이 완화  
- **모델 크기 확장**: 6B→175B 파라미터 증가 시 RL 이점 크게 증대, BC 대비 격차 확대.[1]
- **학습 커리큘럼 개선**: 높낮이(depth)별 데이터 비중 최적화, 온·오프 정책 혼합으로 일반화 더욱 강화 기대.  

## 4. 미래 연구 영향 및 고려사항  
- **스케일러블 오버사이트** 전 영역 적용: 요약을 넘어 복잡한 의사결정·추론 작업에도 재귀 분해+인간 피드백 병합 가능.  
- **고차원 RL 안정화**: 트리 상위 단계에 특화된 보상 모델·커리큘럼 설계 필요.  
- **동적 분해 학습**: 학습 과정에서 분해 전략 자체를 모델에게 학습시켜, 중요 정보가 흩어진 장문에서도 맥락 보존성 향상 검토.  
- **실제 활용 시 주의**: 부정확 정보 확산 방지를 위해 인간 검수 체계 유지 필수.  

---  
 Wu et al., “Recursively Summarizing Books with Human Feedback” (2109.10862v2)[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/fb163835-64c4-4023-8367-645961c4098c/2109.10862v2.pdf)
