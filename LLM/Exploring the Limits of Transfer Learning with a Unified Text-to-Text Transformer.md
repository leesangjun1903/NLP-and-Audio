# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

**핵심 주장 및 주요 기여**  
이 논문은 NLP의 다양한 과제를 모두 “텍스트→텍스트” 형태로 통합하고, 표준화된 Transformer 기반 모델(T5)을 통해 전이 학습의 주요 요소(사전학습 목표, 데이터셋, 모델 구조 등)를 체계적으로 비교·분석함으로써, 전이 학습의 현재 한계와 확장 가능성을 밝힌다. 주요 기여는 다음과 같다.[1]
- **텍스트→텍스트 통합 프레임워크**: 번역·요약·분류·질문응답 등 모든 과제를 단일 입력–출력 포맷으로 통합.  
- **사전학습 데이터셋(C4)**: Common Crawl을 정제·클리닝하여 750GB 규모의 대규모 언어 코퍼스를 공개.  
- **전이 학습 요소 비교**: 사전학습 목표(denoising vs. LM 등), 아키텍처(encoder-decoder vs. decoder-only), 데이터 정제 수준, 다중 과제 학습 방법 등을 일관된 환경에서 ablation 실험.  
- **스케일링**: 최대 11B 파라미터 모델과 1조 토큰 사전학습을 통해 다양한 NLP 벤치마크에서 SOTA 성능 달성.  

***

## 1. 해결하고자 하는 문제  
전이 학습이 NLP 성능을 획기적으로 향상시켰지만,  
1) 모델 구조·사전학습 목표·데이터 전처리·학습 방식이 매우 다양하고,  
2) 서로 다른 설정 간 비교가 어려워 “어떤 요소가 진짜 성능 차이를 만드는가”가 불명확하다.  
이를 체계적으로 분리·분석하고, 대규모 스케일링을 통해 전이 학습의 한계를 탐색하고자 한다.

***

## 2. 제안 방법  

### 2.1 텍스트→텍스트 프레임워크  
모든 과제를 입력 텍스트 앞에 태스크 프롬프트(prefix)를 붙여 하나의 문장열로 처리하고, Transformer로 정답 텍스트를 생성하도록 학습한다.  
- 입력: `"translate English to German: That is good."`  
- 출력: `"Das ist gut."`  

### 2.2 모델 아키텍처  
표준 encoder–decoder Transformer (Vaswani et al. 2017) 사용  
- **모델 크기(Base)**  
  - Encoder, Decoder 각각 L=12, d_model=768, d_ff=3072, 12-head attention (총 220M 파라미터)  
- **변형 실험**  
  - 파라미터 공유 encoder–decoder  
  - Decoder-only LM, Prefix-LM (encoder 없이 입력 부분만 full-attention)  
  - 레이어 수 축소/확장  

### 2.3 사전학습 목표  
주요 목표는 denoising(토큰 마스킹·복원)이며,  
범용 목표로 i.i.d. 토큰 마스킹, span 마스킹, BERT-style MLM 등을 비교했다.  
- i.i.d. 15% 마스킹: 각 토큰을 확률적으로 [MASK]로 대체하고 원문 복원  
- Span 마스킹: 연속된 평균 길이 3의 span 15% 마스킹 → 짧은 타깃 시퀀스  
- 언어모델링(autoregressive LM) 대비 denoising이 모든 벤치마크에서 우수함을 확인  

#### Masked Denoising Objective  
입력 시퀀스 $$x$$에서 마스킹된 span을 $$\langle \mathrm{X}\_i\rangle$$ 로 대체하고,  
타깃 $$y$$는 마스킹된 원문 span만 순서대로 나열하여 학습  

$$
\mathcal{L}=-\sum_{t=1}^{|y|}\log p(y_t\mid x_{\mathrm{masked}}).
$$

### 2.4 C4 데이터셋  
Common Crawl의 2019.04 월별 웹 텍스트에서 문장 길이·불건전어·코드·중복 등 필터링을 거쳐 750 GB 확보.  
- 대조: 무필터링 6.1 TB, 뉴스 전용 35 GB, Reddit 기반 17 GB, Wikipedia + 책 20 GB  
- **결과**: 대규모·다양성 높은 C4가 반복 학습(overfitting) 없이 최적(750 GB)  

### 2.5 학습 전략  
1. **단일 과제 사전학습→개별 파인튜닝**(기본): 전통적 방식이며 대부분 과제에서 최고 성능  
2. **다중 과제 학습(MTL)**: GLUE·SuperGLUE·번역·요약·denoising 혼합 → 불균형(over/under-train) 문제  
3. **MTL 후 파인튜닝**: MTL 뒤에 개별 과제 파인튜닝 실시 시 단일-과제 파인튜닝과 비슷한 성능 달성  

### 2.6 스케일링  
- **사전학습 토큰 수**: Base(3.4 ×10¹⁰ 토큰) → 1 조 토큰  
- **모델 규모**: 0.06 B → 0.22 B → 0.77 B → 3 B → 11 B 파라미터  
- **연산 배분**: 토큰 수×4 vs. 배치 크기×4 vs. 모델 크기×4 vs. 앙상블×4 비교  
  - 모델 크기×연산 배분 방식이 전반적 우위  
  - N개 앙상블은 또 다른 효과적 확장 수단  

***

## 3. 성능 향상 및 한계  

| 모델 | GLUE↑ | SuperGLUE↑ | CNN/DM R-2 F1↑ | SQuAD EM↑ | En→De BLEU↑ |
|:----:|:-----:|:----------:|:--------------:|:---------:|:------------:|
| Base (220M) | 83.3 | 71.4 | 19.2 | 80.9 | 27.0 |
| + longer pre-train (1T tok) | 85.3 | 74.7 | 19.3 | 82.5 | 27.1 |
| Large (770M) | 86.4 | 77.2 | 19.5 | 84.2 | 27.5 |
| T5-3B | 88.5 | 86.4 | 19.6 | 84.2 | 28.4 |
| T5-11B | **90.3** | **88.9** | **21.6** | **91.3** | **32.1** |

- **SOTA 달성**: GLUE 90.3 (+0.9), SuperGLUE 88.9 (+4.3), CNN/DM ROUGE-2 21.6 (+1.3), SQuAD 91.3 (+1.3).[1]
- **일반화 한계**: COPA·WSC 등 매우 저자원 과제(수백 예제)에선 100% 인간 성능 대비 격차 존재.  
- **제한점**:  
  - 연산·메모리 비용 상승  
  - 비영어·도메인 특화 과제(번역, 도메인별 독해)에서 영어 사전학습의 한계  
  - 최대 규모 모델이 비즈니스·임베디드 환경에 비현실적  

***

## 4. 향후 연구 및 고려사항  

1. **효율적 전이 학습**: 작은 모델과 적은 사전학습 자원으로도 성능을 내는 지식 증류·조건부 계산 연구 필요.  
2. **사전학습 목표 혁신**: denoising 외 “실제 vs. 생성 텍스트 구분” 등 더욱 효율적인 일반 지식 추출 방식 탐색.  
3. **도메인·언어 무관 모델**: 영어 의존 탈피, 다국어·도메인 간 전이 한계를 극복할 수 있는 언어·도메인-agnostic 아키텍처 고찰.  
4. **과제 유사도 정량화**: 사전학습 데이터와 하위 과제 간 “도메인·표현 유사도” 정량적 지표 개발로 사전학습 준비성 판단 지원.  

이 논문은 **통합 프레임워크**와 **체계적 비교**를 통해 전이 학습의 구조적 이해를 돕고, **대규모 스케일링**으로 실제 SOTA를 재정의했다. 앞으로는 규모뿐 아니라 **효율성과 범용성**을 균형 있게 발전시키는 연구가 중요할 것이다.[1]

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9ddede54-ada0-4ae0-a991-480e7fb5a66b/1910.10683v4.pdf
