# SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks 

## 주요 주장 및 기여  
**SUPER-NATURALINSTRUCTIONS**는 1,616개의 다양한 NLP 과제와 그에 대한 전문가 수준의 자연어 지시문을 모은 대규모 메타 데이터셋을 제안한다. 이 벤치마크는 76개의 상이한 과제 유형(분류, 추출, 채우기, 태깅, 재작성, 생성 등)을 포함하며 55개 언어, 33개 도메인을 아우른다.  
- **데이터셋 규모와 다양성**: 기존 NATINST(61개 과제) 대비 26배 이상 과제 수 증가, 과제 정의 길이는 평균 56.6단어로 상세함.  
- **모델 Tk-INSTRUCT**: T5 계열 기반으로 메타 학습을 수행, 11B 파라미터 모델이 InstructGPT(175B) 대비 9.9 ROUGE-L 포인트 우월한 성능 달성.  
- **분석 결과**:  
  - 훈련 과제 수와 모델 크기에 따른 성능이 로그-선형적으로 증가  
  - 과제별 인스턴스 수는 64개 이상부터 포화  
  - 정의+양성 예시 조합이 일반화 성능에 결정적 기여

## 해결하고자 하는 문제  
- **명시적 지시 기반 크로스-태스크 일반화 한계**: 대규모 언어 모델은 지시문이나 예시를 통해 새로운 과제를 처리할 수 있지만, 해당 메커니즘에 사용된 데이터셋과 학습 기법은 투명하지 않고 공개되지 않음.  
- **연구 확장성 부재**: InstructGPT와 같은 모델의 사전학습 데이터 및 파라미터가 공개되지 않아 학계 재현 및 확장이 어려움.

## 제안 방법  
1. **메타 데이터셋 구축**  
   - GitHub 공개 기여 및 크라우드소싱 검수 과정을 거쳐 1,616개 과제와 지시문 수집  
   - 각 지시문은 ‘정의(DEFINITION)’, ‘양성 예시(POSITIVE EXAMPLES)’, ‘음성 예시(NEGATIVE EXAMPLES)’ 구조  
2. **Tk-INSTRUCT 학습**  
   - 입력: `〈지시문〉 + 〈과제 입력〉` 형태로 텍스트 인코딩  
   - 모델: T5 기반 인코더-디코더, 정의와 두 개 양성 예시를 컨텍스트로 사용  
   - 학습 목표: 보이지 않은 과제에 대해 지시문만으로 출력을 생성하도록 메타 트레이닝  
3. **수식적 정의**  
   - 과제 $$t$$의 지시문 $$I_t$$와 인스턴스 $$(x,y)$$에 대해 모델 $$M$$이 $$M(I_t, x)=y$$를 만족하도록 학습  
4. **평가**  
   - 영어 119개, 다국어 35개 과제 총 15,310 인스턴스  
   - 자동 평가지표: ROUGE-L  
   - 휴먼 평가: 생성물 선호도 비교

## 모델 구조 및 학습 세부사항  
- **베이스 아키텍처**: T5-1.1 모델(11B 파라미터)  
- **입력 포맷**:  
  ```
  Definition: <정의>
  Positive Example 1 — input: ... output: ...
  Positive Example 2 — ...
  Now complete the following example — input: <x> output:
  ```
- **학습 하이퍼파라미터**: TPU v3-256, 1e-5 학습률, 배치 1M 토큰, 1,000 스텝  
- **멀티링귀얼 버전(mTk-INSTRUCT)**: mT5-13B 기반  

## 성능 향상 분석  
- **전반적 성능**:  
  - Tk-INSTRUCT(11B) ROUGE-L: 62.0 (영어) vs. InstructGPT(175B) 52.1  
  - mTk-INSTRUCT(13B) ROUGE-L: 66.1 (비영어) vs. InstructGPT 52.8  
- **스케일링 경향**:  
  1) 과제 수 증가 → 로그-선형 성능 향상  
  2) 파라미터 수 증가 → 로그-선형 성능 향상  
  3) 인스턴스 수(과제당) 증가 → 64개 이후 포화  
- **지시문 요소 기여도**:  
  - **정의 + 양성 예시** 조합이 핵심  
  - 음성 예시는 소폭 도움, 부가 설명문은 오히려 성능 저하  

## 한계 및 고려점  
- **언어 및 과제 분포 왜곡**: 영어 및 단기 응답 과제 비중 과도  
- **ROUGE-L 평가 한계**: 재작성, 오류 교정 과제에서 입력 복사 편향  
- **연산 자원 제약**: 더 큰 모델 및 장기 생성 과제 미실험  

## 향후 연구에 미치는 영향 및 고려사항  
- **범용성 연구 촉진**: 대규모 공개 지시 기반 데이터셋으로 학계 재현 가능성 제고  
- **지시문 설계 최적화**: 정의와 예시 수의 균형, 음성 예시 활용 방안  
- **장기 및 구조적 응답 과제 확장**: 긴 생성, 복잡 구조 과제 일반화 연구  
- **평가 지표 발전**: 단순 겹침 기반 지표를 넘어선 의미적 평가 필요  

**SUPER-NATURALINSTRUCTIONS**는 지시 기반 크로스-태스크 일반화의 새로운 기준을 제시하며, 향후 지시문 설계와 범용 모델 학습 연구의 토대를 마련한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b82d0c94-0b7f-4066-9371-3250bb3d5bdf/2204.07705v3.pdf)
