# RPTQ: Reorder-based Post-training Quantization for Large Language Models

# 핵심 요약

**RPTQ**(Reorder-based Post-training Quantization)는 대형 언어 모델(LLM)의 활성화(activation) 양자화에서 채널별 값 범위 차이가 주요 원인임을 규명하고, 채널을 범위 유사도에 따라 군집화(cluster)한 뒤 재배열(reorder)하여 군(cluster) 단위로 양자화함으로써 3비트 수준의 활성화 양자화를 처음으로 가능하게 한 방법이다. 이를 통해 OPT-175B 모델의 메모리 사용량을 최대 80%까지 감소시키면서도 퍼플렉시티(perplexity) 손실을 1.5 이하로 억제하는 성과를 달성했다.

***

## 1. 해결하고자 하는 문제

대형 언어 모델은 활성화 텐서의 각 채널마다 최대·최소값 차이가 수백 배에 달해, 동일한 양자화 파라미터를 적용할 경우

- 범위가 작은 채널은 모든 양자화 눈금이 촘촘하게 배치되지 못해 높은 오차 발생  
- 범위가 큰 채널은 절단(truncation) 손실이 커짐  

기존 PTQ(Post-Training Quantization) 기법들은 아웃라이어 처리, 스무딩, 벡터 단위 파라미터 부여 등으로 개선을 시도했으나, 채널 간 범위 차이를 동시에 해결하지 못했다.

***

## 2. 제안하는 방법

### 2.1 채널 군집화 및 재배열

1. **채널별 범위 계산**  

$$
     X_{\min, i} = \min_{b,n} X_{b,n,i},\quad
     X_{\max, i} = \max_{b,n} X_{b,n,i}
   $$

2. **K-평균 군집화**  

$$(X_{\min,i}, X_{\max,i})$$ 좌표를 기준으로 $$g$$개의 클러스터 $$S_1,\dots,S_g$$로 분할.

3. **채널 재배열**  
   군집 순으로 채널 인덱스 벡터 $$S=[S_1,\dots,S_g]$$를 연결하여 입력 활성화 $$X$$의 채널 차원을 재정렬:  

$$\widetilde X_{:,:,j}=X_{:,:,S_j}$$.

4. **군집별 양자화 파라미터 산출**  
   각 클러스터 $$i$$에 대해 Min–Max 기준으로 스케일 $$s_i$$, 제로포인트 $$z_i$$를 구해 군집 단위 양자화:

$$
     X^q_i = \mathrm{clamp}\bigl(\mathrm{round}(X_i/s_i)+z_i,\,-2^{k-1},\,2^{k-1}-1\bigr).
   $$

### 2.2 오버헤드 경감

- **LayerNorm과 재배열 융합**: LayerNorm 출력 시 메모리 쓰기 주소를 채널별 재배열 인덱스로 오프셋해 곧바로 재배열된 활성화를 생성.  
- **선형 계층 가중치 재정렬**: 배포 전(weight preprocessing)에 입력·출력 채널 차원 모두 재배열해 런타임 오버헤드를 제거.

***

## 3. 모델 구조 및 적용

- 기본 Transformer 블록 내의 두 개 LayerNorm, Q-K 매트릭스 곱, 소프트맥스, 선형 투영(queries, keys, values, output, feed-forward)에 모두 RPTQ 적용  
- 특히 Q-K 매트릭스 곱에서는 쿼리와 키가 동일한 재배열을 공유해 어긋남(misalignment) 방지  
- 최종 출력 투영과 잔차 연결(residual) 지점은 재배열을 적용하지 않아 채널 정합 유지  

***

## 4. 성능 향상 및 한계

### 4.1 성능 향상

- **퍼플렉시티**: OPT-175B에서 3비트 활성화(W3A3KV) 적용 시, WikiText2·PT·C4에서 퍼플렉시티 손실 ≤1.5  
- **메모리 절감**: 동일 모델을 시퀀스 길이 8,192, 배치 64에서 실행 시 메모리 80% 절감(≈2.75 TB→0.59 TB)  
- **영점 샷(zero-shot) 성능**: LAMBADA·PIQA·ARC·BoolQ 등 자연어 이해 과제에서 FP16 대비 수% 이내 성능 저하로 실용적 유지  

### 4.2 한계

- **군집 수 민감도**: 클러스터 수가 적으면 대형 모델에서 분산 캡처가 부정확해 퍼플렉시티가 불안정(특히 R2, R3 단계)  
- **교정 데이터 의존성**: 소량의 교정 샘플은 채널 범위 추정의 분산을 키워 군집화 오차 초래  
- **하드웨어 지원**: 3비트 정수 연산 미지원 GPU에서 4비트/8비트로 캐스팅해야 해 이론 성능 절반만 활용  

***

## 5. 일반화 성능 개선 관점

RPTQ의 군집화 기반 재배열은 특정 채널 분포 패턴을 학습시키지 않고도, 활성화 분포 차이를 정량적으로 분리하여 오차를 최소화한다. 이로 인해:

1. **다양한 입력 도메인**에서도 주요 채널 범위를 재정렬·양자화 파라미터를 군집별로 고정 적용하므로 퍼플렉시티 안정성 상승  
2. **미세 조정(fine-tuning) 없이** 저비트 양자화만으로 zero-shot 태스크 일반화 성능이 FP16 대비 크게 저하되지 않음  
3. **채널 통계 왜곡**에 강건: 소수의 아웃라이어 채널이 전체 범위를 지배하는 전통적 Min–Max PTQ 대비 일반화 오차가 낮음  

***

## 6. 향후 연구의 영향 및 고려 사항

- **하드웨어 친화적 설계**: 3비트 연산 지원 가속기 개발 시 RPTQ의 최대 이점을 활용  
- **동적 군집화**: 추론 도중 입력 배치별로 실시간 군집 재구성을 통한 적응형 양자화 연구  
- **교정 데이터 최소화**: 소량·다양성 높은 샘플 세트 선정 기법 개발로 군집화 오차 억제  
- **다양한 아키텍처 적용**: ViT, MLP-Mixer 등 다른 대규모 모델의 활성화 분포에도 RPTQ 적용 가능성 탐색  

RPTQ는 LLM 양자화 분야에 있어 채널 단위 분포 차이를 정량적으로 다루는 새로운 패러다임을 제시하였으며, 향후 저전력·고성능 추론 가속을 위한 핵심 기법으로 자리매김할 전망이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/001b26f6-2838-4bf7-b245-63a55c2dfb16/2304.01089v4.pdf
