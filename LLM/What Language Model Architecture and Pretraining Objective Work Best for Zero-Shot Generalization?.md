# What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?

## 1. 핵심 주장 및 주요 기여  
이 논문은 대규모 언어 모델의 **제로샷 일반화(학습되지 않은 작업 수행 능력)** 측면에서  
-  **아키텍처(구조)**: 인코더-디코더(Encoder-Decoder), 인과적 디코더(Causal Decoder), 비(非)인과적 디코더(Non-Causal Decoder)  
-  **프리트레이닝 목표(Objective)**: 전언어 모델링(Full LM), 접두어 언어 모델링(Prefix LM), 마스킹 언어 모델링(MLM)  
조합의 영향을 체계적으로 비교했다.  
주요 발견은  
- 순수 프리트레이닝 후에는 **인과적 디코더 + 전언어 모델링**이 제로샷 성능 최상  
- **멀티태스크 파인튜닝**을 거치면 **인코더-디코더 + MLM**이 최상  
- 프리트레이닝 목적과 구조 간 **적응(Adaptation)** 단계를 도입해 두 모델의 장점을 모두 획득 가능  
라는 점이다.

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 향상 및 한계  
### 문제 정의  
제로샷 일반화에 최적화된 언어 모델을 설계하려면  
1. 어떤 **Transformer 구조**가 좋은가?  
2. 어떤 **프리트레이닝 목표**가 좋은가?  
3. 멀티태스크 파인튜닝 시 이 조합은 어떻게 바뀌는가?  
라는 질문에 대한 **전면적 비교**가 필요하다.  

### 제안 방법  
1. **프리트레이닝**  
   - 데이터: 1680억 토큰(C4)  
   - 모델 규모: 디코더-only ≈5B 파라미터, ED ≈11B  
   - 목표별 학습량:  
     -  FLM: 전체 토큰 손실 → 100%  
     -  PLM: 접두어 제외 → 평균 50%  
     -  MLM: 15% 마스킹 → ≈18%  
2. **멀티태스크 파인튜닝** (MT-F)  
   - T0-Train 데이터 130억 토큰  
   - 자연어 형태 프롬프트 제공  
3. **모델 적응(Adaptation)**  
   - **언어 모델링 적응(LM-A)**: MLM-ND → FLM-CD  
   - **비인과적 MLM 적응(NC-A)**: FLM-CD → MLM-ND  

#### 수식  
- 전체 프리트레이닝 손실:  

$$ \mathcal{L}\_{FLM} = -\sum_{t=1}^T \log p(x_t \mid x_{ < t}) $$  

- MLM(span corruption) 손실:  

$$ \mathcal{L}\_{MLM} = -\sum_{s \in S} \log p(x_s \mid x_{\setminus S}) $$  

### 모델 구조  
- **Causal Decoder (CD)**: 좌측 인과 마스킹  
- **Non-Causal Decoder (ND)**: 입력 부분만 비인과적 가시성  
- **Encoder-Decoder (ED)**: 전체 입력 비인과적, 출력 인과적  

### 성능 향상  
- **프리트레이닝 직후**: CD+FLM 최고  
- **MT-F 이후**: ED+MLM 최고, ND+MLM 근소 차이  
- **적응 활용 시**:  
  -  LM-A: FLM 학습 1.6× 단축  
  -  NC-A: MLM 학습 3.3× 단축  
  → 단일 모델 이상의 제로샷 및 생성 성능 동시 확보 가능  

### 한계  
- 프리트레이닝 데이터 중 **T0/EAI 평가 데이터 누수** 확인 안 됨  
- **Bias/Toxicity** 등 윤리·사회적 영향 분석 미흡  
- 자원 제약으로 **더 다양한 아키텍처 변형** 및 규모 실험 제한  

## 3. 제로샷 일반화 성능 향상 관점  
- **프리트레이닝 목표**:  
  -  FLM은 순수 생성 능력 우수  
  -  MLM은 파인튜닝 후 제로샷 일반화 우수  
- **구조**:  
  -  CD 구조가 즉시 제로샷에 강함  
  -  ED 구조는 다양한 문맥 이해와 파인튜닝에 뛰어남  
- **적응 단계**를 활용해 사전학습 모델을 상호 전환하면,  
  **최적의 제로샷 + 생성 성능**을 가장 **효율적**으로 달성  

## 4. 향후 연구에 미치는 영향 및 고려 사항  
- **연구 영향**:  
  -  아키텍처·목표 조합에 따른 **체계적 설계 지침** 제공  
  -  적응(Adaptation) 기법을 통해 **모델 재활용 및 자원 효율화** 가능성 제시  
- **고려 사항**:  
  -  **윤리·사회적 영향**(편향·유해성) 분석 필수  
  -  **데이터 중복·누수** 여부 점검  
  -  어떠한 작업에도 강인한 **일반화 아키텍처** 탐색  
  -  **대규모·다국어** 확장성 및 비용-성능 균형 연구

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6fc5ba15-2af5-4ff2-9395-558ba3b4122d/2204.05832v1.pdf)
