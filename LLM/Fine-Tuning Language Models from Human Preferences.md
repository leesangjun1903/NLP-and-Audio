# Fine-Tuning Language Models from Human Preferences

## 핵심 주장 및 주요 기여  
**Fine-Tuning Language Models from Human Preferences**는 인간의 선호를 학습한 보상 모델을 활용해 사전학습된 언어 모델을 강화학습(PPO)으로 미세조정함으로써,  
1) **감정·묘사 스타일 제어**와 2) **추상적 요약** 과제에서 인간의 평가에 부합하는 출력을 얻을 수 있음을 보였다.[1]
- 매우 적은 수(5천~6만)의 인간 비교 레이블만으로도 학습이 가능  
- KL 제약을 통해 정책(π)과 사전학습 모델(ρ) 간 거리를 제한하여 자연스러움 유지  
- 텍스트 생성 품질(인간 선호)에 있어 기존 제로샷·지도학습 대비 유의미한 개선 달성  

## 문제 정의  
- 전통적 강화학습은 보상이 명시적 함수(BLEU·ROUGE)에 의존하나, **진정한 목표를 반영하지 못함**  
- 인간의 질적 판단만으로 정의된 보상이 필요, 직접 최적화 불가 → **보상 모델(r)** 학습 후 최적화  

## 제안 방법  
### 1. 보상 모델 학습  
- 인간 평가자에게 $$x$$ 문맥에 대한 4가지 생성 $$\{y_i\}_{i=0}^3$$ 중 최고 $$y_b$$ 선택 → $$(x,\{y_i\},b)$$ 데이터 수집  
- 손실 함수:  

$$
\mathcal{L}(r)=\mathbb{E}_{(x,\{y_i\},b)}\Big[-\log\frac{e^{r(x,y_b)}}{\sum_i e^{r(x,y_i)}}\Big]
$$  

- 보상 모델은 사전학습 언어모델 ρ의 마지막 임베딩 위에 선형층으로 구현, 출력 정규화  

### 2. 정책 미세조정 (PPO)  
- 정책 $$\pi$$ 초기화: $$\pi\leftarrow\rho$$  
- 수정 보상:  

$$
R(x,y)=r(x,y)-\beta\,\mathrm{KL}(\pi(\cdot|x)\|\rho(\cdot|x))
$$  

- Proximal Policy Optimization으로 ρ로부터 크게 벗어나지 않으며 $$r$$ 최대화  
- $$\beta$$ 동적 조절로 KL 목표 유지  

### 3. (옵션) 온라인 데이터 수집  
- 정책 변화로 보상 모델 분포 이동 우려 → 주기적(총 20회) 인간 비교 추가 수집 후 $$r$$ 재학습[1]

## 모델 구조  
| 구성 요소    | 상세                            |
|------------|---------------------------------|
| 사전학습 모델 $$\rho$$ | GPT-2 (774M 파라미터, 36층)   |
| 보상 모델 $$r$$     | $$\rho$$ 최종 임베딩 + 선형층  |
| 정책 $$\pi$$       | $$\rho$$ 아키텍처 동일, PPO 미세조정 |
| 하이퍼파라미터    | $$\beta$$=0.03~0.1, 에피소드=2M 등 |

## 성능 향상 및 한계  
### 스타일 연속 생성  
- 감정: 5천 레이블로 제로샷 대비 86% 우위 확보  
- 묘사: 5천 레이블로 77% 우위 달성  
- **소량 데이터**로도 인간 선호 학습 가능[1]

### 요약 과제  
- TL;DR·CNN/DM에서 6만 레이블 온라인 RL 미세조정 모델, 제로샷·지도학습 대비 인간 선호 우위 확보  
- 그러나 모델은 **“스마트 복사(extractive)”**에 불과: 문장 단위 복사가 71~98%에 달함  
- **한계**: 추상화 능력 부족·과도한 복사, 오히려 복사 전략이 사실성(truthfulness)을 높이는 모순 발생[1]

## 일반화 성능 향상 가능성  
- **배치형(active) 데이터 수집**으로 품질·소프트웨어 복잡도 완화  
- **모델 공유 파라미터** 연구 필요: 보상 모델·정책 간 과적합 방지  
- **명확한 라벨링 지침** 설계로 주관적 편향 최소화  
- 추상적 생성 강화: **보상 모델에 사실성·다양성 지표** 추가 고려  

## 향후 영향 및 연구 고려 사항  
- 인간 선호 기반 RL 미세조정은 **대화**, **문서 생성**, **AI 안전** 분야에 적용 가능  
- **인간 라벨링 인터페이스 개선**, **라벨링 비용 최적화**, **정책·보상 네트워크 공유 설계**가 핵심 과제  
- 복사 편향을 넘는 **진정한 추상화 요약** 달성을 위해 복합 보상 설계 및 대규모 인간 평가 필요  

**참고문헌**  
 Ziegler et al., “Fine-Tuning Language Models from Human Preferences,” arXiv:1909.08593v2, 2020.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6bcd1350-5f97-41f4-a156-71d18d63418e/1909.08593v2.pdf)
