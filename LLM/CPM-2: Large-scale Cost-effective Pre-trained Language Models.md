# CPM-2: Large-scale Cost-effective Pre-trained Language Models

## 1. 핵심 주장과 주요 기여 (간결 요약)
CPM-2는 기존 거대 언어모델의  
- **사전학습 비용**,  
- **파인튜닝 저장소 크기**,  
- **추론 시 요구 컴퓨팅 자원**  
를 획기적으로 절감하면서도 대규모 성능을 유지하는 **비용-효율적 파이프라인**을 제안한다.  
주요 기여:
1. **지식 상속(Knowledge Inheritance)** 기반 다단계 사전학습으로 연산량 대폭 감소  
2. **프롬프트 튜닝(Prompt Tuning)** 활용해 파인튜닝 파라미터 99.996% 축소  
3. **INFMOE**: 단일 GPU에서 수십억–수백억 파라미터 MoE 모델 추론 가능케 하는 동적 오프로드 프레임워크  
4. 성능 비교: mT5-XXL 대비 전반적 CUGE 점수 4%p 이상 우수  

## 2. 문제 정의·제안 기법·모델 구조·성능 및 한계

### 2.1 해결 과제
- 사전학습: 초대형 PLM은 수천 GPU, 수주 소요  
- 파인튜닝: 모델당 수백 GB 저장 부담  
- 추론: 다GPU 요구로 실제 응용 어려움  

### 2.2 제안 방법
#### 2.2.1 지식 상속 기반 다단계 사전학습  
1) Chinese Stage: 순수 중국어 코퍼스로 기초 학습  
2) Bilingual Stage: 영어 토큰 임베딩 초기화 및 한·영 데이터 비율 2:1 유지하며 추가 학습  
3) MoE Stage: Bilingual 모델 복제 후 전문가(Experts) 분기 게이팅 고정한 채 Mixture-of-Experts 확장  
> 학습 단계별 손실 함수:  
> $$\mathcal{L} = \sum_{i\in\text{stages}} \mathrm{MLM}_{i} $$  

#### 2.2.2 프롬프트 튜닝  
- 입력 앞뒤·문장 사이에 100개의 연속 소프트 프롬프트 삽입  
- 오직 이 프롬프트 임베딩(약 0.0037% 파라미터)만 업데이트  
- 풀파라미터 튜닝 대비 GPU 메모리 사용량 최대 50% 절감 (단, 수렴 속도는 느림)

#### 2.2.3 INFMOE: 동적 오프로드 추론  
- 전문가 파라미터를 CPU↔GPU 오프로드  
- 각 전문가의 연산량 αᵢ, 통신비 β를 예측해 “(t–1)α ≥ (t–1)β” 및 “(t–1)α ≤ (t+K–1)β”를 만족하는 그리디 스케줄링으로 완전 중첩 달성  
- 수십억 파라미터 MoE 모델을 단일 GPU에서 연산  

### 2.3 모델 구조
- Transformer 인코더-디코더 아키텍처  
- CPM-2: 11B 파라미터(24층, 64헤드)  
- CPM-2-MoE: 전문가 수 32, 총 198B 파라미터  

### 2.4 성능 향상
- 전반 CUGE 점수: CPM-2 198 vs. mT5-XXL 190 (+4p)  
- 계산능력(Math23K): +10%p 향상  
- 프롬프트 튜닝 성능 저하 최소화(−0.7~−2.1%p)하며 메모리 50% 절감  

### 2.5 한계 및 도전 과제
- 프롬프트 튜닝: 수렴 속도 느림  
- Sogou-Log(대조학습)에서 성능 급락  
- INFMOE: 극단적 전문가 불균형 시 스케줄링 불가  
- Pangu-α 등 비공개 모델과 직접 비교 불가  

## 3. 일반화 성능 향상 관점
- **지식 상속**: 사전학습 단계별로 언어 지식 계층적 누적 → 더 광범위한 도메인·언어 일반화  
- **MoE 아키텍처**: 전문가별 특화 표현 학습으로 파라미터 대비 표현력 강화  
- **프롬프트 튜닝+풀튜닝 두 단계 전략**: 1단계 연속 프롬프트로 초기 컨텍스트 탐색, 2단계 전파하여 소수 데이터셋일 때도 과적합 최소화  
- **상대 위치 임베딩 고려**: 문장 간 장거리 관계 보강을 위한 중간 프롬프트 삽입으로 멀티시퀀스 이해력 강화  

## 4. 향후 연구 영향 및 고려 사항
- **지속적 지식 상속**: 보다 다양한 언어·도메인 PLM으로 확장, 연쇄 상속 기법 연구  
- **프롬프트 튜닝 수렴 가속**: 학습률 스케줄링·적응형 옵티마이저 결합  
- **MoE 전문가 균형화**: 동적 게이팅·부하 분산 알고리즘 개선  
- **추론 최적화**: 메모리 제약 하에서 동시 다층 오프로드, 프루닝·양자화 병행  
- **공정성·안전성 평가**: 대형 모델 기반 추론 시스템에서 생길 수 있는 편향·남용 방지 가이드라인 수립  


[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/cf143ef4-f2ae-4802-a47d-2ca593b22aec/2106.10715v3.pdf
