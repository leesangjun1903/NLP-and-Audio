# MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer

**핵심 주장 및 주요 기여**  
최신 다국어 사전학습 모델은 용량 한계로 인해 저자원 언어 및 사전학습에 포함되지 않은 언어에서 성능이 크게 떨어진다. MAD-X는 *언어 어댑터*, *과제 어댑터*, *가역 어댑터*를 모듈식으로 결합하여 고성능 전이를 파라미터 효율적으로 달성하는 프레임워크를 제안한다.  
1. **모듈화된 어댑터 구조**를 통해 사전학습 모델 가중치를 고정한 채 소수의 추가 파라미터만으로 다양한 언어·과제 전이를 지원.  
2. **가역 어댑터(invertible adapter)** 설계를 도입하여 입력·출력 임베딩을 같은 모듈로 효율적 변환, 어휘 불일치 문제 완화.  
3. **언어·과제 어댑터 분리 학습**으로 새로운 타깃 언어·과제에 신속 재활용 가능.  
4. 저자원·비포함 언어에서 기존 방법 대비 F1 기준 평균 5점 이상 성능 향상.  

***

## 1. 해결하고자 하는 문제  
대형 다국어 모델(mBERT, XLM-R)은 수백만 단어의 어휘를 공유하지만  
- 모델 용량 제약으로 모든 언어를 균일하게 표현 불가  
- 저자원 및 사전학습 미포함 언어에서 전이 성능 극심 저하  

***

## 2. 제안 방법 및 모델 구조  

### 2.1 어댑터 세 가지 종류  
1) **언어 어댑터 (Language Adapter)**  
   - Transformer 각 층에 삽입된 작은 다운·업 투영 모듈  
   - MLM으로 각각 언어 특화 파라미터 학습  

2) **과제 어댑터 (Task Adapter)**  
   - 언어 어댑터 출력 위에 추가된 모듈  
   - 소스 언어 과제 데이터로만 미세조정  

3) **가역 어댑터 (Invertible Adapter)**  
   - 입력 임베딩 직후, 출력 임베딩 직전 위치  
   - NICE coupling 구조 기반으로 $$F,G$$ 함수 학습  

- 순방향:  

$$
       o_1 = F(e_2) + e_1,\quad o_2 = G(o_1) + e_2,\quad o=[o_1,o_2]
     $$  
   
- 역방향:

$$
       e_2 = o_2 - G(o_1),\quad e_1 = o_1 - F(e_2),\quad e=[e_1,e_2]
     $$  

### 2.2 학습 및 전이 과정  
- **단계별 학습**  
  1) 언어·가역 어댑터: 타깃 언어 위키피디아 데이터로 MLM 학습  
  2) 과제 어댑터: 소스 언어 과제 데이터로 과제 미세조정  
- **제로샷 전이**: 인퍼런스 시 소스 언어 어댑터를 타깃 언어 어댑터로 교체  

***

## 3. 성능 향상 및 평가 결과  
| 평가 과제        | 저자원·비포함 언어 전이 성능 향상 |
|----------------|------------------------------|
| NER (WikiANN) | 평균 F1 +5.0 이상             |
| CCR (XCOPA)   | 정확도 +1.8                   |
| QA (XQuAD)    | F1/EM 동등 혹은 소폭 개선     |

- **저자원 언어** 및 **사전학습 미포함 언어** 간 전이에서 가장 큰 효과  
- 고자원 언어 전이에서도 기존 대비 **비열등성** 확인  

***

## 4. 한계 및 고려 사항  
- **학습 비용**: 언어 어댑터·가역 어댑터 학습에 수만~수십만 단계 필요  
- **모듈 관리**: 언어별·과제별 어댑터 수가 증가하면 운영 복잡도 상승  
- **언어 커버리지**: 극히 저자원 언어는 단일 위키피디아 데이터만으로 학습 한계  

***

## 5. 일반화 성능 향상 관점  
- 모듈화된 어댑터는 **재사용성**이 높아, 새로운 과제·언어 추가 시 전체 모델 재학습 불필요  
- 가역 어댑터로 **어휘 불일치** 문제 완화하여 언어 간 표현 차이를 줄임  
- 과제 어댑터 분리 학습을 통해 **언어 중립적 과제 지식** 확보, 다양한 언어에 안정적 전이  

***

## 6. 향후 연구에 미치는 영향 및 고려 사항  
- **다양한 모델 적용**: GPT, T5 등 다른 사전학습 모델로 MAD-X 확장 가능성  
- **적응형 어댑터 경량화**: 언어·과제 특성에 따른 동적 모듈 선택 연구  
- **극저자원 언어 처리**: 언어 간 유사성 기반 어댑터 초기화 및 메타러닝과 결합  
- **어댑터 조합 자동화**: 자동 검색(AutoML) 기법으로 최적 어댑터 구성 탐색  

*본 논문은 다국어 전이 학습의 파라미터 효율성과 범용성 측면에서 새로운 설계를 제시하며, 실무 및 후속 연구에서 모듈식 어댑터 활용 방향을 제시한다.*

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9ce48d90-52f2-43ae-8d66-a63805b05fc5/2005.00052v3.pdf)
