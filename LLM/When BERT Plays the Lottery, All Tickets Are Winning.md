# When BERT Plays the Lottery, All Tickets Are Winning

## **핵심 주장 및 주요 기여**

이 논문은 BERT에 대한 **복권 가설(Lottery Ticket Hypothesis)**의 적용을 체계적으로 검증한 최초의 연구로, 다음과 같은 중요한 발견을 제시합니다:[1]

### **주요 발견**
- **"모든 복권이 당첨권"**: 구조적 가지치기(structured pruning)에서 최악의 부분네트워크조차도 강력한 학습 가능성을 유지하며, 이는 사전 훈련된 BERT 가중치의 대부분이 잠재적으로 유용함을 의미합니다[1]

- **좋은 부분네트워크의 불안정성**: 무작위 초기화에 따라 "좋은" 부분네트워크가 크게 달라지며, 이들의 성공이 특정 언어학적 지식보다는 최적화 표면과 더 관련이 있음을 시사합니다[1]

- **언어학적 패턴의 부재**: 생존한 attention head들이 해석 가능한 언어학적 기능을 우선적으로 인코딩하지 않으며, 성능 향상이 특정 언어 지식보다는 다른 요인에 기인함을 발견했습니다[1]

## **해결하고자 하는 문제 및 제안 방법**

### **문제 설정**
기존 연구들은 Transformer 모델이 대부분의 attention head와 layer를 제거해도 성능 손실이 크지 않다는 것을 보여주었으나, 이를 복권 가설의 관점에서 체계적으로 분석한 연구는 부족했습니다.[1]

### **제안 방법**

#### **1. Magnitude Pruning (M-pruning)**
전체 모델에서 임베딩을 제외하고 가장 낮은 크기의 가중치 10%를 반복적으로 제거하는 방법입니다:[1]

- 각 반복에서 dev set 점수를 확인
- 전체 fine-tuned 모델 성능의 90% 이상을 유지하는 동안 계속 가지치기

#### **2. Structured Pruning (S-pruning)**
BERT의 구조적 블록(attention head와 MLP)을 중요도에 따라 마스킹하는 방법입니다:[1]

**Attention Head 중요도 계산**:

$$I_{h,l} = |E_x[\nabla L(x) \cdot \epsilon_{h,l}]| $$[1]

**MLP 중요도 계산**:

$$I_{l}^{mlp} = |E_x[\nabla L(x) \cdot \epsilon_l]| $$[1]

여기서:
- $$x$$는 데이터 분포 $$X$$의 샘플
- $$L(x)$$는 네트워크 출력의 손실
- $$\epsilon_{h,l}$$과 $$\epsilon_l$$은 각각 head와 MLP의 마스킹 변수

**마스킹 구현**:

$$MHAtt^l(x) = \sum_{h=1}^{N_h} \epsilon_{h,l} \cdot Att^l(W_k^{h,l}, W_q^{h,l}, W_v^{h,l}, W_o^{h,l})(x) $$[1]

## **모델 구조 및 성능 향상**

### **BERT 아키텍처**
BERT는 기본적으로 Transformer 인코더 레이어의 스택으로, 각 레이어는 다음과 같이 구성됩니다:[1]

- **Multi-head Self-Attention (MHAtt)**: $$N_h$$개의 독립적으로 매개변수화된 헤드
- **MLP**: 두 개의 feed-forward 층으로 구성
- **Residual Connection**: 각 블록 주위에 적용

**Attention Head 매개변수**:
각 attention head $$h$$는 layer $$l$$에서 $$W_k^{h,l}, W_q^{h,l}, W_v^{h,l} \in \mathbb{R}^{d \times d_h}$$, $$W_o^{h,l} \in \mathbb{R}^{d \times d_h}$$로 매개변수화됩니다.[1]

### **성능 결과**

#### **압축률 비교**
- **M-pruning**: 7개 작업 중 9개에서 상당히 높은 압축률 달성 (10-15% 더 많은 가중치 제거)[1]
- **S-pruning**: 더 큰 부분네트워크를 생성하지만 전체 네트워크 성능에 약간 뒤처짐[1]

#### **부분네트워크 성능**
- **좋은 부분네트워크**: 전체 모델과 비교 가능한 성능 달성[1]
- **무작위 부분네트워크**: S-pruning에서 좋은 부분네트워크와 거의 동등한 성능[1]
- **나쁜 부분네트워크**: S-pruning에서도 BiLSTM+ELMO와 비교 가능한 강력한 성능 유지[1]

## **모델의 일반화 성능 및 안정성**

### **일반화 성능의 한계**
연구진은 좋은 부분네트워크의 **불안정성**을 발견했습니다:[1]

- **Fleiss kappa**: 5개 무작위 시드에서 head 생존률에 대해 0.15-0.32 범위[1]
- **Cochran Q test**: 동일 작업에 대한 5개 무작위 시드로 얻은 이진 마스크가 0.05 수준에서 유의미하게 유사하지 않음[1]

이는 좋은 부분네트워크가 **특정 작업에 대한 사전 훈련된 가중치의 유용성보다는 무작위 초기화에 더 의존**한다는 것을 의미합니다.[1]

### **언어학적 지식과의 관계**
**Super-survivor head들의 분석 결과**:
- 이질적(heterogeneous) 패턴과 super-survivor 여부 간의 Pearson 상관계수가 원시 attention에서 0.015, 정규화된 attention에서 0.025로 매우 낮음[1]
- 많은 중요한 head들이 대각선 attention 패턴을 보여 중복적임을 시사[1]

### **최적화 표면 가설**
연구진은 BERT의 성공이 **특정 언어학적 지식보다는 더 평평하고 넓은 최적화 최적점(flatter and wider optima)**과 관련이 있을 가능성을 제기합니다.[1]

## **향후 연구에 미치는 영향 및 고려사항**

### **연구에 미치는 영향**

#### **1. 모델 해석 패러다임의 전환**
- **개별 구성요소 분석의 한계**: 기존의 individual component 분석에서 **구성요소 간 상호작용** 연구로의 전환 필요성 제시[1]
- **언어학적 지식 vs 최적화**: 모델 성능이 특정 언어 지식보다는 최적화 특성에 더 의존할 가능성 시사[1]

#### **2. 모델 압축 기술 발전**
- **구조적 가지치기의 우수성**: 나쁜 부분네트워크도 강력한 학습 가능성을 보이므로, 더 적극적인 모델 압축 전략 가능[1]
- **사전 훈련 가중치의 재평가**: 대부분의 사전 훈련된 가중치가 잠재적으로 유용하다는 발견[1]

### **향후 연구 시 고려사항**

#### **1. 방법론적 개선**
- **더 정교한 탐사 기법**: 전통적인 탐사 분류기의 false positive 문제를 피하는 **minimum description length와 같은 정보 이론적 탐사** 방법 활용 필요[1]
- **구성요소 조합 분석**: 개별 BERT 요소가 아닌 **요소들의 조합**에서 언어학적 정보가 인코딩될 가능성 탐구[1]

#### **2. 실험 설계 고려사항**
- **일반화 검증**: Dev set을 마스크 찾기와 테스트 모두에 사용하는 한계를 극복하기 위한 별도 검증 전략 필요[1]
- **다양한 초기화**: 무작위 초기화에 대한 안정성 확보를 위한 더 robust한 실험 설계[1]

#### **3. 이론적 탐구 방향**
- **비언어학적 사전 훈련**: MIDI 음악 같은 비언어학적 태스크로의 사전 훈련이 언어 모델링에 도움이 되는지에 대한 추가 연구[1]
- **최적화 표면 특성**: 다양한 사전 훈련 태스크에서 유사한 loss landscape를 얻을 수 있는지에 대한 탐구[1]

#### **4. 실무적 함의**
- **모델 선택 기준**: 단순히 SOTA 모델이 더 나은 언어 지식을 가진다고 자동으로 가정하지 않는 신중한 접근 필요[1]
- **효율적 fine-tuning**: 대부분의 가중치가 유용하다는 발견을 활용한 더 효율적인 fine-tuning 전략 개발[1]

이 연구는 기존의 모델 해석 접근 방식에 근본적인 의문을 제기하며, **모델의 성공이 특정 언어학적 능력보다는 최적화 특성에 더 의존할 가능성**을 시사함으로써 향후 연구 방향에 중요한 영향을 미칠 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/d3d2eafb-2118-40f6-b0dd-5ed18f206140/2005.00561v2.pdf)
