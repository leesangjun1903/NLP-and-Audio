# Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing

## 1. 핵심 주장과 주요 기여

**Funnel-Transformer**는 기존 Transformer 모델의 중요한 비효율성을 해결하는 혁신적인 아키텍처입니다. 핵심 통찰은 **시퀀스-레벨 태스크에서 전체 길이의 토큰-레벨 표현을 유지하는 것의 불필요한 중복성**을 해결하는 것입니다.[1]

**주요 기여들:**
- **점진적 시퀀스 압축**: 레이어가 깊어질수록 숨겨진 상태의 시퀀스 길이를 점진적으로 줄임
- **Pool-Query-Only 설계**: 압축을 더욱 표현력 있게 만드는 혁신적인 어텐션 메커니즘
- **FLOP 재투자**: 절약된 계산 자원을 더 깊거나 넓은 모델 구축에 재투자하여 모델 용량 향상
- **디코더 메커니즘**: 사전훈련 목표를 위해 압축된 표현에서 토큰-레벨 표현 복구

## 2. 해결하고자 하는 문제와 제안 방법

### 해결하고자 하는 문제

기존 Transformer는 **모든 레이어에서 전체 길이의 시퀀스를 유지**하여 상당한 계산 비용이 발생합니다. 특히 텍스트 분류나 랭킹과 같은 시퀀스-레벨 태스크에서는 단일 벡터 표현만 필요하므로 토큰-레벨 세분성의 모든 정보를 보존할 필요가 없습니다.[1]

### 제안하는 방법

**인코더-디코더 아키텍처:**

**인코더**: 점진적 압축을 통한 효율성 향상
- 여러 블록으로 구성되며, 블록 간 이동 시 시퀀스 길이를 절반으로 줄임
- **Pool-Query-Only 설계**:

$$h \leftarrow \text{LayerNorm}(h' + \text{S-Attn}(Q = h', KV = h))$$
  
  여기서 $$h'$$는 풀링된 시퀀스, $$h$$는 풀링되지 않은 시퀀스

**디코더**: 토큰-레벨 표현 복구
- 업샘플링을 통해 전체 길이 시퀀스 복구:

$$\forall i = 1, \ldots, T, \quad h^{up}_i = h^M_{i//2^{M-1}}$$

- 저수준과 고수준 표현 융합: $$g = h^1 + h^{up}$$

**복잡도 분석:**
단일 Transformer 레이어의 복잡도는 $$O(T^2D + TD^2)$$이므로, 시퀀스 길이가 절반으로 줄어들 때마다 **초선형적 복잡도 감소**를 얻습니다.[1]

## 3. 모델 구조와 성능 향상

### 아키텍처 세부사항

**3-블록 설계** (예: B6-6-6H768):
- 첫 번째 블록: 6개 레이어 (전체 길이)
- 두 번째 블록: 6개 레이어 (길이 1/2)
- 세 번째 블록: 6개 레이어 (길이 1/4)

**핵심 설계 요소:**
1. **상대적 위치 어텐션**: 풀링 작업으로 인한 위치 정보 손실 방지
2. **[CLS] 토큰 분리**: 사전훈련 구조 보존
3. **Strided Mean Pooling**: 언어학적 직관에 따른 인접 토큰 병합

### 성능 향상

**Base-scale 결과** (BERT 크기):
- B6-6-6 (0.88x FLOPs): GLUE 평균 85.3 vs L12H768 84.4
- 텍스트 분류에서 일관된 향상 달성

**Large-scale 결과**:
- B10-10-10: GLUE 평균 90.0 (이전 SOTA 89.5 대비)
- RACE 독해: 85.7 (XLNet-Large 85.4 대비)

## 4. 일반화 성능 향상 가능성

### 핵심 일반화 메커니즘

**1. 효율적인 계산 자원 활용**
- 절약된 FLOPs를 모델 깊이/폭 증가에 재투자
- 더 깊은 네트워크를 통한 표현 학습 능력 향상

**2. 계층적 표현 학습**
- CNN의 공간 해상도 감소와 유사한 개념
- 언어학적 직관에 따른 점진적 의미 단위 병합

**3. 정규화 효과**
- 압축 과정에서 노이즈 제거 및 핵심 정보 보존
- 과적합 방지를 통한 일반화 성능 향상

**4. 멀티스케일 정보 활용**
- 저수준(세부 토큰 정보)과 고수준(압축된 의미) 표현의 효과적 결합
- Skip connection을 통한 그래디언트 흐름 개선

### 실증적 일반화 증거

**다양한 태스크에서의 일관된 성능:**
- 텍스트 분류, 언어 이해, 독해 등에서 광범위한 개선
- 특히 긴 텍스트와 복잡한 추론이 필요한 RACE에서 큰 향상
- 작은 모델에서 더 두드러진 일반화 효과

## 5. 한계점

**1. 토큰-레벨 태스크 성능 제한**
- SQuAD와 같은 토큰-레벨 예측에서 상대적 성능 저하
- 세부 토큰 정보가 중요한 태스크에서 압축으로 인한 정보 손실

**2. 파라미터 수 증가**
- 깊이 증가로 인한 1.5배 파라미터 증가
- 분산 훈련에서 통신 비용 및 메모리 소비 증가

**3. 블록 레이아웃 최적화 미해결**
- 최적의 블록 구성에 대한 체계적 연구 부족
- 태스크별 최적화 필요성

## 6. 미래 연구에 미치는 영향과 고려사항

### 연구에 미치는 영향

**1. 효율적 Transformer 설계 패러다임**
- 시퀀스 압축을 통한 효율성 향상의 새로운 방향 제시
- Computer Vision의 hierarchical 구조를 NLP에 성공적으로 적용

**2. 사전훈련-파인튜닝 최적화**
- 태스크 특성에 따른 아키텍처 선택의 중요성 강조
- 계산 효율성과 성능의 새로운 균형점 제시

**3. 멀티모달 확장 가능성**
- 시퀀스 데이터 처리의 일반적 프레임워크 제공
- 시계열, 비디오 등 다양한 도메인 적용 가능

### 앞으로 고려할 점

**1. 압축 메커니즘 개선**
- 더 정교한 풀링 전략 개발
- 정보 손실 최소화를 위한 학습 가능한 압축 방법

**2. 동적 압축률 조정**
- 입력과 태스크에 따른 적응적 압축
- 토큰 중요도 기반 선택적 압축

**3. 지식 증류와의 결합**
- 모델 압축 기법과의 시너지 효과 탐구
- 실용적 임팩트 향상을 위한 통합 접근

**4. 자동화된 아키텍처 탐색**
- Neural Architecture Search를 통한 최적 블록 레이아웃 발견
- 태스크별 맞춤형 아키텍처 자동 설계

Funnel-Transformer는 **효율성과 성능의 새로운 균형점**을 제시하며, 특히 **일반화 성능 향상을 위한 계층적 표현 학습**의 중요성을 보여줍니다. 이는 향후 언어 모델 설계에서 **태스크별 최적화와 계산 효율성**을 동시에 고려하는 새로운 패러다임을 제시합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8d7f4425-6fa4-4e8c-a915-fddfe97495f9/2006.03236v1.pdf)
