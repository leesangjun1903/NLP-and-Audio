# Qwen2.5 Technical Report

## 주요 주장 및 기여  
- **데이터 확장**: 사전학습 토큰 수를 기존 7T에서 18T로 대폭 확대하여 상식, 전문 지식, 추론 능력 강화.  
- **후학습 개선**: 100만 개 이상의 감독적 미세조정(SFT) 예시와 오프라인 DPO·온라인 GRPO 강화학습 도입으로 인간 선호도 및 지시 이행 능력 향상.  
- **모델 다양성**: 0.5B‒72B 파라미터의 공개형(dense) 모델 7종과 MoE 기반 Turbo·Plus 서비스형 모델 2종 제공.  
- **장문 처리**: Turbo 모델은 최대 1M 토큰, 나머 모델은 131K 토큰까지 처리. YARN·DCA 기법으로 장문 문맥 성능 확보.  
- **비용 대비 효율**: Qwen2.5-72B-Instruct가 Llama-3-405B-Instruct와 성능 대등, Turbo·Plus는 GPT-4o-mini/4o와 경쟁력 있는 효율성 제공.  

## 해결 과제  
- 대규모 데이터 부족에 따른 지식·추론 한계  
- 긴 문맥 생성 및 이해 능력 제한  
- 구조화된 데이터 처리 취약  
- 인간 선호도 및 지시 이행 편차  

## 제안 방법  
1. **사전학습 데이터 스케일링**  
   - 18T 토큰 구성: 고품질 텍스트 + 수학·코드 전문·합성 데이터  
   - 도메인별 과소·과다표집 균형 조정  
2. **하이퍼파라미터 스케일링 법칙**  
   - 최적 학습률 µ*, 배치 크기 B* 예측:  

$$ \mu_{\mathrm{opt}} \propto N^{-0.2}D^{-0.1},\quad B_{\mathrm{opt}}\propto N^{0.75}D^{-0.25} $$  

3. **장문 맥락 훈련**  
   - 초기 4K→최종 32K(일반), Turbo: 32K→262K→1M 단계적 확장  
   - YARN, DCA 적용해 장문 성능 유지  
4. **후학습**  
   - 감독적 미세조정: 1M 예시, 최대 32K 시퀀스, 두 에폭  
   - DPO(Direct Preference Optimization)로 150K 쌍 학습  
   - GRPO(Group Relative Policy Optimization)로 온라인 RL 수행  

## 모델 구조  
- **Dense 모델**: Transformer 디코더 기반, GQA, SwiGLU, RoPE, QKV bias, RMSNorm 사용  
- **MoE 모델**: 일부 FFN 레이어를 MoE 레이어로 대체, top-K 전문가 라우팅  

## 성능 향상  
- MMLU, BBH, MATH, GSM8K, HumanEval, MBPP 등 다수 벤치마크에서 전작(Qwen2) 대비 5–15% 절대 개선  
- Qwen2.5-72B-Instruct는 Llama-3-405B-Instruct와 대등 성능 달성  
- Turbo 모델은 1‒4배 추론 속도 향상, 12.5배 어텐션 연산 절감  

## 한계  
- 실험적 Reward 모델 평가 편향: 단일 RM 벤치마크 최적화 시 다른 평가 성능 저하 가능(굿하트 법칙)  
- 초장문 RL 보상 신호 희소: 장문 RLHF는 비용·보상모델 한계로 오프라인 RL 중심  
- 모델 거대화에 따른 자원 요구량 여전  

## 일반화 성능 향상 가능성  
- 대규모·다양 도메인 데이터로 사전학습해 도메인 적응성 및 추론 일반화 강화  
- MoE 구조로 활성 전문가 분기 통해 파라미터 효율적 활용  
- YARN·DCA로 미지 길이·형식의 문맥 처리 능력 확대  

## 향후 연구 영향 및 고려사항  
- **보상 모델 평가**: 다중 RM 벤치마크·출력 일관성 기반 평가 프레임워크 필요  
- **멀티모달 확장**: 텍스트·비전·오디오 통합 모델로 연구 확대  
- **추론 스케일링**: 동적 스파스 어텐션·2단계 RLHF로 비용 효율적 장문 추론 개선  
- **데이터 윤리·편향**: 대규모 학습 데이터의 편향·유해 콘텐츠 필터링 강화  

위 개선 방향을 통해 Qwen2.5는 높은 일반화 역량과 효율성을 바탕으로 AI 연구 및 응용 전반에 기여할 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/cc76fb1b-f185-471d-aef8-e0764cf9fe56/2412.15115v2.pdf)
