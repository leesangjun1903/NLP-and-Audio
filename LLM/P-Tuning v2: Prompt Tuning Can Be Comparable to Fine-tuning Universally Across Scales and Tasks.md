# P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks

## 1. 핵심 주장 및 주요 기여  
**P-Tuning v2**는 **연속적 프롬프트(prompt tuning)** 방식이 모델 크기(330M–10B)와 과제 종류(단순 분류부터 시퀀스 라벨링까지)에 관계없이 **파인튜닝(fine-tuning) 수준의 성능**을 보이며, 전체 매개변수 중 **0.1%–3%만 학습**된다는 점을 입증했다.[1]

***

## 2. 문제 정의 및 제안 기법  

### 2.1 해결하고자 하는 문제  
기존의 프롬프트 튜닝(prompt tuning)은
- **중간 규모(100M–1B)** 모델에서 파인튜닝 대비 성능이 낮고,  
- **시퀀스 라벨링** 같은 어려운 NLU 과제에서는 특히 성능 저하  
가 발생한다.[1]

### 2.2 제안 방법  
P-Tuning v2는 **Deep Prompt Tuning**을 기반으로, **Transformer의 모든 층(layer) 또는 일부 심층 층**에 걸쳐 연속적 프롬프트를 삽입하여 해결한다.

수식으로 요약하면, 사전학습된 언어모델 $$M$$의 임베딩 층 $$e(\cdot)$$에 대해, 입력 시퀀스 $$x$$와 마스크 토큰 $$\texttt{[MASK]}$$ 주변에 연속 임베딩 $$\{h^{(l)}_i\}$$를 다음과 같이 삽입한다:  

```math
\text{Layer-}l\text{ 입력} = [\,e(x),\,h^{(l)}_1,\,\dots,\,h^{(l)}_{k_l},\,e(\texttt{[MASK]})\,]
```

여기서 $$l=1,\dots,L$$ (Transformer 층 수), $$k_l$$는 층 $$l$$에 할당된 프롬프트 길이다.[1]

### 2.3 모델 구조  
- **Multi-layer prompts**: 기존 방식이 입력층만 사용하는 것과 달리, P-Tuning v2는 모든 층(또는 상위 층)에 프롬프트 삽입  
- **선형 분류 헤드**: BERT의 $$\texttt{[CLS]}$$ 토큰 위에 랜덤 초기화된 선형 헤드를 사용하여 범용성 강화  
- **(선택적) 재매개변수화**: 프롬프트 임베딩에 MLP를 적용할지 여부는 과제별 최적화에 따라 달라짐  

| 요소                    | 기존 Prompt Tuning               | P-Tuning v2                            |
|------------------------|----------------------------------|----------------------------------------|
| 프롬프트 위치           | 입력층만                          | 입력층 + 중간/상위 층                  |
| 분류 헤드               | 언어 모델 헤드(verbalizer)       | $$\texttt{[CLS]}$$ + 선형 헤드         |
| 재매개변수화(MLP) 적용 | 일관되게 적용                     | 과제별로 선택적                        |
| 학습 파라미터 비율      | 0.01%–0.1%                       | 0.1%–3%                                |

***

## 3. 성능 향상 및 한계  

### 3.1 성능 향상  
- **모델 크기별(SuperGLUE)**: BERT-large·RoBERTa-large에서 파인튜닝과 유사하거나 더 우수한 성능 확보.[1]
- **시퀀스 라벨링(NER, QA, SRL)**: CoNLL·SQuAD·PropBank 등에서 미미한 성능 격차(≤2%) 또는 동등한 성능 달성.[1]
- **멀티태스크 학습**을 적용 시 더욱 안정적인 초기화와 성능 개선 가능.

### 3.2 한계  
- **프롬프트 길이**와 **재매개변수화 사용 여부**가 과제별로 최적값이 상이하여, 하이퍼파라미터 탐색 비용 존재  
- **초대형 모델(10B 이상)**에서 파인튜닝 대비 큰 이점이 줄어들며, 단순 프롬프트 튜닝과 성능 차이가 감소  
- 여전히 **few-shot** 시나리오가 아닌 **풀 데이터 슈퍼바이즈드** 환경에서 검증됨

***

## 4. 일반화 성능 향상 가능성  
P-Tuning v2의 **심층적 프롬프트 배치**는 모델 내부 표현학습 과정 전반에 영향을 주어,  
- **다양한 과제**에 대한 적응력을 높이고,  
- **고난도 시퀀스 태깅**에서도 안정적 성능을 보장한다.[1]
특히, 프롬프트를 상위 층에 배치할수록 출력에 직접적인 영향을 미쳐 성능이 향상되는 경향을 관찰했다.[1]

***

## 5. 향후 연구에 미치는 영향 및 고려 사항  
- **파인튜닝 대체 수단**: 차세대 대형 언어모델 활용 시, 전체 파라미터 업데이트 없이 효율적 과제 적응 가능  
- **하이퍼파라미터 자동화**: 프롬프트 길이, 삽입 층, 재매개변수화 선택 등을 자동 탐색하는 메타러닝 기법 필요  
- **Few-shot / Zero-shot** 확장: 소수 예제 환경에서 P-Tuning v2의 잠재력 검증 및 최적화  
- **다국어·멀티모달** 과제 적용: 언어 간 일반화 및 이미지·텍스트 융합 모델로의 확장 가능성  

이로써 P-Tuning v2는 **파라미터 효율성**과 **과제 범용성**을 중시하는 향후 연구 방향에 중요한 기준점을 제시한다.[1]

 2110.07602v3.pdf[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7aabb588-c8aa-436f-8c17-1459793975da/2110.07602v3.pdf)
