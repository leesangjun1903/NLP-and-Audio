# On the Sentence Embeddings from Pre-trained Language Models

**주요 주장 및 기여 요약**  
사전학습된 언어 모델(BERT 등)의 문장 임베딩이 미세조정 없이 활용될 때 의미적 유사도를 제대로 반영하지 못하는 이유를 규명하고, **정규화 흐름(normalizing flows)** 기반의 무감독 보정 방법인 **BERT-flow**를 제안하여 성능을 크게 향상시킨다.[1]

## 1. 해결하고자 하는 문제  
사전학습된 BERT 문장 임베딩은 단순 평균 또는 CLS 벡터 추출 방식일 때, GloVe와 같은 비문맥화 모델보다도 낮은 의미 유사도 평가 성능을 보인다. 이는 임베딩 공간이 **비등방성(anisotropic)** 으로 편향되어 있어, 의미 정보가 고르게 분포되지 못하고 단어 빈도 정보에 과도하게 영향을 받기 때문이다.[1]

## 2. 제안하는 방법  
### 2.1 이론적 배경  
- BERT의 마스크 언어 모델(Masked Language Model) 목표는 문맥-단어 상호작용을 학습하나, 문장 간 유사도 정보는 직접 최적화 대상이 아니다.  
- **Dot-product** 또는 **코사인 유사도** 기반 측정 시, 임베딩 공간의 비등방성으로 인해 단어 빈도·문자열 유사도(edit distance)가 과도하게 반영된다.[1]

### 2.2 BERT-flow: 정규화 흐름 기반 보정  
- **목표**: BERT 문장 임베딩 $$u$$를 정규분포 잠재변수 $$z$$ 공간으로 가역 변환하여  

$$
    \max_{f^{-1}} \mathbb{E}_{u\sim U}\bigl[\log p_Z(f^{-1}(u)) + \log\bigl|\det\nolimits J_{f^{-1}}(u)\bigr|\bigr]
  $$  
  
을 무감독 학습한다.[1]
- **구조**: Glow 아키텍처를 단순화한 **additive coupling layer**, **랜덤 순열**, **액트노름(actnorm)**의 스택으로 구성.  
- **특징**: BERT 파라미터는 고정하고 흐름 네트워크만 학습하여, 정보 손실 없이 등방성(isotropic) 임베딩 공간을 유도한다.

## 3. 모델 구조  
- 입력 문장을 BERT-base 또는 BERT-large의 마지막 두 레이어 평균 임베딩($$-last2avg$$)으로 초기 문장 벡터화.  
- Flow 모델:  
  - 레벨(levels) 2, 깊이(depth) 3, 커널 폭(width) 32의 3-레이어 CNN 기반 residual coupling 적용.  
  - 학습률 1e-3, 에포크 1 (타깃 데이터) 또는 2e-5, 에포크 0.15 (NLI 데이터)로 Adam 최적화.[1]

## 4. 성능 향상 및 한계  
- **STS 벤치마크** 7개 데이터셋에서 BERT-base 대비 평균 +8.16포인트, BERT-large 대비 +5.88포인트 개선.[1]
- **NLI 무감독 학습**(SNLI+MNLI) 대비, 타깃 데이터 무감독 학습이 더 효과적(예외: SICK-R).[1]
- **질문-답변 함축(QNLI)** 무감독 평가에서 AUC +6포인트 이상 상승.[1]
- **한계**:  
  - Flow 모델 아키텍처 및 학습 스케줄에 따라 민감도 존재.  
  - 대규모 NLI 데이터를 활용할 때 계산 비용 및 메모리 요구량 증가.

## 5. 일반화 성능 관점  
BERT-flow는 문맥·단어 빈도 편향을 완화하여, 문장 임베딩이 의미 공간에서 더욱 고르게 분포하도록 보정한다. 이로 인해 **레이블 없는 새로운 도메인**(예: QNLI, STS-16)에서도 의미 유사도 평가 성능이 크게 개선되며, 모델의 **도메인 일반화 능력**이 향상됨이 실험적으로 입증되었다.[1]

## 6. 향후 연구 영향 및 고려사항  
- **흐름 기반 보정의 확장**: 다른 사전학습 모델(RoBERTa, GPT) 및 다국어 환경 적용.  
- **지도 정보 활용**: 제한적 레이블 환경에서 가중 무감독 학습(hybrid flow) 가능성 탐색.  
- **경량화**: 온디바이스 및 실시간 서비스 적용을 위한 flow 모델 경량화 및 효율화 연구.  
- **이상 탐지 및 편향 완화**: 임베딩 공간의 비등방성 이외에도, 사회적 편향 등 고차원 문제 보정에의 확장.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/49c52c37-8520-4db7-b342-74be4f2da0f9/2011.05864v1.pdf)
