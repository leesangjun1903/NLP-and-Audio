# GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference

## 1. 핵심 주장 및 주요 기여 요약  
GOBO는 사전 학습된 어텐션 기반 NLP 모델(BERT 계열 등)의 99.9% 이상의 32비트 부동소수점 가중치를 추가 학습 없이 3비트 인덱스로 압축하면서도 정확도를 유지하는 후처리 양자화 기법이다.  
- **아웃라이어-인식 양자화**: 전체 가중치를 “가우시안 분포(‘G’ 군) 내 비율 99.9%”와 “극단값(‘O’ 군)”으로 분리하여, 극소수(0.1%) 아웃라이어는 FP32로 그대로 저장  
- **비균일 대표값 선택**: ‘G’ 군 가중치를 빈 등분 클러스터링→L1 거리 기반 반복 재할당으로 8개(3비트) 대표값 선정  
- **무재학습 적용**: 미세조정 없이 10분 이내에 양자화 완료  
- **하드웨어 적용 사례**  
  1. 기존 TPU, Eyeriss, TensorCore-like 아키텍처의 메모리 압축(10× 용량·대역폭 향상→최대 10× 성능)  
  2. GOBO 전용 가속기: 3비트 상태 유지 연산, 대부분의 곱셈을 덧셈으로 대체해 동일 면적 대비 7× 성능·3× 에너지 효율 개선  

## 2. 상세 설명

### 2.1 해결 과제  
- BERT-Large: 1.12 GB, BERT-Base: 326 MB FP32 가중치로 메모리 대역·지연 제한 심각  
- 기존 양자화 기법: 8비트 고정소수점(Q8BERT), 사전미세조정·Hessian 기반 Q-BERT(4–16값) 등  
- 문제점: 미세조정 필요, 재학습 시간 증가, 아웃라이어 보존 미흡, 精度 저하  

### 2.2 제안 기법  
1) **아웃라이어 검출**  
   - 각 레이어 가중치 μ,σ 계산 후 Gaussian PDF $$\mathrm{pdf}(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/(2\sigma^2)}$$ 로 로그확률 < –4인 값 O 군(FP32)  
2) **‘G’ 군 양자화**  
   - 비균일 빈 분할: 가중치 정렬→동일 개수 클러스터(bin)  
   - 반복 최적화:  
     - 단계1: L1 거리 기준 가중치→가장 가까운 대표값 클러스터 재할당  
     - 단계2: 클러스터별 평균값(centroid) 재계산  
     - 종료조건: L1 합 최소화 달성 시  
   - 8개 대표값(3비트)으로 0.7% 이하 정확도 손실, 16개(4비트)로 손실 0%  
3) **메모리 레이아웃**  
   - 서브매트릭스(SM) 16×16 단위로 헤더(메타데이터+centroid 테이블) – 양자화 가중치(3비트 인덱스) – 아웃라이어(FP32) 분리 저장  
   - 연속 스트리밍 방식으로 LUT 복원 + 아웃라이어 덮어쓰기  
4) **전용 연산 가속기**  
   - PE: 3비트 인덱스별 누적용 8-entry 레지스터 파일 + FP32 덧셈기  
   - SPU: 제한적 FP32 MAC + 아웃라이어 처리  
   - 연산 변환:  
     $$O\!A = \sum_i A_i \times W_i = \sum_j C_j \times V_j$$,  
     여기서 $$C_j=\sum_{i:W'_i=j}A_i$$, $$V_j$$는 j번째 대표값  
   - 동일 면적 대비 TFLOPS↑, 에너지↓  

### 2.3 성능 평가  
- **정확도**: MNLI(84.45%) 3비트 모델 손실 0.7% 미만, 4비트 무손실  
- **압축률**: 가중치 최대 9.8×, 임베딩 10× 이상  
- **하드웨어 가속**  
  - TPU+: 최대 10× 속도  
  - Eyeriss+: 7×  
  - TensorCore+: 4×  
  - GOBO 전용: TensorCore 동면적 대비 평균 7× 속도·3× 에너지 효율  
- **한계**: 매우 극단적인 아웃라이어 분포 시 대표값 선정 민감도, 학습 중 양자화 미고려로 세부 미세조정 불가  

## 3. 일반화 성능 향상 관점  
- GOBO 비훈련 후처리 특성상, 타NLU/NLG 과제(번역·질문응답 등) 및 다양한 트랜스포머 구조(SpanBERT, HAT)에도 즉시 적용 가능  
- 아웃라이어 보존으로 세부 정보 유지, 과도한 근사로 인한 도메인 편향 최소화  
- 향후 도메인별 분포 특성 반영한 동적 임계치·빈 크기 조정 연구 기대  

## 4. 향후 연구 영향 및 고려 사항  
- **영향**:  
  - 모델 배포 비용 대폭 절감→엣지·모바일 NLP 확산  
  - 학습·추론 분리 접근 촉진, 후처리 경량화 연구 증가  
  - 하드웨어·컴파일러 간 협업강화: 양자화·데이터플로우 공동 최적화  
- **고려 점**:  
  - 분포 비정상 레이어 탐지 및 적응형 빈 수 설계  
  - 프루닝·지식증류 결합해 초경량·고정밀 모델 달성  
  - 양자화 오차 민감 태스크(생성·추론)에서 복원 메커니즘 개선

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9b4579c6-acbf-417d-9121-3c414eb3e3e9/2005.03842v2.pdf
