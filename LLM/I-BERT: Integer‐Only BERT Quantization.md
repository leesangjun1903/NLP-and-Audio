# I-BERT: Integer‐Only BERT Quantization

## 1. 핵심 주장과 주요 기여  
**핵심 주장**  
Transformer 기반 모델(BERT, RoBERTa)의 전체 추론 과정을 순수 정수 연산만으로 처리함으로써, 메모리·지연시간·전력 소모를 획기적으로 줄이면서도 원본 FP32 모델과 동등 이상의 성능을 유지할 수 있음을 보인다.  

**주요 기여**  
- 모든 연산을 INT8·INT32 정수 연산만으로 수행하는 완전 정수 전용 양자화(I-BERT) 기법 제안.  
- 비선형 연산(GELU, Softmax, LayerNorm)에 대해 둘째차 다항식 근사를 통해 정수만으로 계산하는 경량 커널 설계.  
- RoBERTa-Base/Large 모델을 GLUE 벤치마크에서 평가하여, 평균 +0.3∼+0.5% 성능 향상 및 최대 4× 추론 속도 개선 달성.  

***

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 2.1 해결하고자 하는 문제  
- 기존 Transformer 양자화는 비선형 연산에서 FP32 로직을 사용하거나 시뮬레이션 양자화(fake quantization)에 의존하여, 정수 전용 하드웨어(예: ARM Cortex-M, Turing Tensor Cores)를 충분히 활용하지 못함.  
- 비선형 함수(GELU, Softmax, LayerNorm)는 정수 연산으로 직접 계산하기 어려워 대부분 부동소수점으로 처리됨.  

### 2.2 제안된 방법  
#### 2.2.1 기본 양자화  
- 모든 가중치·활성화를 8비트 정수(INT8)로 표현하고, 행렬곱·임베딩은 INT8×INT8→INT32 누적 방식으로 수행.  
- 연산 후 필요 시 INT32→INT8 재양자화(requantization) 적용.  

#### 2.2.2 비선형 함수의 정수 근사  
1) **GELU 근사 (i-GELU)**  
   - 원함수:  

$$ \mathrm{GELU}(x) = x\,\frac{1+\mathrm{erf}(x/\sqrt{2})}{2}. $$  
   
   - erf를 둘째차 다항식 $$L(x)=a(x+b)^2+c$$로 근사하여:  

$$ \mathrm{i\text{-}GELU}(x) = x\,\frac{1 + \mathrm{sgn}(x)[a(\mathrm{clip}(|x|,{-}b)+b)^2 + 1]}{2}, $$  
     
  여기서 $$a=-0.2888,\,b=-1.769$$.  
   - 평균 절대 오차 $$\ell_2=8.2\times10^{-3}$$, $$\ell_\infty=1.8\times10^{-2}$$.  

2) **Softmax 근사 (i-Softmax)**  
   - 입력에서 최대값을 빼 안정화 후,  

$$ \exp(\tilde x) = 2^{-z}\exp(p),\quad z=\lfloor -\tilde x/\ln2\rfloor,\;p=\tilde x+z\ln2. $$  
   
   - $$\exp(p)$$를 둘째차 다항식

$$ L(p)=0.3585(p+1.353)^2+0.344 $$  
     
  으로 근사.  
   - 최댓값 편차 1.9×10⁻³로 양자화 오차 내에 존재.  

3) **LayerNorm 정수 구현**  
   - 평균 $$\mu$$는 INT32 합산 후 덧셈·나눗셈으로 계산.  
   - 표준편차 $$\sigma$$의 제곱근은 뉴턴법 기반 반복 정수 알고리즘으로 $$\lfloor\sqrt{n}\rfloor$$을 계산(최대 4회 반복).  

### 2.3 모델 구조  
- RoBERTa-Base/Large 구조 유지  
- 모든 self-attention·피드포워드 블록의 MatMul·임베딩은 INT8, 비선형층만 INT32  
- 파라미터·활성화 및 스케일링 상수는 정적 양자화(static quantization)  

### 2.4 성능 향상  
| 모델               | FP32 평균 GLUE | INT8 평균 GLUE | Diff (%) | 최대 추론 속도 개선 |
|--------------------|---------------:|--------------:|---------:|--------------------:|
| RoBERTa-Base      | 86.0           | 86.3          | +0.3      | 3.08×              |
| RoBERTa-Large     | 89.0           | 89.5          | +0.5      | 3.56× (최대 4.00×)|  

### 2.5 한계  
- 가중치 양자화 비트폭은 8비트에 고정되어 있어 초저비트(≤4비트) 성능은 미검증.  
- TensorRT 플러그인과의 완전 호환성 미구축으로, 추가 소프트웨어 최적화 가능성 남음.  
- 제안된 근사 다항식은 훈련 단계에는 미적용되어, 정수 전용 학습에 대한 영향은 미검증.  

***

## 3. 모델의 일반화 성능 향상 가능성  
- **오류 누적 억제**: 낮은 차수 다항식 근사에도 불구, GLUE 전반에 걸쳐 원본 성능을 유지·향상시켜 전이학습(task transfer)에 강건함을 입증.  
- **정수 전용 연산 안정성**: INT8·INT32 연산만으로 다양한 문장 길이·배치 크기에서 일관된 성능 보존, 추론 환경 변화에도 일반화 가능성 제시.  
- **비선형 근사의 범용성**: 제안된 i-GELU, i-Softmax 는 Transformer 외 다른 NLP·CV 모델의 유사 비선형 연산에도 적용 가능, 광범위한 모델에 일반화될 잠재력.  

***

## 4. 향후 연구에 미치는 영향 및 고려 사항  
- **정수 전용 학습(Integer-Only Training)**: i-GELU·i-Softmax 근사를 학습 단계에 적용하여, 학습 속도·메모리 효율 개선 연구.  
- **초저비트 양자화**: 4비트 이하 비트폭·혼합 정밀도(mixed precision) 도입 시 근사 오차와 성능 트레이드오프 분석.  
- **하드웨어 친화적 설계**: 정수 전용 마이크로컨트롤러·엣지 디바이스 상 구현·벤치마크를 통한 전력·지연 최적화.  
- **다항식 근사 최적화**: 적응형(polynomial fitting) 또는 자동화된 검색(neural architecture search) 기법으로 각 연산별 최적 근사식 탐색.  

이 연구는 Transformer 양자화에서 **정수 전용 추론** 패러다임을 제시함으로써, 경량화∙저전력 AI 시스템의 새로운 가능성을 열었으며, 후속 작업으로 정수 전용 학습·초저비트 양자화·하드웨어 구현 연구가 기대된다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6e53622c-c1dd-4e73-ae67-490c84650fcc/2101.01321v3.pdf
