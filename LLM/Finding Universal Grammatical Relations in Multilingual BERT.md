# Finding Universal Grammatical Relations in Multilingual BERT

**핵심 주장 및 주요 기여**  
이 논문은 **Multilingual BERT(mBERT)가 명시적 언어감독 없이도 공통적인 구문 구조와 의존성 레이블을 학습**한다는 것을 입증한다. 주요 기여는 다음과 같다.[1]
- **다중언어 구조 프로빙 확장**: 11개 언어에 대해 BERT 내의 구문적 거리 정보를 선형 변환으로 복원하고, 기존 단일언어 연구를 다중언어 설정으로 확장했다.[1]
- **공유된 구문 하위공간 발견**: 서로 다른 언어에서 학습된 구문 하위공간이 **제로샷 전이**를 통해 타언어 구문을 복원할 수 있음을 보였다.[1]
- **비지도적 의존성 레이블 클러스터링**: 구문 하위공간에서 헤드–종속어 벡터 차이를 클러스터링하여, Universal Dependencies(UD) 레이블과 높은 일치도를 보이는 군집을 발견했다.[1]

***

## 1. 해결하고자 하는 문제  
다국어 사전학습 언어모델(mBERT)은 언어 간 제로샷 전이 능력을 보이나, **어떤 구문적 표현이 언어 간 공유되는지**는 명확하지 않다. 본 연구는  
1) BERT 내에 언어별로 구문 정보를 담은 하위공간이 존재하는지  
2) 그 하위공간이 언어 간에 공유되는지  
3) UD 의존성 레이블 정보가 비지도적으로 획득되는지  
를 규명하고자 한다.[1]

***

## 2. 제안하는 방법  
### 2.1 구조 프로브(Structural Probe)  
각 문장의 단어 표현 $$h_1^n$$에서 선형 변환 행렬 $$B\in\mathbb{R}^{k\times m}$$을 학습하여, BERT 표현 $$\{h_i\}$$ 간의 **제곱 L2 거리**가 구문 트리상의 거리 $$d_T(w_i,w_j)$$를 근사하도록 최적화한다.[1]

$$
d_B(h_i,h_j) = \|B h_i - B h_j\|_2^2
$$

$$
\underset{B}{\arg\min}\,\sum_{(i,j)}\bigl(d_T(w_i,w_j) - d_B(h_i,h_j)\bigr)^2
$$

이때 $$B$$의 행은 차원 축소된 **구문 하위공간**의 기저를 정의하며, 이 공간이 구문 정보를 압축한다고 간주한다.[1]

### 2.2 하위공간 전이 실험  
- **언어 내 학습(In-language)**: 각 언어별로 $$B$$를 학습하고 동일 언어 문장에 평가.  
- **단일 언어 전이(Single-transfer)**: 한 언어로 학습한 $$B$$를 다른 언어에 제로샷 평가.  
- **홀드아웃(All-but-one)/공동(All-langs)**: 특정 언어를 제외하거나 모든 언어를 합쳐 학습한 후 평가.  

***

## 3. 모델 구조  
- **모델**: BERT-Base, Multilingual Cased(110M 파라미터)  
- **언어**: 11개(영어, 프랑스어, 독일어, 스페인어, 체코어, 핀란드어, 라트비아어, 아랍어, 페르시아어, 인도네시아어, 중국어)  
- **데이터**: Universal Dependencies v2 트리뱅크  
- **측정 지표**  
  - UUAS(무향 비라벨 연결 정확도)  
  - $$\text{Spearman}(d_T,d_B)$$ 거리 상관(DSpr.)  

***

## 4. 성능 향상 및 일반화  
### 4.1 In-language 성능  
구문 프로브는 모든 언어에서 베이스라인 대비 평균 **UUAS +22점**, **DSpr. +0.175**를 달성하여, mBERT가 강력한 구문 표현을 학습했음을 보였다.[1]

### 4.2 제로샷 전이  
최적의 단일 언어 전이를 선택하면, 평균 **UUAS +14점**, **DSpr. +0.128**의 제로샷 성능 향상이 나타났다.[1]
홀드아웃 학습 시 **UUAS +16점**, **DSpr. +0.137**, 다언어 공동 학습 시 **UUAS +19점**, **DSpr. +0.156**를 보여, 구문 하위공간이 언어 간 **높은 공유도**를 지님을 입증했다.[1]

### 4.3 한계  
- UUAS만으로는 장거리 경로 복원이 아닌 인접 엣지 예측으로도 좋은 점수를 얻을 수 있으므로, **거리 상관(DSpr.) 평가가 필수**다.  
- 서브워드 중첩 여부가 전이 성능에 미치는 영향과 파라미터 공유의 기여도를 분리하기 어렵다.  

***

## 5. 모델의 일반화 성능 향상 가능성  
- **구문 하위공간 전이** 실험을 통해, mBERT 내부의 구문 표현은 **언어 특이적이지 않고** 보편적임이 입증되었다.[1]
- **의존성 레이블 클러스터링** 결과, 섬세한 문법적 구분(예: 전치형 vs 후치형 형용사, 기사형태, 부정사 등)이 **비지도적으로** 확보되어, 모델이 학습 데이터에 없는 구조도 내재적으로 일반화할 잠재력을 시사한다.[1]

***

## 6. 앞으로의 영향 및 고려사항  
이 연구는 **다국어 사전학습 모델이 언어 간 보편적 구문 지식을 자발적으로 학습**함을 밝힘으로써,  
- **다국어 모델 설계**: 언어 간 공동 학습 전략·하위공간 통합 방안 개발  
- **효율적 전이 학습**: 소자원 언어 제로샷·소량지도 학습에의 활용  
- **프로빙 해석성 연구**: 비지도적 클러스터링 기법으로 모델 내부 구조를 더 깊이 이해  

등에 기여할 것으로 기대된다. further work에서는 XLM-R 등 다른 모델에 대한 검증, 의존성 레이블 복원에 대한 **정량적 평가**, 프로빙의 인과적 해석성 연구가 필요하다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/53c9c30f-fe89-4976-925b-270e9fee51c2/2005.04511v2.pdf)
