# Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling

## 1. 핵심 주장 및 주요 기여  
“Outlier Suppression+”는 트랜스포머 기반 대형 언어 모델의 **포스트 트레이닝 양자화(Post-training quantization)** 시 발생하는 극단적 아웃라이어(outlier) 문제를 해결하기 위해, 채널별 비대칭성을 제거하는 **채널별 쉬프팅(Shifting)** 과 아웃라이어 집중을 완화하는 **채널별 스케일링(Scaling)** 을 제안한다. 이 연산들을 후속 모듈의 가중치와 편향으로 일관되게 “마이그레이션”하여(이동) 원래 부동소수점(FP) 모델의 동등성(equivalence)을 유지하면서도 8-bit, 6-bit, 심지어 4-bit 양자화에서도 **거의 FP 성능에 근접하거나** 뛰어난 결과를 달성한다.

주요 기여:
- 채널별 아웃라이어가 **양·음 비대칭**으로 분포함을 규명하고 이를 해소하는 쉬프팅 기법 제안  
- 아웃라이어가 일부 채널에 집중된 현상을 완화하기 위한 스케일링 기법 및 효율적 최적화 절차 제시  
- 쉬프팅·스케일 연산을 후속 선형 레이어로 마이그레이션해 FP 모델과 동등한 출력을 보장  
- BERT, OPT, BLOOM, LLaMA 등 다양한 모델에서 8-bit, 6-bit, 4-bit 양자화 성능 획기적 개선  

## 2. 문제 정의 및 제안 방법  
### 2.1 해결하려는 문제  
대형 언어 모델의 **포스트 트레이닝 양자화(PTQ)** 시, 활성화(activation) 텐서 내 소수의 채널에 극단적 outlier가 존재해 전체 분포 범위가 과도하게 확장된다.  
- 각 채널별 최소·최대값이 비대칭으로 치우치면(per-tensor quantization) 양자화 단계를 거친 후 정밀도가 급격히 저하  
- 일부 채널만 아웃라이어를 표현하기 위해 전체 비트 레벨을 크게 할당해야 하는 비효율  

### 2.2 제안 기법  
1) 채널별 쉬프팅(Shifting)  

$$X' = X - z$$,  

$$z_j = \tfrac{\max_t X_{t,j} + \min_t X_{t,j}}{2}$$  
   
   채널별 분포 중심을 0으로 정렬해 양·음 비대칭성 제거  
2) 채널별 스케일링(Scaling)  

$$\widetilde X = (X - z)\oslash s$$,  
   
   스케일 벡터 
   
$$s_j = \max\bigl(1,\tfrac{\max_t(X_{t,j}-z_j)}{t^*}\bigr)$$  
   
   단일 임계치 $$t^*$$ 를 그리드 검색해 아웃라이어 채널만 축소  
3) 동등성 유지 마이그레이션(Equivalent Migration)  
   쉬프팅·스케일 효과를 후속 선형 레이어(또는 잔차 연결)의 가중치·편향에 반영  

$$\displaystyle W' = W \odot s^\top,\quad b' = b + zW^\top$$  
   
   모델 출력 변화 없이 FP 모델과 완전 동등  

### 2.3 최적화 절차  
- 활성화·가중치 양자화 후 최종 출력 변화량 MSE를 목적함수로 설정  
- 여러 레이어(어텐션의 Q, K, V) 동시 고려하는 식으로 softmax 기반 손실 구성  
- 단일 임계치 $$t$$ 그리드 탐색으로 $$s$$ 벡터 효율적 산출  

## 3. 모델 구조 및 성능 향상  
- **적용 모델**: BERT-base/large, OPT(13B-175B), BLOOM, BLOOMZ, LLaMA(7B-65B)  
- **결과 요약**:  
  - 8-bit 양자화: FP 대비 성능 1% 이내 차이  
  - 6-bit 양자화: 대부분 1–2% 성능 저하  
  - 4-bit 양자화(BERT-base): 기존 기법 대비 15.5% 포인트 개선  
  - LLaMA-65B 4-bit: per-token 양자화에서 Winogrande 10%↑, WikiText2 PPL 10 감소  
- **한계**:  
  - 아웃라이어 발생 원인(학습 파이프라인·하이퍼파라미터)에 대한 근본적 이해 부족  
  - LLaMA 특수 구조(FFN 내 엘리먼트곱) 양자화 시 쉬프팅이 약간의 오버헤드 유발 가능성  

## 4. 일반화 성능 향상 관점  
- 채널별 쉬프팅·스케일링은 **데이터 분포 왜곡 완화** 효과가 있어, 다양한 입력 도메인에서 모델의 **강건성(robustness)** 을 높일 잠재력  
- 아웃라이어가 줄어든 분포는 양자화뿐 아니라 **미세조정(fine-tuning)** 시 일반화 성능 향상에도 기여할 것으로 기대  
- 후속 연구에서 도메인 적응, 소량 샷(few-shot) 학습 등 다양한 일반화 시나리오에 대한 실험 필요  

## 5. 향후 연구 방향 및 고려 사항  
- **아웃라이어 기원 규명**: 사전 학습(pre-training) 과정에서 아웃라이어가 왜, 어떻게 생성되는지 분석  
- **동적·토큰별 조합**: 채널별 기법과 per-token, per-group 양자화를 결합해 더욱 미세한 분포 제어  
- **경량 하드웨어 구현**: 쉬프팅·스케일 이동 연산을 저전력 임베디드 디바이스에서 효율적으로 지원  
- **일반화 및 안전성 테스트**: 아웃라이어 축소가 모델의 편향(bias)·안정성에 미치는 영향 정밀 평가  

Outlier Suppression+는 대형 언어 모델 양자화 분야에서 **아웃라이어 특성을 과학적으로 규명**하고, 쉬프팅·스케일링의 **수학적 최적화**를 통해 높은 비트 절감 효과를 달성했다는 점에서, 앞으로 경량화·일반화 연구의 중요한 기반이 될 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/dad85f2b-fed8-4b0c-998d-4eb53455ec81/2304.09145v3.pdf
