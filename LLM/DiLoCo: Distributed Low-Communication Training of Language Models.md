# DiLoCo: Distributed Low-Communication Training of Language Models

### 1. 논문의 핵심 주장 및 주요 기여

**DiLoCo(Distributed Low-Communication Training)** 논문은 지리적으로 분산되어 있고 낮은 대역폭으로 연결된 컴퓨팅 아일랜드에서 대규모 언어모델을 효율적으로 훈련하는 새로운 분산 최적화 알고리즘을 제안합니다.[1]

이 연구의 핵심 기여는 다음과 같습니다:

- **통신 효율성 획기적 개선**: C4 데이터셋에서 8개 워커를 사용할 때 완전히 동기화된 최적화와 비교하여 **500배 적은 통신**을 하면서도 동일하거나 더 나은 성능 달성[1]
- **이중 수준 최적화 프레임워크**: 내부 최적화(Inner Optimization)와 외부 최적화(Outer Optimization)를 조합한 새로운 접근 방식
- **강건한 성능**: 비균등 데이터 분포(non-IID), 동적 컴퓨팅 리소스, 통신 실패 등 현실적 제약 조건에서의 견고성 입증[1]
- **실무적 가치**: 벽시계 시간 기준으로 8배 더 빠른 훈련 달성

***

### 2. 문제 정의 및 제안 방법

#### 해결하고자 하는 문제[1]

현대의 LLM 훈련은 여러 심각한 과제를 직면하고 있습니다:

1. **인프라 복잡성**: 수천 개의 가속기가 같은 위치에 물리적으로 배치되어야 하고, 고대역폭 케이블로 상호 연결되어야 함
2. **지연시간과 동기화**: 매 최적화 단계마다 기울기와 중간 상태를 모든 기기에서 교환해야 하는 높은 통신 오버헤드
3. **장애 취약성**: 기기 수가 증가할수록 훈련 중단 위험 증가
4. **지리적 제약**: 낮은 대역폭의 연결로 떨어져 있는 클러스터들을 활용하기 어려움

#### 제안 방법의 핵심 원리

DiLoCo는 Federated Learning의 변형으로, 두 가지 수준의 최적화를 결합합니다:

**알고리즘 구조**:

$$\text{내부 최적화}: \theta_i^{t,h} = \text{InnerOpt}(\theta_i^{t,h-1}, \nabla L_i(\theta_i^{t,h-1}))$$

$$\text{외부 기울기}: g^t = \frac{1}{K}\sum_{i=1}^{K}(\theta_i^{t+1,0} - \theta_i^{t,0})$$

$$\text{외부 최적화}: \theta^{t+1,0} = \text{OuterOpt}(\theta^{t,0}, g^t)$$

여기서:
- **H**: 내부 최적화 단계 수 (기본값: 500단계)
- **K**: 워커(레플리카) 개수
- **InnerOpt**: AdamW (표준 트랜스포머 최적화기)
- **OuterOpt**: Nesterov Momentum

**핵심 설계 결정**:

1. **대규모 내부 단계**: 500 단계의 긴 내부 훈련으로 통신 빈도 대폭 감소
2. **적응형 내부 최적화기**: Adam 계열의 적응형 학습률이 각 워커의 로컬 훈련에 효과적
3. **외부 모멘텀**: Nesterov 모멘텀이 외부 기울기의 장기 스팬(hundreds of steps)에서 성능 향상

**수식 표현**:[1]

수렴을 위한 기울기 계산:

$$\nabla L(\theta) = \frac{1}{N}\sum_{i=1}^{N}\nabla_{\theta}f(x_i, y_i; \theta)$$

외부 기울기 평균화 (Algorithm 1, Line 12):

$$\bar{g}^t = \frac{1}{K}\sum_{i=1}^{K}(\theta_i^{t} - \theta_i^{t-1})$$

***

### 3. 모델 구조

#### 트랜스포머 아키텍처[1]

실험에서 사용된 모델은 Chinchilla 스타일의 디코더-온리 트랜스포머입니다:

| 파라미터 | 60M | 150M | 400M |
|---------|-----|------|------|
| 레이어 수 | 3 | 12 | 12 |
| Hidden Dimension | 896 | 896 | 1,536 |
| 어텐션 헤드 수 | 16 | 16 | 12 |
| KV 크기 | 64 | 64 | 128 |
| 어휘 크기 | 32,000 | 32,000 | 32,000 |

#### 분산 훈련 구조[1]

**Figure 1**: DiLoCo는 다음과 같이 동작합니다:

1. 초기 모델 $\theta_0$을 K개 워커에 복제
2. 각 워커는 자신의 데이터 샤드에서 H단계의 독립적 훈련 수행
3. H단계 후, 각 워커의 외부 기울기를 중앙 서버에서 평균화
4. 평균화된 기울기로 중앙 모델 업데이트 후 워커에 재배포

#### 최적화기 상세 설정[1]

**내부 최적화기 (AdamW)**:
- 학습률: $4e^{-4}$ (warmup 1,000 단계)
- Weight decay: 0.1
- Batch size: 512 (sequence length: 1,024)

**외부 최적화기 (Nesterov)**:
- 외부 학습률: 0.7
- 외부 모멘텀: 0.9
- 각 외부 단계에서 기울기 평균 계산 후 적용

실험 결과, Nesterov가 SGD나 Adam보다 우월함이 확인되었습니다 (Figure 6).

***

### 4. 성능 향상 및 일반화 능력

#### 성능 비교 분석[1]

| 모델 | 통신 | 벽시계시간 | 계산량 | 데이터 | Perplexity |
|-----|------|---------|--------|--------|-----------|
| Baseline | 0× | 1× | 1× | 1× | 16.23 |
| Baseline, 8× 배치 (데이터 병렬) | 8× | 1× | 8× | 8× | 15.30 |
| Baseline, 8× 배치 (마이크로배칭) | 0× | 8× | 8× | 8× | 15.30 |
| Baseline, 8× 업데이트 | 0× | 8× | 8× | 8× | 14.72 |
| **DiLoCo** | **0.016×** | **1×** | **8×** | **8×** | **15.02** |

**주요 발견**:
- DiLoCo는 배치 크기 8배 기준선 대비 **더 나은 일반화** (PPL 15.02 vs 15.30)
- 동일한 벽시계 시간과 계산량에서 달성하면서 **500배 통신 감소**
- 8배 업데이트 기준선보다는 성능이 약간 낮지만 8배 빠름 (compute efficiency 트레이드오프)

#### 일반화 성능 향상 메커니즘[1]

**1. 사전훈련의 영향**:

Figure 3에서 보이듯이, DiLoCo는 사전훈련 없이도 시작 가능합니다:
- 무사전훈련: PPL 변화 -0.1 (최소 저하)
- 기존 Local-SGD 연구와 달리 성능 저하 미미
- 이는 큰 배치 설정에서의 과훈련(overtraining) 환경에서 더 나은 성능

**2. 데이터 분포의 견고성**[1]

| 설정 | 최종 PPL |
|-----|---------|
| i.i.d. 데이터 | 초반 빠른 수렴, 최종 15.08 |
| non-i.i.d. 데이터 | 초반 느린 수렴, 최종 15.02 |

**직관적 설명**: 
- i.i.d. 설정: 모든 워커 기울기 상관성 높음 (코사인 유사도 거의 0 분산)
- non-i.i.d. 설정: 워커 기울기 직교성 높음 (상관성 역으로 비례)
- 직교 기울기 평균화가 **더 나은 일반화** 제공 (후기 훈련에서 더 빠른 수렴)

**3. 모델 크기 영향**[1]

| 모델 크기 | 상대 개선 (%) | 절대 PPL |
|----------|------------|---------|
| 60M | 4.33% | 1.01 |
| 150M | 7.45% | 1.21 |
| 400M | 7.49% | 1.01 |

**2가지 메커니즘**:
- 선형 연결성 이론: 더 큰 모델이 파라미터 평균화 시 간섭 더 적음
- 과훈련 설정: 큰 모델이 동일 데이터양 피팅에 더 효율적

**4. 통신 빈도 영향**[1]

Figure 4 분석:

$$\text{PPL 상대 증가} \approx 2.9\% \text{ (1000 vs 50 단계)}$$

최적 지점: **500 단계** (일반화 성능 vs 통신 비용 트레이드오프)

#### 특이한 성능 현상

**선형 연결성의 발현**: 
논문의 핵심 통찰은 큰 내부 단계에서 각 워커의 모델이 파라미터 공간의 같은 방향으로 수렴한다는 것입니다. 이는 선형 모드 연결성(Linear Mode Connectivity) 문헌을 활용하여 설명됩니다:

$$\text{최종 모델} = \text{평균}(\theta_1, \theta_2, \ldots, \theta_K)$$

이 평균이 우수한 일반화를 달성하는 이유는 각 워커가 독립적으로 학습한 다양한 최적화 궤적의 이점을 활용하기 때문입니다.

***

### 5. 주요 한계점

논문에서 명시된 한계는 다음과 같습니다:[1]

#### 기술적 한계

1. **스케일링 한계**:
   - 8 워커 이상에서 수확체감 관찰 (Table 3)
   - 64 워커 시: 15.0 PPL (vs 8 워커: 15.02)
   - 추가 계산 활용 능력 제한

2. **모델 크기 제약**:
   - 최대 실험: 400M 파라미터
   - 현재 최첨단 모델: 수십억~수조 파라미터
   - 대규모 모델 성능 불확실

3. **비동기 훈련 미지원**:
   - 모든 워커가 동일 단계 수 완료 대기 필요
   - 이질적 기기 환경에서 느린 기기 병목

4. **계산 효율성 저하**:[1]
   - Wall-clock time 기준 8배 빠르나
   - FLOP/데이터 효율성 감소 (Figure 2의 "8 updates" 기준선과 비교)
   - 외부 업데이트의 과도한 배치 크기 문제

#### 실무적 한계

5. **데이터 효율성**:
   - 매우 긴 내부 단계에서 과훈련 가정
   - 데이터 효율이 중요한 시나리오에서 성능 저하 가능성

6. **아키텍처 및 도메인 일반화**:
   - 트랜스포머만 실험
   - CNN 같은 다른 아키텍처의 동작 불명확 (선형 연결성이 덜함)
   - 언어모델링 외 도메인 미검증

7. **통신 실패 견고성**:
   - 50% 드롭 확률 시에도 PPL 2.1% 저하
   - 완벽한 동기화 불가능 시 성능 저하

***

### 6. 최신 관련 연구 비교 분석 (2020년 이후)

#### 2020-2023년: 기초 연구 및 프레임워크

**주요 선행 연구와의 위치**:

| 연구 | 주요 특징 | DiLoCo와의 비교 |
|-----|---------|---------------|
| **Local SGD (Lin et al., 2020)** | 큰 배치 일반화 개선 | DiLoCo: 언어모델 특화, 500배 통신 감소 |
| **FedMom (Huo et al., 2020)** | Nesterov 외부 최적화 | DiLoCo: 더 큰 모델(400M vs LSTM), 100배 통신 감소 |
| **Local SGD at Scale (Ortiz et al., 2021)** | 비전 도메인, ResNet | DiLoCo: 언어모델에서 더 나은 스케일링, pretrain 불필요 |
| **Federated Averaging (FedAvg, McMahan et al., 2017)** | 기본 알고리즘 | DiLoCo: 더 강력한 내/외부 최적화기 |

#### 2024년: 현재 최신 연구 동향

**1. LLM 중심의 연합학습 프레임워크**

**OpenFedLLM (2024)**:[2]
- 명령어 튜닝(Instruction Tuning)과 가치 정렬(Value Alignment) 지원
- 7가지 대표 FL 알고리즘 포함
- 금융 벤치마크: Llama2-7B이 GPT-4 능가

**FATE-LLM (2023)**:[3]
- 산업급 프레임워크
- 지적재산권 보호 메커니즘
- 파라미터 효율적 미세조정 통합

**FedLLM-Bench (2024)**:[4]
- 현실적 벤치마크 (38-747 클라이언트)
- 8개 훈련 방법, 4개 데이터셋, 6개 평가 지표
- 다언어, 품질, 길이 이질성 포함

**2. 효율성 및 개인화**

**Fisher Information-based Curriculum FedLLM (FibecFed, 2024)**:[5]
- 적응형 데이터 샘플링: Fisher 정보 기반
- 동적 레이어 선택: LoRA와 결합
- 성능: 최대 98.61% 빠름 (17개 기준선 대비)

**Personalized Wireless Federated Fine-tuning (PWFF, 2024)**:[6]
- 무선 환경 최적화
- 어댑터 + LoRA 기술
- 다중 목표 정렬

**FeDeRA (2024)**:[7]
- 가중치 분해 기반 LoRA 확장
- SVD 초기화
- Non-IID 데이터에서 우수한 성능

**3. 실제 배포 및 확장성**

**Titanic (2024)**:[8]
- 클라이언트 디바이스에서 직접 LLM 미세조정
- 모델 파티셔닝 자동화
- 프로덕션 레벨 시스템

**FwdLLM (2024)**:[9]
- 역전파 불필요한 훈련
- 모바일 NPU 활용
- 수렴 속도: 최대 1,000배 가속
- 메모리: 4.6배 감소

**FL-GLM (2024)**:[10]
- 분할 학습 기반 LLM 연합학습
- ChatGLM 프레임워크 적용
- 중앙집중식 모델과 유사 성능

**4. 통신 효율성 고급 기법**

**Distributed Lion (2024)**:[11]
- 이진(Binary) 또는 저정밀도 벡터 통신
- AdamW 대비 대역폭 획기적 감소
- Vision과 Language 작업 모두 우수

**Distributed Sign Momentum with Local Steps (2024)**:[12]
- Lion 옵티마이저 활용
- 로컬 스텝과 결합
- GPT-2 사전훈련에서 우수

**R-SFLLM (2024)**:[13]
- 분할 연합학습의 보안 강화
- 무선 채널 재밍 대응
- 센싱 기반 빔포밍

**5. 신뢰성 및 개인화**

**FedPrompt (2024)**:[14]
- 프롬프트 튜닝 활용
- 통신: 0.01% 감소 (풀 모델 대비)
- IID/Non-IID 모두 적용 가능

#### DiLoCo의 차별화 요인

비교 분석 결과, DiLoCo의 독특한 위치:

| 차원 | DiLoCo | 2024년 연구들 |
|-----|--------|------------|
| **통신 효율성** | 500배 감소 (아키텍처 고정) | 이진화/프롬프트로 0.01-10배 |
| **모델 크기** | 60-400M | 주로 대규모 미세조정 |
| **프레임워크** | 알고리즘 중심 | 기술 스택 (LoRA+FL) |
| **성능** | 기준선 능가 | 기준선 동등 추구 |
| **일반화** | 과훈련 환경 최적 | 데이터 효율성 강조 |

***

### 7. 모델의 일반화 성능 향상 메커니즘

#### 이론적 기초: 선형 모드 연결성[1]

DiLoCo의 핵심 통찰은 **선형 모드 연결성(Linear Mode Connectivity)**에 기반합니다:

**정의**: 여러 신경망 모델들 사이를 선형으로 보간할 때, 보간된 중간 모델들도 낮은 손실을 유지하는 현상

**수식**:

$$L(\alpha \theta_1 + (1-\alpha)\theta_2) \approx \min(L(\theta_1), L(\theta_2)), \quad \forall \alpha \in $$[1]

DiLoCo는 훈련 **중간에** 선형 연결성을 활용합니다:

1. 각 워커가 독립적으로 500 단계 훈련 → $\theta_1^t, \ldots, \theta_K^t$
2. 매개변수 공간에서 이들 사이의 손실 장벽이 낮음
3. 이들의 평균 $\bar{\theta}^t = \frac{1}{K}\sum_i \theta_i^t$도 우수한 성능 유지

**왜 작동하는가?**:

Wortsman et al. (2021-2023)의 연구에 따르면, Transformer는 다른 아키텍처보다 선형 연결성이 강함. DiLoCo는 이를 활용하여:

- 워커들의 **다양한 최적화 궤적** 통합 (앙상블 효과)
- **정규화 효과**: 각 워커의 과적합을 제한
- **교차 검증 효과**: 각 워커는 다른 데이터 분포에서 훈련

#### 외부 기울기의 통계 분석[1]

**코사인 유사도 분석** (Appendix 6.2):

i.i.d. vs non-i.i.d. 환경에서 외부 기울기의 상관성 차이:

$$\text{코사인 유사도} = \frac{\bar{g}_i^t \cdot \bar{g}_j^t}{||\bar{g}_i^t|| \cdot ||\bar{g}_j^t||}$$

- **i.i.d. 설정**: 평균 유사도 높음, 분산 거의 0 (모든 워커 동일 신호)
- **non-i.i.d. 설정**: 평균 유사도 낮음, 분산 높음 (워커별 고유 신호)

**직관**: non-i.i.d에서 직교하는 기울기들의 평균이 더 나은 방향을 제시 (Gradient Noise Reduction 효과)

#### 배치 크기와 일반화[1]

큰 배치 훈련 환경에서의 일반화 향상:

$$\text{일반화 갭} = \text{훈련 손실} - \text{검증 손실}$$

DiLoCo의 설정:
- 총 배치 크기: $K \times 512$ (8 워커 × 512 = 4,096)
- 비동기 훈련으로 인한 **노이즈 추가** → 정규화 효과

**SAM (Sharpness Aware Minimization) 관점**:
- 각 워커: 로컬 데이터에 특화
- 외부 평균화: 날카로운 loss valley 회피 → 더 평탄한 모델

***

### 8. 향후 연구에 미치는 영향 및 고려 사항

#### A. 이론적 영향

**1. 분산 최적화 이론의 확장**:

- **수렴 분석 필요성**: 500단계 같은 매우 큰 내부 단계에서의 수렴율 정량화
- **이질성 처리**: Non-IID 데이터에서 외부 기울기 집산(aggregation)의 이론
- **선형 연결성 정량화**: Transformer에서 선형 연결성의 정도를 수치화하는 이론

**2. 일반화 이론**:

- Fisher 정보량 관점: DiLoCo의 정규화 효과를 정형적으로 분석
- PAC 학습 이론: 분산 훈련이 샘플 복잡도에 미치는 영향

#### B. 실무적 임팩트

**1. 미래 연구 시 고려할 점**:

**(1) 스케일링 문제 해결**:
- 8 워커 이상에서 성능 저하 분석
- 제안: 계층적 애그리게이션 또는 적응형 내부 단계

**(2) 비동기 훈련 확장**:

$$\text{알고리즘 개선}: \theta^{t+1} = \text{OuterOpt}(\theta^t, \bar{g}^{t-\tau_i})$$

여기서 $\tau_i$는 워커 $i$의 지연(staleness). SPIDER, SCAFFOLD 같은 제어 변량(control variate) 기법 적용 가능.

**(3) 모델 파라미터 압축**:
- 현재: 전체 파라미터 통신 (O(N) 복잡도)
- 제안: LoRA, adapter와 결합하여 0.01-10% 만 통신
- 최신 논문 (FibecFed, FedPrompt): 이미 98% 통신 감소 달성

**(4) 대규모 모델 검증**:
- 수십억 파라미터 모델에서의 동작 확인
- 저자의 추측: 간섭 더 적어 더 나은 성능, 그러나 실증 필요

#### C. 응용 분야 확대

**1. 도메인 확장**:

| 도메인 | 고려사항 |
|--------|---------|
| **Vision** | CNN의 낮은 선형 연결성 (Jordan et al., 2023) |
| **Multimodal** | Vision-Language 모델의 선형 연결성 확인 필요 |
| **Speech** | Conformer 아키텍처의 강건성 검증 |

**2. 무선 네트워크 배포**:

최신 연구(PWFF, R-SFLLM, FwdLLM)에서 이미 확장 중:
- 모바일 클라이언트 훈련 (FwdLLM: 역전파 불필요)
- 무선 채널 최적화 (R-SFLLM)
- 에너지 효율성 (PWFF)

#### D. 개방된 연구 질문

**1. 통신-계산 트레이드오프**:

Wall-clock time은 빠르나 FLOP 효율성 감소:

$$\text{개선 제안}: \text{적응형 외부 학습률} = f(\text{워커 간 기울기 일관성})$$

높은 일관성 → 높은 학습률 (빠른 집렴), 낮은 일관성 → 낮은 학습률 (안정성)

**2. 데이터 효율성**:

과훈련 가정에서 벗어난 정상 훈련 시나리오에서의 성능

**3. 동적 클러스터 환경**:

워커 입퇴장이 빈번한 실제 환경에서의 안정성

***

### 결론

DiLoCo는 분산 LLM 훈련의 패러다임을 바꾸는 연구입니다. **500배 통신 감소와 동등 이상의 성능**을 달성함으로써 지리적으로 분산된 저대역폭 환경에서 LLM 훈련의 가능성을 입증했습니다.

핵심 기여는:
1. **이중 수준 최적화**의 효과 입증 (특히 언어모델 도메인)
2. **선형 모드 연결성**을 훈련 중 활용하는 새로운 관점
3. **강건한 실무 성능** (non-IID, 동적 리소스, 통신 실패)

그러나 제한점도 명확합니다:
- 8 워커 이상 스케일링 한계
- 대규모 모델(수백억 이상) 미검증
- 계산 효율성 vs 벽시계 시간의 트레이드오프

2024년 이후 최신 연구들은 DiLoCo의 기초 위에서 **파라미터 효율성, 무선 최적화, 프라이버시 강화** 등으로 확장 중입니다. 향후 연구는 스케일링, 비동기 훈련, 모델 압축 기법의 통합에 집중할 필요가 있습니다.

***

### 참고문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/11b4a272-1441-49df-ad4e-7a2c6299f97e/2311.08105v3.pdf)
[2](https://dl.acm.org/doi/10.1145/3637528.3671582)
[3](https://arxiv.org/pdf/2310.10049.pdf)
[4](https://arxiv.org/abs/2406.04845)
[5](https://arxiv.org/abs/2410.00131)
[6](https://arxiv.org/abs/2404.13238)
[7](https://arxiv.org/abs/2404.18848)
[8](https://ieeexplore.ieee.org/document/10621164/)
[9](https://www.usenix.org/conference/atc24/presentation/xu-mengwei)
[10](https://aclanthology.org/2024.emnlp-main.303.pdf)
[11](https://arxiv.org/pdf/2404.00438.pdf)
[12](https://arxiv.org/pdf/2411.17866.pdf)
[13](https://ieeexplore.ieee.org/document/11104091/)
[14](https://arxiv.org/pdf/2409.15723.pdf)
[15](https://arxiv.org/abs/2404.15182)
[16](https://dl.acm.org/doi/10.1145/3639478.3643533)
[17](https://peerj.com/articles/cs-2993/)
[18](https://linkinghub.elsevier.com/retrieve/pii/S2666389924001053)
[19](http://arxiv.org/pdf/2406.04845.pdf)
[20](http://arxiv.org/pdf/2409.15723.pdf)
[21](http://arxiv.org/pdf/2405.16682.pdf)
[22](https://arxiv.org/pdf/2402.06954.pdf)
[23](https://arxiv.org/pdf/2409.11585.pdf)
[24](http://arxiv.org/pdf/2208.11625.pdf)
[25](https://arxiv.org/html/2305.11414)
[26](https://fid3024.github.io/papers/2020%20-%20Communication-Efficient%20Distributed%20Deep%20Learning:%20A%20Comprehensive%20Survey.pdf)
[27](https://proceedings.iclr.cc/paper_files/paper/2025/file/a1e0d6fa0c30b7d4f75dd9c7ed6189f2-Paper-Conference.pdf)
[28](https://conferences.sigcomm.org/events/apnet2024/papers/UnderstandingCommunication.pdf)
[29](https://arxiv.org/html/2403.07585v1)
[30](https://aclanthology.org/anthology-files/pdf/D/D19/D19-5608.pdf)
[31](https://aclanthology.org/2024.findings-naacl.59/)
[32](https://arxiv.org/abs/2406.09831)
[33](https://pubmed.ncbi.nlm.nih.gov/39776850/)
[34](https://arxiv.org/abs/2404.06114)
[35](https://arxiv.org/html/2502.00213v1)
[36](https://arxiv.org/html/2507.20424v1)
[37](https://arxiv.org/html/2507.17501v1)
[38](https://research-explorer.ista.ac.at/download/17490/17492/Thesis_final_version_pdfa2.pdf)
[39](https://www.sciencedirect.com/science/article/abs/pii/S0743731520304068)
