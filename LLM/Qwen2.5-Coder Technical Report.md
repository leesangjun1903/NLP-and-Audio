# Qwen2.5-Coder Technical Report

## 1. 핵심 주장과 주요 기여

Qwen2.5-Coder는 코드 전문 대규모 언어 모델의 새로운 세대로, 다음과 같은 핵심 기여를 제시합니다:[1]

**강력한 성능(Powerful)**: 32B 버전은 현재 오픈소스 코드 모델 중 최고 수준(SOTA)으로, GPT-4o에 필적하는 코딩 능력을 보유하며, 동시에 일반 및 수학적 능력도 우수합니다.[1]

**다양한 모델 크기(Diverse)**: 0.5B, 1.5B, 3B, 7B, 14B, 32B의 6가지 모델 크기를 제공하여 다양한 개발자의 요구사항을 충족합니다.[1]

**실용성(Practical)**: 코드 어시스턴트와 Artifacts와 같은 실제 시나리오에서 실용성을 탐구하며, 5.5조 토큰 이상의 대규모 코드 특화 사전학습 데이터셋을 구축했습니다.[1]

## 2. 해결하고자 하는 문제

### 2.1 문제 정의

기존 코드 LLM들은 최신 독점 모델(Claude-3.5-Sonnet, GPT-4o)에 비해 성능이 뒤처지는 문제가 있었습니다. 특히 다음과 같은 한계가 존재했습니다:[1]

- 제한적인 다중 프로그래밍 언어 지원
- 긴 컨텍스트 이해 능력 부족
- 일반 지식과 수학적 능력 손실
- 실제 환경에서의 적용 가능성 한계

### 2.2 제안하는 방법

**사전학습 데이터 구성**:[1]

Qwen2.5-Coder는 5가지 핵심 데이터 유형으로 구성된 5.5조 토큰의 대규모 데이터셋을 구축했습니다:

1. **소스 코드 데이터**: GitHub에서 2024년 2월 이전에 생성된 92개 프로그래밍 언어의 공개 저장소, Pull Requests, Commits, Jupyter Notebooks, Kaggle 데이터셋[1]

2. **텍스트-코드 연결 데이터(Text-Code Grounding Data)**: Common Crawl에서 수집한 코드 관련 문서, 튜토리얼, 블로그를 포함하며, 계층적 필터링 방식을 사용하여 품질 관리[1]

3. **합성 데이터(Synthetic Data)**: CodeQwen1.5를 사용하여 생성하며, 환각(hallucination) 위험을 완화하기 위해 실행 가능한 코드만 유지[1]

4. **수학 데이터**: Qwen2.5-Math의 사전학습 코퍼스를 통합하여 수학적 능력 향상[1]

5. **텍스트 데이터**: Qwen2.5 모델의 일반 자연어 데이터를 포함하되, 코드 중복을 피하기 위해 코드 세그먼트를 제거[1]

**데이터 혼합 비율**:[1]

모델은 코드, 텍스트, 수학 데이터의 최적 비율을 실험적으로 결정했습니다. 표준화된 실험을 통해 **70% 코드, 20% 텍스트, 10% 수학**의 비율이 가장 우수한 성능을 보였습니다:[1]

| 토큰 비율 (Code:Text:Math) | Coding | Math | General | Average |
|---|---|---|---|---|
| 100:0:0 | 49.8 | 10.3 | 42.8 | 31.3 |
| 85:15:5 | 43.3 | 26.1 | 56.8 | 48.9 |
| **70:20:10** | **48.3** | **33.2** | **62.9** | **55.0** |

이는 수학과 텍스트 데이터가 특정 임계값에 도달했을 때 코드 성능에 긍정적으로 기여할 수 있음을 시사합니다.[1]

**3단계 학습 파이프라인**:[1]

1. **파일 수준 사전학습(File-Level Pretraining)**: 
   - 최대 시퀀스 길이: 8,192 토큰
   - 학습 목표: 다음 토큰 예측 + Fill-in-the-Middle (FIM)
   - FIM 형식:
   ```
   <fim_prefix>code_pre<fim_suffix>code_suf<fim_middle>code_mid<|endoftext|>
   ```

2. **저장소 수준 사전학습(Repo-Level Pretraining)**:
   - 컨텍스트 길이를 8,192에서 32,768 토큰으로 확장
   - RoPE base frequency를 10,000에서 1,000,000으로 조정
   - YARN 메커니즘 적용으로 최대 128K 토큰까지 처리 가능[1]
   - 300B 토큰의 긴 컨텍스트 코드 데이터 사용
   - Repo-Level FIM 형식:
   ```
   <repo_name>repo_name
   <file_sep>filepath1
   file_content1
   <file_sep>filepath2
   file_content2
   <file_sep>filepath3
   <fim_prefix>code_pre<fim_suffix>code_suf<fim_middle>code_fim<|endoftext|>
   ```

3. **후학습(Post-training)**:
   - Coarse-to-fine 미세조정: 먼저 수천만 개의 다양하지만 낮은 품질의 instruction 샘플로 학습 후, 수백만 개의 고품질 샘플로 정제[1]
   - Mixed Tuning: 표준 SFT 데이터와 FIM instruction 샘플을 혼합하여 긴 컨텍스트 능력 유지[1]
   - Direct Preference Optimization (DPO): 다중 언어 코드 샌드박스로 코드 실행 피드백을 제공하고, LLM을 judge로 사용하여 선호도 정렬[1]

### 2.3 모델 구조

Qwen2.5-Coder는 Qwen2.5 아키텍처를 직접 계승하며, Grouped Query Attention (GQA)과 SwiGLU activation을 사용합니다:[1]

| 구성 | 0.5B | 1.5B | 3B | 7B | 14B | 32B |
|---|---|---|---|---|---|---|
| Hidden Size | 896 | 1,536 | 2,048 | 3,584 | 5,120 | 5,120 |
| Layers | 24 | 28 | 36 | 28 | 48 | 64 |
| Query Heads | 14 | 12 | 16 | 28 | 40 | 40 |
| KV Heads | 2 | 2 | 2 | 4 | 8 | 8 |
| Head Size | 128 | 128 | 128 | 128 | 128 | 128 |
| Vocabulary Size | 151,646 | 151,646 | 151,646 | 151,646 | 151,646 | 151,646 |
| Trained Tokens | 5.5T | 5.5T | 5.5T | 5.5T | 5.5T | 5.5T |

**특수 토큰**: 코드 데이터를 더 잘 이해하기 위해 다음과 같은 특수 토큰을 추가했습니다:[1]

- `<|endoftext|>`: 텍스트/시퀀스 종료
- `<fim_prefix>`, `<fim_middle>`, `<fim_suffix>`: Fill-in-the-Middle 구현
- `<fim_pad>`: FIM 패딩
- `<repo_name>`: 저장소 이름
- `<file_sep>`: 파일 구분자

## 3. 성능 향상

### 3.1 코드 생성 성능

**HumanEval & MBPP**:[1]

| 모델 | 크기 | HumanEval+ | MBPP+ | BigCodeBench |
|---|---|---|---|---|
| Qwen2.5-Coder-7B | 7B | 53.0 | 62.9 | 45.8 |
| DS-Coder-6.7B-Base | 6.7B | 39.6 | 56.6 | 41.1 |
| Qwen2.5-Coder-32B | 32B | 60.4 | 68.2 | 53.6 |
| DS-Coder-33B-Base | 33B | 47.6 | 60.7 | 49.1 |

Qwen2.5-Coder-7B는 이전 최고 성능의 밀집 모델인 DS-Coder-33B를 모든 5가지 지표에서 능가합니다.[1]

**다중 프로그래밍 언어 (MultiPL-E)**:[1]

Qwen2.5-Coder-32B는 8개 주류 언어에서 평균 63.9%를 달성하며, 5개 언어에서 60% 이상을 기록했습니다:[1]

- Python: 65.9
- C++: 68.3
- Java: 70.9
- JavaScript: 67.1
- TypeScript: 66.0

### 3.2 코드 완성 성능

**HumanEval-FIM**:[1]

Qwen2.5-Coder-7B는 평균 86.2%로 동급 모델 중 최고 성능을 달성했으며, 33B 파라미터의 DS-Coder-33B-Base와 동등한 성능을 보였습니다.[1]

**CrossCodeEval & CrossCodeLongEval**:[1]

크로스 파일 컨텍스트를 요구하는 벤치마크에서 Qwen2.5-Coder-32B는 평균 57.1% EM (Exact Match)과 86.8% ES (Edit Similarity)를 기록하며 SOTA를 달성했습니다.[1]

### 3.3 코드 추론 성능

**CRUXEval**:[1]

| 모델 | Input-CoT | Output-CoT |
|---|---|---|
| Qwen2.5-Coder-7B | 56.5 | 56.0 |
| DS-Coder-6.7B-Base | 39.0 | 41.0 |
| Qwen2.5-Coder-32B | 62.5 | 69.4 |
| DS-Coder-V2-Base | 62.7 | 67.4 |

실행 가능한 품질에 중점을 둔 코드 정제 프로세스 덕분에 높은 추론 성능을 달성했습니다.[1]

### 3.4 Instruction 모델 성능

**LiveCodeBench (2024.07-2024.11)**:[1]

Qwen2.5-Coder-32B-Instruct는 31.4% Pass@1을 달성하여 모든 오픈소스 코드 생성 모델을 능가하며, 많은 독점 API와 비교할 수 있는 수준에 도달했습니다.[1]

**Human Preference Alignment (CodeArena)**:[1]

GPT-4o를 평가 모델로 사용한 A vs. B 승률 비교에서:
- Qwen2.5-Coder-32B-Instruct vs. DS-Coder-V2-Instruct: 68.9% 승률
- Qwen2.5-Coder-32B-Instruct vs. Codestral-22B: 78.1% 승률

## 4. 일반화 성능 향상 가능성

### 4.1 Out-of-Distribution 일반화

**BigCodeBench**: 이 벤치마크는 도구 사용과 복잡한 instruction 수행 능력을 평가하는 out-of-distribution (OOD) 평가에 적합합니다. Qwen2.5-Coder-32B는 53.6%를 기록하며 강력한 일반화 잠재력을 보여줍니다.[1]

**LiveCodeBench**: 지속적으로 LeetCode, AtCoder, CodeForces에서 새로운 문제를 수집하는 오염 없는(contamination-free) 벤치마크입니다. 2024년 7월-9월 데이터에서 Qwen2.5-Coder-32B-Instruct는 31.4%를 달성하여 실제 경쟁 프로그래밍 작업에서 우수한 일반화 능력을 입증했습니다.[1]

### 4.2 긴 컨텍스트 일반화

**Needle in the Code**: Megatron 코드베이스의 다양한 위치에 간단한 커스텀 함수를 삽입하고, 모델이 코드베이스 끝에서 해당 함수를 복제할 수 있는지 테스트했습니다. Qwen2.5-Coder는 128K 토큰 범위 내에서 이 작업을 성공적으로 완료할 수 있음을 보여줍니다.[1]

**Repository-Level 작업**:[1]
- RepoEval에서 Qwen2.5-Coder-32B는 평균 51.6% EM과 78.5% ES를 기록하여 DS-Coder-33B-Base 대비 7.9% EM, 4.2% ES 향상을 달성했습니다[1]
- 이는 저장소 수준의 긴 컨텍스트 이해 능력이 뛰어남을 의미합니다

### 4.3 다중 작업 일반화

**수학 추론 유지**:[1]

코드 전문 모델임에도 불구하고 수학적 능력을 유지했습니다:

| 모델 | MATH | GSM8K | MMLU-STEM |
|---|---|---|---|
| Qwen2.5-Coder-7B | 46.6 | 83.9 | 67.6 |
| DS-Coder-6.7B-Base | 10.3 | 21.3 | 34.2 |
| Qwen2.5-Coder-32B | 57.2 | 91.1 | 75.1 |

**일반 자연어 이해 유지**:[1]

| 모델 | MMLU | HellaSwag | ARC-Challenge |
|---|---|---|---|
| Qwen2.5-Coder-7B | 68.0 | 76.8 | 60.9 |
| DS-Coder-6.7B-Base | 36.4 | 53.8 | 36.4 |
| Qwen2.5-Coder-32B | 79.1 | 83.0 | 70.5 |

이러한 결과는 **70:20:10 (코드:텍스트:수학) 데이터 혼합 전략**의 효과성을 입증하며, 코드 전문성을 유지하면서도 일반 능력을 손실하지 않았음을 보여줍니다.[1]

### 4.4 데이터 품질과 일반화

**계층적 필터링의 효과**:[1]

Text-Code Grounding Data에 대한 4단계 반복 필터링을 통해:
- 단계 1: 582B 토큰 → 평균 성능 41.6
- 단계 4: 118B 토큰 → 평균 성능 46.8

이는 고품질 데이터가 모델의 일반화 성능 향상에 직접적으로 기여함을 보여줍니다.[1]

**합성 데이터 검증**:[1]

합성 데이터 생성 시 실행기(executor)를 도입하여 실행 가능한 코드만 유지함으로써, 환각(hallucination) 위험을 완화하고 일반화 성능을 향상시켰습니다.[1]

## 5. 한계

논문에서 명시적으로 언급된 한계는 다음과 같습니다:

### 5.1 데이터 오염 제거의 한계

10-gram 중복 기반 오염 제거를 수행했으나, 이 방법은 의역되거나 약간 변형된 테스트 샘플을 완전히 제거하지 못할 수 있습니다.[1]

### 5.2 모델 크기의 한계

32B 파라미터가 현재 가장 큰 모델이지만, 독점 모델들(GPT-4o, Claude-3.5-Sonnet)과 비교했을 때 일부 작업에서 여전히 성능 격차가 존재합니다:[1]

- Aider (코드 편집): Claude-3.5-Sonnet-20241022가 71.4% vs. Qwen2.5-Coder-32B-Instruct 60.9%
- HumanEval (Instruct): o1-preview가 95.1% vs. Qwen2.5-Coder-32B-Instruct 92.7%

### 5.3 특정 언어에서의 성능 편차

MultiPL-E 평가에서 일부 언어의 성능이 상대적으로 낮습니다:[1]
- Bash: 39.9%
- Perl: 낮은 성능 예상 (long-tail 언어)

이는 학습 데이터에서 주류 언어(Python, Java, C++)에 비해 long-tail 언어의 비중이 적었기 때문으로 추정됩니다.[1]

### 5.4 코드 완성의 Exact Match 한계

함수 완성(Function completion) 작업에서 모든 모델이 낮은 Exact Match (EM) 결과를 보였습니다:[1]
- CrossCodeLongEval 함수 완성: Qwen2.5-Coder-32B 16.4% EM

이는 다중 라인 코드 스니펫을 정확히 매칭하기 어렵다는 복잡성을 반영합니다.[1]

### 5.5 추론 능력의 한계

코드 추론 작업에서 여전히 개선의 여지가 있습니다:
- CRUXEval Output-CoT: o1-mini가 96.2% vs. Qwen2.5-Coder-32B 83.4%

## 6. 향후 연구에 미치는 영향 및 고려사항

### 6.1 연구에 미치는 영향

**스케일링 법칙의 재확인**:[1]

논문은 "Scaling is All You Need"라는 섹션을 통해 모델 크기와 성능 간의 긍정적 상관관계를 입증했습니다. 이는 향후 연구자들이 더 큰 코드 LLM 탐구를 지속할 동기를 제공합니다.[1]

**데이터 혼합의 중요성**:[1]

70:20:10 비율이 100:0:0 비율보다 우수한 성능을 보인 것은, 단순히 코드 데이터만 늘리는 것이 아니라 다양한 데이터 유형의 균형이 중요함을 시사합니다. 향후 연구는 최적 비율 메커니즘을 더 효율적으로 탐색할 필요가 있습니다.[1]

**긴 컨텍스트 능력의 표준화**:[1]

128K 토큰 지원은 저장소 수준 이해와 코드 에이전트 구축의 핵심 기술로, 향후 코드 LLM의 표준 기능이 될 것으로 예상됩니다.[1]

**다중 언어 지원의 확장**:[1]

92개 프로그래밍 언어 지원과 다중 에이전트 협업 프레임워크는 향후 연구가 더 광범위한 언어 생태계를 포괄하도록 유도할 것입니다.[1]

### 6.2 향후 연구 시 고려할 점

**1. 데이터 품질과 다양성의 균형**:
- 계층적 필터링 방식을 참고하여 각 필터 단계의 책임을 명확히 분리[1]
- 표면적 특징에 집중하는 작은 모델(예: fastText)이 더 효과적일 수 있음을 고려
- 합성 데이터 생성 시 실행기를 통한 검증 필수[1]

**2. 최적 데이터 혼합 비율 탐색**:
- 수학 및 텍스트 데이터가 특정 임계값에 도달해야 코드 성능에 기여한다는 가설을 다른 도메인에도 적용 가능한지 검증[1]
- 더 효율적인 비율 탐색 메커니즘 개발 필요

**3. 긴 컨텍스트 학습 전략**:
- 파일 수준 → 저장소 수준의 점진적 학습 접근법 활용[1]
- RoPE base frequency 조정과 YARN 메커니즘을 통한 외삽(extrapolation) 능력 향상[1]
- 긴 컨텍스트 능력 유지를 위한 Mixed Tuning (FIM + SFT) 적용[1]

**4. 평가 벤치마크의 다양화**:
- 오염 없는 벤치마크(LiveCodeBench) 활용으로 진정한 일반화 능력 평가[1]
- OOD 평가(BigCodeBench)를 통한 실제 환경 성능 검증[1]
- Human preference alignment (CodeArena)로 실용성 평가[1]

**5. 모델 정렬과 선호도 최적화**:
- Coarse-to-fine 미세조정 전략: 먼저 다양성 확보, 이후 품질 정제[1]
- 다중 언어 샌드박스를 통한 코드 실행 피드백과 LLM-as-a-judge 결합[1]
- DPO를 통한 오프라인 선호도 최적화[1]

**6. 추론 능력 강화**:
- 논문이 향후 연구 방향으로 명시한 "추론 능력 향상"에 집중[1]
- Chain-of-Thought 접근법의 개선
- 코드 실행 시뮬레이션 능력 강화

**7. 효율성과 성능의 균형**:
- 7B 모델이 33B 모델을 능가하는 사례를 통해 모델 아키텍처 최적화의 중요성 인식[1]
- Grouped Query Attention과 같은 효율적 아키텍처 적용[1]

**8. 오염 제거 강화**:
- 10-gram 중복 외에 의미론적 유사성 기반 오염 제거 방법 개발[1]
- 학습 데이터와 평가 데이터의 독립성 보장

**9. Long-tail 언어 지원 개선**:
- 주류 언어와 long-tail 언어 간 성능 격차 해소[1]
- 언어별 특화 에이전트 활용을 통한 크로스 언어 지식 증류[1]

**10. 실제 애플리케이션 검증**:
- 코드 어시스턴트, 코드 에이전트 등 실제 시나리오에서의 성능 검증[1]
- 사용자 피드백을 통한 지속적 개선

이러한 고려사항들은 Qwen2.5-Coder의 성공 요인을 다른 연구 프로젝트에 적용하고, 동시에 발견된 한계를 극복하는 데 도움이 될 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/182c1997-10c7-4cd8-9ce3-52e4a5b5177f/2409.12186v3.pdf)
