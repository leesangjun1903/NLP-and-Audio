# Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks

**주요 주장**  
대규모 범용 언어 모델(RoBERTa 등)은 다양한 소스의 방대한 데이터로 사전학습되었으나, 특정 도메인 또는 과제에 더욱 적합하도록 추가 학습(continued pretraining)을 시행하면 성능 향상이 가능하다.

**주요 기여**  
1. 네 개 도메인(생의학, 컴퓨터과학, 뉴스, 리뷰)·여덟 과제(각 도메인당 2개 분류 과제)에 대해  
   - 도메인 적응 전처리(DAPT)  
   - 과제 적응 전처리(TAPT)  
   - 두 기법의 결합  
   을 종합 비교.  
2. DAPT·TAPT 각 단계별 계산 비용과 성능 이득 분석.  
3. 제한된 자원 환경에서 자동 데이터 선택(kNN-TAPT) 기법 제안.  
4. 대규모 인간 주도 무라벨 데이터(“Curated-TAPT”) 활용 시 추가 이득 확인.

***

## 1. 해결 문제  
- **도메인 불일치(Domain Shift)**  
  범용 LM이 학습한 데이터와 실제 과제 데이터 간 통계 분포 차이로 인한 성능 저하.  
- **과제 특화 부족(Task Specificity)**  
  도메인 단위가 아닌 개별 과제의 특유한 분포를 포착하지 못함.

***

## 2. 제안 방법

### 2.1 도메인 적응 전처리(Domain-Adaptive Pretraining, DAPT)  
- RoBERTa를 특정 도메인(생의학·컴퓨터과학·뉴스·리뷰) 코퍼스에 대해 추가로 12.5K 스텝 학습.  
- 손실 감소로 도메인 언어 특성 포착.  
- 결과: 전반적 분류 성능 상승.[1]

### 2.2 과제 적응 전처리(Task-Adaptive Pretraining, TAPT)  
- 해당 과제의 무라벨 학습 데이터에 대해 RoBERTa를 100 에포크 추가 학습(각 에포크마다 15% 마스킹 토큰 변경).  
- DAPT보다 훨씬 저렴한 비용으로 과제 특화 분포 반영.  
- 결과: DAPT와 유사하거나 더 나은 성능.

### 2.3 두 단계 결합  
- RoBERTa → DAPT → TAPT 순으로 연속 적용.  
- 도메인 및 과제 궤도를 모두 반영하여 각 과제에서 최상의 결과 달성.

### 2.4 자동 데이터 선택(kNN-TAPT)  
- 대규모 인-도메인 코퍼스에서 VAMPIRE 임베딩을 사용해 과제 문장별 k-최근접 문장 추출(k=50,150,500).  
- 추출 문장과 원 과제 문장을 합쳐 TAPT 수행.  
- k 증가에 따라 성능 점진 상승, DAPT 수준 근접.

### 2.5 인간 주도 무라벨 데이터 활용(Curated-TAPT)  
- RCT-500, Hyperpartisan, IMDB 과제에 기존 태스크 설계자가 수집한 거대 무라벨 데이터(라벨링 전) 추가.  
- TAPT 대비 평균 2% 이상 성능 향상.

***

## 3. 모델 구조 및 수식  
- **기반 아키텍처:** RoBERTa-base (12층 Transformer, 768 차원, 12 헤드).[1]
- **목표 함수:** 마스킹된 언어 모델링(Masked LM) 손실  

$$ \mathcal{L}_\text{MLM} = - \sum_{i \in M} \log P(x_i | x_{\backslash M}) $$  
  
  여기서 $$M$$은 마스킹된 토큰 인덱스 집합, $$x_{\backslash M}$$은 나머지 입력 토큰.  
- **분류기:** 최종 CLS 토큰 표현 $$\mathbf{h}_\text{CLS}$$를 피드포워드 레이어에 통과시켜 예측.  

$$ \hat{y} = \mathrm{softmax}(W\,\mathbf{h}_\text{CLS} + b) $$

***

## 4. 성능 향상 및 한계

### 4.1 성능 요약  
- **DAPT**: 평균 1–7% 절대 향상.[1]
- **TAPT**: 평균 0.5–5% 향상, 일부 과제는 DAPT 초과.  
- **DAPT → TAPT**: 모든 과제에서 최상.  
- **kNN-TAPT**: k=500 시 DAPT에 근접.  
- **Curated-TAPT**: 최소 TAPT 대비 1.5–3% 추가 향상.

### 4.2 한계  
- **계산 비용**: DAPT(12.5K 스텝, 47 GB) vs. TAPT(100 에포크, 수십 MB) 큰 격차.  
- **과제 간 전이(Transfer-TAPT)**: 동일 도메인 내 과제 간 선호 방향 불일치로 오히려 성능 하락.  
- **데이터 불균형**: 소규모 과제(Hyperpartisan 등)에서 랜덤 시드 민감도 큼.  
- **자동 선택 품질**: kNN-TAPT의 이득이 도메인과 과제 특성에 따라 달라짐.

***

## 5. 일반화 성능 향상 관점

- **도메인 특화 학습**은 모델이 범용 코퍼스에 없는 용어·표현을 학습하도록 도와 **언어 분포 차이**를 줄인다.  
- **과제 특화 학습**은 실험 데이터의 레이블 분포 및 패턴을 정확히 모방하여 **과제 일반화**를 강화한다.  
- **연속 처리(DAPT→TAPT)**는 도메인과 과제 분포 간 간극을 줄이는 **커리큘럼 학습** 효과를 제공해 사전학습된 표현을 안정적으로 특화시킨다.  
- **자동 데이터 선택**은 추가 데이터가 부족한 경우에도 노이즈를 최소화하며 **관련성 높은** 학습 예시를 제공해 일반화 능력 확보를 지원한다.

***

## 6. 향후 연구 및 고려 사항

1. **효율적 전처리 순서 탐색**: TAPT→DAPT 순서, 교차 반복, 및 병렬 학습 커리큘럼 최적화.  
2. **고급 데이터 선택**: 표현 학습 기반 군집화, 적대적 예시 선택, 다양성·관련성 균형 기법.  
3. **소규모·다중 과제**: 태스크 간 전이를 고려한 다태스크 TAPT, 메타러닝 접근.  
4. **범용성 평가**: 추가 도메인(법률, 금융 등)으로 확장해 DAPT·TAPT 일반성 검증.  
5. **자원 효율성**: 경량화된 추가 학습 방법(프롬프트 튜닝, LoRA 등) 적용.

이 논문은 **도메인 및 과제 특화의 중요성**을 명확히 입증함으로써, 대용량 범용 모델 이외에도 **맞춤형 전처리 단계**를 통합하는 연구가 향후 NLP 실무 및 연구에서 필수 요소임을 시사한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/48373186-3c35-43d6-b27f-b7c3222aa537/2004.10964v3.pdf)
