# “A Survey on Large Language Model based Autonomous Agents” 

## 1. 핵심 주장과 주요 기여
“LLM 기반 자율 에이전트” 연구 동향을 체계적으로 정리하여  
- **통합 프레임워크**(프로파일링·메모리·플래닝·액션 모듈) 제시  
- 다양한 **응용 분야**(사회과학·자연과학·공학) 분류  
- **평가 전략**(주관 평가·객관 평가)와 벤치마크 정리  
를 통해, LLM 에이전트 설계 원리와 발전 방향을 한눈에 조망하도록 함.

## 2. 해결하고자 하는 문제
기존 자율 에이전트 연구는
- 제한된 환경·단순 정책 함수  
- 고립된 도메인 학습  
으로 인간 수준의 복합 의사결정과 일반화에 한계가 있었음.  
이를 극복하기 위해  
- 방대한 웹 지식 내재화한 LLM을 컨트롤러로 활용  
- 메모리·계획·행동 모듈 설계로 동적 환경 적응  
- 인간과 유사한 자연어 인터페이스 제공  
등을 통해 개방 도메인에서의 **인간 수준 자율성**을 달성하고자 함.

## 3. 제안하는 방법론
### 3.1 통합 에이전트 프레임워크  
1) **프로파일링**: 역할·성격·사회관계 등 에이전트 특성 정의  
2) **메모리**:  
   - 구조: 단일 메모리(컨텍스트창) vs. 단기·장기 하이브리드  
   - 형식: 텍스트·임베딩·데이터베이스·리스트  
   - 연산: 읽기·쓰기·반성(고차원 통찰 생성)  
3) **플래닝**:  
   - 피드백 없는 단일·다중 경로 추론(Chain-of-Thought, Tree-of-Thought)  
   - 환경·인간·모델 피드백을 통한 반복 계획 수정  
4) **액션**:  
   - 목표: 과제 수행·커뮤니케이션·탐험  
   - 생산: 메모리 회상 기반 vs. 계획 기반  
   - 공간: 도구 호출(API·DB·모델) vs. 내부 지식  
   - 영향: 환경 변화·내부 상태 업데이트·추가 행동 촉발  

### 3.2 수식 예시  
메모리 읽기 시 최근성·관련성·중요도 결합 가중합으로 핵심 메모리 선별:  

$$
m^* = \arg\max_{m\in M}\bigl[\alpha\,s_{\text{rec}}(q,m) + \beta\,s_{\text{rel}}(q,m) + \gamma\,s_{\text{imp}}(m)\bigr]
$$

## 4. 모델 구조와 성능 향상
- **모듈화 아키텍처**: 네 개 모듈 간 결합으로 유연성과 확장성 확보  
- **하이브리드 메모리**: 벡터 DB 활용한 장기 메모리로 맥락 창 한계 극복  
- **피드백 루프**: 환경·인간·모델 피드백으로 장기 과제에 반복 계획 수립  
- **도구 연계**: API·전문 모델 호출로 LLM의 외연 확장  
이들 설계는 다양한 벤치마크(IGLU, ALFWorld, ToolBench 등)에서  
전통적 LLM 대비 과제 성공률·일관성·인간 유사성 지표를 **10~50%** 이상 향상시켰으나,  
대규모 에이전트 구축 시 연산 비용·추론 지연·메모리 관리 한계가 남아 있음.

## 5. 한계 및 일반화 성능
- **컨텍스트 길이 제약**: 하이브리드 메모리로 보완했으나, 임의 환경 전반의 정보 누락 위험  
- **도메인 의존성**: 외부 플래너·도구에 과도히 의존 시 일반화 저하 가능  
- **프롬프트 민감도**: 모듈 간 상호영향으로 일관된 프롬프트 설계 어려움  
- **보편적 인간 정렬**: 다양한 시뮬레이션 목적에 맞춘 가치·행동 재조율 필요  
이러한 점들이 장·단기 대상 과제로 작용하여, **일반화 성능** 확보를 위해  
메타-학습, 유연한 프롬프트 튜닝, 멀티도메인 학습 전략이 중요함을 강조.

## 6. 향후 연구 영향 및 고려사항
- **메타-에이전트 설계**: 자율 모듈 선택·조합을 스스로 학습하는 **메타 제어기** 도입  
- **효율성 최적화**: 분산 추론·지연 학습·온디맨드 메모리 페이징 기법 개발  
- **안전·윤리 규제**: 시뮬레이션용 부정 행위 시나리오도 허용하는 **범용 가치 정렬** 틀 구축  
- **일반화·적응학습**: 소수 샷·온라인 학습으로 도메인 전환 시 성능 저하를 최소화  
이 논문은 에이전트 모듈화 관점의 **체계적 분류**와 **벤치마크 정리**로 미래 자율 에이전트  
설계·평가의 핵심 로드맵을 제시하며, **효율·안전·확장성**을 동시 달성하는 연구에 기여할 전망이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7f48c2ba-afcc-4b31-b7c1-770c02684496/2308.11432v7.pdf

# 2 LLM 기반 자율 에이전트 구축 자세 해설

LLM 기반 자율 에이전트 구축은 크게 두 단계로 나뉩니다.  
1) **아키텍처 설계(Agent Architecture Design)**  
2) **에이전트 역량 획득(Capability Acquisition)**  

아래에 각 단계와 그 하위 요소를 목차별로 상세히 설명합니다.

***

## 2.1 에이전트 아키텍처 설계  
LLM을 활용해 에이전트를 구현할 때는 네 가지 핵심 모듈을 조합합니다.  
```
프로파일링 ──▶ 메모리 ──▶ 플래닝 ──▶ 액션
       ╲───────────────────╱
```

### 2.1.1 프로파일링 모듈(Profile)  
- **목적**: 에이전트가 수행할 “역할(role)”을 정의하여 LLM의 출력을 해당 역할에 맞게 유도  
- **프로파일 정보**  
  - 인구통계(나이·성별·경력)  
  - 성격·심리(외향·내향·의사결정 스타일)  
  - 사회관계(다른 에이전트/사용자와의 관계)  
- **생성 방식**  
  1. **수작업**: 디자이너가 직접 “당신은 친절한 상담사입니다” 등 프롬프트 작성  
  2. **LLM 자동 생성**: 소수 시드 프로파일을 주고 나머지를 LLM에게 생성 요청  
  3. **데이터셋 정렬**: 실제 인구설문(예: ANES) 데이터를 자연어로 가공해 프로파일링  

### 2.1.2 메모리 모듈(Memory)  
에이전트가 주변 환경을 기억·활용하도록 설계

1) **구조(Structure)**  
   - **단일 기억(Unified)**: 컨텍스트창(prompt)에 최근 정보만 보존  
   - **하이브리드(Hybrid)**:  
     - 단기 메모리(컨텍스트창)  
     - 장기 메모리(벡터 DB 기반 외부 저장)

2) **형식(Formats)**  
   - **자연어**: 로그·행동 관찰을 텍스트로 저장  
   - **임베딩**: 텍스트를 벡터로 인코딩해 유사도 검색  
   - **데이터베이스**: SQL 등 구조화된 쿼리로 관리  
   - **구조화 리스트**: 목표·행동 쌍을 계층 리스트로 저장  

3) **연산(Operations)**  
   - **읽기(Reading)**:  

$$
       m^* = \arg\max_{m\in M}\bigl[\alpha\,s_{\text{rec}}(q,m) + \beta\,s_{\text{rel}}(q,m) + \gamma\,s_{\text{imp}}(m)\bigr]
     $$  
     
  – 최근성(sₙ), 관련성(sᵣ), 중요도(sᵢ)를 가중합하여 핵심 메모리 선택  
   - **쓰기(Writing)**: 중복 통합·낡은 항목 삭제(FIFO 등)  
   - **반성(Reflection)**: 과거 경험 요약→추상 인사이트 생성  

### 2.1.3 플래닝 모듈(Planning)  
LLM에게 “계획 세우기→실행→검증” 사이클을 부여

1) **피드백 없는 계획(Planning w/o Feedback)**  
   - **단일 경로 추론(Single-path)**  
     - Chain-of-Thought: 예시 단계 삽입→한 번에 전체 계획 생성  
     - Zero-shot CoT: “한 단계씩 생각” 트리거 프롬프트  
   - **다중 경로 추론(Multi-path)**  
     - Self-Consistent CoT: 여러 경로 생성→빈도 최대 답 선택  
     - Tree of Thoughts: 트리 탐색(BFS/DFS)으로 단계별 분기  
   - **외부 플래너(External Planner)**  
     - PDDL 변환 후 전통적 플래너 호출  

2) **피드백 기반 계획(Planning w/ Feedback)**  
   - **환경 피드백**: 관측·오류 메시지로 계획 수정 (ReAct, Voyager)  
   - **인간 피드백**: 사용자 질의·평가 반영 (Inner Monologue)  
   - **모델 피드백**: LLM 스스로 오류 검출·자기 수정(Self-Refine)  

### 2.1.4 액션 모듈(Action)  
계획을 실제 행동으로 전환

1) **행동 목표(Action Goal)**  
   - 과제 수행, 커뮤니케이션, 환경 탐색 등 다양

2) **행동 생성(Action Production)**  
   - **메모리 회상 기반**: 과거 성공 경험→현재 행동 예시  
   - **계획 준수 기반**: 사전 생성된 계획 순차 실행  

3) **행동 공간(Action Space)**  
   - **외부 도구**: API 호출, 데이터베이스 조회, 전문 모델 이용  
   - **내부 지식**: LLM의 계획·대화·상식 능력 활용  

4) **행동 영향(Action Impact)**  
   - 환경 상태 변경, 내부 메모리·계획 업데이트, 후속 행동 유발  

***

## 2.2 에이전트 역량 획득 전략  
하드웨어(아키텍처) 외에 ‘소프트웨어’로서의 경험·지식 축적 방안

### 2.2.1 파라미터 튜닝 기반(With Fine-tuning)  
- **사람 주석 데이터**: 인간 라벨링된 대화·행동 로그로 직접 파인튜닝  
- **LLM 생성 데이터**: ChatGPT 등으로 대규모 시뮬레이션→튜닝  
- **실세계 데이터**: 웹 탐색·SQL 로그 등 현실 샘플로 도메인 특화  

### 2.2.2 비튜닝 기법(Without Fine-tuning)  
1) **프롬프트 엔지니어링**  
   - CoT 예시 삽입, 심리 상태 묘사, 메모리·반성 프롬프트 강화  

2) **메커니즘 설계(Mechanism Engineering)**  
   - **시도-오류**: 행동 후 크리틱 피드백→재생산 루프  
   - **크라우드소싱**: 다수 에이전트 토론→합의 답안  
   - **경험 누적**: 성공 행동 기억→재사용  
   - **자가 주도 진화**: 에이전트 목표 설정→환경 탐색·보상 학습  

***

이상의 프로파일링→메모리→플래닝→액션 모듈 설계와, 파인튜닝·프롬프트·메커니즘 엔지니어링을 결합하면, LLM 기반 자율 에이전트가 복잡한 열린 환경에서도 인간과 유사한 계획·학습·실행 능력을 구현할 수 있습니다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7f48c2ba-afcc-4b31-b7c1-770c02684496/2308.11432v7.pdf
