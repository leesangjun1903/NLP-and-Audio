# Qwen3 Technical Report

**주요 주장**  
Qwen3는 **사고(Thinking) 모드**와 **비사고(Non-Thinking) 모드**를 통합한 대형 언어 모델(LLM) 시리즈로, 복합 추론과 빠른 응답을 하나의 프레임워크에서 동적으로 전환 가능하게 설계되었다. 또한, **사고 예산(Thinking Budget) 메커니즘**을 도입해 사용자가 추론에 할당할 계산 자원을 조절할 수 있으며, 이를 통해 대기 시간과 성능 간 균형을 최적화한다. Qwen2.5 대비 훈련 데이터 규모를 대폭 확장하고, **밀도(Dense) 모델과 전문가 혼합(MoE) 모델**을 모두 제공하여 파라미터 수 0.6B~235B 범위를 지원한다.[1]

**주요 기여**  
- **통합 모드 설계**: 별도 모델 전환 없이 사고 및 비사고 모드를 지원, 사용자 요구에 따른 유연한 동작 제공.[1]
- **사고 예산 제어**: 최대 컨텍스트 길이 및 예산 기반 추론 중단 기능으로 실시간 응용에 적합.[1]
- **데이터 및 다국어 확장**: 36조 토큰, 119개 언어·방언으로 전처리·합성 데이터 확충, 일반화 성능 강화.[1]
- **효율적 경량화**: 대형 모델 지식 전수를 통해 작은 모델도 높은 성능 확보, 계산 자원 절감.[1]
- **공개 접근성**: 모든 모델을 Apache 2.0 라이선스로 공개, 재현성 및 커뮤니티 연구 지원.[1]

# 문제 정의 및 제안 방법 상세 설명

## 해결하고자 하는 문제  
기존 LLM은 주로 추론 모델과 대화 모델이 분리되어 있거나, 단일 모드에서만 최적화되어 있어  
- 복합 추론 시 전용 모델로 전환해야 하는 불편  
- 단일 응답 지연과 성능 균형 제어 부족  
- 다국어 및 소규모 모델의 일반화 한계  

## 제안 방법  
Qwen3는 다음 요소로 구성된다:[1]

1. **이중 모드 통합**  
   - *사고 모드*와 *비사고 모드*를 하나의 아키텍처에 구현  
   - 사용자 또는 템플릿 플래그(`think`/`no_think`)로 모드 전환  

2. **사고 예산 메커니즘**  
   - 사용자 정의 토큰 예산에 도달하면 추론 중단 후 응답 생성  
   - 예: “Considering the limited time... think..” 지시어로 중단  

3. **사전 학습(Pre-training)**  
   - 36조 토큰(119개 언어)  
   - 3단계 전략: 일반→추론 강화→장문 컨텍스트(32,768 토큰)  
   - RoPE 주파수 확장, Dual Chunk Attention 등으로 긴 문맥 처리  

4. **사후 학습(Post-training)**  
   - 4단계: Long-CoT Cold Start → Reasoning RL → Thinking Mode Fusion → General RL  
   - Strong-to-Weak Distillation으로 경량 모델에 대형 모델 지식 전수  

5. **모델 구조**  
   - Dense: Qwen3-0.6B~32B (Grouped Query Attention, SwiGLU, QK-Norm 등)  
   - MoE: Qwen3-30B-A3B, Qwen3-235B-A22B (128 전문가, 활성화 전문가 수 8/22B)[1]

6. **수식**  
   - 일반 Transformer 스케일링 법칙 기반 학습률 및 배치 크기 예측  
   - 전역 배치 로드 밸런싱 손실(Global-Batch Load Balancing Loss)[1]

## 성능 향상  
- **기존 SOTA 모델 대비** Qwen3-235B-A22B는 대부분 벤치마크에서 최고·차선 성능 달성  
- **모델 규모 대비 효율성**: MoE 모델은 활성화 파라미터 수 15B만으로 Dense 모델과 동등 성능  
- **사고 예산 증대 시** 수학·코딩 과제에서 일관된 성능 향상 관찰  

## 한계  
- **다양한 일반 RL 훈련** 후 일부 복잡 과제(예: AIME, LiveCodeBench) 사고 모드 성능 소폭 감소  
- **RL과 Distillation** 비교 시, Distillation은 추론 잠재력 확장에 유리하나 RL은 특정 전문화 과제에 강점  

# 일반화 성능 향상 관점

Qwen3의 일반화 강화는 다음 요인에 기인한다:
- **인스턴스 수준 데이터 혼합**: 세밀한 데이터 레이블로 소형 프록시 모델 ablation 통해 최적 데이터 배합.[1]
- **강화된 다국어 데이터**: 29→119개 언어 확장으로 교차 언어 이해·생성 능력 증대  
- **MoE 전문가 세분화**: 전문가 간 분업을 통한 특화 표현 학습, 추론 다양성 향상  
- **사고 모드 예산 제어**: 불필요한 과도 추론 방지로 과적합 감소  

# 영향 및 향후 연구 고려사항

Qwen3는 **오픈 소스 LLM** 발전의 중요한 이정표로, 다중 모드·예산 제어를 통한 **실시간 응용** 및 **경량 모델 배포** 가능성을 확장한다.  
향후 연구에서는  
- **고품질·다양성 향상** 데이터 투자  
- **초장문 맥락 처리**와 모델 압축 연구  
- **Agent 기반 RL** 자원 확충 및 환경 상호작용 학습  
- **일반화 강화**를 위한 새로운 전문가 혼합·데이터 혼합 기법 개발  

이러한 방향은 LLM의 활용 범위를 의료, 교육, 산업 전반으로 확대하며, 효율성과 유연성을 동시에 제공할 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/9adde115-3fd2-4860-b1d8-304f80a3ce18/2505.09388v1.pdf)
