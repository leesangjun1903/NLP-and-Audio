# Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information

**핵심 메시지:** mRASP는 *다국어 신경 기계 번역*에서 **정렬 정보(Alignment Information)**를 활용한 *랜덤 정렬 치환(Random Aligned Substitution, RAS)* 기법으로 사전 학습(pre-training)된 범용 모델을 만들고, 이를 임의의 언어 쌍에 미세조정(fine-tuning)함으로써 저자원·중간자원·고자원·심지어 사전 학습에 전혀 포함되지 않은 이국어(exotic) 번역까지도 **일관된 성능 향상**을 실현한다.[1]

***

## 1. 핵심 주장 및 주요 기여  
1. **범용 다국어 MT 모델 제안 (mRASP):** 32개 언어(64개 방향)의 병렬말뭉치(197M 문장쌍)로 Transformer-large 아키텍처를 사전 학습하여, 어떠한 언어 쌍에도 파라미터 공유 및 동일한 학습 목표로 미세조정 가능함을 입증했다.[1]
2. **랜덤 정렬 치환(RAS) 기법 도입:** 소스 문장 내 단어를 *다른 언어*의 대응 단어로 무작위 치환함으로써 언어 간 의미 표현 간극을 좁히는 추가 손실을 학습 목표에 포함했다.[1]
3. **다양한 시나리오 전반에서 성능 향상 검증:**  
   - 극저자원(≤100K)부터 고자원(≥10M), 미세조정하지 않은 제로샷(Zero-Shot) 실험, 이국어 번역(Exotic Pair/Source/Target/Full)까지 모두 유의미한 BLEU 향상을 달성했다.[1]
4. **범용성 및 효율성:** mRASP는 기존의 MASS·mBART 등보다 적은 양의 병렬말뭉치(수백만 문장쌍)만으로도 *풍부한* 자원 언어에서도 성능 이득을 보였고, 백트랜슬레이션과도 상호보완적임을 보였다.[1]

***

## 2. 해결하려는 문제  
기존 MT 사전 학습의 한계점은 다음과 같다:[1]
- 사전 학습 목표와 번역 목표 간 불일치(Auto-encoding vs. Translation)  
- BERT류 모델의 *직접 미세조정* 어려움  
- 다국어 사전 학습 시 *저자원* 언어 증진에 집중하고, *고자원* 언어에서는 오히려 성능 저하를 보이는 경향  

mRASP는 **모든 언어 쌍**에 일관된 학습 목표를 적용하고, *정렬 정보를 통한 명시적 의미 격차 해소*로 이 문제를 극복하고자 한다.

***

## 3. 제안 방법  
### 3.1 모델 구조  
- **Transformer-large**  
  - Encoder/Decoder 각 6개 레이어, 모델 차원 1024, 헤드 수 16  
  - Feedforward 활성화 함수: GeLU, 위치 임베딩: 학습 가능(positional embeddings)  
- **언어 표시 토큰(Language Indicator):** 입력·출력에 `[src_lang]`·`[tgt_lang]` 토큰 추가.[1]

### 3.2 학습 목표 (수식)  
1. **기본 번역 손실:**  

$$
   \mathcal{L}_{pre}(\theta) = \sum_{(x_i,x_j)\in\mathcal{D}} -\log P_\theta(x_j\mid x_i)
   $$  
   
   여기서 $$\mathcal{D}$$는 다국어 병렬 말뭉치 집합, $$\theta$$는 모델 파라미터.[1]

2. **랜덤 정렬 치환(RAS) 손실:**  
   - 소스 문장 $$x_i$$의 단어 인덱스 $$t$$를 무작위로 선택, 사전(dictionary)을 통해 다른 언어 $$L_k$$의 대응 단어 $$d_{i,k}(x_i^t)$$로 치환  
   - 치환된 문장 쌍 $$(C(x_i),x_j)$$에도 동일한 번역 손실을 적용  

$$
   \mathcal{L}_{RAS}(\theta) = \sum_{(x_i,x_j)\in\mathcal{D}} -\log P_\theta\bigl(x_j\mid C(x_i)\bigr)
   $$  

3. **총 손실:**  

$$
   \mathcal{L}(\theta) = \mathcal{L}_{pre} + \mathcal{L}_{RAS}
   $$  

### 3.3 사전 학습 세부 사항  
- 병렬 데이터: TED, WMT, Europarl, ParaCrawl, OpenSubtitles, QED 등  
- 총 197M 문장쌍, 32개 언어·64방향  
- BPE 공유 어휘(32K merge ops), 어휘 크기 64,808  
- Adam 최적화(learning rate warm-up 4K steps, 총 150K steps)  
- RAS: 상위 1,000개 빈도 단어 중 30% 확률로 치환, 다의어(polysemy) 대응 위해 무작위 선택.[1]

***

## 4. 성능 향상 및 한계  
### 4.1 성능 향상  
- **극저자원(≤100K):** 최대 +22 BLEU 점수 향상  
- **저·중·고자원:** 모든 자원 규모에서 미세조정 대비 일관된 성능 상승  
- **제로샷(미세조정 없음):** 사전 학습만으로도 의미 있는 번역 가능, 특히 저자원에서 랜덤 초기화 대비 우위  
- **이국어(exotic) 번역:** 사전 학습에 전혀 포함되지 않은 언어 쌍에서도 +3.3~+14.1 BLEU 개선  
- **비교 실험:** MASS, mBART, XLM 등 주요 기법 대비 동등 내지 우수 성능.[1]

### 4.2 한계  
- **사전 학습 자원 의존:** 여전히 수백만 문장쌍 필요  
- **정렬 사전 품질 의존:** MUSE 등 사전 품질이 낮으면 효과 감소 가능  
- **계산 비용:** Transformer-large 기반 사전 학습의 높은 GPU·시간 요구량  

***

## 5. 일반화 성능 향상 관점  
- **RAS 적용 효과:** 서로 **유사한 언어(En-De)** 및 **비유사 언어(En-Zh)** 모두에서 단어 임베딩 간 **코사인 유사도** 증가 확인, 의미 공간 격차 축소.[1]
- **데이터 규모 민감도:** 미세조정을 위한 병렬쌍이 1K개 수준에서도 24 BLEU 달성, 랜덤 초기화 대비 압도적 우위.[1]
- **미세조정 vs. 사전 학습 기여:** 사전 학습만으로 상당 성능 확보, 미세조정은 추가 이득 제공하며 주로 언어별 임베딩 세부 조정에 기여하는 것으로 분석됨.[1]

***

## 6. 향후 연구 영향 및 고려 사항  
mRASP는 **다국어 MT 사전 학습** 분야에 다음과 같은 시사점을 제공한다.  
- **범용 사전 학습 모델 발전:** NLP의 BERT·GPT·BART 패러다임에 상응하는 MT 특화 모델 가능성 제시  
- **정렬 정보 활용 확대:** RAS 외에도 *구조적 정렬*, *문장 차원 정렬* 등 다양한 정렬 정규화 기법 탐색 필요  
- **저자원·이국어 번역 지원:** 소규모·신생 언어 자원 부족 문제에 효과적  
- **효율적 모델 경량화:** 대규모 프리트레인 계산비용 절감 위해 지식 증류·프루닝 연구와 결합 고려  

향후 연구에서는 **더 큰 규모의 언어 집합**, **고품질 사전 정렬 자원**, **경량화된 Transformer 아키텍처**를 결합하여 mRASP 기반 MT의 *표준 프레임워크*로 발전시킬 수 있을 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/d1c71b48-cddd-4d4a-9cbe-a4ad90851300/2010.03142v3.pdf)
