# Galactica: A Large Language Model for Science

**주요 주장 및 기여 요약**  
Meta AI가 제안한 Galactica는 **과학 지식 전용으로 엄선된 대규모 코퍼스**(논문, 교과서, 단백질·화합물 시퀀스, 학술 웹사이트 등 1060억 토큰)를 활용해 사전학습된 120억~1200억 매개변수 규모의 디코더형 트랜스포머 모델이다. Galactica는 일반 LLM 대비 다음과 같은 성과를 달성했다.  
- LaTeX 수식 재생성 정확도 68.2%로 GPT-3의 49.0% 대비 40% 상대 개선  
- MMLU 수학 영역(chain-of-thought)에서 41.3%로 Chinchilla(35.7%) 대비 우수  
- MATH 벤치마크(5-shot chain-of-thought)에서 20.4%로 PaLM 540B(8.8%) 대비 2.3배 달성  
- PubMedQA(77.6%)·MedMCQA dev(52.9%) 등 과학 QA에서 최첨단 성능 경신  

***

## 1. 해결하려는 문제  
- **정보 과부하**: 매일 수백 편의 논문, 수조 개 생물·화학 시퀀스 데이터가 축적되어 연구자가 개별 자료를 탐색·조합해 통찰을 얻기 어려움  
- **검색 기반 한계**: 검색 엔진은 키워드 매칭만 수행, 문헌 간 연결·추론·종합 기능 부족  
- **모델의 전문성**: 범용 LLM은 잡음 많은 웹 크롤 메타데이터에 학습되어 과학 지식에 특화된 성능 부족  

***

## 2. 제안 방법  
Galactica는 대규모 **정제 과학 코퍼스**와 **프롬프트 사전학습** 기법을 결합해 과학 지식 저장·결합·추론 능력을 강화함.

### 2.1 데이터셋 구성  
- 48M 논문(full-text+초록), 8M 교과서·강의노트·백과, 2M 화합물·단백질 시퀀스, 1M 고품질 크롤링  
- 시퀀스(단백질·DNA·SMILES)→특수 토큰 래핑, LaTeX·계산 스크립트(✏️〈work〉…〈/work〉) 삽입

### 2.2 모델 구조  
- **Decoder-only Transformer** (12→96 layer, d_model 768→10240)  
- 컨텍스트 윈도우 2048, 50K BPE 어휘, GeLU 활성화  
- 사전학습에 4.25 epochs(≈4500억 토큰) 반복 사용  

### 2.3 사전학습 목표  
- **언어모델 손실**: 토큰 예측  
- **프롬프트 사전학습**: QA·요약·개체추출·추론 데이터 삽입(358M 토큰)로 zero-shot 성능 강화  
- **〈work〉 토큰**: 체인-오브-생각 유도 및 필요 시 코드 오프로딩(예: 계산 스크립트 실행)

***

## 3. 성능 개선 및 한계  

### 3.1 성능 개선  
- **지식 프로브**: LaTeX 수식(68.2%), 화학반응(43.1%), 단백질·은하수·광물 분류 등에서 일반 LLM 대비 우수  
- **추론**: MMLU·MATH 수학 영역에서 Chinchilla·PaLM 대비 30B 모델이 18× 작은 규모로 동급 성능 달성  
- **QA**: PubMedQA(77.6%)·MedMCQA dev(52.9%)·BioASQ·MedQA-USMLE에서 최첨단 성능  
- **Citation**: 문헌 인용 예측 정확도 최대 51.9%로 sparse/dense 리트리벌 초과  
- **멀티모달**: SMILES→IUPAC 명명 정확도 39.2%, MoleculeNet 분류·단백질 서열 기반 기능 키워드·설명 생성에서 스케일업에 따라 향상  

### 3.2 한계  
- **코퍼스 범위**: 비(非)오픈 액세스 논문·교재 미포함, 단백질·화합물 시퀀스도 축소 샘플  
- **조정 필요**: 데이터 누수 최소화·사전학습 프롬프트 편향·인용 분포 왜곡 보정  
- **구조적 지식 부족**: 화학 3D 구조·단백질 3D 접힘 정보 미반영  
- **추론경로 투명성**: 〈work〉 토큰에서 과다한 코드 생성 오류 위험  

***

## 4. 일반화 성능 향상 가능성  
- **프롬프트 사전학습 효과**: zero-shot·few-shot 전 영역 성능 개선→유도 메시지 의존도↓  
- **반복 학습 토큰의 가치**: 4회 반복 사전학습 후에도 오버핏 적음→고품질 데이터로 일반화 강화  
- **모듈화 추론**: 〈work〉 토큰+코드 오프로딩 전략 확장 시 거대 모델 한계 보완 가능  
- **추가 모달리티**: 이미지, 3D 구조, 그래프 정보 통합 시 과학 문헌·실험 데이터 일반화↑  
- **지속 학습**: 신규 과학 지식겸 반영을 위한 점진적·메모리 기반 업데이트  

***

## 5. 향후 연구에의 영향 및 고려사항  
- **과학 지식 인터페이스**: Galactica는 연구자와 과학 데이터베이스 간 교량 역할, 자동 문헌 요약·연구 기획 지원 도구로 확장 전망  
- **하이브리드 검색·생성 시스템**: LLM 내장 지식+외부 리트리벌 결합해 정확성·신뢰성 강화  
- **안정성·검증 메커니즘**: 팩트 체크·추론 검증·인용 분포 보정 연구 필요  
- **윤리·편향**: 과학 분야 편향·오독 위험 관리, 투명성·책임소재 명확화  
- **오픈 사이언스 기여**: 모델·데이터·코드 공개를 통한 커뮤니티 협력·재현성 향상  

Galactica는 “과학 특화 LLM은 범용 LLM을 대체”라는 비전을 제시하며, 정제된 과학 데이터와 추론 인터페이스를 통해 연구 생산성을 획기적으로 높일 수 있음을 입증했다. 앞으로 다중 모달리티 통합, 추론 검증, 지속 업데이트 방안을 중심으로 연구를 확장해야 한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d7b65c79-3c31-4878-98da-be4e3d1f6394/2211.09085v1.pdf)
