Introduction
1. Foundation Model Overview  
2.1. Language Foundation Models  
 2.1.1. Model Architectures  
 2.1.2. Representative Models and Downstream Tasks  
 2.1.3. Cost Analysis  
2.2. Vision Foundation Models
 2.2.1. Model Architecture
 2.2.2. Representative Models and Downstream Tasks
 2.2.3. Cost Analysis
2.3. Multimodal Foundation Models
 2.3.1. Key Architectures
 2.3.2. Representative Models and Downstream Tasks
 2.3.3. Cost Analysis
Resouce-Efficient Architectures
3.1. Efficient Attention
 3.1.1. Sparse Attention
 3.1.2. Approximate Attention
 3.1.3. Attention-Free Approaches
3.2. Dynamic Neural Network
 3.2.1. Mixture of Experts
 3.2.2. Early Exiting
3.3. Diffusion-specific Optimization
 3.3.1. Efficient Sampling
 3.3.2. Diffusion in Latent Space
 3.3.3. Diffusion Architecture Variants
3.4. ViT-specific Optimizations
Resouce-Efficient Algorithms
4.1. Pre-training Algorithms
 4.1.1. Training Data Reduction
 4.1.2. Neural Architecture Search
 4.1.3. Progressive Learning
 4.1.4. Mixed Precision Training
4.2. Finetuning Algorithms
 4.2.1. Additive Tuning
 4.2.2. Selective Tuning
 4.2.3. Re-parameter Tuning
4.3. Inference Algorithms
 4.3.1. Opportunistic Decoding
 4.3.2. Input Filtering and Compression
 4.3.3. Key-Value Cache
 4.3.4. Long Context
4.4. Model Compression
 4.4.1. Pruning
 4.4.2. Knowledge Distillation
 4.4.3. Quantization
 4.4.4. Low-Rank Decomposition
Resouce-Efficient Systems
5.1. Distributed Training
5.2. Federated Learning
 5.2.1. Frameworks & Benchmarks
 5.2.2. PEFT-based Approaches
 5.2.3. Model Decomposition
 5.2.4. Backprop-free Approaches
5.3. Serving on Cloud
 5.3.1. Inference Accelerating
 5.3.2. Memory Saving
 5.3.3. Emerging Platforms
5.4. Serving on Edge
Conclusion and Future Directions
