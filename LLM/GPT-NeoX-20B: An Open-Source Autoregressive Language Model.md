# GPT-NeoX-20B: An Open-Source Autoregressive Language Model

**핵심 주장**  
GPT-NeoX-20B는 20억 매개변수 규모의 순차 생성 언어 모델로, 완전 공개된 가중치와 학습·평가 코드를 제공하여 누구나 대형 언어 모델 연구와 안전·해석·스케일링 법칙 분석에 활용할 수 있음을 입증한다.  

**주요 기여**  
1. 20B 파라미터의 개방형 공개: 현재까지 공개된 밀집(​dense) 오토리그레시브 모델 중 최대 규모이며, 학계·산업계 모두 가중치를 자유롭게 활용 가능하도록 라이선스를 부여.  
2. 아키텍처·토크나이저 개선  
   - Rotary 위치 인베딩(RoPE) 적용(전체 차원의 25%): 상대 위치 정보를 정적 회전 행렬로 처리.  
   - 어텐션과 FF 레이어의 병렬 연산: 잔차 결합 전 합산 형태로 15% 처리량 향상.  
   - BPE 기반 새 토크나이저: 공백 처리 일관화, 최대 24칸 연속 공백 토큰화 도입으로 코드·문서 장황 부분 압축.  
3. 대규모 학습 동적 체크포인트 공개: 1,000 스텝 간격의 중간 가중치도 모두 배포, 학습 과정·하이퍼파라미터 최적화·훈련 동역학 연구 촉진.  
4. 성능 및 일반화 실험  
   - 언어 이해·지식·수리 과제 32종 중 22종에서 FairSeq · GPT-3 동급 모델 대비 우수한 제로·파이브샷 성능 달성.  
   - 복합수학(MATH) 및 기본 산술 과제에서 Pile 데이터 기반 수식 노출 덕분에 부가적 이점 확인.  
   - Few-shot 학습에서 GPT-3 대비 3–10배 큰 성능 향상 폭 관찰, “파인튜닝 없이 일반화” 능력 대폭 개선.  

# 문제 정의·제안 방법·모델 구조·성능·한계

## 1. 해결 과제  
기존 10B~175B급 언어 모델들은 기업 내부 API로만 제공되어 연구자가 모델 파라미터·학습 데이터·토크나이저·훈련 동역학을 상세히 분석하기 어려웠다.  
→연구·안전·해석 커뮤니티 접근성 부재.

## 2. 제안 방법  
– 아키텍처  
  -  기반: GPT-3 유사 트랜스포머 디코더 구조  
  -  **Rotary Positional Embedding**:  

$$
      \text{RoPE}(x_{m}, x_{n}) = x_{m}^\top W_q R_{\Theta,(n-m)} W_k x_n
    $$  
    
  \- 상대 위치 $$n-m$$에 비례해 회전 행렬 $$R_{\Theta,(n-m)}$$ 적용  
  
  -  **병렬화된 어텐션+피드포워드**:  

$$
      y = x + \text{Attn}(\mathrm{LN}_1(x)) + \mathrm{FF}(\mathrm{LN}_2(x))
    $$  
    
  두 서브블록을 독립 레이어 노름 뒤 병렬 연산하여 통신 오버헤드 감소  
  
  -  **초기화**:  

$$\mathrm{FF}$$ 출력 가중치 스케일링 $$2/\sqrt{d}$$, 그 외 소규모 초기화  

– 토크나이저  
  -  BPE 50,257 토큰, 25% 공백 기반 접두어 처리 일관화  
  -  최대 24칸 반복 공백 토큰 포함 → 코드·LaTeX 대용량 공백 압축  

– 학습  
  -  데이터: 22개 소스 825GiB ‘The Pile’  
  -  배치: 3.15M 토큰, 2048 길이 컨텍스트 ×1538 문맥  
  -  옵티마이저: AdamW + ZeRO Stage 1, ZeRO-Offload 없이  
  -  병렬화: 텐서 병렬 2, 파이프라인 병렬 4  
  -  학습률: $$9.7\times10^{-5}$$로 코사인 스케줄, 150K 스텝  

## 3. 성능 향상  
– **제로 샷**: FairSeq 13B 대비 22/32 과제 우수, 평균 3–5%p 높은 정확도  
– **파이브 샷**: GPT-J-6B는 +5.26%p, GPT-NeoX-20B는 +5.98%p 향상, FairSeq 6.7B·13B는 각각 +0.51·+1.83%p[표 5]  
– **수학 과제**: Pile 내 수식 빈도와 상관성 강함, 기본 산술에서 GPT-3·FairSeq 대비 3–4배 높은 정답률  
– **일반화**: Few-shot에서 비약적 성능 상승 통해 고차원 문맥 일반화 가능성 시사  

## 4. 한계  
1. 하이퍼파라미터 최적화 미완: 13B→175B 보간 기반, 최적값 아님.  
2. Deduplication 미적용: 2회 에포크 데이터 중복에도 검증 손실 감소 지속, 하지만 실제 배포 모델에선 데이터 누출 위험 잔존.  
3. 코드 과제 평가는 재정 제약으로 미실시.  
4. 거대한 모델 추론·파인튜닝 비용: A6000·RTX3090Ti 이상 하드웨어 필요.  

# 일반화 성능 개선 관점  
-  Pile의 다양한 도메인(코드·대화·학술·웹문서) 덕분에 문맥·양식 변화에 강건한 일반화 능력 확보  
-  RoPE 기반 상대 위치 임베딩과 병렬 레이어 구성으로 장문·다양한 프롬프트 패턴 학습 유연성 증대  
-  Few-shot 시 모델이 제시된 예제 문맥을 더 효과적으로 활용하여 즉시 적응하는 능력 크게 향상  

# 향후 연구 영향 및 고려 사항

– **투명성·안전 연구 활성화**: 공개 가중치로 메커니즘 해석·탈취 공격·보상 모델 연구 가능.  
– **리소스 접근성**: 고성능 하드웨어가 필요하므로, 경량화·지연 저감·파라미터 효율화 연구 강화 권장.  
– **데이터 품질**: Deduplication·편향 완화·데이터 누출 방지 위한 전처리·후처리 전략 필수.  
– **토크나이저 일반화**: 멀티링구얼·바이트 수준 토크나이저 연구하여 비영어·비라틴문자 일반화 확대.  

GPT-NeoX-20B는 **공개형 대형 언어 모델**로서 연구 커뮤니티에 큰 전환점을 제공하며, 향후 안전·해석·효율화 연구의 토대로 활용될 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7abc59a1-e163-4496-8a8b-d09d34761a6d/2204.06745v1.pdf)
