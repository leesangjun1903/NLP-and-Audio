> ## Building Large Language Models and Lightweight Language Models, Language Models Algorithm.

# Lecture
Deep Learning Bible - 8. Large Language Models : https://wikidocs.net/book/14965

# PaLM + RLHF - Pytorch
https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main?tab=readme-ov-file

## LangChain
- 랭체인(LangChain) 입문부터 응용까지 : https://wikidocs.net/book/14473  
- <랭체인LangChain 노트> - LangChain 한국어 튜토리얼 :  https://wikidocs.net/book/14314  , https://github.com/teddylee777/langchain-kr?tab=readme-ov-file\
- RAG From Scratch : https://github.com/langchain-ai/rag-from-scratch

## papers
### Main Architecture : Transformers
- Transformer : Attention Is All You Need
: https://github.com/huggingface/transformers/tree/main

- (Survey) A Survey of Resource-efficient LLM and Multimodal Foundation Models :  https://wikidocs.net/237419
- (Survey) Parameter-Efficient Fine-Tuning for Large Models : A Comprehensive Survey
- (Survey) Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment
- (Survey) A Survey of Large Language Models : https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file#table-of-contents , https://wikidocs.net/237619, https://arxiv.org/abs/2303.18223
- Large Language Models: A Survey(2024) : https://velog.io/@sohtks/Paper-Review-Large-Language-Models-A-Survey, https://arxiv.org/abs/2402.06196
- (Survey) A Survey on Large Language Model based Autonomous Agents

- AlphaGeometry : Solving Olympiad Geometry without Human Demonstrations
- AlphaGeometry2 : Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2
- LoRA: Low-Rank Adaptation of Large Language Models | LLM, Fine-tuning, Mathematical Reasoning
- LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models | Instruction Following, Fine-tuning, Question answering
- iVQA : Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis Tool | Question answering, Reinforcement Learning
- GPT-1 : Improving Language Understanding by Generative Pre-Training
- GPT-2: Language Models are Unsupervised Multitask Learners
- GPT-3 : Language Models are Few-Shot Learners
- GPT-4 Technical Report
- DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
- PAS: Data-Efficient Plug-and-Play Prompt Augmentation System
- DeepSeek-V3 Technical Report
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
- Transformer-Squared: Self-Adaptive LLMs
- Titans: Learning to Memorize at Test Time
- PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels
- Parallelizing Linear Transformers with the Delta Rule over Sequence Length
- DFloat11 : 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float
- Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval | Quety classification
- Mitigating Language-Dependent Ethnic Bias in BERT
- Learning to Summarize from Human Feedback
- PaLM: Scaling Language Modeling with Pathways
- A Length-Extrapolatable Transformer
- Understanding R1-Zero-Like Training: A Critical Perspective
- Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
- RoBERTa: A Robustly Optimized BERT Pretraining Approach
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer
