> ## Building Large Language Models and Lightweight Language Models, Language Models Algorithms, Metrics, RNN, LSTM Language Models. 

# Lecture
- Deep Learning Bible - 8. Large Language Models : https://wikidocs.net/book/14965
- PaLM + RLHF - Pytorch : https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main?tab=readme-ov-file
- 랭체인(LangChain) 입문부터 응용까지 : https://wikidocs.net/book/14473  
- <랭체인LangChain 노트> - LangChain 한국어 튜토리얼 :  https://wikidocs.net/book/14314  , https://github.com/teddylee777/langchain-kr?tab=readme-ov-file\
- RAG From Scratch : https://github.com/langchain-ai/rag-from-scratch

## papers
### Main Architecture : Transformers
- Transformer : Attention Is All You Need | 2017 · 195827회 인용
: https://github.com/huggingface/transformers/tree/main

- (Survey) A Survey of Resource-efficient LLM and Multimodal Foundation Models :  https://wikidocs.net/237419
- (Survey) Parameter-Efficient Fine-Tuning for Large Models : A Comprehensive Survey
- (Survey) Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment
- (Survey) A Survey of Large Language Models : https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file#table-of-contents , https://wikidocs.net/237619, https://arxiv.org/abs/2303.18223
- Large Language Models: A Survey(2024) : https://velog.io/@sohtks/Paper-Review-Large-Language-Models-A-Survey, https://arxiv.org/abs/2402.06196
- (Survey) A Survey on Large Language Model based Autonomous Agents
- (Survey) Towards Reasoning in Large Language Models: A Survey
- (Survey) Reasoning with Language Model Prompting: A Survey

- AlphaGeometry : Solving Olympiad Geometry without Human Demonstrations | 2024 · 662회 인용
- AlphaGeometry2 : Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2 | 2025 · 35회 인용
- LoRA: Low-Rank Adaptation of Large Language Models | 2021 · 18787회 인용, LLM, Fine-tuning, Mathematical Reasoning
- LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models | 2023 · 473회 인용, Instruction Following, Fine-tuning, Question answering
- iVQA : Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis Tool | 2018 · 30회 인용, Question answering, Reinforcement Learning
- GPT-1 : Improving Language Understanding by Generative Pre-Training | 2018 . 15874회 인용
- GPT-2: Language Models are Unsupervised Multitask Learners | 2019 . 18593회 인용
- GPT-3 : Language Models are Few-Shot Learners | 2020 · 54787회 인용
- GPT-4 Technical Report | 2023 · 18158회 인용
- DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models | 2024 · 1815회 인용
- PAS: Data-Efficient Plug-and-Play Prompt Augmentation System | 2024 · 5회 인용
- DeepSeek-V3 Technical Report | 2024 · 1896회 인용 
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning | 2025 · 3869회 인용
- Transformer-Squared: Self-Adaptive LLMs | 2025 · 13회 인용
- Titans: Learning to Memorize at Test Time | 2024 · 85회 인용
- PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels | 2023 · 13회 인용
- DeltaNet : Parallelizing Linear Transformers with the Delta Rule over Sequence Length | 2024 · 111회 인용
- DFloat11 : 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float | 2025 · 6회 인용
- Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval | 2015 · 521회 인용, Quety classification
- Ethnic Bias in BERT : Mitigating Language-Dependent Ethnic Bias in BERT | 2021 · 151회 인용
- Summarize from Human Feedback : Learning to Summarize from Human Feedback | 2020 · 2737회 인용
- PaLM: Scaling Language Modeling with Pathways | 2022 · 7437회 인용
- LEX : A Length-Extrapolatable Transformer | 2022 · 198회 인용
- Understanding R1-Zero : Understanding R1-Zero-Like Training: A Critical Perspective | 2025 · 246회 인용
- Limet of Reinforcement Learning with Verifiable Rewards (RLVR) : Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? | 2025 · 188회 인용
- RoBERTa: A Robustly Optimized BERT Pretraining Approach | 2019 · 22034회 인용
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter | 2019 · 10092회 인용
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | 2019 · 26734회 인용
- mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer | 2020 · 3035회 인용
- PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation | 2021 · 295회 인용
- CPM-2: Large-scale Cost-effective Pre-trained Language Models | 2021 · 102회 인용
- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | 2019 · 14161회 인용
- RoFormer: Enhanced Transformer with Rotary Position Embedding | 2021 · 3504회 인용
- DPO : Direct Preference Optimization: Your Language Model is Secretly a Reward Model | 2023 · 5298회 인용
- KTO: Model Alignment as Prospect Theoretic Optimization | 2024 · 628회 인용
- Noise Contrastive Alignment of Language Models with Explicit Rewards | 2024 · 58회 인용
- SpinQuant: LLM quantization with learned rotations | 2024 · 181회 인용
- CBQ: Cross-Block Quantization for Large Language Models | 2023 · 22회 인용
- LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid | 2024 · 6회 인용
- KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization | 2024 · 268회 인용
- QBB: Quantization with Binary Bases for LLMs | 2024 · 3회 인용
- ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification | 2024 · 38회 인용
- DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs | 2024 · 67회 인용
- KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization | 2024 · 43회 인용
- qllm_eval : Evaluating Quantized Large Language Models | 2024 · 107회 인용
- SqueezeLLM: Dense-and-Sparse Quantization | 2023 · 289회 인용
- KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache | 2024 · 265회 인용
- AQLM : Extreme Compression of Large Language Models via Additive Quantization | 2024 · 125회 인용
- BiLLM: Pushing the Limit of Post-Training Quantization for LLMs | 2024 · 141회 인용
- OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models | 2023 · 365회 인용
- LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models | 2023 · 238회 인용
- SpQR: Near-Lossless LLM Weight Compression | 2023 · 314회 인용
- QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models | 2023 · 231회 인용
- LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models | 2022 · 171회 인용
- IR-QLoRA : Accurate LoRA-Finetuning Quantization of LLMs via Information Retention | 2024 · 117회 인용
- QuIP: 2-Bit Quantization of Large Language Models With Guarantees | 2023 · 276회 인용
- PEQA : Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization | 2023 · 146회 인용
- Atom: Low-bit Quantization for Efficient and Accurate LLM Serving | 2023 · 207회 인용
- AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration | 2023 · 1334회 인용
- LLM-QAT: Data-Free Quantization Aware Training for Large Language Models | 2023 · 394회 인용
- Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling | 2023 · 165회 인용
- RPTQ: Reorder-based Post-training Quantization for Large Language Models | 2023 · 142회 인용
- The Case for 4-Bit Precision: k-Bit Inference Scaling Laws | 2022 · 275회 인용
- SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models | 2022 · 1409회 인용
- GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers | 2022 · 1720회 인용
- BiBERT: Accurate Fully Binarized BERT | 2022 · 152회 인용
- Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models | 2022 · 204회 인용
- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale | 2022 · 1485회 인용
- ZeroQuant: Eﬃcient and Aﬀordable Post-Training Quantization for Large-Scale Transformers | 2022 · 589회 인용
- Compression of Generative Pre-trained Language Models via Quantization | 2022 · 109회 인용
- BinaryBERT: Pushing the Limit of BERT Quantization | 2020 · 289회 인용
- Transformer Quantization : Understanding and Overcoming the Challenges of Efficient Transformer Quantization | 2021 · 191회 인용
- TernaryBERT: Distillation-aware Ultra-low Bit BERT | 2020 · 244회 인용
- GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference | 2020 · 253회 인용
- Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT | 2019 · 700회 인용
- Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model | 2019 · 165회 인용
- Q8BERT: Quantized 8Bit BERT | 2019 · 660회 인용
- T-Few : Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning | 2022 · 1207회 인용
- ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | 2019 · 9440회 인용
- UNILM: Unified Language Model Pre-training for Natural Language Understanding and Generation | 2019 · 2049회 인용
- ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | 2020 · 5240회 인용
- GLM-130B: An Open Bilingual Pre-trained Model | 2022 · 852회 인용
- ST-MoE: Designing Stable and Transferable Sparse Expert Models | 2022 · 293회 인용
- OPT: Open Pre-trained Transformer Language Models | 2022 · 3784회 인용
- BLOOM: A 176B-Parameter Open-Access Multilingual Language Model | 2022 · 2132회 인용
- GLaM: Efficient Scaling of Language Models with Mixture-of-Experts | 2021 · 922회 인용
- Megatron-DeepSpeed : Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model | 2022 · 779회 인용 : https://github.com/deepspeedai/Megatron-DeepSpeed
- Scaling Language Models: Methods, Analysis & Insights from Training Gopher | 2021 · 1422회 인용
- Chinchilla : Training Compute-Optimal Large Language Models | 2022 · 2804회 인용
- LaMDA: Language Models for Dialog Applications | 2022 · 1979회 인용
- LLaMA: Open and Efficient Foundation Language Models | 2023 · 19904회 인용
- BloombergGPT: A Large Language Model for Finance | 2023 · 1335회 인용
- GPT-NeoX-20B: An Open-Source Autoregressive Language Model | 2022 · 1026회 인용
- PaLM 2 Technical Report | 2023 · 2059회 인용
- Llama 2: Open Foundation and Fine-Tuned Chat Models | 2023 · 18311회 인용
- Model Card and Evaluations for Claude Models
- Is ChatGPT a General-Purpose Natural Language Processing Task Solver? | 2023 · 917회 인용
- Benchmarking Large Language Models for News Summarization | 2023 · 707회 인용
- News Summarization and Evaluation in the Era of GPT-3 | 2022 · 508회 인용
- Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine | 2023 · 433회 인용
- Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT | 2023 · 398회 인용
- Atlas: Few-shot Learning with Retrieval Augmented Language Models | 2022 · 594회 인용
- Solving math word problems with process- and outcome-based feedback | 2022 · 416회 인용
- Large Language Models Encode Clinical Knowledge | 2023 · 3140회 인용
- Scaling Laws for Neural Language Models | 2020 · 4656회 인용
- Emergent Abilities of Large Language Models | 2022 · 4005회 인용
- Is GPT-3 a Good Data Annotator? | 2022 · 329회 인용
- Want To Reduce Labeling Cost? GPT-3 Can Help | 2021 · 344회 인용
- GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation | 2021 · 301회 인용
- ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks | 2023 · 1451회 인용
- G-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment | 2023 · 1658회 인용
- GPTScore: Evaluate as You Desire | 2023 · 705회 인용
- Is ChatGPT a Good NLG Evaluator? A Preliminary Study | 2023 · 492회 인용
- Prefix-Tuning: Optimizing Continuous Prompts for Generation | 2021 · 5539회 인용
- P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks | 2021 · 1786회 인용
- ZeRO: Memory Optimizations Toward Training Trillion-Parameter Models | 2019 · 1906회 인용
- Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | 2019 · 2609회 인용
- Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM | 2021 · 1099회 인용
- Reducing Activation Recomputation in Large Transformer Models | 2022 · 368회 인용
- Calibrate Before Use: Improving Few-Shot Performance of Language Models | 2021 · 1708회 인용
- Do Prompt-Based Models Really Understand the Meaning of Their Prompts? | 2021 · 459회 인용
- GPT-4o System Card | 2024 · 2177회 인용
- Finetuned Language Models Are Zero-Shot Learners | 2021 · 4689회 인용
- Cross-Task Generalization via Natural Language Crowdsourcing Instructions | 2021 · 830회 인용
- Scaling Instruction-Finetuned Language Models | 2022 · 4723회 인용
- OPT-IML : Scaling Language Model Instruction Meta Learning through the Lens of Generalization | 2022 · 132회 인용
- A General Language Assistant as a Laboratory for Alignment | 2021 · 384회 인용
- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback | 2022 · 2808회 인용
- Training Language Models to Follow Instructions with Human Feedback | 2022 · 18040회 인용
- Red Teaming Language Models with Language Models | 2022 · 875회 인용
- Constitutional AI: Harmlessness from AI Feedback | 2022 · 2102회 인용
- SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions | 2022 · 2633회 인용
- CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis | 2022 · 1478회 인용
- SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks | 2022 · 738회 인용
- UL2: Unifying Language Learning Paradigms | 2022 · 413회 인용
- Galactica: A Large Language Model for Science | 2022 · 954회 인용
- CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X | 2023 · 577회 인용
- Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling | 2023 · 1450회 인용
- GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding | 2020 · 1593회 인용
- Evaluating Large Language Models Trained on Code | 2021 · 6313회 인용
- ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | 2021 · 685회 인용
- WebGPT: Browser-assisted Question-Answering with Human Feedback | 2021 · 1608회 인용
- Competition-Level Code Generation with AlphaCode | 2022 · 1705회 인용
- Improving alignment of dialogue agents via targeted human judgements | 2022 · 615회 인용
- Defending Against Neural Fake News | 2019 · 1421회 인용
- DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters | 2020 · 1743회 인용
- Deduplicating Training Data Makes Language Models Better | 2021 · 802회 인용
- Deduplicating Training Data Mitigates Privacy Risks in Language Models | 2022 · 363회 인용
- GLM: General Language Model Pretraining with Autoregressive Blank Infilling | 2021 · 1900회 인용
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Eﬃcient Sparsity | 2021 · 3020회 인용
- Long Range Language Modeling via Gated State Spaces | 2022 · 349회 인용
- Hungry Hungry Hippos: Towards Language Modeling with State Space Models | 2022 · 677회 인용
- Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation | 2021 · 929회 인용
- VLLM : Efficient Memory Management for Large Language Model Serving with PagedAttention | 2023 · 3203회 인용
- What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? | 2022 · 242회 인용
- Program Synthesis with Large Language Models | 2021 · 2311회 인용
- Show Your Work: Scratchpads for Intermediate Computation with Language Models | 2021 · 825회 인용
- A Systematic Evaluation of Large Language Models of Code | 2022 · 901회 인용
- InCoder: A Generative Model for Code Infilling and Synthesis | 2022 · 834회 인용
- CodeT: Code Generation with Generated Tests | 2022 · 443회 인용
- StarCoder: May the Source Be With You! | 2023 · 1244회 인용
- Language Models of Code are Few-Shot Commonsense Learners | 2022 · 239회 인용
- Multi-Task Deep Neural Networks for Natural Language Understanding | 2019 · 1602회 인용
- Muppet: Massive Multi-task Representations with Pre-Finetuning | 2021 · 318회 인용
- The Flan Collection: Designing Data and Methods for Eﬀective Instruction Tuning | 2023 · 932회 인용
- LIMA: Less Is More for Alignment | 2023 · 1494회 인용
- Fine-Tuning Language Models from Human Preferences | 2019 · 2243회 인용
- Teaching Language Models to Support Answers with Verified Quotes | 2022 · 290회 인용
- Parameter-Efficient Transfer Learning for NLP | 2019 · 6129회 인용
- MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer | 2020 · 714회 인용
- AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts | 2020 · 2331회 인용
- GPT Understands, Too | 2021 · 1594회 인용
- The Power of Scale for Parameter-Efficient Prompt Tuning | 2021 · 5252회 인용
- Parameter-efficient Fine-tuning of Large-scale Pre-trained Language Models | 2023 · 1136회 인용
- AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning | 2023 · 858회 인용
- LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-Initialized Attention | 2023 · 941회 인용
- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models | 2023 · 288회 인용
- An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels | 2022 · 209회 인용
- What Makes Good In-Context Examples for GPT-3? | 2021 · 1660회 인용
- Learning To Retrieve Prompts for In-Context Learning | 2021 · 798회 인용
- Active Example Selection for In-Context Learning | 2022 · 243회 인용
- Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity | 2021 · 1382회 인용
- An Explanation of In-context Learning as Implicit Bayesian Inference | 2021 · 948회 인용
- Data Distributional Properties Drive Emergent In-Context Learning in Transformers | 2022 · 411회 인용
- Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? | 2022 · 1733회 인용
- What learning algorithm is in-context learning? Investigations with linear models | 2022 · 628회 인용
- Larger Language Models Do In-Context Learning Differently | 2023 · 418회 인용
- Automatic Chain of Thought Prompting in Large Language Models | 2022 · 1343회 인용
- STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning | 2022 · 971회 인용
- Large Language Models are Zero-Shot Reasoners | 2022 · 6382회 인용
- Complexity-Based Prompting for Multi-Step Reasoning | 2022 · 509회 인용
- Language Models are Multilingual Chain-of-Thought Reasoners | 2022 · 450회 인용
- Least-to-Most Prompting Enables Complex Reasoning in Large Language Models | 2022 · 1848회 인용
- Multimodal Chain-of-Thought Reasoning in Language Models | 2023 · 710회 인용
- Self-Consistency Improves Chain of Thought Reasoning in Language Models | 2022 · 2844회 인용
- Large Language Models Can Self-Improve | 2022 · 716회 인용
- Training Verifiers to Solve Math Word Problems | 2021 · 5310회 인용
- Large Language Models are Better Reasoners with Self-Verification | 2022 · 272회 인용
- Teaching Small Language Models to Reason | 2022 · 333회 인용
- Large Language Models Are Reasoning Teachers | 2022 · 406회 인용
- Minerva: Solving Quantitative Reasoning Problems with Language Models | 2022 · 1085회 인용
- Challenging BIG-Bench tasks and whether chain-of-thought can solve them | 2022 · 1033회 인용
- Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models | 2023 · 343회 인용
- PROGPROMPT: Generating Situated Robot Task Plans using Large Language Models | 2022 · 1053회 인용
- VOYAGER: An Open-Ended Embodied Agent with Large Language Models | 2023 · 1326회 인용
- Reflexion: Language Agents with Verbal Reinforcement Learning | 2023 · 2263회 인용
- Decomposed Prompting: A Modular Approach for Solving Complex Tasks | 2022 · 601회 인용
- Toolformer: Language Models Can Teach Themselves to Use Tools | 2023 · 2325회 인용
- HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face | 2023 · 1492회 인용
- Faithful Chain-of-Thought Reasoning | 2023 · 358회 인용
- LLM+P: Empowering Large Language Models with Optimal Planning Proficiency | 2023 · 574회 인용
- ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | 2024 · 828회 인용
- OpenAssistant Conversations - Democratizing Large Language Model Alignment | 2023 · 716회 인용
- Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena | 2023 · 5464회 인용
- Qwen2.5 Technical Report | 2024 · 3175회 인용
- Improving Language Models by Retrieving from Trillions of Tokens | 2021 · 1583회 인용
- Mistral 7B | 2023 · 2772회 인용
- Regularizing and Optimizing LSTM Language Models | 2017 · 1455회 인용
- Factorization Tricks for LSTM Networks | 2017 · 181회 인용
- Understanding Back-Translation at Scale | 2018 · 1514회 인용
- Non-Autoregressive Neural Machine Translation | 2017 · 975회 인용
- Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets
- Natural Language Inference over Interaction Space | 2017 · 230회 인용
- Convolutional Sequence to Sequence Learning | 2017 · 4671회 인용
- Unsupervised Machine Translation Using Monolingual Corpora Only | 2017 · 1356회 인용
- Unsupervised Neural Machine Translation with Weight Sharing | 2018 · 153회 인용
- Language Models Are Realistic Tabular Data Generators | 2022 · 356회 인용
- KoBERT : Korean BERT pre-trained cased (KoBERT) : https://github.com/SKTBrain/KoBERT?tab=readme-ov-file#kobert
- KoBART : https://github.com/SKT-AI/KoBART
- CPM: A Large-scale Generative Chinese Pre-trained Language Model | 2020 · 139회 인용
- Multilingual Alignment of Contextual Word Representations | 2020 · 206회 인용
- TABERT: Pretraining for Joint Understanding of Textual and Tabular Data | 2020 · 759회 인용

## Agent
- Generative Agents: Interactive Simulacra of Human Behavior
- ReAct: Synergizing Reasoning and Acting in Language Models
- Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents

