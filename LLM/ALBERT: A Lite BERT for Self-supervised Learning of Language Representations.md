# ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

## 논문의 핵심 주장과 주요 기여

ALBERT는 BERT의 메모리 제약과 훈련 속도 문제를 해결하면서도 더 나은 성능을 달성하는 혁신적인 아키텍처를 제안합니다. 논문의 핵심 주장은 **모델 크기가 커질수록 성능이 향상되지만, 하드웨어 한계로 인한 확장성 문제를 해결해야 한다**는 것입니다.[1]

### 주요 기여점

1. **매개변수 효율성 극대화**: BERT-large 대비 18배 적은 매개변수로 더 나은 성능 달성[1]
2. **훈련 속도 개선**: ALBERT-large는 BERT-large 대비 약 1.7배 빠른 데이터 처리량[1]
3. **새로운 사전훈련 손실 함수**: 문장 순서 예측(SOP)을 통한 담화 일관성 학습[1]
4. **SOTA 성능 달성**: GLUE(89.4), RACE(89.4), SQuAD 2.0 F1(92.2)에서 최고 성능 기록[1]

## 해결하고자 하는 문제

### 핵심 문제점
- **메모리 한계**: 수억 개 매개변수를 가진 모델의 GPU/TPU 메모리 제약[1]
- **통신 오버헤드**: 분산 훈련 시 매개변수 수에 비례하는 통신 비용[1]
- **훈련 시간 증가**: 모델 크기 확장에 따른 훈련 속도 저하[1]
- **NSP의 비효율성**: BERT의 Next Sentence Prediction 손실의 한계[1]

## 제안하는 방법론

### 1. 인수분해 임베딩 매개변수화 (Factorized Embedding Parameterization)

**핵심 아이디어**: 어휘 임베딩 크기(E)와 은닉층 크기(H)를 분리[1]

**수학적 표현**:
- 기존 BERT: $$O(V \times H)$$ 매개변수
- ALBERT: $$O(V \times E + E \times H)$$ 매개변수 (단, $$H \gg E$$)[1]

**장점**:
- 모델링 관점: WordPiece 임베딩(문맥 독립적)과 은닉층 임베딩(문맥 의존적)의 역할 분리[1]
- 실용적 관점: $$H$$ 증가 시 임베딩 매트릭스 크기 급격한 증가 방지[1]

### 2. 교차층 매개변수 공유 (Cross-layer Parameter Sharing)

**전략**: 모든 Transformer 층에서 attention과 feed-forward 매개변수 공유[1]

**효과 분석**: 
- L2 거리와 코사인 유사도 측정 결과, ALBERT는 BERT보다 층간 전환이 더 부드러움[1]
- 매개변수 안정화 효과로 정규화 역할 수행[1]

### 3. 문장 순서 예측 (Sentence-Order Prediction, SOP)

**기존 NSP의 문제**: 주제 예측과 일관성 예측을 혼재하여 학습 효율성 저하[1]

**SOP 접근법**:
- 긍정 예시: 연속된 두 세그먼트
- 부정 예시: 순서를 바꾼 동일한 두 세그먼트[1]

**성능 비교**:
- NSP는 SOP 태스크에서 무작위 수준 성능(52.0%)[1]
- SOP는 NSP 태스크에서 합리적 성능(78.9%) 달성[1]

## 모델 구조

### ALBERT 구성 비교

| 모델 | 매개변수 | 층 수 | 은닉 크기 | 임베딩 크기 | 매개변수 공유 |
|------|----------|-------|-----------|-------------|---------------|
| BERT-base | 108M | 12 | 768 | 768 | False |
| BERT-large | 334M | 24 | 1024 | 1024 | False |
| ALBERT-base | 12M | 12 | 768 | 128 | True |
| ALBERT-large | 18M | 24 | 1024 | 128 | True |
| ALBERT-xlarge | 60M | 24 | 2048 | 128 | True |
| ALBERT-xxlarge | 235M | 12 | 4096 | 128 | True |

### 아키텍처 특징
- **백본**: Transformer 인코더 + GELU 활성화 함수[1]
- **피드포워드 크기**: 4H
- **어텐션 헤드 수**: H/64[1]
- **최적 임베딩 크기**: 실험 결과 E=128이 최적[1]

## 성능 향상 및 일반화 능력

### 정량적 성능 개선

**BERT-large 대비 ALBERT-xxlarge 성능 향상**:
- SQuAD v1.1: +1.9%
- SQuAD v2.0: +3.1%
- MNLI: +1.4%
- SST-2: +2.2%
- RACE: +8.4%[1]

### 일반화 성능 향상 메커니즘

1. **정규화 효과**: 매개변수 공유가 자연스러운 정규화 역할 수행[1]
2. **안정적 훈련**: 층간 매개변수 공유로 인한 훈련 안정성 향상[1]
3. **향상된 담화 이해**: SOP 손실을 통한 문장간 일관성 학습 개선[1]
4. **드롭아웃 효과**: 대규모 Transformer 모델에서 드롭아웃 제거 시 성능 향상 확인[1]

### 훈련 설정 최적화

**추가 데이터 활용**: XLNet과 RoBERTa에서 사용한 추가 데이터 활용으로 MLM 정확도 대폭 향상[1]

**n-gram 마스킹**: 최대 3-gram까지의 완전 단어 마스킹으로 더 도전적인 학습 태스크 구성[1]

## 한계점

### 계산 비용 문제
- ALBERT-xxlarge는 BERT-large보다 매개변수는 적지만 **계산 비용이 더 높음**[1]
- 더 큰 구조로 인한 추론 속도 저하 (약 3배 느림)[1]

### 확장성의 한계
- 48층 네트워크에서 성능 감소 현상 관찰[1]
- 매우 큰 은닉 크기(6144)에서 성능 저하[1]

### 수렴 특성
- DQE와 달리 임베딩이 평형점으로 수렴하지 않고 진동하는 패턴[1]

## 미래 연구에 미치는 영향

### 즉각적 영향

1. **효율적 아키텍처 설계**: 매개변수 효율성과 성능 간의 균형점 제시
2. **사전훈련 손실 함수 재고**: NSP보다 SOP가 더 효과적임을 입증
3. **정규화 메커니즘**: 매개변수 공유의 정규화 효과 확인

### 장기적 연구 방향

1. **희소 어텐션**: sparse attention과 block attention을 통한 계산 효율성 개선[1]
2. **어려운 예시 마이닝**: hard example mining을 통한 추가적 표현 능력 확보[1]
3. **새로운 자기지도 학습**: 현재 손실 함수가 포착하지 못하는 차원 탐색[1]

### 연구 시 고려사항

1. **매개변수 효율성 vs 계산 효율성**: 매개변수 수 감소가 반드시 계산 효율성으로 이어지지 않음
2. **아키텍처별 최적화**: Transformer 변형에서 드롭아웃 효과 재검증 필요[1]
3. **스케일링 법칙**: 층 수와 은닉 크기 증가의 한계점 고려
4. **도메인 적응**: Wikipedia 기반 모델의 도메인 외 성능 한계 인식[1]

ALBERT는 단순히 BERT의 경량화 버전이 아닌, **매개변수 효율성과 성능 향상을 동시에 달성한 혁신적 아키텍처**로서 향후 대규모 언어 모델 연구의 중요한 이정표가 되었습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/ae44409b-2efb-486e-8fbf-6988380dc0ef/1909.11942v6.pdf)
