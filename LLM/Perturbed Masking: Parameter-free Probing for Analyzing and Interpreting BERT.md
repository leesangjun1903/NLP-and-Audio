# Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT

# 핵심 요약 및 주요 기여

**주요 주장**: BERT와 같은 사전학습 언어모델이 문법적·구조적 언어 지식을 내재화했음을, 추가 파라미터나 감독 없이도 **Perturbed Masking** 기법으로 증명할 수 있다.[1]
**주요 기여**:  
- 파라미터 프리(parameter-free) 방식으로 토큰·스팬 간 상호 영향(inter-word correlations)을 측정하는 **Perturbed Masking** 기법 제안.[1]
- 해당 영향 행렬로부터 그래프 기반 알고리즘을 통해 **구문 트리(dependency·constituency)** 와 **담화 구조(discourse)** 를 유도하여, 기존 비지도 파싱 기법과 비교 가능한 성능 달성.[1]
- 유도된 트리를 감정 분류 같은 다운스트림 태스크에 적용했을 때, 인간 설계 구조와 동등하거나 그 이상의 성능 향상 확인.[1]

# 문제 정의 및 제안 방법

## 해결하고자 하는 문제  
- *Probe* 기반 분석에서, 하위 네트워크(추가 파라미터)가 학습하는 정보와 사전학습 모델 자체가 인코딩한 지식을 구분하기 어려움.[1]
- BERT가 언어 구조(구문 및 담화)를 얼마나 내재화했는지, 파라미터 추가 없이 평가할 필요성.  

## Perturbed Masking 기법  
1. **토큰 레벨**: 입력 문장 $$x=x_1,\dots,x_T$$ 에서 목적 토큰 $$x_i$$ 만 마스킹 후 BERT 출력 $$H_{\text{mask}(i)}$$ 획득.  
2. 이어서 영향을 평가할 컨텍스트 토큰 $$x_j$$ 도 추가 마스킹하여 $$H_{\text{mask}(i),\text{mask}(j)}$$ 획득.  
3. 두 표현 차이를 거리 함수 $$d(\cdot,\cdot)$$ 로 계산:  

$$
     f(x_i,x_j) = d\bigl(H_{\text{mask}(i)},\,H_{\text{mask}(i),\text{mask}(j)}\bigr)
   $$  
   
   - 거리 함수는 **유클리드 거리(Dist)** 또는 **예측 확률 차이(Prob)** 사용 가능.[1]
4. 모든 $$(i,j)$$ 쌍에 적용해 **영향 행렬** $$F\in\mathbb{R}^{T\times T}$$ 구성.[1]

5. **스팬(Level) 확장**: 문장 내 여러 토큰을 한꺼번에 마스킹하고 평균 풀링하여 구간 간 상호 영향을 측정.[1]

## 모델 구조  
- **BERT-base** uncased 사용, 마스킹 비율·전처리 동일.[1]
- 파라미터 추가 없이, 단순히 입력에 마스킹만 적용해 영향 행렬 산출.  

# 성능 향상 및 한계

## 구문(Dependency) 파싱  
- **UAS** 기준, BERT 기반 Eisner+Dist 기법이 WSJ10-U 58.6, PUD 41.7 기록하며 랜덤 BERT 대비 대폭 개선.[1]
- 그러나 **간단한 right-chain**(gold root 설정) 대비 성능 우위는 크지 않아, 학습된 구조가 주로 국소적 패턴에 의존함을 시사.[1]
- UUAS·NED에선 더 큰 이득(각각 +12.2, +28.4).[1]
- **한계**: 전체 프로젝트성(non-projective) 구조 학습 한계, 복잡한 구문 학습 부족.

## 구성(Constituency) 파싱  
- PTB23에서 **F1 42.1** 기록, ON-LSTM·PRPN 등 전용 비지도 파서와 유사한 수준 달성.[1]
- NP·VP·PP 식 품사별 정확도 경쟁력 있으나, 세부 구문 복잡도에선 전용 기법 대비 차이 존재.[1]

## 담화(Discourse) 구조 파싱  
- SciDTB에서 UAS 34.4, 인접 스팬간 관계 식별 정확도 63.8 달성, 랜덤 대비 +28.[1]
- Left-chain 기반 문장 구조 우위로 인해 전체 퍼포먼스는 한계 존재.[1]

## 다운스트림 태스크(ABSC) 개선  
- Aspect-based Sentiment Classification에 유도 트리 적용 시, SpaCy 제공 구조와 동등하거나 근소한 우위 달성.[1]
- 긴 문장(노트북 리뷰)에서 직접 유도된 구조가 더 경쟁력 있음을 확인.

# 일반화 성능 개선 관점

- **파라미터 프리** 특성 덕분에 추가 과적합 위험 없이 다양한 데이터셋에 적용 가능하며, **스팬 레벨**로 확장 시 문서 전반 구조 학습에 유리.[1]
- 마스킹 시점에 토큰 순서와 전후 문맥을 모두 반영하므로, **long-range dependency** 와 **담화적 문서 구성** 학습 잠재력.[1]
- 그러나 현 단계에서는 **국소 패턴** 위주 파싱 성능으로, 보다 정교한 일반화 위해 거리 함수나 트리 유도 알고리즘 개선이 필요.

# 향후 연구 방향 및 고려 사항

- **거리 함수** $$d(\cdot,\cdot)$$ 및 **유도 알고리즘**(Eisner, Chu-Liu-Edmonds)의 최적화로 더 복잡한 언어 구조 탐색.  
- **스팬 확장**을 넘어 **문맥 뭉치** 단위로 일반화 실험, 다양한 언어·도메인에 적용성 검증.  
- **하위 트리 신뢰도**와 **필터링 기법** 도입으로 노이즈 제거, 다운스트림 태스크에 최적화된 구조 자동 선택.  
- 핵심 과제인 **Probe 과적합** 이슈: 제안 기법을 통해 BERT 내재 지식의 진정한 범위를 밝히고, **다양한 언어 구문·담화 태스크**로 확장 연구 권장.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3de71bd5-618c-4474-9703-47afc10c3693/2004.14786v3.pdf)
