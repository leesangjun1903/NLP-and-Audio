# SqueezeLLM: Dense-and-Sparse Quantization

**주요 주장 및 기여**  
SqueezeLLM은 대규모 언어 모델(LLM) 추론의 **메모리 대역폭 병목**을 극복하기 위해 제안된 초저비트 비손실 양자화 프레임워크입니다.  
1. **민감도 기반 비균일 양자화**  
   – 2차 정보(피셔 정보)를 활용해 중요한(weight마다 Hessian 근사) 값 주변에 양자화 수준(k-means 군집화 센트로이드)을 배치  
2. **Dense-and-Sparse 분해**  
   – 극소수(≈0.45%)의 outlier·민감치(weight)를 FP16 희소 행렬로 분리하고, 나머지는 저비트로 양자화  

이로써 3-bit LLaMA-7B에서 FP16 대비 최대 2.3× 추론 속도 향상과 0.4 perplexity 이내의 성능 보존을 달성했습니다.

***

## 1. 문제 정의  
- **메모리 벽(Memory Wall)**: 생성형 LLM 단일 배치 추론은 행렬-벡터 연산으로 메모리 대역폭 의존도가 높아, 연산량 대비 메모리 로드가 병목을 일으킵니다.  
- 기존 PTQ(Post-Training Quantization) 기법들은 **균일 양자화(uniform)** 또는 **그룹화**에 의존해 낮은 비트로 압축하나, 비균일 분포와 희귀 아웃라이어를 충분히 고려하지 못해 성능 저하가 큽니다.

## 2. 제안 방법  
### 2.1. 민감도 기반 비균일 양자화  
- 원래 k-means:  

$$Q^*(w)=\arg\min_Q\sum_i(w_i-Q(w_i))^2$$  

- 민감도 반영: 피셔 정보(diag(F))로 가중치하여 최종 손실 변화 최소화:  

$$Q^*(w)=\arg\min_Q\sum_i F_{ii}(w_i-Q(w_i))^2$$  

- 피셔 정보 $$F\approx\frac1{|D|}\sum_{d\in D}g_dg_d^\top$$를 샘플 데이터로 근사

### 2.2. Dense-and-Sparse 분해  
- 전체 가중치 행렬 $$W=D+S$$로 분해  
  -  $$S$$: 크기가 큰 outlier·민감치만 포함하는 희소(FP16)  
  -  $$D$$: 나머지 값, 작은 값 범위에서 저비트 양자화  
- 희소 비율 0.45%로도 range를 ≈10× 축소하여 양자화 정밀도 향상  

### 2.3. 커널 구현  
- LUT 기반 비균일 3/4-bit CUDA 커널 + CSR 기반 균형 희소 행렬-벡터 곱 커널

## 3. 성능 향상  
|모델|비트|메모리 절감|속도 향상|C4 Perplexity 변화|  
|---|---|---|---|---|  
|LLaMA-7B|3-bit (0.45% sparsity)|≈4.9×|≈2.1×|FP16 대비 +0.48|  
|LLaMA-13B|3-bit (0.45%)|≈4.9×|≈2.2×| +0.31|  
|Vicuna-7B|4-bit (0.45%)|≈4.3×|–|Zero-shot MMLU ±0.3% 유지|  

- **3-bit**에서 SqueezeLLM은 GPTQ/AWQ 대비 **0.3–1.8 perplexity** 우위  
- **추론 속도**: A6000 GPU에서 최대 **2.3×** 가속  
- **일반화 성능**: MMLU, Vicuna instruction-following 과제에서도 FP16 대비 **거의 손실 없는** 성능 유지

## 4. 한계 및 고려 사항  
- **Hessian 근사**·피셔 정보 계산 메모리 비용(최대 300GB)과 시간(65B 기준 ≈80분) 필요  
- **2-bit 이하** 양자화 시 아웃라이어 영향 커, 추가 희소화 필요  
- 제안 기법은 **디코더 아키텍처**에 주로 평가되었으며, 인코더 등 타 구조 적용성 추가 검증 필요

## 5. 향후 연구 영향 및 제언  
- **메모리 대역폭 병목 극복**: 메모리-연산 트레이드오프 관점의 양자화 연구 확장  
- **민감도 기반 최적화**: 2차 정보 활용한 다양한 압축·프루닝 기법으로 일반화 가능  
- **희소·저비트 하이브리드**: 더 낮은 비트(2-bit 이하)에서도 성능 확보를 위한 동적 희소화 연구  
- **다양한 아키텍처**: 인코더, 멀티모달 모델 등에도 적용하여 범용성 검증 필요

> SqueezeLLM은 **메모리 병목 해소**를 목표로, **민감도 기반 비균일 양자화**와 **Dense-and-Sparse 분해**를 결합해 초저비트에서도 LLM의 성능과 효율을 동시에 달성한 획기적 방안을 제시합니다. 앞으로 메모리-연산 최적화, 동적 희소화, 다양한 네트워크 구조 적용 연구가 활발해질 것으로 기대됩니다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/104e857d-e45c-4726-9719-020731e76fdd/2306.07629v4.pdf
