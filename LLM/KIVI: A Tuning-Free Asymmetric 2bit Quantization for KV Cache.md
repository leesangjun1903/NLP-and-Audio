# KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache

**핵심 주장 및 주요 기여**  
이 논문은 대형 언어 모델(LLM) 추론 시 KV(Key-Value) 캐시가 메모리와 속도의 병목이 되는 문제를 해결하기 위해, “키 캐시는 채널별(per-channel), 값 캐시는 토큰별(per-token)로 양자화”하는 **KIVI** 알고리즘을 제안한다. 이로써 2비트 극저비트 양자화만으로도 모델 품질 저하를 최소화하며, 최대 2.6× 메모리 절감과 최대 3.47× 처리량 향상을 달성한다.

***

## 1. 해결하고자 하는 문제  
- **메모리 병목**: 롱 컨텍스트·대규모 배치에서 KV 캐시가 TB 단위로 늘어나 모델 파라미터보다 더 큰 메모리를 차지하며,  
- **속도 병목**: 매 토큰마다 GPU SRAM으로 KV 캐시를 로드하는 동안 연산 유휴 상태 발생  
  
기존 연구는 4비트 토큰별 양자화, 헤드 수 축소, 토큰 축출, 페이징 등으로 접근했으나, **KV 캐시 요소 분포의 불균형**을 심층적으로 분석하거나 2비트 이하로 압축하는 방법을 제시하지 못했다.

***

## 2. 제안 방법: KIVI 알고리즘  
### 2.1. 요소 분포 분석  
- **키 캐시**: 특정 채널에 고(高)크기 아웃라이어가 지속적으로 존재 → 채널별 양자화가 적합  
- **값 캐시**: 아웃라이어 패턴 부재, 희소한 어텐션 가중치와 결합 시 한 토큰 내 오차만 중요 → 토큰별 양자화 권장  

### 2.2. 비대칭 양자화 설계  
- 키 캐시: B비트 정수 양자화 $$Q(X) = \lfloor (X - z)/s \rceil$$, $$z=\min X$$, $$s=(\max X-\min X)/(2^B-1)$$ 를 **채널별** 그룹화($$G$$ 채널씩)  
- 값 캐시: 동일 연산을 **토큰별**로  

### 2.3. 스트리밍·시스템 지원 구조  
1. **그룹화/잔여 분할**  
   - 전체 토큰을 $$G$$개 단위 그룹과 $$R$$개 잔여(residual)로 분할  
   - 그룹은 양자화, 잔여는 FP32 유지  
2. **디코딩 시점**  
   - 새 토큰 키/값을 잔여 버퍼에 추가 → 잔여가 $$R$$ 도달 시 양자화 후 그룹에 합류  
   - 어텐션 계산 단계에서 그룹화된 양자화값(저정밀)과 잔여(FP32)를 합산  
3. **하드웨어 최적화**  
   - CUDA로 양자화 및 탈양자화-곱셈 융합(Q_MatMul), Triton으로 그룹 양자화 커널 구현  

***

## 3. 성능 향상  
| 모델            | 메모리 절감 | 최대 처리량 향상      | 정확도 손실(최대) |
|-----------------|-------------|-----------------------|------------------|
| Llama-2-7B      | 2.6×        | 2.35×∼3.47×           | ≤2%              |
| Falcon-7B       | 4비트 사용  | 유사한 처리량 유지    | 미미             |
| Mistral-7B      | 2.6×        | 비슷                  | ≤2%              |

- **LongBench** 장문 이해·생성 벤치마크에서도 2비트 KIVI가 FP 대비 평균 1% 미만 성능 저하  
- **GSM8K** 수학 추론처럼 하드한 과제는 FP 슬라이딩 윈도우($$R$$개 토큰) 유지 덕분에 정확도 유지  

***

## 4. 한계 및 고려 사항  
- **멀티쿼리 어텐션(Falcon)**: KV 헤드 수가 적은 구조는 2비트에서 정확도 저하 심해 4비트 양자화 필요  
- **잔여 버퍼 크기(R)**: 너무 작으면 하드 과제 성능 저하, 너무 크면 메모리 절감 효과 감소  
- **추가 최적화**: 양자화 연산과 기존 GEMM 연산 융합, 하드웨어 특화 레이어 구현  

***

## 5. 향후 연구 영향 및 고려할 점  
- **범용 양자화 설계**: KIVI의 비대칭 양자화 원칙을 다른 스트리밍 텐서(예: 메모리-집약적 인코더)에도 적용 가능  
- **시스템 통합**: vLLM·S3 등의 메모리 관리 기법과 결합하여 추가 처리량 및 메모리 이점 모색  
- **아키텍처 적응**: Mixture-of-Experts, Retrieval-Augmented Generation 등 복합 구조에서의 KV 캐시 양자화 영향 평가  
- **자동 튜닝**: $$G, R$$ 값 자동 최적화로 모델·하드웨어 특성에 맞춘 동적 양자화 스킴 개발  

이러한 방향을 통해 LLM 추론 비용을 혁신적으로 낮추고, 실시간·대규모 컨텍스트 서비스의 대중화를 앞당길 수 있을 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/3a532fe5-304e-413a-98dd-33ea7fcc9846/2402.02750v2.pdf
