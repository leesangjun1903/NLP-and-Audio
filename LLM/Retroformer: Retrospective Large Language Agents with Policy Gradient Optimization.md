
# Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization

## 1. 핵심 주장과 주요 기여

**Retroformer**는 대규모 언어 모델(LLM) 기반 에이전트의 성능을 체계적으로 향상시키기 위해 **정책 그래디언트 최적화**를 도입하는 혁신적인 프레임워크입니다.[2][1]

### 핵심 주장[1]

기존의 언어 에이전트들(ReAct, Reflexion 등)은 환경 보상을 활용하지 않거나 **자기 반성(self-reflection)**을 통해 정성적 피드백만 제공합니다. 이들은 **신경망 가중치 업데이트가 불가능한 구조**를 가지고 있어 환경 피드백으로부터의 학습이 제한적입니다. 특히 frozen LLM에서 생성된 자기 반성은 **신용 할당 문제(credit assignment problem)**를 제대로 해결하지 못하고 동일한 실패를 반복하는 경향이 있습니다.[1]

### 주요 기여[2][1]

1. **정책 그래디언트 기반 에이전트 학습**: 환경 보상을 직접 활용하는 첫 번째 원칙 기반 프레임워크 제안
2. **회고적 모델(Retrospective Model) 최적화**: 행위자 LLM의 파라미터에 접근할 수 없을 때, 작은 로컬 LLM(예: Llama-7b)만 미세조정하는 방식으로 계산 효율성 확보
3. **다중 환경·다중 태스크 학습**: HotPotQA, AlfWorld, WebShop 등 다양한 환경에서 일반화 가능한 에이전트 구축
4. **플러그인 모듈 특성**: 다양한 클라우드 기반 LLM(GPT, Bard 등)에 통합 가능한 유연성[1]

***

## 2. 문제 정의 및 제안 방법

### 해결하고자 하는 문제[1]

| 측면 | 문제점 | Retroformer의 해결책 |
|------|--------|-------------------|
| **신용 할당** | Frozen LLM은 실패 원인을 정확히 진단하지 못함 | 에이전트 궤적과 보상 신호로부터 근본 원인 학습 |
| **학습 신호** | 정성적 피드백만 존재 | 환경 보상(F1 점수, 성공/실패 등)을 직접 활용 |
| **계산 복잡성** | LLM 전체 미세조정이 불가능 | 작은 회고적 모델만 최적화 |
| **확장성** | 각 환경마다 별도 프롬프트 엔지니어링 필요 | 여러 환경의 데이터로 일반화된 모델 학습 |

### 수학적 표현[1]

**기본 설정**: LLM 기반 에이전트를 함수 $$M_a: X \to A$$로 정의 ($$X$$는 프롬프트 공간, $$A$$는 행동 공간)

**환경 모델**:

$$\tau_o: S \times A \to S \text{ (상태 전이 함수)}$$

$$R: S \to \mathbb{R} \text{ (보상 함수)}$$

**회고적 모델의 역할**:

$$y_{k,i} = M_r(\{s_{k,i,t}, a_{k,i,t}, r_{k,i,t}\}_{t=1}^{T-1}, G_{k,i} \mid \theta_r, x_{k,i})$$

여기서 $$y_{k,i}$$는 반성 응답, $$\theta_r$$는 회고적 모델의 파라미터입니다.[1]

**목표 함수**:

$$\arg\max_{\theta_l} \mathbb{E}_{\omega,\zeta,\rho} \left[ \sum_{t=1}^{T} R_t \mid s_{t+1} \sim T_\zeta(s_t), L(\theta_l, s_i, a_i, r_i, x_i^u)_{i=1}^{t}, t=1,\ldots,T-1 \right]$$

**핵심**: 오직 회고적 모델의 파라미터 $$\theta_r$$만 학습 가능하며, 행위자 LLM의 파라미터는 고정됩니다.[1]

***

## 3. 모델 구조 및 작동 메커니즘

### 전체 아키텍처[1]

Retroformer는 **이중 LLM 구조**로 구성됩니다:

```
┌─────────────────────────────────────────────────────────────┐
│                  Actor LLM (Frozen)                         │
│  - ReAct 프롬프트 기반                                      │
│  - 행동(action) 생성                                        │
└──────────────────────┬──────────────────────────────────────┘
                       │
              ┌────────▼──────────┐
              │  Environment      │
              │ (HotPotQA/        │
              │  AlfWorld/        │
              │  WebShop)         │
              └────────┬──────────┘
                       │
        ┌──────────────▼──────────────┐
        │ Reward Signal r_{k,i}       │
        │ Episode Return G_{k,i}      │
        └──────────────┬──────────────┘
                       │
┌──────────────────────▼──────────────────────────────────────┐
│      Retrospective LLM (Fine-tuned: Llama-7b)              │
│  - 회고적 프롬프트 생성                                    │
│  - 실패 원인 분석 및 개선 계획 수립                        │
│  - PPO로 최적화                                            │
└──────────────────────┬──────────────────────────────────────┘
                       │
           ┌───────────▼────────────┐
           │  개선된 프롬프트       │
           │  (Long-term memory)    │
           └───────────┬────────────┘
                       │
              다음 시행으로 피드백
```

### 핵심 컴포넌트[1]

**1. 행위자 모델 (Actor Model)**[1]
$$a_{k,i,t} = M_a(s_{k,i,\leq t}, a_{k,i,\leq t-1}, r_{k,i,\leq t-1}, s_{k,i,t})$$

GPT-3 또는 GPT-4 등 클라우드 기반 모델로, 파라미터는 완전히 고정됨.

**2. 회고적 모델 (Retrospective Model)**[1]
$$y_{k,i} = M_r(s_{k,i,\leq T-1}, a_{k,i,\leq T-1}, r_{k,i,\leq T-1}, G_{k,i})$$

7B 파라미터 규모의 LongChat 모델 기반, LoRA(Low-Rank Adaptation)로 미세조정:
- LoRA rank=1: 0.53M 파라미터 (Llama-7b의 0.015%)
- LoRA rank=4: 2.25M 파라미터

**3. 메모리 모듈**[1]

| 메모리 타입 | 설명 | 역할 |
|-----------|------|------|
| 단기 메모리 (Short-term) | 현재 에피소드 궤적 $$\tau_i$$ | 즉각적인 의사결정 |
| 장기 메모리 (Long-term) | 이전 시행의 반성 응답 $$y_{k,j}$$ | 반복 오류 방지 및 누적 학습 |
| 리플레이 버퍼 | $$(x_{k,i}, y_{k,i}, G_{k,i})$$ 저장 | RL 최적화를 위한 데이터셋 |

***

## 4. 정책 그래디언트 최적화[1]

### 응답 등급 산정[1]

연속 시행 간의 에피소드 반환값 차이를 보상 신호로 사용:

$$r(x_{k,i}, y_{k,i}) = G_{k,i+1} - G_{k,i}$$

- $$\Delta G > 0$$: 행위자가 개선됨 → 높은 점수
- $$\Delta G \leq 0$$: 행위자가 악화됨 → 낮은 점수

### PPO (Proximal Policy Optimization) 최적화[1]

$$\mathcal{L}_{PPO} = \mathbb{E}_{x,y \sim D_{RL}} \left[ r(x,y) \log \pi_{RL}(y|x) - \beta \text{KL}(\pi_{RL}(y|x) \| \pi_{ref}(y|x)) \right]$$

여기서:
- $$r(x,y)$$: 응답 등급 (위에서 정의)
- $$\pi_{RL}$$: 미세조정된 회고적 모델
- $$\pi_{ref}$$: 참조 모델 (최적화 과정 중 격차 제한)
- $$\beta$$: KL 발산 계수 (배포 이동 제어)

**세 단계 프로세스**:[1]

1. **감독학습(Supervised Fine-tuning)**: 긍정 등급 샘플로 2 에포크 사전학습
2. **보상 모델 학습**: 수용/거절 응답 분류기 훈련
3. **PPO 미세조정**: 학습된 보상 모델로 정책 최적화 (20,000 최대 스텝)

### 온라인 실행 (Online Execution)[1]

Best-of-N 샘플링 사용:
$$y^* = \arg\max_{y \sim \{\text{top-N samples}\}} \hat{r}(x,y)$$

***

## 5. 성능 향상 및 실험 결과[1]

### 실험 환경[1]

| 환경 | 구성 | 평가 방식 |
|------|------|---------|
| **HotPotQA** | 100 검증 태스크, 위키피디아 검색 기반 | 성공률 (%) |
| **AlfWorld** | 134 태스크, 텍스트 기반 임체디드 로봇 제어 | 성공률 (%) |
| **WebShop** | 100 태스크, 웹 브라우징 기반 쇼핑 | 성공률 (%) |

### 주요 결과[1]

**HotPotQA (질문 응답 태스크)**:
- **ReAct (GPT-4)**: 40% 성공률
- **Reflexion (GPT-4, 4 재시도)**: 52% 성공률  
- **Retroformer (GPT-4, 4 재시도)**: 54% 성공률 (전체 100% 달성)
- **개선율**: 18% with 4 retries vs. ReAct[1]

**AlfWorld (의사결정 태스크)**:
- **ReAct (GPT-3)**: 62.69%
- **Reflexion (GPT-3, 4 재시도)**: 84.33%
- **Retroformer (GPT-3, 4 재시도)**: 100% 성공률
- **개선율**: 36% with 3 retries vs. ReAct[1]

**WebShop (웹 브라우징 태스크)**:
- **ReAct (GPT-3)**: 33%
- **Reflexion (GPT-3, 4 재시도)**: 35%
- **Retroformer (GPT-3, 4 재시도)**: 36-45% (LoRA 구성에 따라)
- **개선율**: 4% with 4 retries (제한적 개선)[1]

### 학습 곡선 분석[1]

초기 시행에서 가장 큰 성능 향상 관찰:
- **Trial 1→2**: Reflexion 대비 5-8% 추가 개선
- **Trial 2→3**: 성능 향상 포화 시작
- **Trial 3→4**: 미미한 추가 개선

이는 **회고적 모델이 빠른 학습자**임을 시사합니다.[1]

### 반성 응답의 질적 개선[1]

| 측면 | Frozen LLM | Retroformer (미세조정) |
|------|-----------|----------------------|
| **근본 원인 분석** | 실패 원인 불명확 | 구체적 실패 진단 (예: 검색 쿼리 불명확) |
| **행동 지침** | 이전 조치 반복 | 새로운 시도 방안 제시 |
| **구조화** | 산문형 혼합 | 반성(Reflection)/계획(Plan) 분리 |
| **허상 제거** | 불필요한 세부사항 포함 | 간결한 형식 유지 |

예시: "다음 시행에서" 등 불필요한 텍스트 자동 제거[1]

***

## 6. 모델의 일반화 성능 향상 가능성[3][1]

### 현재 논문의 한계[1]

1. **환경 내 일반화(In-domain Generalization)**:
   - 동일 환경 내 새로운 태스크에 대해 테스트하지 않음
   - 학습 데이터의 다양성이 제한적 (HotPotQA: 3,000 태스크)

2. **크로스 도메인 일반화(Cross-domain Generalization)**:
   - HotPotQA에서 학습된 모델이 AlfWorld/WebShop으로 전이 불가
   - 각 환경별로 별도 회고적 모델 필요

3. **분포 외 일반화(Out-of-Distribution)**:
   - 새로운 유형의 태스크에 대한 강건성 미검증
   - 프롬프트 재표현(paraphrasing)에 대한 강건성 부족

### 최신 연구 기반 개선 방향[4][3]

**2025년 AgentRM 연구**: 보상 모델 기반 일반화 강화[3]

- **핵심 발견**: 정책 모델 직접 미세조정보다 **보상 모델 미세조정**이 더 강건
- **성과**: 
  - 일반 에이전트 대비 **8.8점 평균 향상**
  - 비미세조정 기본 정책 모델 대비 **4.0점 추가 개선**
  - 약강 일반화(weak-to-strong): 강력한 정책 모델(LLaMA-3-70B)에서 **12.6점 개선**

**크로스도메인 전이**: 도구 통합 강화학습의 일반화[4]

- **TGRL 프레임워크**: 표준화된 도구 인터페이스 + 이중 보상 시스템 + XML 기반 템플릿
- **성과**: 수학 문제 해결에서 학습된 도구 사용 방식이 다른 추론 영역으로 효과적 전이
- **핵심**: 도메인 불가지론적(domain-agnostic) 행동 학습 강조

**RetroAct (2025년 후속연구)**:[5]

- **개선점**: Retroformer의 작업 계획과 자기 반성을 **공동 최적화**
- **성과**: 기본 Reflexion 대비 **8%** 추가 개선, 최고 미세조정 방법 대비 **13.4%** 개선
- **일반화 메커니즘**: 작업 계획과 반성 능력의 상호 강화

### Retroformer의 일반화 가능성 평가[3][4][1]

**긍정적 요인**:
1. **다중 환경 학습**: 세 가지 이질적 환경에서 동시 학습 (전이 학습 잠재력)
2. **작은 모델 크기**: 7B 규모로 빠른 재미세조정 및 특화 가능
3. **문제 불가지론성**: 임의의 보상 신호 활용 가능 (도메인 별 적응 용이)

**부정적 요인**:
1. **프롬프트 의존성**: 각 환경별 특화 프롬프트 필요
2. **분포 외 강건성 미검증**: 패러프레이징 공격에 취약할 가능성
3. **데이터 효율성**: 긍정 등급 샘플 비율 낮음 (HotPotQA: 32%, 1,084/3,383)

***

## 7. 한계 및 제약 사항[1]

### 기술적 한계[1]

| 한계 | 영향 | 대안 |
|------|------|------|
| **행동 생성 오류** | LLM이 행동 공간 외 명령어 생성 | 구속 된코딩 또는 파싱 강화 |
| **프롬프트 길이 제한** | 장기 에피소드에서 컨텍스트 오버플로우 | 메모리 압축 또는 계층적 요약 |
| **온도 고정** | 온도=0으로 설정하여 탐색 능력 제한 | 적응형 온도 또는 혼합 전략 |
| **성능 포화** | WebShop에서 명백한 개선 불가 | 구조적 탐색 알고리즘 통합 |

### WebShop 환경의 특수성[1]

웹 쇼핑 환경에서 개선율이 4%에 그친 이유:
1. 높은 **탐색 복잡성**: 광대한 상품 공간
2. **정밀한 검색 쿼리** 필요: 일반적 반성으로는 불충분
3. **다단계 의사결정**: 프롬프트 기반 피드백의 한계

**논문의 인정**: "언어 피드백 기반 접근은 이 환경에 최적이 아님"[1]

### 데이터 수집의 비효율성[1]

**리플레이 버퍼 구성**:
- HotPotQA: 3,383 반성 샘플 중 **1,084개만 긍정 등급** (32%)
- AlfWorld: 523 중 불명확 비율
- WebShop: 267 중 불명확 비율

**함의**: 대부분의 샘플이 부정 신호 → 학습 효율 개선 필요

***

## 8. 향후 연구 영향 및 고려사항

### Retroformer의 학술적 영향[2][1]

1. **정책 그래디언트의 도입**: LLM 에이전트 최적화에 **고전 RL 기법** 적용의 선구적 역할
2. **플러그인 아키텍처 개념**: 접근 불가능한 상용 LLM의 대리 모델(proxy model) 학습 방식 확립
3. **신용 할당 문제 해결**: LLM 컨텍스트에서의 **명시적 보상 신호** 활용 체계화

### 후속 연구 방향 (2025년 기반)[6][5][4][3]

**1. 일반화 개선 (AgentRM, 2025)**[3]
- **핵심 인사이트**: 정책 모델보다 **보상 모델 미세조정**이 크로스도메인 일반화에 더 효과적
- **적용 가능성**: Retroformer의 보상 모델 학습 단계를 강화하면 **8-12% 성능 향상** 예상

**2. 공동 최적화 (RetroAct, 2025)**[5]
- **개선책**: 작업 계획과 자기 반성 **동시 최적화**
- **성과**: 기존 Retroformer 대비 **13.4% 추가 개선**
- **Retroformer에 적용**: 회고적 모델에 작업 계획 성분 추가

**3. 크로스도메인 전이 학습 (TGRL, 2025)**[4]
- **제안**: 도메인 불가지론적 보상 설계
- **구성**:
  - 표준화된 도구 인터페이스
  - 이중 보상: $$r_{task} + \lambda r_{efficiency}$$
  - XML 기반 구조화 프롬프트

**4. 프롬프트 최적화**[7]
- **연구**: 강화학습 기반 프롬프트 개선 (RPO, ICLR 2026 제출)
- **방법**: Turn-by-turn 피드백 + 경험 리플레이로 **프롬프트 자동 재작성**
- **Retroformer 확장**: 회고적 모델 출력 → 행위자 모델 프롬프트 직접 최적화

***

## 9. 실용적 고려사항

### 구현 시 권장사항[1]

**1. 모델 선택**:
```python
# 행위자 모델: 클라우드 기반 (GPT-3.5/GPT-4 권장)
actor_model = "gpt-4"

# 회고적 모델: 로컬 개배 가능한 소형 모델
retrospective_model = "meta-llama/Llama-2-7b-chat"  # 또는 LongChat-7b-16k
```

**2. LoRA 구성**:
- **메모리 제약 환경**: LoRA rank=1 (0.53M 파라미터)
- **성능 중시**: LoRA rank=4 (2.25M 파라미터, 작은 오버헤드)

**3. 하이퍼파라미터**:[1]
- 감독학습: 학습률 1e-5, 배치 크기 32, 2 에포크
- 보상 모델링: 학습률 2.5e-5, 배치 크기 32, 최대 스텝 20,000
- PPO 미세조정: 학습률 1.4e-5, 배치 크기 64, PPO 에포크 4

### 배포 시나리오[1]

**시나리오 1: 클라우드 기반 LLM**
```
사용자 프롬프트 → Frozen Actor (GPT-4) → 행동
                                ↓
                          환경 상호작용
                                ↓
                         보상 신호 (r)
                                ↓
                   로컬 회고적 모델 최적화
                   (온디바이스 또는 엣지)
```

**시나리오 2: 멀티 에이전트 협력**
```
에이전트 1 (HotPotQA 특화) ┐
에이전트 2 (AlfWorld 특화)  ├─→ 공유 보상 모델
에이전트 3 (WebShop 특화)  ┘     학습 (메타러닝)
```

***

## 결론

**Retroformer**는 LLM 기반 에이전트의 구조적 한계(접근 불가능한 파라미터)를 우회하면서도 **환경 보상 신호로부터 체계적으로 학습**하는 혁신적 프레임워크입니다.[2][1]

그러나 **일반화 성능의 근본적 도전**이 남아있습니다:
- 환경 내 new task에 대한 일반화 미검증
- 크로스도메인 전이의 어려움
- 분포 외 강건성 부족

최신 연구(2025)는 이러한 한계를 **보상 모델 기반 방법론**, **공동 최적화**, **도메인 불가지론적 설계**로 극복하는 방향을 제시합니다. **Retroformer 위에 이들 기법을 층화하면 LLM 에이전트의 일반화 능력을 획기적으로 향상**시킬 수 있을 것으로 예상됩니다.[5][4][3]

***

## 참고 자료

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/96533ddc-f139-4e5d-89cc-2f51ae32f713/2308.02151v3.pdf)
[2](https://arxiv.org/pdf/2308.02151.pdf)
[3](http://arxiv.org/pdf/2502.18407.pdf)
[4](https://arxiv.org/abs/2510.11184)
[5](https://aclanthology.org/2025.naacl-long.6.pdf)
[6](https://arxiv.org/pdf/2503.01490.pdf)
[7](https://openreview.net/forum?id=Xt6K4cw36l)
[8](http://arxiv.org/pdf/2502.10325.pdf)
[9](https://arxiv.org/abs/2503.02519)
[10](https://arxiv.org/pdf/2210.03821.pdf)
[11](https://arxiv.org/pdf/2410.04612.pdf)
[12](http://arxiv.org/pdf/2405.14751.pdf)
[13](https://www.emergentmind.com/papers/2308.02151)
[14](https://arxiv.org/html/2509.23586v1)
[15](https://arxiv.org/abs/2308.02151)
[16](https://ci.acm.org/2025/wp-content/uploads/Academic_Simulacra_Final.pdf)
[17](https://aclanthology.org/2022.emnlp-main.222/)
[18](https://openreview.net/forum?id=KOZu91CzbK)
[19](https://aclanthology.org/2025.acl-long.945.pdf)
[20](https://www.emergentmind.com/topics/reinforcement-learning-for-prompt-tuning)
[21](https://proceedings.iclr.cc/paper_files/paper/2024/file/29f421fbdcc82aeb349d784d3aaccdb3-Paper-Conference.pdf)
[22](https://arxiv.org/pdf/1811.07871.pdf)
[23](https://arxiv.org/pdf/2502.19328.pdf)
[24](http://arxiv.org/pdf/2502.12130.pdf)
[25](https://arxiv.org/pdf/2312.03881.pdf)
[26](https://arxiv.org/html/2502.04864v1)
[27](http://arxiv.org/pdf/2404.01131.pdf)
[28](https://aclanthology.org/2025.acl-long.945/)
[29](https://arxiv.org/abs/2502.18407)
[30](https://github.com/weirayao/Retroformer)
[31](https://arxiv.org/html/2505.23847v1)
[32](https://chatpaper.com/paper/176341)
[33](https://www.ijcai.org/proceedings/2025/1198.pdf)
[34](https://openreview.net/pdf?id=xCXRs4WtHC)
