# OPT-IML : Scaling Language Model Instruction Meta Learning through the Lens of Generalization

**핵심 주장 및 기여**  
OPT-IML은 2000여 개의 NLP 과제로 구성된 대규모 지시-메타학습 벤치마크(OPT-IML Bench)를 제안하여  
1) 완전히 신규 카테고리, 2) 기존 카테고리 내 신규 과제, 3) 기존 과제 내 신규 인스턴스로 나뉜 **세 수준의 일반화**를 체계적으로 평가한다.  
이를 통해 지시-튜닝 시  
- 과제·카테고리 스케일  
- 데이터 샘플링 전략  
- 체인 오브 땀프(Reasoning)·대화(Dialogue) 데이터 포함  
- 데모(시연) 활용 방식  
등 주요 결정 요소들의 성능 Trade-off를 분석하고, 이를 바탕으로 OPT-IML 30B/175B 모델을 학습하여 여러 벤치마크에서 최첨단 성능을 달성한다.

***

## 1. 해결하고자 하는 문제 및 제안 방법

### 문제 정의  
- **지시-튜닝 효과**는 보고되었지만, 데이터 스케일·다양성·샘플링·데모 활용·전용 데이터 포함 등 각 결정 요소가 **일반화 성능**에 미치는 영향을 종합적으로 이해하기 어려움.  
- 특히 “새로운 과제”와 “새로운 카테고리”로의 제로/소수샷 일반화 차원에서 **비교 정량화 부족**.

### OPT-IML Bench 구성  
- 8개 기존 벤치마크 합산 1,991과제 → 100여개 카테고리로 군집 및 분류  
- Train/Dev/Test 분할  
  -  완전차단(fully held-out) 카테고리  
  -  부분차단(partially held-out) 카테고리 내 신규 과제  
  -  Train 과제 내 신규 인스턴스

### 지시-튜닝 수식  
모델 θ에 대해 과제 i의 지시문+입력(sᵢ)을 소스, 정답 토큰(tᵢ)만 타겟으로 삼아 교차엔트로피 최소화:  

$$
L(D; θ) = -\sum_i \sum_j \log p_θ(t_{ij}\mid s_i, t_{i < j})
$$

- **Packing & 문서 단위 마스킹**: 여러 예시를 2048 토큰 시퀀스에 pack, 블록-삼각형 마스크 사용  
- **샘플링**: 각 벤치마크별 비율(proportion)과 예시별 최대 샘플 개수(EPS) 조정  

***

## 2. 모델 구조 및 학습 세부

- **Base 모델**: OPT 30B/175B (decoder-only Transformer)  
- **하이퍼파라미터**  
  -  Batch size: 30B→256, 175B→128  
  -  LR 5e-5, Warm-up 60 step, Total step: 30B→4k, 175B→8k  
  -  Mixed-precision, Fully Sharded Data Parallel, Megatron-LM 병렬화  
- **데이터 구성**  
  -  Train mix: Super-NaturalInst 20%, FLAN 25%, PromptSource 45%, 기타 10%  
  -  사전학습 데이터 5%, 체인오브땀프 1% 포함  
  -  데모(meta-ICL)·대화 데이터는 degeneration 우려로 제외  

***

## 3. 성능 향상 및 한계

### 정량적 성능  
- **OPT 대비 제로샷**: 30B +6%p, 175B +7%p 평균 정확도 개선  
- **T0/Tk-Instruct 등 비교**:  
  -  PromptSource 0/5-shot 모두 상회  
  -  FLAN-137B 대비 0-shot +3%p, 5-shot 유사 성능  
  -  Super-NatInst 0-shot 우위, few-shot에서는 벤치 특화 모델에 근접  
- **일반화 수준별**  
  -  완전차단 카테고리에서 꾸준한 개선  
  -  부분차단 과제에서  과제수·카테고리 스케일 증가 시 큰 폭 향상  
  -  Train 과제 인스턴스 일반화는 성능 안정 유지

### 한계 및 고려사항  
- PaLM/FLAN-PaLM 시리즈 대비 BBH·MMLU 챌린징 벤치마크에서 여전히 격차 존재  
- 메타-ICL, 대화 데이터 포함 시 특정 카테고리 성능 저하 관찰  
- 카테고리 분류 주관적, 다른 분류 기준 시 튜닝 최적점 변화 가능  

***

## 4. 일반화 성능 향상 관점 집중

- **과제 수 스케일**: 16→1024과제 시 제로샷 완전차단에서 Rouge-L +40%p 이상  
- **카테고리 스케일**: 4→93카테고리 시 부분차단·완전차단 0-shot 평균 +5%p  
- **Chain of thought**: 1% 포함만으로도 논리·상식 추론 과제에서 Rouge-L +19%p  
- **샘플링 최대치(EPS)**: 4096으로 설정 시 벤치 간 과도한 편향 방지하며 일반화 안정화  
- **사전학습 데이터 일부 포함**: 5% 선에서 신뢰도·표현력 증진, 과다 시 classification 성능 저하  

***

## 5. 향후 영향 및 연구 고려사항

- **프롬프트 및 과제 다양성**를 활용한 **범용 LLM** 구축 전략 제시  
- **세 수준 일반화 평가 체계**는 후속 지시-튜닝 연구의 표준으로 채택 가능  
- 메타-ICL, 대화 데이터, RLHF 병합 시 **degeneration 극복** 연구 필요  
- 카테고리·토큰 예산·데이터 구성 상호작용 고려한 **다변량 최적화** 기법 개발  
- **소형 모델**(≤30B)에서도 적절 지시-튜닝으로 대형 모델 성능 근접 가능, **환경적 지속가능성** 측면 기여  

OPT-IML은 지시-튜닝의 대규모화와 일반화 평가를 통해 **지시-튜닝 최적화** 로드맵을 제시하며, 후속 연구에서 보다 **상호작용적·효율적** 지시-학습 기법 개발에 핵심적 토대를 제공할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6646958b-2d56-4efc-960f-b39fb8ef5aa1/2212.12017v3.pdf)
