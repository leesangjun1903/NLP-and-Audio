# A Systematic Evaluation of Large Language Models of Code

## 1. 핵심 주장 및 주요 기여  
**핵심 주장**  
*대규모 언어 모델(Large Language Models, LLMs)이 코드 생성 및 완성에서 뛰어난 성능을 보이지만, 최상위 모델 대부분이 비공개이므로 학계와 중소기업의 연구·응용이 제한된다.*[1]

**주요 기여**  
1. **기존 모델 비교 분석**: Codex, GPT-J, GPT-Neo, GPT-NeoX, CodeParrot 등 주요 모델을 **언어별 perplexity**와 **HumanEval 벤치마크**로 체계적 비교 수행.[1]
2. **PolyCoder 제안**: 12개 프로그래밍 언어(총 254 GB 코드)로만 학습된 2.7B 파라미터 모델 ‘PolyCoder’ 공개. C 언어에서 Codex를 능가하는 perplexity 달성.[1]
3. **다중언어 학습 효과 검증**: 자연어와 코드 혼합 학습 모델 대비, **코드 전용 다중언어 학습**이 특히 저자원 언어에서 일반화 성능을 높일 가능성 시사.[1]

## 2. 문제 설정 및 제안 방법  
### 2.1 해결하고자 하는 문제  
- **비공개성**: Codex 등 최첨단 모델은 가중치·데이터 미공개, 연구 확장 제한.  
- **학습 데이터 설계 불투명**: 모델별 데이터 규모·언어 분포·전처리 방식이 명확히 비교되지 않음.  

### 2.2 제안 방법  
1. **데이터 수집·전처리**  
   - GitHub 인기 리포지토리(별 ≥ 50) 12개 언어, 147K 리포지토리, 24M 파일, 254 GB.[1]
   - 파일 크기(> 1 MB)·길이(< 100토큰) 필터링, 중복 제거(hash 기반) 수행.  
2. **모델 구조**  
   - GPT-2 기반 트랜스포머 아키텍처  
   - 2.7B 파라미터: 32-layer, hidden size=2560, context window=2048, batch=262K 토큰.[1]
3. **학습 수식**  
   - 언어 모델링 목표: $$\displaystyle \max_\theta \sum_{t=1}^T \log p_\theta(x_t \mid x_{<t})$$[1]
   - 최적화: AdamW, learning rate decay (cosine), warm-up steps=1600.  
4. **평가 지표**  
   - **HumanEval**: Pass@k 성능(코드 생성 정확도)  
   - **Perplexity**: 언어별 일반화 능력 측정  

## 3. 성능 향상 및 한계  
### 3.1 성능 향상  
- **C 언어 perplexity**: PolyCoder(2.33) < Codex(2.55)로 최고.[1]
- **다중언어 전이 효과**: Python 성능은 GPT-Neo(2.7B) 대비 다소 낮으나, C/C++·Rust·Scala 등 비주류 언어에서 GPT-Neo와 유사하거나 우수.[1]
- **HumanEval**: Pass@1 ~5.6%, Pass@100 ~17.7%로 Python 전용 모델(CodeParrot)보다 저조하지만, 모델 크기 대비 다중언어 학습의 가능성 시사.[1]

### 3.2 한계  
1. **학습량 부족**: 150K 스텝(39B 토큰) 학습에도 수렴하지 않아 underfit 발생.[1]
2. **데이터 불균형**: C/C++ 데이터 비중 과다로 특정 언어 편향 발생 가능.  
3. **자연어 부재**: 코드 전용 학습으로 Stack Exchange 등 기술문서 텍스트의 언어적 맥락 부재.  

## 4. 일반화 성능 향상 가능성  
- **다중언어 학습**은 프로그래밍 언어 간 유사 키워드·구문 전이를 촉진하여 저자원 언어에서도 일반화 도움.[1]
- **자연어 혼합 학습**(GPT-NeoX, GPT-J)은 기술 문서 내 코드 설명 패턴 학습으로 코드 생성 시 문맥 이해 강화 기대.  
- **더 긴 학습**과 **균형 데이터 샘플링**을 통해 underfit 해소 및 편향 완화 가능성이 높음.  

## 5. 향후 연구 방향 및 고려 사항  
- **자원 효율적 학습**: 중간 규모 모델의 더 긴 사전학습과 지식 증류(knowledge distillation)로 대규모 모델 성능 접근.  
- **데이터 다양화**: 코드+문서+테스트케이스 혼합 전이 학습으로 자연어-코드 상호작용 강화.  
- **공개 벤치마크 확장**: 저자원 언어 평가 세트 구축으로 다중언어 모델 일반화 성능 객관적 측정.  
- **안정성 및 편향 분석**: 코드 생성 중 보안 취약점이나 라이선스 위반 등 윤리적 리스크 저감 기법 연구.  

이 논문은 **대규모 코드 언어 모델의 민주화**와 **다중언어 학습의 일반화 가능성**을 제시하며, 후속 연구에서 **학습 자원 최적화**, **데이터 편향 완화**, **자연어-코드 융합** 등을 고려해야 함을 강조한다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/88292186-dfa4-4419-98d9-31563d8ec4cc/2202.13169v3.pdf)
