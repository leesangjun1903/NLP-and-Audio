# AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning

**핵심 주장 및 주요 기여**  
AdaLoRA는 사전학습된 대형 언어모델을 다운스트림 태스크에 효율적으로 파인튜닝할 때, 모든 파라미터에 균등하게 예산을 분배하는 기존 저랭크(LoRA) 기반 기법의 한계를 극복한다. 각 가중치 행렬의 중요도에 따라 파라미터 예산(랭크)을 동적으로 재할당함으로써, 동일 예산 대비 성능을 크게 향상시킨다.

***

## 1. 해결하고자 하는 문제  
- **파인튜닝 예산의 균등 분배 한계**  
  - LoRA 등 기존 방법은 모든 가중치 행렬에 동일한 랭크 $$r$$를 할당하여 미세조정하지만, 실제로는 모듈 및 층마다 중요도가 다르다.  
  - 중요도가 낮은 곳에 예산을 낭비하면 과적합을 유발하거나, 중요한 영역에 충분한 용량을 할당하지 못해 성능이 저하된다.

***

## 2. 제안 방법  

### 2.1 SVD 기반 파라미터화  
각 증분 업데이트 행렬 $$\Delta_k\in\mathbb{R}^{d_1\times d_2}$$를 저랭크 SVD 형태로 표현:  

$$
W = W^{(0)} + \Delta,\quad
\Delta_k = P_k\,\Lambda_k\,Q_k,
$$  

여기서  
- $$P_k\in\mathbb{R}^{d_1\times r},\,Q_k\in\mathbb{R}^{r\times d_2}$$  
- $$\Lambda_k=\mathrm{diag}(\lambda_{k,1},\dots,\lambda_{k,r})$$  

추가로 $$P_k,Q_k$$의 직교성을 유지하기 위해 정규화 항을 손실에 더한다:  

$$
R(P_k,Q_k) = \|P_k^\top P_k - I\|_F^2 + \|Q_k Q_k^\top - I\|_F^2.
$$

### 2.2 중요도 기반 랭크 할당  
각 트리플릿 $$\{P_{k, * i},\,\lambda_{k,i},\,Q_{k,i *}\}$$의 중요도 점수 $$S_{k,i}$$를 계산하여, 전체 예산 $$b(t)$$ 내에서 상위 점수에만 랭크를 유지하고 하위는 제거한다.  

$$
S_{k,i}
= s(\lambda_{k,i})
+\frac1{d_1}\sum_j s(P_{k,ji})
+\frac1{d_2}\sum_j s(Q_{k,ij}),
$$  

$$ s(w) = I(w)\,U(w),\quad
I(w)=|w\,\partial_w \mathcal{L}|,\quad
U(w)=\mathrm{EMA\,uncertainty}.
$$  

이후 매 스텝마다 예산 스케줄 $$b(t)$$에 따라 랭크를 동적으로 축소하며, 초기 과잉 예산에서 점차 목표 예산 $$b(T)$$에 수렴시키는 큐빅 스케줄러를 적용한다.

***

## 3. 모델 구조  
- 기존 Transformer의 각 층 Self-Attention 모듈($$W_q,W_k,W_v,W_o$$) 및 FFN 모듈($$W_{f1},W_{f2}$$)에 대해 모두 SVD 기반 증분 업데이트를 적용.  
- 증분 행렬마다 $$P,\Lambda,Q$$ 세 파라미터만 학습하고, 원본 가중치는 고정.

***

## 4. 성능 향상 및 한계  
- **GLUE, SQuAD, XSum/CNN-DailyMail** 등 7개 벤치마크에서 동일 예산 대비 일관된 성능 향상  
- 특히 예산이 0.1% 이하인 극저예산 설정에서도 기존 LoRA 대비 최대 1.2% F1 상승  
- **일반화 성능**  
  - 중요도 기반 예산 할당으로 과적합 위험이 낮아지고, 드물게 높은 랭크가 필요한 모듈에 충분한 용량 제공  
  - 상대적으로 작은 예산에서도 높은 안정적 성능을 보이며, 예산 범위 전반에 걸쳐 일관된 결과 유지  
- **한계**  
  - SVD 파라미터화 및 중요도 계산으로 인해 LoRA 대비 10~15% 추가 학습 시간 소요  
  - 중요도 추정(gradient sensitivity) 불확실성 관리 필요  
  - 매우 저차원 예산에서는 과도한 랭크 축소로 복원 불가능한 정보 손실 가능성

***

## 5. 일반화 성능 향상 가능성  
- 중요도 기반 동적 예산 조정이 다양한 태스크에 걸쳐 과적합 제어 및 특화된 학습 용량 배분에 효과적  
- 작은 예산에서도 높은 안정성을 보이며, 미니 배치 노이즈에 의한 중요도 불확실성 완화를 위한 스무딩 기법이 일반화에 기여  
- 추후 다양한 예측 태스크(예: 멀티모달, 비전-언어)로 확장 시에도 유사한 일반화 혜택 기대

***

## 6. 향후 연구 영향 및 고려점  
- **가중치 중요도 측정의 정확도 개선**  
  - 더 정교한 불확실성 추정, 어텐션 기반 중요도, 정보 이론적 지표 탐색  
- **연산 효율화**  
  - 중요도 계산 및 랭크 재할당 비용 감소 기법 개발  
- **확장성**  
  - 초대형 모델(GPT-4급) 및 멀티태스크/연속 학습 시나리오에의 적용  
- **복원력 확보**  
  - 과도한 파라미터 축소 시 정보 손실 방지를 위한 재활성화(Re-enable) 메커니즘 연구  

AdaLoRA는 파라미터 효율성과 일반화 사이 균형을 재정의함으로써, 향후 효율적 대규모 모델 적응 연구에 새로운 방향을 제시한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/eec3abdb-c2cb-497b-b0a1-fb2c0b6b20f6/2303.10512v2.pdf)
