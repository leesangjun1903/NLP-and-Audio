# Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?

**핵심 주장**  
Reinforcement Learning with Verifiable Rewards(RLVR)가 LLM의 표집 효율성을 높여 평균 성능(pass@1)을 개선하지만, 다수 샘플링(pass@k, k≫1) 시에는 오히려 베이스 모델의 잠재적 해결 범위를 좁힌다. 즉, RLVR는 새로운 추론 패턴을 학습하는 것이 아니라 기존 베이스 모델의 분포 내에서 올바른 경로를 더 자주 샘플링할 뿐이며, 이로 인해 근본적인 추론 능력 확장은 이루어지지 않는다.

**주요 기여**  
1. **추론 능력 경계 측정**  
   - pass@k 메트릭을 수학·프로그래밍·시각추론 벤치마크 전반에 적용하여 RLVR 모델과 베이스 모델의 해결 범위를 비교.  
   - k가 작을수록 RLVR이 우세하나, k가 커질수록 베이스 모델이 일관되게 능가함을 규명.  
2. **추론 경로 분석**  
   - RLVR 모델이 생성하는 올바른 체인-오브-생각(CoT) 경로는 모두 베이스 모델의 출력 분포에 이미 존재함을 퍼플렉서티 분석으로 증명.  
3. **샘플링 효율성 격차 정량화**  
   - ∆SE(샘플링 효율성 격차) 정의: RLVR pass@1과 베이스 pass@256 간 차이로, 다양한 RL 알고리즘(PPO, GRPO, Reinforce++, RLOO, ReMax, DAPO) 모두 큰 격차(약 40%p 이상)를 보이며 최적과 거리가 멂을 확인.  
4. **증류(distillation) 비교**  
   - 강력한 교사 모델로부터 CoT를 증류한 경우(distillation)는 베이스 모델의 추론 범위를 실제로 확장함을 실험적으로 확인.  

# 문제 정의 및 제안 방법  

1. **해결하고자 하는 문제**  
   RLVR가 LLM에 자율적 탐색 및 새로운 추론 전략 학습을 유도하여 베이스 모델을 초월하는 추론 능력을 획득하는지 검증.

2. **평가 지표 및 수식**  
   - pass@k: 문제 하나당 k개의 샘플 중 정답이 하나라도 나오면 1로 간주.  
   - unbiased estimator:  

$$ \text{pass@k} = \mathbb{E}_{x\sim D}\Bigl[1 - \frac{\binom{n - c(x)}{k}}{\binom{n}{k}}\Bigr] $$  
     
여기서 n ≥ k 샘플 수, c(x)는 그 중 정답 개수.  

3. **모델 구조 및 학습 설정**  
   - 시작 모델: 다양한 크기의 Qwen2.5(7B/14B/32B)ㆍLLaMA-3.1-8B 등.  
   - RLVR 알고리즘: PPO, GRPO, Reinforce++, RLOO, ReMax, DAPO (KL 페널티 일부 실험).  
   - 학습 환경: zero-RL(수학), instruct-tuned 시작점(코드·시각).  
   - 하이퍼파라미터: temperature=0.6, top-p=0.95, rollout 8개(확장 실험에서 32개).  

4. **성능 향상**  
   - pass@1 기준 평균 정확도 최대 40%p 이상 향상.  
   - 그러나 pass@256 기준 베이스 모델 대비 최대 9%p 감소(예: Minerva 32B).  
   - ∆SE 분석에서 모든 알고리즘이 40%p 이상 격차 유지, 샘플링 효율 최적화 여지 큼.

5. **한계**  
   - **추론 범위 축소**: RLVR 훈련 진행 시 pass@k(k 큰 값) 지속적으로 감소.  
   - **새로운 패턴 부재**: CoT 경로·퍼플렉서티 분석으로 RLVR이 기존 분포 내 경로만 강화함 확인.  
   - **탐색 전략 부족**: 광대한 언어 행동 공간에서 효과적 탐색 미비.  
   - **단일턴 상호작용 한계**: Multi-turn 환경 상호작용 부재로 경험 확장 어려움.  

# 일반화 성능 향상 가능성  

- **증류와의 차별성**: distillation은 강교사 모델의 CoT를 학습하여 베이스 모델의 추론 능력을 실제로 확장, 즉 새로운 패턴을 도입함.  
- **향후 RLVR 방향**:  
  1. **효율적 탐색 전략** 개발로 베이스 모델 분포 외 영역 탐색 촉진.  
  2. **멀티턴 RL 에이전트** 패러다임 도입해 환경과 상호작용하며 새로운 경험 학습.  
  3. **계속적 스케일링** 및 롤아웃 다양성 확대를 통한 안정적 확장 검증.  

# 향후 연구에 미치는 영향 및 고려 사항  

- **추론 능력 확장 메커니즘 재고**: RLVR만으로는 한계가 명확하므로, 증류·지도학습·RL의 하이브리드 접근 필요.  
- **탐색 vs. 착취 균형**: 거대한 언어 공간에서 탐색(exploration)을 늘리고, 유효 보상 신호를 효과적으로 설계하는 연구 필수.  
- **환경 상호작용 디자인**: 실세계·시뮬레이터 기반 다중 턴 상호작용으로 에이전트 경험 강화.  
- **평가 프로토콜 다양화**: pass@k 외 추가 메트릭(예: 탐색 다양성, 반성적 행동 평가) 도입 검토.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/11da2c6d-f09e-4ce9-9cbb-c91f7be94458/2504.13837v2.pdf
