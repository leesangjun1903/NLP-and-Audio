# Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation

# 핵심 요약

**주장:** Transformer 모델의 위치 표현 방식이 입력 길이 **외삽(extrapolation)** 성능을 결정하며, 기존의 사인파 위치 임베딩은 훈련 시보다 긴 시퀀스에 대해 성능이 급격히 저하된다.  
**기여:**  
- **ALiBi (Attention with Linear Biases)**: 키-쿼리(dot-product) 어텐션 점수에 거리에 비례하는 선형 바이어스를 추가하여, 훈련 시보다 긴 시퀀스에 대한 성능 저하 없이 외삽을 가능하게 함.[1]
- 구현이 단순하며, 추가 파라미터 없이(메모리 오버헤드는 미미) 기존 코드 몇 줄만 수정해 적용 가능.  
- 위키텍스트 및 대규모 코퍼스 실험에서, 짧은 훈련 시퀀스(L)로도 긴 평가 시퀀스(Lvalid&gt;L)에서 동등하거나 더 나은 퍼플렉서티를 달성.

# 상세 설명

## 1. 해결하고자 하는 문제  
Transformer는 훈련 시 고정된 길이 L의 시퀀스를 사용하며, 추론 시 더 긴 시퀀스를 처리할 때 성능이 급격히 저하되는 **외삽 능력 부족**이 문제였다.[1]

## 2. 제안하는 방법: ALiBi  
- **핵심 아이디어:** 위치 임베딩을 더하지 않고, 각 헤드마다 거리에 비례하는 고정된 선형 바이어스 $$m_h$$를 어텐션 점수에 더함.  
- **수식:**  

$$
    \text{Attention}(Q,K,V)
    = \mathrm{softmax}\bigl(QK^\top + B\bigr)V,
  $$  
  
  여기서 $$B_{i,j}^{(h)} = -m_h \times (i - j)$$ (단, causal masking 포함).  
- **슬로프 설정:** 8~16 헤드 모델에 대해 $$m_h$$를 기하급수적으로 분포시키며, 별도 학습 없이 고정 사용.

## 3. 모델 구조  
기존 Transformer 구조에서 **위치 임베딩 추가 제거** 후, 쿼리-키 곱에만 선형 바이어스 행렬 $$B$$를 합산. 이외의 어텐션 블록·피드포워드 등은 변함없음.

## 4. 성능 향상  
- **위키텍스트-103**:  
  - L=512 훈련 시, Lvalid=3072에서 퍼플렉서티 18.40으로, L=3072 사인파 대비 동등 혹은 우수.[1]
  - 훈련 속도 1.8배 향상, 메모리 절감.  
- **대규모 코퍼스(1.3B 파라미터)**:  
  - L=512→Lvalid=1024 평가에서 사인파 L=1024 대비 퍼플렉서티 차이 0.06점, 메모리 6% 절감.  
  - L=1024→Lvalid=2048 평가에서 사인파 L=2048 대비 0.09점 우위, 속도 11% 향상.[1]

## 5. 한계  
- 외삽 시 실제로 L보다 긴 맥락을 학습적으로 활용하는지 여부 불분명(초기 토큰 혜택 감소가 주요 원인일 수 있음).  
- 슬로프 값은 고정하나, 최적 값 탐색을 완전히 배제하면 특정 도메인에서 최적화 여지 존재.

## 6. 일반화 성능 향상 관점  
ALiBi는 위치 정보 주입 방식을 변경함으로써, **짧은 훈련 시퀀스로도 더 긴 문맥에 외삽** 가능하게 하여, 모델의 맥락 의존성 일반화 능력을 크게 확장한다. 이는 특히 긴 문서 이해, 프로그래밍 코드 생성 등 장거리 의존성이 중요한 작업에 유리하다.

# 향후 연구에 미치는 영향 및 고려 사항

- **후속 연구 방향:**  
  - 외삽 시 실질적 맥락 이용 분석 및 개선 (예: 초기 토큰 커브스 완전 해소).  
  - 다양한 NLP 태스크(번역, 요약, 대화)에서 ALiBi 결합 효과 검증.  
  - 동적·컨텍스트 의존적 바이어스 학습 방식 도입으로 성능 극대화.  
- **고려점:**  
  - 모델 복합화에 따른 메모리 오버헤드 및 속도 영향.  
  - 타 위치 표현 기법과의 조합 가능성 및 상호 보완성.  

References  
 Press et al., “Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation,” ICLR 2022.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/47cf6cd6-c64a-4ab3-8cc1-79f29137f629/2108.12409v2.pdf)
