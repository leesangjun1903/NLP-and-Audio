# Teaching Language Models to Support Answers with Verified Quotes

## 1. 핵심 주장과 주요 기여

### 핵심 주장
이 논문의 핵심 주장은 대규모 언어 모델이 사실적 질문에 답할 때 발생하는 **환각(hallucination) 문제**를 해결하기 위해, 모델이 답변과 함께 **검증 가능한 인용문(verified quotes)**을 제공하도록 훈련시키는 것입니다. 이를 통해 사용자가 모델의 답변을 신뢰하기 전에 사실 확인을 할 수 있도록 지원합니다.

### 주요 기여
1. **Self-Supported Question Answering (SQA)** 태스크 정의 및 구현
2. **Inline Evidence 구문** 개발로 답변과 인용문을 하나의 문자열로 생성
3. **GopherCite**: 280B 파라미터 모델로 높은 품질의 지원 증거 생성
4. **강화학습 기반 훈련 파이프라인** 구축
5. **선택적 답변 거부 메커니즘** 개발

## 2. 문제 정의 및 제안 방법

### 해결하고자 하는 문제
- **환각 문제**: 언어 모델이 그럴듯하지만 거짓인 정보를 생성하는 현상
- **신뢰성 부족**: 사용자가 모델의 답변을 맹목적으로 수용하거나 직접 검증해야 하는 부담
- **증거 부족**: 기존 모델은 답변에 대한 구체적인 근거를 제공하지 않음

### 제안 방법

#### Inline Evidence 구문
모델이 다음과 같은 특수 구문을 사용하여 답변과 증거를 생성:
```
%<Claim>%(Document title)%[Quote from document]%
```

#### 수식적 접근
자회귀 언어 모델의 조건부 확률 분해:

$$ P(\text{answer, evidence}|\text{question, context}) = P(\text{answer}|\text{question, context}) \times P(\text{evidence}|\text{answer, question, context}) $$

#### 4단계 훈련 파이프라인

**Step 1: 데이터 수집**
- Few-shot prompting으로 초기 데이터 생성
- 인간 평가자가 품질 평가

**Step 2: 지도 학습 (SFT)**
- 인간이 긍정적으로 평가한 샘플로 모델 미세조정
- 60 SGD 스텝으로 제한하여 원본 모델의 능력 보존

**Step 3: 보상 모델 (RM) 훈련**
- 인간 선호도 데이터로 보상 모델 훈련
- 답변의 "Plausible"과 "Supported" 여부 예측

**Step 4: 강화학습 (RL)**
- A2C 알고리즘 사용
- KL divergence 페널티로 다양성 유지

## 3. 모델 구조

### 기반 모델
- **Gopher 280B**: DeepMind의 대규모 언어 모델 기반
- SentencePiece 토크나이저 (32,000 어휘)

### 검색 시스템
- Google Search API 활용
- 최대 4096 토큰의 컨텍스트 제공
- 여러 문서에서 정보 통합

### 제약 샘플링 (Constrained Sampling)
- 인용문이 원본 문서에서 정확히 추출되도록 보장
- 유한 상태 기계로 구현

## 4. 성능 향상 및 평가

### 주요 성능 지표
- **NaturalQuestions**: 80% Supported & Plausible 달성
- **ELI5**: 67% Supported & Plausible 달성
- **선택적 답변**: 70% 질문만 시도할 때 90% (NQ), 80% (ELI5) 성능

### 평가 방법
1. **Plausible**: 답변이 대화에서 합리적인 응답인지
2. **Supported**: 제공된 증거가 답변을 충분히 뒷받침하는지

### 성능 향상 요인
- **리랭킹**: 여러 후보 중 최고 보상 점수 선택
- **강화학습**: 인간 선호도 최적화
- **모델 크기**: 280B > 7B > 1.4B 순으로 성능 향상

## 5. 한계점

### 주요 한계
1. **Source Trustworthiness**: 신뢰할 수 없는 웹 소스 문제
2. **Cherry-picked Evidence**: 편향적 증거 선택 가능성
3. **Limited Reasoning**: 단일 인용문만 사용, 복합적 추론 부족
4. **TruthfulQA 성능**: "Supported"와 "True"의 불일치 문제

### 구체적 실패 사례
- Red Bull이 "날개를 준다"는 은유적 표현을 literal하게 해석
- 픽션 내용을 사실로 인용 (예: Fahrenheit 451의 내용)

## 6. 일반화 성능 향상 가능성

### 긍정적 측면
1. **다양한 도메인 적용**: NaturalQuestions와 ELI5에서 모두 효과 입증
2. **확장 가능한 구조**: 다른 언어 모델에도 적용 가능한 일반적 프레임워크
3. **실시간 정보 접근**: 검색 엔진 연동으로 최신 정보 활용
4. **모듈화된 설계**: 각 구성 요소를 독립적으로 개선 가능

### 제한적 측면
1. **도메인 특화성**: 사실적 QA에 특화되어 있음
2. **언어 의존성**: 영어 중심 평가
3. **검색 품질 의존**: Google Search 결과에 크게 의존

## 7. 향후 연구에 미치는 영향

### 긍정적 영향
1. **신뢰성 있는 AI**: 설명 가능하고 검증 가능한 AI 시스템 개발 방향 제시
2. **멀티모달 확장**: 텍스트 외 다른 모달리티로 확장 가능
3. **산업 응용**: 실제 제품에서 활용 가능한 실용적 접근법
4. **평가 방법론**: 새로운 평가 기준 (Supported & Plausible) 제시

### 연구 방향성
- **WebGPT**와 **LaMDA** 등 동시대 연구와의 비교 분석
- **Debate** 기반 접근법과의 결합 가능성
- **Constitutional AI** 방법론과의 통합

## 8. 향후 연구 시 고려사항

### 기술적 고려사항
1. **다중 증거 통합**: 여러 소스의 증거를 종합하는 방법
2. **추론 능력 강화**: 단순 인용을 넘어선 논리적 추론
3. **실시간 팩트체킹**: 동적 사실 검증 시스템
4. **편향 완화**: 체리피킹 및 확증 편향 해결

### 윤리적 고려사항
1. **정보 출처 신뢰성**: 악의적 정보원 필터링
2. **문화적 민감성**: 다양한 관점과 가치 반영
3. **투명성**: 모델 결정 과정의 명확한 설명

### 실용적 고려사항
1. **계산 비용**: 검색 및 리랭킹으로 인한 비용 증가
2. **지연 시간**: 실시간 응답을 위한 최적화 필요
3. **사용자 경험**: 복잡한 인용 정보의 직관적 제시

이 논문은 언어 모델의 신뢰성 문제를 해결하기 위한 실용적이고 확장 가능한 접근법을 제시하며, 향후 Trustworthy AI 연구의 중요한 기초를 마련했습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/8262b24a-b9dc-4c27-919a-7fe58d20f49d/2203.11147v1.pdf)
