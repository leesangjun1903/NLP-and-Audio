# Gemma: Open Models Based on Gemini Research and Technology

**주요 주장:**  
Gemma는 Google DeepMind의 Gemini 기술 및 연구를 바탕으로 개발된 경량·최신성 오픈 대형언어모델(LLM) 패밀리로, 2 B와 7 B 파라미터 두 가지 크기로 제공된다.[1]

**주요 기여:**  
-  Gemini 연구·기술을 계승한 아키텍처와 학습레시피 적용으로, 유사 규모 오픈 모델 대비 18개 텍스트 과제 중 11개에서 우수한 성능 달성.[1]
-  사전학습(pretrained) 및 지침추가 튜닝(instruction-tuned) 체크포인트 모두 공개하여 연구자·개발자에게 폭넓은 실험·응용 환경 제공.[1]
-  대화·추론·수학·코딩 등 다양한 벤치마크에서 척도별 최고성능 기록하고, 안전성·책임성 평가를 병행 수행.[1]
-  사전학습 데이터 필터링, 메모리제거 기법, RLHF(인간피드백강화학습) 등으로 일반화 및 책임 있는 배포 전략 제시.[1]

# 문제 정의 및 제안 방법

## 해결하고자 하는 문제  
- **오픈소스 LLM의 성능·안전성 한계 극복:** 기존 공개 모델들은 규모 및 성능 측면에서 대형·폐쇄형 모델에 밀리거나, 안전성 평가가 부족했다.  
- **다양한 응용에서의 일반화 및 책임 있는 배포:** 연구자·개발자가 안전성 우려 없이 광범위한 도메인에 모델을 적용할 수 있도록 지원 필요.[1]

## 제안 방법  

1. **Transformer 디코더 기반 아키텍처**  
   - 토큰 시퀀스 $$x$$에 대해, 각 레이어에서 self-attention과 피드포워드 네트워크(FFN)를 반복 적용  
   - Self-attention:  

$$
       \mathrm{Attn}(Q,K,V) = \mathrm{softmax}\Bigl(\frac{QK^\top}{\sqrt{d_k}}\Bigr)V
     $$  
     
  ($$Q,W_Q;\ K,W_K;\ V,W_V$$는 입력 $$x$$에 대한 선형투영).[1]

2. **아키텍처 개선**  
   - 다중-쿼리(Multi-Query) 어텐션: 2B 모델은 $$\text{num kv heads}=1$$, 7B 모델은 멀티-헤드 사용.[1]
   - RoPE(Rotary Positional Embeddings): 계층 단위 회전 위치 인코딩.[1]
   - GeGLU 활성화($$\mathrm{GeGLU}(x) = (W_1 x)\odot\mathrm{GELU}(W_2 x)$$).[1]
   - RMSNorm: 각 서브레이어 입력 정규화.[1]

3. **대규모 사전학습 및 필터링**  
   - 2 B 모델: 3 T 토큰, 7 B 모델: 6 T 토큰 학습  
   - 유해·저품질·평가 데이터 오염 방지 필터링 및 단계적 데이터 혼합(ablation으로 최적화).[1]

4. **지침추가 튜닝 및 RLHF**  
   - SFT: 합성·인간 제작 프롬프트-응답 데이터로 감독학습  
   - RLHF: Bradley–Terry 보상 모델 및 자체 강화학습으로 보상해킹 방지.[1]

# 모델 구조

| 파라미터             | 2 B 모델 | 7 B 모델 |
|----------------------|----------|----------|
| $$d_{\text{model}}$$ | 2048     | 3072     |
| 레이어 수            | 18       | 28       |
| FFN 숨김 유닛        | 32768    | 49152    |
| 헤드 수              | 8        | 16       |
| KV 헤드 수           | 1        | 16       |
| 어휘 크기            | 256128   | 256128   |
| 최대 문맥 길이       | 8192 토큰| 8192 토큰|

*Table 1: 핵심 모델 파라미터*[1]

# 성능 향상 및 평가

1. **학술 벤치마크**  
   - MMLU(5-shot): 64.3% → LLaMA2 13B(54.8%) 및 Mistral 7B(62.5%) 상회.[1]
   - GSM8K(수학, maj@1): 46.4% → 7 B 공개 모델 최상.[1]
   - HumanEval(pass@1): 32.3% → CodeLLaMA보다 높은 32.3%.[1]

2. **인간 선호도 평가**  
   - Gemma 7B IT vs. Mistral 7B: 도움말·안전성 과제에서 각각 61.2%, 63.5% 승리율.[1]

3. **안전성 벤치마크**  
   - RealToxicity, CrowS-Pairs, BBQ, Winogender 등 9개 지표 평가  
   - Gemma 7B IT, Mistral 대비 BBQ Ambiguous(1-shot) 86.06% vs. 58.97%.  

4. **메모리제거(Over-Memorization) 평가**  
   - 샘플 10 000문서 → 연속 50토큰 생성 매칭  
   - PaLM2 유사 모델과 비슷한 수준의 낮은 암기율.[1]
   - 민감 개인정보 암기 0건 관찰.[1]

# 한계 및 일반화 성능 개선 포인트

- **한계:**  
  - 주로 영어 데이터만 학습, 다국어·멀티모달 미지원.[1]
  - 모든 잠재적 사용 시나리오 안전성 보장 불가.[1]

- **일반화 성능 개선:**  
  - 스테이지별 데이터 혼합 및 필터링으로 도메인 특화 비율 조정 → 말뭉치 후반부 고품질 가중치 상승.[1]
  - GeGLU·RoPE·Multi-Query Attention 도입으로 소규모 모델에서도 추론능력 극대화.[1]
  - SFT+RLHF 병행으로 실제 대화·지침추종 과제서 인간 유사 응답 패턴 학습.[1]

# 향후 연구 영향 및 고려 사항

**영향:**  
- 오픈소스 LLM 연구 활성화: 사전·튜닝 모델·코드 공개로 투명성 증대 및 후속 안전 연구 촉진.[1]
- AI 생태계 접근성 확대: 경량 모델을 CPU·온디바이스 환경까지 적용 가능.[1]
- 학계·산업 협력 기반 제공: 책임 있는 툴킷·모델 카드·윤리지침 배포로 공동 대응 강화.[1]

**고려 사항:**  
- 더욱 도전적이고 견고한 벤치마크 개발 필요(복합 추론·사실성·적대적 입력).[1]
- 오픈 가중치 모델의 악용·보상 해킹 방지 위해 추가적 안전·거버넌스 메커니즘 도입 고려.  
- 다국어·멀티모달·저리소스 언어 적용을 위한 데이터·아키텍처 확장 연구 추진.  

Gemma는 오픈 LLM의 최전선에서 성능·안전성·책임성을 획기적으로 향상시켰으며, 향후 연구자·개발자의 공동 혁신과 안전한 AI 발전에 중요한 초석이 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/efc2dcda-0192-41bb-8eeb-72cd8c000db1/2403.08295v4.pdf)
