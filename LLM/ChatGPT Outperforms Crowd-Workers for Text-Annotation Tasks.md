# ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks

**핵심 주장 및 주요 기여**  
이 논문은 대형 언어 모델(LLM)인 ChatGPT가 전통적인 크라우드소싱(예: MTurk) 방식보다 텍스트 주석(annotation) 작업에서 더 높은 정확도와 일관성을 보이며, 비용은 약 30배 더 저렴하다는 점을 입증했다. 네 가지 서로 다른 데이터셋(콘텐츠 중재 트윗·뉴스·미국 의회 트윗·2023년 트윗)을 대상으로 0.2와 1.0 두 온도 설정으로 제로-샷 분류를 수행한 결과, ChatGPT는 대부분의 과제에서 MTurk 대비 약 25%p 높은 정확도를 달성했고, 두 번 실행 간의 상호부호 일치(intercoder agreement)는 최대 97%에 이르렀다.

***

## 1. 해결하고자 하는 문제  
- **문제 정의**: 연구자들이 텍스트 분류(classification)나 개념적 범주화를 위해 대규모 라벨링 데이터를 필요로 할 때, 기존에는 학습된 연구조교와 크라우드워커(MTurk) 방식이 주로 사용.  
- **한계**:  
  - 연구조교: 높은 품질이지만 비용과 시간이 많이 소요  
  - 크라우드워커: 저렴하지만 복잡한 작업에 품질 저하 우려  

***

## 2. 제안 방법  
ChatGPT API(‘gpt-3.5-turbo’)를 활용한 **제로-샷(zero-shot) 주석** 절차를 제시  
  
### 2.1 모델 입력 및 파라미터  
- **프롬프트**  
  “Here’s the [문장], please label it as [코드북 지침].”  
- **온도(temperature)**  
  - $$T=1.0$$ (기본값, 랜덤성 높음)  
  - $$T=0.2$$ (출력 일관성 증가)  

### 2.2 주석 태스크  
- **과제**:  
  1. 관련성(relevance)  
  2. 주제(topic)/정책 프레임(policy frames)  
  3. 스탠스(stance)  
  4. 문제/해결 프레임(problem/solution)  
- **데이터셋**:  
  - 2020–21 콘텐츠 중재 트윗  
  - 2020–21 뉴스 기사  
  - 2017–22 미국 의회 트윗  
  - 2023년 콘텐츠 중재 트윗  

***

## 3. 모델 구조  
ChatGPT 자체는 Transformer 기반 대규모 언어 모델로, 추가 학습 없이 프롬프트만을 통해 분류 태스크에 활용  
- **파라미터 수**: 공개되지 않음  
- **추가 컴포넌트**: 사용자 정의 입력 프롬프트 및 온도 조정 외 별도 모듈 없음  

***

## 4. 성능 향상 및 평가 지표  
| 데이터셋               | MTurk 정확도 | ChatGPT 정확도 (T=1) | ChatGPT 정확도 (T=0.2) | MTurk 상호부호 | ChatGPT 상호부호 (T=1) | ChatGPT 상호부호 (T=0.2) |
|------------------------|--------------|----------------------|------------------------|---------------|------------------------|--------------------------|
| 트윗 (2020–21)         | 약 45%       | 약 70%               | 약 70%                 | 56%           | 91%                    | 97%                      |
| 뉴스 (2020–21)         | 약 56%       | 약 81%               | 약 81%                 | 56%           | 91%                    | 97%                      |
| 의회 트윗 (2017–22)    | 약 58%       | 약 83%               | 약 83%                 | 56%           | 91%                    | 97%                      |
| 트윗 (2023)            | 약 34%       | 약 59%               | 약 59%                 | 56%           | 91%                    | 97%                      |

- **정확도(accuracy)**: 훈련된 연구조교의 골드 스탠다드와의 일치율  
- **상호부호(intercoder agreement)**: 동일 조건에서 두 번 주석한 결과의 일치율  
- **비용**:  
  - ChatGPT: 주석당 약 \$0.003  
  - MTurk: 주석당 약 \$0.09  

***

## 5. 모델 일반화 성능 향상 가능성  
- **온도 조정 효과**: 낮은 온도($$T=0.2$$)에서 일관성이 극대화되며, 정확도 저하 없이 재현성 확보  
- **과제 난이도 상관성**: 훈련조교의 상호부호가 낮은(난이도 높은) 과제일수록 ChatGPT의 MTurk 대비 성능 격차가 더 크게 나타나, 복잡한 태스크의 일반화 가능성 시사  
- **프롬프트 품질**: 예시가 부족한 특정 사례에서 오분류 발생 → 체인오브생각(chain-of-thought)·Few-shot 전략 적용으로 일반화 및 세부 정확도 추가 개선 여지  

***

## 6. 한계  
- **메모리 위험(memorization)**: 2023년 신규 트윗에서도 큰 이슈 없음  
- **다국어 적용성**: 본 연구는 영어 데이터셋만 사용  
- **Few-shot·체인오브생각 기법 미적용**: 추가 실험 필요  
- **모델 비교군 제한**: GPT-3.5-turbo 외 다른 LLM 비교 부족  

***

## 7. 연구적·실무적 영향 및 향후 고려사항  
**영향**  
- 대규모 주석 워크플로우에서 비용·품질 이중 이점 → 학계·산업에서 라벨링 파이프라인 혁신  
- 크라우드소싱 플랫폼 비즈니스 모델 재검토 촉발  

**향후 연구 시 고려점**  
- Few-shot 및 체인오브생각 프롬프트 설계로 복잡 태스크 일반화 강화  
- 비영어권 언어·다양한 도메인 적용성 평가  
- GPT-4·LLaMA·Claude 등 다양한 LLM과 비교 분석  
- 모델 내부 편향·신뢰성·투명성 확보 위한 윤리적·기술적 평가

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/878f152e-5a0b-4dd3-b10b-7055a92fadd2/2303.15056v2.pdf)
