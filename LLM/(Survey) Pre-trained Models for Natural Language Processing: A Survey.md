# Pre-trained Models for Natural Language Processing: A Survey

**주요 주장 및 기여 요약**  
이 논문은 자연어처리(NLP) 분야에서 **사전학습 모델(Pre-trained Models, PTMs)** 의 현황을 총망라하여 정리한다. 주요 기여는 다음과 같다.  
- **종합적 검토**: 분산 표현, 인코더 구조, 사전학습 과제, 확장 기법, 전이 및 응용을 아우르는 포괄적 리뷰를 제시한다.  
- **새로운 분류체계**: PTM을 (1) 표현 유형, (2) 모델 구조, (3) 사전학습 과제, (4) 시나리오별 확장 네 가지 관점으로 체계적으로 분류한다.  
- **풍부한 리소스 제공**: 오픈소스 구현체, 시각화 도구, 말뭉치, 논문 목록 등 연구·개발에 유용한 자료를 집대성하였다.  
- **미래 방향 제시**: 현황의 한계와 향후 연구 과제를 명확히 정리하였다.[1]

***

## 1. 문제 정의  
- **데이터 부족과 일반화 한계**: 대부분의 감독형 NLP 과제는 말뭉치가 작아 심층 신경망이 과적합되기 쉽다. 전통적 신경망은 얕았으며 컨텍스트 부재 문제가 있었다.  
- **사전학습의 필요성**: 대규모 비지도 말뭉치로부터 언어 표현을 학습하여 하위 과제에 전이하면, 초기화 개선·수렴 가속·과적합 방지 효과를 얻는다.[1]

***

## 2. 제안된 방법  
### 2.1 모델 구조  
- 비순차적 **Transformer**(자기-어텐션 기반)와 순차적 RNN/CNN을 포함.  
- Transformer는 장거리 의존성 포착에 유리하나 계산량이 $$O(n^2)$$이다.[1]

### 2.2 사전학습 과제  
1) **언어 모델링(LM)**: 순방향·역방향·양방향 방식으로 토큰 확률을 최대화.  
2) **마스킹 언어 모델링(MLM)**: 입력의 일부 토큰을 마스킹하고 예측.  

$$\displaystyle \mathcal{L}\_{\text{MLM}}=-\sum_{m=1}^M\log p(x_m\mid x_{\setminus m})$$.[1]

3) **Permutation LM(PLM)**: 토큰 순서를 임의 배치하여 예측 순서를 학습.  
4) **잡음 제거 오토인코더(DAE)**: 문장을 변형 후 원문 복원 학습.  
5) **대비 학습(Contrastive Learning)**: 긍정·부정 샘플 구분으로 표현 학습.  
6) **문장 관계 예측(NSP, SOP)**: 다음 문장 예측과 문장 순서 예측으로 문장 간 관계 학습.  

### 2.3 전이 학습  
- **피처 추출**: PTM을 고정하고 상위 레이어만 학습.  
- **미세조정**: PTM 전체 파라미터를 하위 과제에 맞춰 조정.  
- **프롬프트 튜닝**: 입력에 자연어 템플릿(프롬프트)을 삽입하여 MLM 과제로 전환함으로써 미세조정 없이도 성능 확보.[1]

***

## 3. 성능 향상 및 한계  
- **성능 향상**: PTM 적용 시 GLUE 평균 점수 80→88, 주요 QA·개체명 인식·요약에서 SOTA 달성 사례 다수.[1]
- **한계**  
  - **계산·메모리 비용**: 깊은 구조와 긴 시퀀스 처리 시 과도한 자원 소모.  
  - **일반화 한계**: 도메인 특화 말뭉치 부재 상황에서 생성·추론 성능 저하.  
  - **해석 가능성 부족**: 내부 메커니즘 불투명, 어텐션 기반 해석 신뢰도 논란.  
  - **취약성**: 텍스트 어택에 민감하여 신뢰도 문제 제기.  

***

## 4. 모델의 일반화 성능 향상 방안  
1) **도메인·지식 주입**: 외부 지식그래프, 도메인 말뭉치로 추가 사전학습 or 어댑터 활용.[1]
2) **모델 압축·경량화**: 지식 증류·모듈 교체·프루닝·양자화·조기 종료 등으로 실시간 추론 가능성 확보.[1]
3) **작업 특화 사전학습**: GSG(요약), Label-Aware MLM(감성분석) 등 과제별 SSL 설계.[1]
4) **프롬프트 기반 효율 학습**: 연산량 적은 프롬프트 튜닝, 제로/소수 샷 전이 학습에서 뛰어난 효율성.[1]

***

## 5. 향후 연구 영향 및 고려 사항  
- **효율적 대규모 학습**: 자원 제약 환경에서 거대 PTM 상한선과 학습 기법 최적화 연구가 필수적.  
- **아키텍처 혁신**: 긴 문맥 효율 모델·약한 계산 복잡도 구조 탐색 필요.  
- **전이 절차 최적화**: 어댑터·프롬프트 등 파라미터 효율적 전이 기법 발전.  
- **신뢰성·해석 가능성 강화**: XAI 기법과 어드버서리 방어 연구로 안전한 적용 보장.  
- **다중모달·다국어 융합**: 상호작용하는 멀티모달 PTM과 고품질 크로스링구얼 모델 확장.  

이 설문조사는 PTM의 **이론, 구현, 전이, 응용**을 망라하고, NLP 연구·개발자로 하여금 **모델 선택**, **전이 전략**, **미래 방향**을 체계적·실용적으로 판단하도록 돕는다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/840cf34d-c360-4b9b-b906-1261a60934c2/2003.08271v4.pdf)
