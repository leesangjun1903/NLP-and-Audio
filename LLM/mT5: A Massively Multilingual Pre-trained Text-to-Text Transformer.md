# mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer

## 핵심 주장 및 주요 기여
mT5는 Google의 T5 모델을 101개 언어로 확장한 대규모 멀티링구얼 텍스트-투-텍스트 사전학습 모델이다.  
주요 기여는 다음과 같다.  
- mC4: Common Crawl 기반 101개 언어의 초대규모 말뭉치 구축  
- 모노링구얼 T5 사전학습 방식을 그대로 계승하여 **텍스트-투-텍스트** 통일 형식을 유지  
- 저자원 언어 부스트를 위한 언어 샘플링 지수 α=0.3 채택  
- 대형 모델(13B 규모)이 멀티태스크 벤치마크 XTREME 전항목에서 최첨단 성능 경신  

## 해결하고자 하는 문제
기존 사전학습 언어모델(T5, BERT, mBERT 등)은 영어 또는 백여 개 언어 위키피디아에 국한되어, **다양한 언어 간 일반화 능력**이 제한적이었다.  
- 특히 인터넷 저자원 언어의 데이터 부족으로 인해 모델이 고자원 언어에 편향됨  
- 제너레이티브 태스크(번역·요약·질문응답)에서 **제로샷** 성능 저하  

## 제안하는 방법 및 수식
1. **mC4 구축**  
   - Common Crawl 71회 월간 스크래핑 데이터에서 cld3 언어 감지  
   - 최소 10,000개 페이지 이상 언어만 포함, 107개(실질 101개) 언어  
   - 라인 길이 필터(200자 이상 3라인)로 노이즈 제거  

2. **언어 샘플링 분포**  

$$
     p(L) \propto |L|^\alpha,\quad \alpha=0.3
   $$  
   
   저자원 언어(|L| 작은 언어)에 대한 샘플링 확률을 부스팅  

3. **모델 구조**  
   - T5.1.1 기반 encoder-decoder Transformer  
   - GeGLU 활성화, 단일 교사강제 최대우도 학습, dropout 제거  
   - 어휘 크기 250K wordpiece, byte-fallback 지원  

4. **사전학습 목표**  
   span-masking(15% 토큰, 평균 span 길이 3)으로 **텍스트 복원**  

5. **제로샷 세대 오류 억제**  
   - fine-tuning 시 XQuAD 등 영어 데이터에만 맞추면 비영어 예측에서 “불법 출력”(문맥에 없거나 영어 번역) 발생  
   - fine-tuning에 mC4 언어 예측 태스크를 100:1 비율로 섞어 학습(α=0.1)함으로써 다언어 생성 능력 보존  

## 성능 향상
- XTREME 벤치마크  
  - XNLI, PAWS-X, WikiAnn NER, XQuAD, MLQA, TyDi QA 등에서 SOTA 달성  
  - 모델 규모 확장에 따라 제로샷과 번역훈련 간 격차 감소(XXL 모델: 제로샷 84.5→번역훈련 87.8)  
- 단일 언어 전용 T5 대비 손실 보상  
  - SQuAD 벤치마크에서 mT5-XXL은 T5-XXL에 근접한 성능 달성  

## 한계 및 일반화 성능 향상 가능성
- **저자원 언어 성능:** α 조정이나 추가 위키피디아 데이터 투입 시 일부 저자원 언어에서 과적합 또는 과소적합 발생  
- **사전학습 데이터 노이즈:** Common Crawl 필터링만으로는 노이즈 완전 제거 어려움  
- **생성 제약 부족:** span 순위화 같은 태스크 특정 제약이 없으면 불법 예측 완전히 억제 불가  
- **일반화 제안:**  
  - **도메인-태스크 적응형 사전학습**(fine-tuning 시 소량 원자료 혼합) 확대 적용  
  - **하이브리드 인퍼런스**로 생성 모델과 추출 모델 결합  
  - **노이즈 필터링 강화**를 위한 언어별 언어모델 기반 점검  

## 향후 연구 영향 및 고려 사항
- **초대규모 다언어 사전학습**의 타당성 입증으로, **극저자원 언어** 및 **코드스위칭** 태스크로 확장 연구 가능  
- **범태스크 텍스트-투-텍스트 통일**이 강력한 제너레이티브 및 분류 성능을 모두 달성할 수 있음을 제시  
- 후속 연구에서는 **효율적인 α 자동 튜닝**, **노이즈 검증 체계**, **제한된 연산 자원 환경**에서의 경량화 연구가 필요하다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/c7bc9000-0093-48d7-8b37-737f052793c6/2010.11934v3.pdf
