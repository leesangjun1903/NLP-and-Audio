# ConvBERT: Improving BERT with Span-based Dynamic Convolution

**주요 주장**  
ConvBERT는 BERT의 **연산적 중복성**을 해소하고 **효율성** 및 **성능**을 동시에 개선하기 위해, 기존의 전(全)어텐션(self-attention)을 지역 의존성(local dependency)에 특화된 **스팬 기반 동적 합성곱(span-based dynamic convolution)**과 혼합하는 새로운 블록을 제안한다.[1]

**주요 기여**  
- **스팬 기반 동적 합성곱**(Span-based Dynamic Convolution): 입력 토큰의 국소 스팬(span)을 고려해 컨볼루션 커널을 동적으로 생성하여, 동일 토큰의 문맥별 의미 차이를 구별 가능하게 함.[1]
- **혼합 어텐션 블록**(Mixed Attention): 글로벌 의존성은 전어텐션으로, 국소 의존성은 스팬 기반 동적 합성곱으로 캡처하여 두 연산의 장점을 결합.[1]
- **병목 구조**(Bottleneck Attention): 키·쿼리·밸류 투영 차원을 축소하고 어텐션 헤드 수를 줄여 중복을 완화하고 연산량을 절감.[1]
- **그룹화 선형 연산**(Grouped Linear Operator): FFN(feed-forward network) 내 선형층을 그룹화해 파라미터 수를 줄이면서 표현력을 유지.[1]
- 실험 결과, **ConvBERTBASE**는 GLUE 평균 점수 86.4를 달성하여 BERTBASE 대비 +5.5, ELECTRABASE 대비 +0.7 향상을 보였으며, 훈련 비용과 파라미터 수가 감소함.[1]

***

## 1. 해결하고자 하는 문제

BERT는 모든 토큰 쌍에 대해 어텐션 가중치를 계산하는 전어텐션의 **이차적 복잡도**로 인해 메모리 및 연산 비용이 크다. 실제로 다수 헤드는 국소 의존성만 학습하며, 전체 입력을 쿼리할 필요 없이 **연산 중복**이 발생한다. 이를 해소하고자, 자연어의 국소 의존성을 더욱 효율적으로 학습할 수 있는 연산으로 일부 어텐션 헤드를 대체하는 방법을 모색한다.[1]

***

## 2. 제안 방법

### 2.1 스팬 기반 동적 합성곱 연산

- **경량 합성곱**: 채널 간 가중치를 공유한 심층 분리 합성곱(depth-wise separable convolution)  

$$
    \mathrm{LConv}(X, W)_i = \sum_{j=1}^k W_j \, X_{i+j-1}
  $$
  
- **동적 합성곱**: 현재 토큰 $$X_i$$로부터 커널을 생성  

$$
    \mathrm{DConv}(X;W_f)_i = \mathrm{LConv}\bigl(X,\ \mathrm{softmax}(W_f\,X_i)\bigr)
  $$
  
- **스팬 기반 동적 합성곱**: 국소 스팬 $$S_i$$에서 키 $$K_s$$를 추출해 커널 생성  

$$
    K_s = \mathrm{Conv_{dw}}(X),\quad
    W_i = \mathrm{softmax}\bigl(W_f \,(Q_i \odot K_{s,i})\bigr)
  $$  

$$
    \mathrm{SDConv}(Q,K_s,V)_i = \mathrm{LConv}\bigl(V,\ W_i\bigr)
  $$

### 2.2 혼합 어텐션 블록

쿼리 $$Q$$는 공유하되, 키 $$K$$는 전어텐션용 $$K$$와 동적 합성곱용 $$K_s$$로 분리하여 연산을 수행한 뒤 출력을 결합한다:

$$
  \mathrm{MixedAttn}(Q,K,K_s,V)
  = W_o\bigl[\mathrm{SelfAttn}(Q,K,V)\;\|\;\mathrm{SDConv}(Q,K_s,V)\bigr]
$$

### 2.3 병목 구조 및 그룹화 선형 연산

- **Bottleneck Attention**: 입력 임베딩 차원을 $$\tfrac{1}{r}$$로 축소하고 헤드 수를 비례 감소.  
- **Grouped Linear Operator**: FFN의 내부 선형층을 $$g$$개 그룹으로 나누어 병렬 처리 후 이어붙임:

$$
    \mathrm{GFFN}(H) = \bigl[\mathrm{FC}_1(H_i)\bigr]_{i=1}^g
  $$

***

## 3. 성능 향상 및 한계

### 3.1 성능 개선  
- **GLUE**  
  ConvBERTBASE(106M 파라미터)는 GLUE 86.4점으로 ELECTRABASE(110M) 대비 +0.7, BERTBASE 대비 +5.5 향상.[1]
- **훈련 비용**  
  FLOPs 기준 약 14×로, ELECTRABASE(49×)보다 낮은 연산량.  

### 3.2 모델 한계 및 구현 이슈  
- **커널 크기**: 커널 크기가 입력 길이를 모두 덮으면 추가 이득이 감소하며, 적절한 크기(k=9)가 필요.[1]
- **GPU/TPU 최적화 부족**: 현재 CPU에서는 속도 개선이 뚜렷하나, GPU/TPU 저수준 최적화는 미흡.[1]
- **문맥 범위 제한**: 합성곱의 국소 한계를 넘어서는 의존성은 여전히 전어텐션에 의존.  
- **하이퍼파라미터 민감도**: 병목 비율, 그룹 수, 커널 크기 등의 조정이 성능에 크게 영향.

***

## 4. 일반화 성능 향상 관점

1. **지역성 학습 강화**  
   스팬 기반 동적 합성곱이 동일 토큰의 다양한 문맥 표현을 구별하여 국소 패턴을 세밀히 포착하므로, 도메인 변경 시에도 문맥 민감도가 유지되어 일반화에 유리하다.
2. **모델 용량 분산**  
   병목 구조와 그룹화 FFN이 과도한 파라미터 집중을 방지하고, 각 헤드 및 그룹이 더 압축된 표현을 학습하여 오버피팅 감소에 기여한다.
3. **어텐션 집중도 완화**  
   BERT 대비 어텐션 대각 집중도(diagonal concentration)가 낮아져 전역 의존성 포착이 향상되고, 국소와 전역 정보를 균형 있게 학습.[1]

***

## 5. 향후 연구 영향 및 고려사항

ConvBERT는 **백본 설계 혁신**의 중요성을 부각하며, 다음 연구에서 고려할 점은 다음과 같다:

- **대규모 모델 확장**: 스팬 기반 합성곱을 대형 트랜스포머(수십억 규모)에 적용하고, 대규모 학습 비용 절감 효과 검증.  
- **하드웨어 최적화**: GPU/TPU를 위한 저수준 합성곱·어텐션 커널 구현으로 실제 추론 속도 개선.  
- **하이브리드 구조**: 합성곱·어텐션 비율, 병목 비율 자동 탐색(AutoML) 기법 연구.  
- **도메인 적응**: 국소 패턴 학습이 중요한 의료·법률·과학 텍스트에 특화된 전이 학습 성능 분석.  
- **보안·프라이버시**: 효율적 모델이 개인 텍스트 분석에 활용되어 발생할 수 있는 **사생활 침해** 위험 완화 방안 연구.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3e2d4e94-2586-45e6-b51d-716a4a7edd7b/2008.02496v3.pdf)
