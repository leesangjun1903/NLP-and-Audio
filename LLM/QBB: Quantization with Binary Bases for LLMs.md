# QBB: Quantization with Binary Bases for LLMs

## 1. 핵심 주장 및 주요 기여 요약  
“Quantization with Binary Bases (QBB)” 논문은 대형 언어 모델(LLM)에 대한 포스트 트레이닝 양자화에서 4비트 이하 저비트 극한 상황을 다루며, **모델의 거의 모든 곱셈 연산을 제거**하고 합산만으로 가중치 연산을 수행하는 새로운 이진 기반 분해 방식을 제안한다. 주요 기여는 다음과 같다.  
- **이진 기저 분해(Binary Bases Decomposition)**: 가중치 행렬 $$W$$를 이진 행렬 $$B_i$$와 스케일 벡터 $$\alpha_i$$의 선형 조합 $$\sum_i \alpha_i B_i$$으로 근사(식 (1)).  
- **분할-블록 반복 최적화(Block Coordinate Descent)**: 각 이진 행렬을 순차적으로 고정·변경하며 $$\ell_2$$ 오차 $$\|W - \sum_i \alpha_i B_i\|^2$$를 최소화(식 (4)).  
- **데이터-프리 교사-학생 지식 증류(Data-Free Distillation)**: 입력 데이터 없이 합성 토큰 시퀀스로부터 교사 모델의 로짓을 생성하여, 스케일 벡터만 추가 보정(식 (7)).  
- **실험적 성능**: LLaMA-2 및 Phi-2 계열에서 GPTQ, OmniQuant 등 기존 최첨단 방법 대비 위키텍스트2 기준 퍼플렉서티(PPL)에서 동등하거나 우수한 결과 달성.  

## 2. 문제 정의, 제안 방법 및 모델 구조  
### 문제 정의  
- LLM 양자화 시 4비트 이하 극저비트에서 곱셈 비용 감소와 정확도 저하 억제가 어려움.  
- 특히 작은 규모(≤7B) 모델에서 4비트 PTQ도 성능 손실이 큼.  

### 제안 방법  
1. **이진 기저 초기화 (Sec. 3.1)**  
   - 첫 번째 기저:  

$$
       B_1 = \mathrm{sign}(W), \quad
       \alpha_1 = \frac{1}{\text{cout}}\|W\|_{1,\text{col}}
     $$  
   
   - 잔차 기반 추가 기저:  

$$
       \Delta_i = W - \sum_{j=1}^{i-1} \alpha_j B_j,\quad
       B_i = \mathrm{sign}(\Delta_i),\quad
       \alpha_i = \frac{1}{\text{cout}}\|\Delta_i\|_{1,\text{col}}
     $$  

2. **반복 최적화**  
   - 손실 함수:  

$$
       L = \bigl\|W - \sum_{i=1}^N \alpha_i B_i\bigr\|_2^2
     $$  
   
   - 블록 좌표 하강법: 각 단계에서 한 개 $$B_i$$만 업데이트하고, 나머지는 고정. 스케일 $$\alpha_i$$는 매단계 모두 갱신(그림 2).  

3. **데이터-프리 홀리스틱 교정 (Sec. 3.2)**  
   - 무작위 토큰으로 시퀀스 생성 후 교사 모델 로짓 $$p^T$$ 산출.  
   - 학생 모델 로짓 $$p^S$$과의 MSE 기반 지식 증류:  

$$
       L_{\text{MSE}} = \sum_{i,v}(p^T_{i,v} - p^S_{i,v})^2,\quad
       L_{\text{feat}} = \sum_{l,i}\|f^T_{i,l} - f^S_{i,l}\|_2^2
     $$  
   
   - 스케일만 최적화하여 빠른 수렴과 데이터 효율성 달성.  

### 모델 구조  
- 기존 Transformer 구조의 선형층 가중치를 대상으로, 각 레이어별로 위 과정을 적용.  
- 입출력 아키텍처 변경 없이 오직 가중치 표현만 이진 기저 조합으로 대체.  

### 성능 향상 및 한계  
- **퍼플렉서티 개선**: Phi-2(2.7B)에서 4기저(QBB N=4) 적용 후 10.40 PPL 달성(기존 4-bit PTQ 대비 0.7 이상 개선)[ Table 2 ].  
- **비용 절감**: 곱셈을 전부 제거하고 합산만으로 구현 가능하여 에너지 효율, 실행 속도 이론적 개선 기대.  
- **한계**:  
  - 실제 하드웨어 구현 미실시로 이론적 이득만 제시.  
  - 원본 모델 바이어스나 데이터 편향이 함께 전이될 수 있음.  
  - 작은 모델일수록 잔차 보정 어려워 수렴 불안정성 존재.  

## 3. 일반화 성능 향상 가능성  
- **잔차 기반 기저 분해**: 전 레이어-입력 무관(init) 후, 교사-학생 증류를 통해 전체 레이어 순차적 보정으로 누적 오차 최소화. 이는 다양한 입력 분포에 대한 일반화 능력을 강화.  
- **합성 데이터 필터링**: 불확실성이 큰 시퀀스 우선 학습으로, 학생 모델이 일반적 어려운 사례에 집중해 학습, 과적합 감소 및 일반화 성능 개선(Fig. 5).  
- **교사 모델과의 블록 교체(swap)**: 훈련 초반 더 약한 신호로부터 점진적 난이도 상승, 학생 모델이 광범위한 중간 성능 범위를 경험하며 학습 곡선 부드럽게 함.  

## 4. 향후 연구에 미치는 영향 및 고려 사항  
- **하드웨어 친화적 양자화**: 완전 곱셈 제거 접근은 저전력·고속 AI 칩 설계에 새로운 패러다임 제시.  
- **다양한 비트 수 확장**: N>4 또는 활성화 이진화(1-bit)로 확장 시, 계산 복잡도·정확도 균형점 재탐색 필요.  
- **동적 기저 개수 적응**: 레이어별 난이도에 따라 기저 개수를 자동 배정하는 적응형 메커니즘 연구.  
- **편향·안정성 연구**: 학생 모델 수렴 불안정 해소 및 편향 완화를 위한 정규화 기법 또는 소규모 데이터셋 기반 보완 훈련 방안 모색.  

QBB는 LLM 양자화 분야에서 저비트 극한 시나리오와 에너지 효율을 동시에 추구하는 새로운 가능성을 제시하며, 하드웨어-알고리즘 공동 설계 연구를 확장하는 중요한 기여라 할 수 있다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/109a417a-23f1-472e-bf7b-29329b802a40/6858_QBB_Quantization_with_Bin.pdf
