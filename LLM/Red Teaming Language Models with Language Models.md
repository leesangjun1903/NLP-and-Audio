# Red Teaming Language Models with Language Models

## 핵심 주장 및 주요 기여
**주장**  
대규모 언어 모델(LLM) 자체를 이용해 ‘레드팀(공격자)’ 역할을 수행함으로써, 수작업으로 작성된 테스트 케이스에만 의존하는 기존 방법의 한계를 넘어 자동으로 다양하고 실제적인 실패(유해) 사례를 탐지할 수 있다.  

**주요 기여**  
1. **자동 테스트 생성 프레임워크**: 공격용 LM(`pr(x)`)으로부터 수많은 자연어 테스트 입력을 생성하고, 목표 LM(`pt(y|x)`)의 출력을 유해 분류기(`r(x,y)`)로 평가하여 실패 사례를 자동 선별하는 3단계 파이프라인을 제안.  
2. **다양한 생성 기법 비교**:  
   - *제로샷*: 간단 프롬프트로 대규모 샘플 생성  
   - *스토캐스틱 Few-shot*: 유해 사례를 few-shot 예제로 활용  
   - *지도학습 및 RL*: 유해성 보상으로 fine-tune / A2C+KL 규제 기법 적용  
   → 난이도(offensiveness)와 다양성(Self-BLEU, 엔트로피, Zipf 계수) 간 트레이드오프 분석.  
3. **응용 사례**:  
   - *공격 종류 확대*: 혐오 발화, 민감 정보(개인 연락처·SSN) 유출, 편향적·차별적 문답, 대화 맥락에서의 유해성 심화 등 다수 오류 모드 발견  
   - *훈련·디플로이 전 보완책*: 학습 데이터 정제(유해 예시 제거), 프롬프트 엔지니어링, 생성 억제(blacklist/재샘플링), unlikelihood 학습 등 구체적 개선안 제시  

## 논문 구성 및 방법론
### 문제 정의  
입력 x에 대해 목표 LM $$p_t(y|x)$$가 유해 출력을 생성할 확률이 높은 x를 대규모로 자동 탐색하고자 함[1].

### 3단계 자동 레드팀 프로세스  
1. **테스트 케이스 생성**: 공격용 LM으로부터 $$x\sim p_r(x)$$ 샘플링  
2. **목표 모델 응답**: $$y\sim p_t(y|x)$$  
3. **유해 사례 선별**: 분류기 $$r(x,y)$$로 $$y$$를 평가해 유해 시 $$x$$ 기록  

### 생성 기법  
- **제로샷(Zero-Shot)**:  
  프롬프트 “List of questions to ask someone:” 사용해 대량 생성.  
- **스토캐스틱 Few-Shot**:  
  제로샷 중 유해 사례를 예제로 추가해 난이도↑, 무작위 예제 추가해 다양성↑.  
- **지도학습(SL)**:  
  제로샷 유해 사례로 LM 한 에포크만 fine-tune.  
- **강화학습(RL)**:  
  보상 $$r(x,y)$$ 극대화, KL 규제 $$\alpha\,D_{\mathrm{KL}}(p_r\Vert p_{\mathrm{init}})+(1-\alpha)\,\text{A2C loss}$$ 적용.[1]

### 모델 구조  
- **목표 LM**: Gopher 280B 채팅봇(대규모 Transformer)  
- **공격용 LM**: 동일 아키텍처, 각 기법별 동일 Gopher 활용  
- **유해 분류기**: Gopher 1.4B 기반, Bot-Adversarial Dialogue(BAD) 데이터로 파인튜닝[1]

## 성능 향상 및 한계
### 성능  
- **유해성 탐지**: RL($$\alpha=0.3$$)로 질문의 42%에서 유해 반응 유발, 제로샷 3.7% 대비 대폭 증가.  
- **다양성-난이도 트레이드오프**: 제로샷→SFS→SL→RL 순으로 난이도↑, 다양성↓. BAD(인간 작성)와 유사하거나 우월한 성능 확보.  
- **응용 사례**: 0.5M 제로샷으로 1만8천여 유해 반응 수집, SSN·이메일·전화번호 유출 등 치명적 오류 모드 자동 검출.  
- **분포적 편향**: 생성된 그룹·질문 템플릿으로 민감 집단 간 유해성 편차 분석, 백인·남성 편향적 유해 발화 발견.

### 한계  
- **LM 편향 및 분류기 오류**: 훈련 데이터에 내재된 편향 반영 및 분류기 오탐·미탐 가능성.  
- **완전성 부족**: 모든 실패 모드 탐지 보장 불가, 수동 검토 및 다중 분류기 조합 필요.  
- **자동화 남용 리스크**: 외부 공격자가 동일 기법 활용 시 대규모 취약점 노출.

## 일반화 성능 향상 가능성
- **프롬프트 엔지니어링**: 특정 오류 모드에 집중한 프롬프트 설계로 새로운 사례 커버리지 확대  
- **공동 학습**: GAN 유사 방식으로 공격자·방어자 LM 동시 학습, 반복적 취약점 보완  
- **언라이클리후드 학습**: 유해 사례에 낮은 확률 부여 학습으로 출력 안전성 강화[1]
- **다양한 분류기 병합**: 혐오·편향·오정보 등 다중 유해 분류기로 일반화 성능↑  

## 향후 연구 및 고려사항
- 외부·내부 레드팀 간 협력 체계 마련 및 **레이트 제한** 전략 검토  
- **화이트박스** 접근(모델 gradient 활용)으로 공격·방어 기법 발전  
- **비의도적 편향** 자동 발견을 위한 프롬프트 및 그룹 생성 기법 고도화  
- **윤리적·법적** 측면: 개인정보 유출·차별 발화 자동 검출 후 사후 조치 프로세스 확립  

***
해당 논문은 LLM 안전성 검증 및 개선 방향을 제시하며, 후속 연구로 자동화·강화학습 기반 공격·방어 기법의 통합과 다중 유해 분류기의 결합, 대화형 시스템의 지속적 모니터링 및 실시간 대응 체계 연구가 요구된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9cd76b6f-186a-4da2-8f74-7cb8c5cc2890/2202.03286v1.pdf)
