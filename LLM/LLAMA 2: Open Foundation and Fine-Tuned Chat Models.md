# Llama 2: Open Foundation and Fine-Tuned Chat Models

## 1. 핵심 주장과 주요 기여
Meta AI가 공개한 Llama 2 모델군은 7B, 13B, 70B 파라미터 규모의 사전학습(pretrained) 모델과 대화용으로 추가 튜닝된(LLama 2-Chat) 모델을 포함한다.  
- **주장**: 고성능의 오픈소스 LLM이 닫힌 모델(예: ChatGPT)에 필적하거나 이를 대체할 수 있다.  
- **기여**:  
  1. 40% 데이터 증량, 컨텍스트 길이 2→4k 토큰 확대, Grouped-Query Attention 도입.  
  2. 지도학습(SFT) 및 RLHF(인간 피드백 기반 강화학습)으로 우수한 대화 성능 및 안전성 확보.  
  3. Ghost Attention 등 멀티턴 일관성 기법 제안.  
  4. 공개된 모델 및 튜닝·책임 사용 가이드 제공.

## 2. 문제 정의·제안 방법·구조·성능·한계
### 2.1 해결 과제  
- 오픈소스 기반 LLM은 전세계적 접근성과 투명성은 높으나, 실제 서비스 수준의 대화 품질과 안전성은 부족했다.  
- 주요 목표: 오픈소스로도 상업용 수준의 대화 모델 구현.

### 2.2 제안 방법  
1. **사전학습(Pretraining)**  
   - 2조 토큰 규모 공개 데이터로 학습(기존 대비 40%↑).  
   - 컨텍스트 길이 4k 토큰, GQA(Grouped-Query Attention) 적용.  
2. **지도학습(SFT)**  
   - 27.5K 고품질 대화 지침 데이터로 초기 튜닝.  
   - 손실 함수: 정답 토큰만 Backpropagate.  
3. **강화학습(RLHF)**  
   - 이진 비교 선호 데이터를 140만 건 수집→보상 모델(Helpfulness RM, Safety RM) 학습.  
   - Proximal Policy Optimization(PPO) + 거부 샘플링(각 프롬프트별 N 샘플 중 최고 보상 선택) 반복  
   - 보상 함수:  

```math
R_c(g|p)=\begin{cases}R_s(g|p)&\text{if is\_safety(p) or }R_s<0.15\\R_h(g|p)&\text{otherwise}\end{cases}
```  

$$\mathit{Reward}= \mathrm{logit}(R_c)$$ 백색화 – β·KL 페널티로 안정화  

4. **Ghost Attention (GAtt)**  
   - 시스템 메시지를 첫 턴에만 입력→이후 단계에서는 손실을 0으로 설정해 토큰 위치 왜곡 없이 주의를 유지.  
5. **안전 미세조정**  
   - 안전 카테고리별 대화 예시 지도학습, 안전 RLHF, 안전 문맥 증류(Context Distillation) 기법으로 강화.

### 2.3 모델 구조  
- 표준 Transformer 기반(예: RMSNorm, SwiGLU, rotary PE).  
- 7B·13B 모델은 통상적 MHA, 34B·70B는 GQA.  
- 토크나이저: 32K BPE.

### 2.4 성능 향상  
- **사전학습 성능**: MMLU·BBH·AGI Eval 등 다수 벤치마크에서 Llama 1 대비 5–8pt↑.  
- **대화 모델 품질**: 휴먼 평가에서 동급 오픈소스 대비 60–75% 우승률. ChatGPT와 유사(36% 승률, 31.5% 무승부).  
- **안전성**: 안전 휴먼 평가에선 ChatGPT·Falcon 대비 동등하거나 낮은 위반율. ToxiGen 독성 0% 달성.  
- **추가 관찰**: 온톨로지적 시간 인식, 도구 사용(제로샷 플러그인) 자발적 등장.

### 2.5 한계  
- 지식 컷오프 이후 정보 부족, 환각 위험 여전.  
- 영어 중심 데이터로 다국어 성능 취약.  
- 안전 튜닝 시 과도한 거부(False Refusal) 가능성.  
- 실제 운영환경·사용자 다양성 미반영 벤치마크의 한계.

## 3. 일반화 성능 향상 가능성 강조
- **스케일과 데이터 증대**: 보상 모델 및 RLHF 데이터 크게 확장할수록 성능 향상 여지 확인(곡선 포화 전).  
- **Ghost Attention**: 멀티턴 제어 기법으로 시스템 메시지 영향력 수십 턴 지속.  
- **추론 중 온-더-플라이 온도 재조정**: RLHF가 프롬프트 유형별 T 최적값 학습→일반화 잠재력.  
- **도구 활용 자발성**: 플러그인 없이도 API 사용법 추론→복합 도구 체인 일반화 가능.

## 4. 영향 및 향후 연구 고려사항
- **오픈소스 민주화**: 상업용 LLM 접근성 확대 및 연구 협력 촉진.  
- **안전성 연구**: 다국어·비영어 프롬프트, 장기 사용자 상호작용 안전성 강조.  
- **보상 함수 개선**: 안전↔유용성 트레이드-오프 균형 위한 다목적 보상 모델 연구 필요.  
- **환각 억제**: 장기 컨텍스트 및 외부 도구 결합 통한 사실성 보증 기법 개발.  
- **윤리·정책**: 오픈 모델 남용, 개인 정보 유출, 사회적 편향 관리 전략 지속 검토·강화.

***

**요약**: Llama 2는 증량된 공개 데이터, 향상된 구조, 고품질 튜닝 파이프라인을 결합해 오픈소스 기반으로도 상업용 대화 품질과 안전성을 달성함으로써, AI 민주화와 안전성 연구에 큰 전기를 마련하였다. 앞으로 멀티스킬·멀티언어 일반화, 안전·정확성 강화, 책임 있는 오픈소스 거버넌스가 핵심 논점이 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/eec7f203-d3a7-4bd3-96bc-c43fdfd91974/2307.09288v2.pdf)
