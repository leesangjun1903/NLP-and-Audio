# Phi-3 Technical Report

**핵심 주장 및 주요 기여**  
소형 언어 모델이 대규모 모델과 유사한 성능을 유지하면서 휴대폰에서도 구동되도록 학습 데이터의 품질과 구성에 집중함으로써, 파라미터 수 3.8억(3.8B) 규모의 phi-3-mini가 Mixtral 8x7B 및 GPT-3.5와 유사한 벤치마크 성능을 달성했다. 또한 7B·14B 파라미터 모델(phi-3-small/medium)과 MoE, Vision, 장문·다국어 특화 모델(phi-3.5 시리즈)을 통해 언어 이해·추론, 장문 처리, 멀티모달 및 다국어 성능을 크게 향상시킨다.[1]

## 1. 해결하고자 하는 문제  
- **모델 크기 vs. 성능 균형**: 대형 언어 모델의 성능은 우수하나 모바일 기기 배포가 불가능할 정도로 무거움.  
- **학습 데이터 품질 한계**: 기존 스케일링 법칙(Scaling Laws)은 고정된 데이터 소스를 전제로 하여 데이터 규모 확대만으로는 소형 모델의 성능 한계가 존재함.

## 2. 제안 방법  
### 2.1 데이터 최적화(regime)  
- **데이터 최적(Regime) 개념**: 모델 규모별로 ‘데이터 최적(regime)’을 정의하고, 소형 모델에 적합한 고품질·엄선된 웹 데이터와 LLM 합성 데이터를 조합.[1]
- **두 단계 사전학습**  
  1) **Phase-1**: 일반 지식·언어 이해 강화 웹 데이터  
  2) **Phase-2**: 엄선된 웹 데이터 + 합성 데이터로 논리 추론·특수 능력 학습  

### 2.2 모델 구조 및 학습  
- **기본 아키텍처**: Transformer 디코더(32 layers, 32 heads, hidden size 3072), LongRope로 128K 컨텍스트 확장 가능.[1]
- **muP(Maximal Update Parametrization)**: GEGLU 활성화와 muP로 소형 모델의 하이퍼파라미터를 프록시 모델로 최적화 후 전이.[1]
- **블록스파스 어텐션**: KV 캐시 절감과 장문 성능 유지 위해 dense·blocksparse layer 교차 배치.[1]
- **MoE(혼합전문가) 구조(phi-3.5-MoE)**: 16 experts 중 토큰당 상위2개 활성화, SparseMixer 기반 스파스 라우터 적용.[1]
- **멀티모달(phi-3.5-Vision)**: CLIP ViT-L14 인코더 + φ3.5-mini 디코더, 동적 크롭 및 블록 토큰화.[1]

### 2.3 수식  
- **스케일링 법칙**: $$\mathrm{Error} \propto N^{-\alpha} $$ (N: 모델 파라미터 수).[1]
- **muP 학습률 전이**:

$$
    \eta_{\text{target}} = \eta_{\text{proxy}} \times \left(\frac{N_{\text{proxy}}}{N_{\text{target}}}\right)^{1/2}
  $$  

- **MoE 활성 파라미터**:

```math
N_{\text{active}} = N_{\text{experts}} \times E_{\text{per\_token}}
```

## 3. 성능 향상  
- **학업 벤치마크**: MMLU 69→75→78, MT-bench 8.38→8.70→8.91로 소형 모델들이 대형 모델과 경쟁.[1]
- **장문·다국어**: RULER·RepoQA 등 128K 컨텍스트 벤치에서 φ3.5-MoE가 Llama-3.1-8B와 유사한 성능 달성.[1]
- **멀티모달**: ScienceQA 91.3, ChartQA 81.8 등 시각·텍스트 복합 벤치에서 SoTA 급.[1]

## 4. 한계 및 일반화  
- **지식 저장 용량 제한**: 소형 모델 특성상 방대한 사실 지식 보유에 제약, 검색엔진 보조 필요.[1]
- **장문 학습 데이터 부족**: 128K 컨텍스트 RULER 성능 하락 관찰, 고품질 장문 데이터 확보 필수.[1]
- **다국어 데이터 부족**: 다국어 벤치에서 성능 격차 존재, 추가 다국어 데이터 요구.[1]
- **홀로세이션 및 안전성**: 강화된 RAI 파인튜닝에도 불구하고 판별 불가능 영역의 환각·오답 발생 가능.[1]

## 5. 일반화 성능 향상 관점  
데이터 최적화(regime)는 모델 규모 변동에도 일반화 성능을 유지하도록 설계되었다. 엄선된 Phase-2 합성 데이터가 논리 추론과 희귀 패턴 학습에 기여하며, muP 전이가 하이퍼파라미터 일관성을 보장한다. MoE 구조는 입력별 전문가 활성화로 범용적 추론 능력을 확장하며, LongRope로 장문 일반화를 촉진한다.

## 6. 향후 연구 및 고려 사항  
- **데이터 믹스 최적화**: 14B 이상 모델에서 벤치마크 개선이 둔화되는 문제 해결을 위한 규모별 데이터 조합 연구.  
- **장문·다국어 데이터 확충**: 고품질 장문·다국어 코퍼스 확보 및 synthetic data 생성 기법 개발.  
- **안전·일반화 트레이드오프**: DPO 데이터 균형 조절을 통해 과잉 안전화와 성능 저하 방지.  
- **검색·증강 기법 통합**: Retrieval-augmented generation으로 사실 지식 격차 해소.  
- **효율적 배포**: 더 저용량 양자화(예: 4-bit) 및 경량 커널 최적화 연구 지속.

Phi-3 시리즈는 데이터 중심 학습이 소형 모델의 일반화 성능을 크게 향상시킬 수 있음을 보여주었으며, 향후 데이터 최적화, 장문·다국어 강화, 안전성 균형 조절에 초점을 맞춘 연구가 필요하다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/d95f2499-f407-417b-99a6-834518fbf425/2404.14219v4.pdf)
