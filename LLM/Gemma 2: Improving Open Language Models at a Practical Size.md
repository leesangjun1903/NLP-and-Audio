# Gemma 2: Improving Open Language Models at a Practical Size

**핵심 주장 및 주요 기여**  
Gemma 2는 2B, 9B, 27B 규모의 경량 오픈 언어 모델로, 지식 증류(knowledge distillation)와 Transformer 아키텍처의 기술적 개량을 결합해, 동일 규모의 기존 모델 대비 성능을 대폭 향상시켰다. 특히 2B·9B 모델은 대형 모델을 교사로 한 지식 증류로 학습하며, 로컬–글로벌 주의(interleaved local–global attention)와 그룹 쿼리 주의(Grouped-Query Attention)를 도입해 2–3× 큰 모델과도 경쟁력 있는 성능을 달성했다.[1]

## 1. 해결하고자 하는 문제  
- **소형 모델의 저조한 학습 효율**: 소형 LLM은 데이터 규모 확대에 로그 스케일 개선만 보이며, 수십 조 토큰 학습해도 성능 향상 폭이 1–2%에 불과하다.[1]
- **학습 자원 한계**: 작은 모델에 15T 토큰 학습을 투입하기 어렵고 효율적 대체 기법이 필요하다.

## 2. 제안 방법  
- **지식 증류 기반 학습**  
  - 대형 모델(teacher) 분포 $$P_T(x\mid x_c)$$로부터 소형 모델(student) 분포 $$P_S(x\mid x_c)$$를 학습:  

$$
      \min_{P_S}\sum_x -P_T(x\mid x_c)\log P_S(x\mid x_c).
    $$
  
  - 2B·9B 모델에 50× 이상 토큰을 증류 학습하여, 토큰 수 부족 문제를 보완하고 일반화 성능을 강화했다.[1]
- **Transformer 구조 개선**  
  - 로컬 슬라이딩 윈도우(4096)와 글로벌 주의(8192)를 교차 적용.[1]
  - 그룹 쿼리 주의(GQA) 적용으로 추론 속도 향상 및 파라미터 절감.[1]
  - 로그잇 소프트캡(logit soft-capping)과 RMSNorm 기반 사전·사후 정규화로 안정적 학습 보장.[1]

## 3. 모델 구조  
Table 1은 핵심 아키텍처 요약이다.[1]
| 파라미터 | 2B | 9B | 27B |
|---|---|---|---|
| d_model | 2304 | 3584 | 4608 |
| Layers | 26 | 42 | 46 |
| Non-linearity | GeGLU | GeGLU | GeGLU |
| Head type | GQA | GQA | GQA |
| Sliding window | 4096 | 4096 | 4096 |
| Global span | 8192 | 8192 | 8192 |

## 4. 성능 향상 및 한계  
- **지식 증류 효과**: 2B 모델 학습 시 토큰 500B 기준 증류 모델이 무증류 모델 대비 평균 벤치마크 점수 60.3→67.7로 향상.  
- **스케일별 일관된 이득**: 모델 크기 증가에도 증류 이득 지속 (500M–1B 토큰 기준, perplexity 저하).  
- **아키텍처 개선 효과**: GQA 도입 시 9B 모델 평균 점수 소폭 향상(50.3→50.8).  
- **한계**  
  - **지속적 과대적합 위험**: 증류는 under-trained 문제 완화하나, 과도한 토큰 수 투입 시 여전히 로그 스케일 개선 한계 존재.  
  - **안정성 검증 필요**: 안전성·악용 위험 평가는 종합적이지만, 모든 시나리오 커버 불가.  
  - **추론 비용**: 27B 모델은 여전히 대규모 하드웨어 요구.

## 5. 일반화 성능 향상 가능성  
지식 증류는 소형 모델이 대형 모델의 **다양한 분포 정보를 직접 학습**하도록 해, 한정된 데이터로도 **더 풍부한 그라디언트**를 제공한다. 이는 특히 작은 모델이 **희소한 패턴**까지 학습해 **다운스트림 태스크**에서의 일반화 성능을 유의미하게 개선한다. 또한, 로컬–글로벌 주의와 GQA는 **장문 컨텍스트 처리**와 **효율적 파라미터 활용**으로 다양한 입력 길이와 태스크에서 강건함을 보인다.[1]

## 6. 향후 연구 영향 및 고려점  
- **증류 기법 심화**: 다단계 증류(multi-stage distillation)나 교사 모델 앙상블(distil from ensemble)을 통한 추가 성능 향상 연구.  
- **안정성·윤리성 강화**: 악의적 사용 및 개인 정보 노출 위험 완화 위해 **더 정교한 필터링·검증 파이프라인** 개발.  
- **경량화 및 효율화**: 모바일·엣지 디바이스 적용 위해 파라미터 공유, 양자화(quantization), 프루닝(pruning) 기법 결합 연구.  
- **다중 모달·언어 확장**: 비영어권·비텍스트 데이터 지원을 위한 **멀티모달 증류** 및 **다국어 증류** 연구.  
- **정책·활용 가이드라인 수립**: 공개 모델의 **책임 있는 개방 전략**과 **사용자 가이드라인** 마련이 필수.

***

Gemma 2는 소형 언어 모델이 현실적인 리소스 한계 내에서 대형 모델에 근접한 성능을 달성할 수 있음을 입증하며, 향후 경량화·효율화·책임 있는 AI 개발 연구의 기반을 제공한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4a6aec5f-4759-4c35-8552-df3594e30ea3/2408.00118v3.pdf)
