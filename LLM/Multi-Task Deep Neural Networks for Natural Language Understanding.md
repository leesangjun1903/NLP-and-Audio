# Multi-Task Deep Neural Networks for Natural Language Understanding

**핵심 주장 및 주요 기여**  
Multi-Task Deep Neural Networks (MT-DNN)은 *다중 과제 학습*(Multi-Task Learning, MTL)과 *사전 학습된 언어 모델*(BERT)의 장점을 결합하여, 범용적 텍스트 표현을 학습함으로써 다양한 자연어 이해(NLU) 과제에서 최첨단 성능을 달성한다. 주요 기여는 다음과 같다.[1]
1. MTL을 통해 GLUE 벤치마크 9개 과제 중 8개에서 새로운 최고 성능(82.7%)을 기록하여 BERT 대비 2.2%p 향상  
2. 소규모 도메인 적응 시 MT-DNN 표현이 BERT 대비 빠른 학습과 높은 성능을 보임  
3. 과제 유형별 맞춤 출력 모듈(예: 순차적 추론 SAN 모듈, 순위 학습 손실) 도입으로 문제 정의를 유연하게 확장  

## 1. 문제 정의  
기존 BERT 기반 모델은 각 과제별로 독립적 파인튜닝만 수행하므로,  
– 과제 간 지식 공유 부족  
– 소량 라벨 데이터 도메인 적응 한계  
를 보인다. 이에 MT-DNN은 다수 과제를 동시에 학습하여 공유 표현층을 정교화하고, 과제 특화 출력층으로 다양한 NLU 과제를 통합 처리하고자 한다.[1]

## 2. 제안 방법  
### 2.1 모델 구조  
– **공유 인코더**: BERT의 WordPiece 임베딩 + 다층 양방향 Transformer  
– **과제별 출력층**:  
  -  단일 문장 분류: softmax 분류(식 (1))  
  -  문장 쌍 유사도(STS): 회귀 $$\text{Sim}(X_1,X_2)=w_{STS}^\top x $$ (식 (2))  
  -  추론(NLI): 다단계 SAN 모듈을 통한 반복 추론 및 평균화(식 (3),(4))  
  -  순위 학습(QNLI): 후보 답안 순위화 $$\mathrm{Rel}(Q,A)=\sigma(w^\top_{QNLI}x)$$, 쌍별 손실 최적화(식 (8),(9))  

### 2.2 학습 절차  
1. **사전 학습**: BERT와 동일한 마스크 언어 모델 및 문장 예측 목적  
2. **다중 과제 학습**: 각 과제 미니배치를 무작위 섞어 반복 학습  
   – 손실 함수: 분류–교차엔트로피(식 (6)), 유사도–MSE(식 (7)), 순위–쌍별 로그우도(식 (8))  

## 3. 성능 향상 및 한계  
### 3.1 GLUE 벤치마크  
| 모델            | GLUE 전체 점수 |
|-----------------|:--------------:|
| BERT<sub>LARGE</sub> | 80.5%         |
| MT-DNN          | **82.7%**      |

– 특히 RTE, SST-2처럼 라벨 데이터가 적은 과제에서 BERT 대비 3~10%p 개선.[1]
– CoLA 과제는 소규모 데이터로 MTL만으로는 과소적합 발생, 파인튜닝 필요.[1]

### 3.2 도메인 적응  
SNLI, SciTail 도메인 적응 시, 훈련 샘플 비율이 작을수록 MT-DNN의 이점이 더욱 두드러진다.  
– SNLI 0.1% 데이터: 52.5%→82.1%, 1%: 78.1%→85.2% 향상  
– SciTail 0.1%: 51.2%→81.9%, 1%: 82.2%→88.3% 향상.[1]

**한계**:  
– MTL 학습 시 과제 간 상호 간섭(task interference) 가능성  
– 계산 자원 및 메모리 소모 증가  
– CoLA와 같은 이질적, 소규모 데이터에 과소적합  

## 4. 일반화 성능 향상 관점  
MT-DNN은 MTL 정규화 효과를 통해 과제 간 공통 표현을 학습함으로써, 새로운 과제나 도메인 적응 시 **파인튜닝 데이터 효율성**을 극대화한다. 특히 샘플 수가 극히 적은 상황에서도 공유된 Transformer 계층이 강력한 초기 표현으로 작용하여 빠르고 안정적으로 수렴한다.[1]

## 5. 향후 연구 영향 및 고려 사항  
MT-DNN은 MTL과 사전 학습 결합의 효과를 입증하여, 이후 연구에서  
– **과제 선택 및 계층 공유 전략** 최적화  
– **다중 목적 사전 학습**(예: 생성+분류) 통합 모델  
– **효율적 MTL 알고리즘**으로 계산 자원 절감  
– **적대적 공격 및 견고성** 평가 강화를 고려해야 한다.[1]

이와 같이 MT-DNN은 범용적 텍스트 표현 학습과 도메인 적응 가능성을 크게 확장하여 후속 연구의 방향타로 작용할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/75c4c297-aa6a-4e02-8184-6e67e837ad5f/1901.11504v2.pdf)
