# Convolutional Sequence to Sequence Learning

**핵심 주장 및 기여**  
Convolutional Sequence to Sequence Learning(ConvS2S)은 기존의 순환 신경망 기반 인코더–디코더 구조를 완전한 컨볼루션 신경망으로 대체하여, 입력·출력 시퀀스 간 매핑을 병렬화하고, 고정된 비선형성 개수로 학습을 안정화하며, 각 디코더 층에 별도의 어텐션 모듈을 도입함으로써 기계 번역·요약 등에서 최첨단 성능을 달성한다.  

## 1. 문제 제기  
- 순환 구조(RNN/LSTM/GRU)는 시퀀스 길이에 따라 연산이 순차적으로 진행되어 병렬화 제약이 크고, 장거리 종속을 학습하기 위한 비선형성 개수가 입력 길이에 비례한다.  
- 기존의 컨볼루션 기반 시퀀스 모델은 병렬 처리 장점이 있지만, 충분한 성능을 보이지 못하거나 디코더에 재귀 구조를 함께 사용했다.  

## 2. 제안 방법  
### 2.1 모델 구조  
- **인코더·디코더 블록**: 1D 컨볼루션 + 게이트형 선형 유닛(GLU) + 잔차 연결  
- **포지션 임베딩**: 입력·출력 단어 임베딩에 위치 정보를 더해 순서 인식  
- **다중 어텐션**: 각 디코더 블록마다 독립적 어텐션 모듈을 두고, 앞선 블록의 어텐션 정보를 누적 반영  
- **정규화 전략**:  
  - 잔차 연결 합 후 $$\times\sqrt{0.5}$$ 스케일링  
  - 어텐션 가중합 후 $$\times m^{1/m}$$ 스케일링 (m = 소스 시퀀스 길이)  
  - 다중 어텐션 시 인코더에 전달되는 기울기 스케일 조정  

### 2.2 수식 요약  
1) 컨볼루션 + GLU  

$$
Y = \mathrm{GLU}(W * X + b),\quad \mathrm{GLU}([A\;B]) = A \otimes \sigma(B)
$$  

2) 포지션 임베딩  

$$
e_j = w_j + p_j,\quad g_i = \text{prev‐target‐embed}_i + \tilde p_i
$$  

3) 다중 어텐션 (블록 $$l$$)  

$$
d^l_i = W^l_d h^l_i + b^l_d + g_i,\quad
\alpha^l_{ij} \propto \exp(d^l_i \cdot z^u_j),\quad
c^l_i = \sum_j \alpha^l_{ij}(z^u_j + e_j),\quad
h^l_i \leftarrow h^l_i + c^l_i
$$  

## 3. 모델 구조 세부  
- 커널 너비(3–5), 블록 수(인코더 15–20층, 디코더 5–20층) 조합  
- 배치 어텐션 연산으로 연산 병렬화  
- 초기화: GLU 연산 특성 반영해 분산 보존하도록 가중치 분포 설계  

## 4. 성능 평가  
- **기계 번역**:  
  - WMT’16 En–Ro: +1.9 BLEU  
  - WMT’14 En–De: +0.5 BLEU  
  - WMT’14 En–Fr: +1.6 BLEU  
  - 엔셈블링 시 추가 향상  
- **생성 속도**: GPU·CPU 모두 RNN 대비 최대 20배 빠른 추론  
- **요약**: Gigaword·DUC-2004에서 기존 RNN MLE 성능 상회  

## 5. 한계 및 일반화 성능  
- **제한된 수용장치 크기**: 고정된 컨텍스트 길이 내에서만 장기 의존 학습  
- **포지션 임베딩 의존**: 문장 길이 상한 필요  
- **추가 기울기 스케일링**: 복잡한 하이퍼파라미터 조율  
- **일반화 가능성**:  
  - 계층적 표현 학습은 언어 외 시계열·음성·생체신호 등에도 확장성  
  - 다중 어텐션 구조는 문서 요약·대화 등 다양한 시퀀스 태스크로 전이 용이  

## 6. 향후 영향 및 고려사항  
- **연구 영향**: 컨볼루션 기반 시퀀스 모델 연구 활성화, 트랜스포머 계열 전조  
- **연구 시 고려**:  
  - 수용장치 확장(장기 의존 캡처)  
  - 어텐션 효율화·하이브리드 구조 탐색  
  - 스케일 아웃 시 초기화·정규화 유지 전략  
  - 강화학습·사전학습 전이로 일반화 성능 극대화

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b032da30-745f-4ba2-8e8f-8442fb89af3c/1705.03122v3.pdf)
