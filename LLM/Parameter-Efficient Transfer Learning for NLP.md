# Parameter-Efficient Transfer Learning for NLP

## 1. 핵심 주장 및 주요 기여  
본 논문은 사전학습된 거대 언어모델(BERT)에 *어댑터 모듈(adapter modules)*만 추가하고, 원본 모델 파라미터는 동결(frozen)한 채 어댑터 파라미터만 미세조정(fine-tuning)함으로써, **기존의 전체 파인튜닝 대비 2~3% 수준의 파라미터만으로 동일에 근접한 성능**을 달성할 수 있음을 보인다.  
- **파라미터 효율성:** 어댑터당 0.5–8% 수준의 소수 파라미터만 추가  
- **확장성(Continual Learning):** 새로운 태스크마다 원본 모델을 재학습하지 않고 어댑터만 적재하여 이전 태스크 성능 보존  
- **성능 보존:** GLUE 벤치마크에서 전체 파인튜닝 대비 평균 성능 격차 0.4% 이내  

***

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 2.1 해결하고자 하는 문제  
- **매 태스크마다 전체 파라미터를 재학습해야 하는 비효율성**  
- **클라우드 서비스 등에서 다수 태스크 순차 학습 시 모델 크기 급증 및 파인튜닝 비용 문제**  

### 2.2 제안 방법 (어댑터 튜닝)  
- 사전학습된 모델 파라미터 $$w$$는 고정  
- 각 태스크마다 작은 병목(bottleneck) 구조의 어댑터 모듈 파라미터 $$v$$만 학습  
- 초기화 시 어댑터를 거의 항등 함수에 가깝게 설정하여 안정적 학습 보장  

수식으로 모델 출력 $$\psi_{w,v}(x)$$는  

$$
\psi_{w,v}(x) = \phi_w(x) + \mathrm{Adapter}_v(\phi_w(x)),
$$  

여기서 $$\phi_w(x)$$는 원본 네트워크, $$\mathrm{Adapter}_v$$는  

```math
\mathrm{Adapter}_v(h) = W_{\text{up}} \,\sigma\bigl(W_{\text{down}}\,h + b_{\text{down}}\bigr) + b_{\text{up}}
```

의 병목 구조를 갖는다. $$W_{\text{down}}\in\mathbb{R}^{m\times d}$$, $$W_{\text{up}}\in\mathbb{R}^{d\times m}$$, $$m\ll d$$.

### 2.3 모델 구조  
- **Transformer 각 레이어**  
  - Multi-Head Attention 후와 Feed-Forward 후, 총 두 곳에 어댑터 삽입  
  - 어댑터 내부는 “down-project → activation(ReLU 등) → up-project”  
  - 레이어 정규화 파라미터도 태스크별 학습  

### 2.4 성능 향상  
- **GLUE**: 어댑터(3.6% 파라미터) vs. 전체 파인튜닝(100%) → 평균 점수 80.0 vs. 80.4  
- **추가 17개 분류 태스크**: 전체 대비 1.14% 파라미터로 전체 파인튜닝 대비 0.4% 내외 성능 차  
- **SQuAD QA**: 어댑터(2% 파라미터) F1=90.4 vs. 전체 파인튜닝 F1=90.7  

### 2.5 한계  
- 어댑터가 **저층부(feature extraction)보다는 고층부(task-specific) 표현**에 주로 의존  
- 어댑터 크기 및 초기화 분포 민감도 존재(초기화가 너무 크면 학습 불안정)  
- 제안 구조 외의 복합 어댑터(병렬 구조, 다양한 활성화) 실험에서 성능 개선 미미  

***

## 3. 모델의 일반화 성능 향상 가능성  
- **동결된 사전학습 표현**을 재활용하되, 어댑터만 학습함으로써 과적합 위험을 완화  
- **레이어 정규화 파라미터**만 조정해도 소폭 성능 향상 관찰 → 경량 파라미터로 세밀한 분포 조절 가능  
- **고층부 어댑터 집중** 경향: 다양한 태스크에 걸쳐 고층부 representation을 재구성해 범용성과 특수성 균형  
- 추후 **어댑터 초기화 스케일** 자동화, **히든 차원 $$m$$** 동적 조정, **활성화 함수** 최적화 등을 통해 일반화 성능 추가 개선 기대  

***

## 4. 향후 연구에 미치는 영향 및 고려 사항  
- **모델 확장성**: 수백 개 태스크 학습 환경에서 모델 크기 억제 가능 → 산업 적용성 제고  
- **Continual Learning**: 어댑터 기반 기억 보존 전략과 기존 메모리 기반 방법 결합 연구  
- **Adapter-Aware Pre-training**: 사전학습 단계부터 어댑터 삽입을 고려한 트레이닝으로 더욱 효율적 미세조정  
- **다중언어, 다중모달** 전이: 텍스트 외에 음성·영상 등에 어댑터 적용 가능성  
- **자원 제약 환경**(엣지 디바이스)에서 경량화된 커스텀 태스크 모델 배포 연구  

以上의 방향을 고려하여, **경량화 전이 학습** 분야에서 어댑터 메커니즘은 향후 주요 패러다임으로 자리매김할 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/41967253-d68b-4a79-8275-a521baa38ca4/1902.00751v2.pdf)
