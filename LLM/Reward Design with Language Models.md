# Reward Design with Language Models

### 1. 핵심 주장 및 주요 기여

**"Reward Design with Language Models"** 논문은 강화 학습(RL)의 근본적인 난제인 **보상 함수 설계 문제**를 혁신적으로 해결하고자 합니다. 논문의 핵심 주장은 다음과 같습니다:[1]

**핵심 통찰(Key Insight):** 대규모 언어 모델(LLM)이 인터넷 규모의 텍스트 데이터로 학습되었다는 사실 자체가 이들을 탁월한 맥락 학습자(in-context learner)로 만들며, 동시에 인간 행동에 대한 의미 있는 상식을 포착할 수 있게 합니다.[1]

**주요 기여:**

1. **LLM을 대리 보상 함수(Proxy Reward Function)로 사용하는 개념 도입:** 기존의 명시적 보상 함수 설계나 지도 학습(Supervised Learning)을 통한 보상 학습의 한계를 극복[1]

2. **일반적 RL 훈련 프레임워크 제안:** 사용된 RL 알고리즘에 무관한 모듈식 프레임워크 개발[1]

3. **실증적 성능 향상:** LLM 기반 접근법이 기존 방법 대비 평균 **35% 성능 개선** 달성[1]

4. **적응형 인터페이스 제공:** 사용자가 자연 언어를 통해 직관적으로 원하는 행동을 명시할 수 있게 함[1]

***

### 2. 해결하고자 하는 문제 및 제안하는 방법

#### 2.1 문제 정의

전통적 RL의 보상 설계는 두 가지 근본적인 문제에 직면합니다:

**문제 1: 명시적 보상 함수 설계의 어려움**
- 인간의 선호도를 수치적 함수로 정의하기 어려움 (예: "다재다능한 협상가"의 정의)
- 보상 해킹(Reward Hacking) 발생 가능성
- 여러 목표 간 균형 조정의 복잡성

**문제 2: 지도 학습 기반 보상 학습의 비효율성**
- 대규모 라벨링 데이터 필요
- 새로운 사용자/목표에 대한 일반화 불가능
- 비용 및 시간 소모 증가

#### 2.2 제안하는 방법론

논문은 LLM을 대리 보상 함수로 사용하는 프레임워크를 제안합니다. 수식적으로 다음과 같이 표현됩니다:[1]

**전통적 보상 함수:**
$$R: S \times A \rightarrow \mathbb{R}$$

**LLM 기반 대리 보상:**
$$R_{LLM} = g(LLM(\alpha_1, \alpha_2, \alpha_3, \alpha_4))$$

여기서:
- $\alpha_1 \in A$: 작업 설명(Task description)
- $\alpha_2 \in A$: 사용자 목표 명시(User objective specification) - 예제 또는 설명 형태
- $\alpha_3 \in A$: 에피소드 결과의 텍스트 표현 ($f: S \times A \rightarrow A$ 파서를 통해 변환)
- $\alpha_4 \in A$: 목표 달성 여부를 묻는 질문
- $g: A \rightarrow \{0, 1\}$: LLM 출력을 정수로 변환하는 파서

#### 2.3 모델 구조 및 학습 프로세스

**훈련 파이프라인(그림 1 기반):**[1]

1. **준비 단계(Setup):** 사용자가 자연 언어로 목표 명시 (몇 예제 또는 설명)
2. **프롬프트 구성:** 네 가지 요소(작업, 목표, 에피소드, 질문)를 연결
3. **LLM 평가:** 구성된 프롬프트를 LLM에 입력하여 보상 신호 생성
4. **보상 파싱:** LLM의 텍스트 출력(예: "Yes"/"No")을 이진 보상으로 변환
5. **RL 업데이트:** 파싱된 보상 신호로 에이전트의 정책 업데이트
6. **반복:** 새 에피소드 샘플링 후 과정 반복

**핵심 마르코프 결정 과정(MDP) 형식화:**[1]

$$M = \langle S, A, p, R, \gamma \rangle$$

- $S$: 상태 공간 (예: DEALORNODEAL의 경우 협상 이력)
- $A$: 행동 공간 (예: 가능한 발화)
- $p(S'|S,A) \in [1]$: 전이 확률
- $\gamma$: 할인 인수

***

### 3. 실험 설계 및 성능 평가

논문은 세 가지 서로 다른 도메인에서 제안된 방법을 평가합니다:

#### 3.1 Ultimatum Game (소수샷 학습, Few-shot)

**작업 설명:** 두 명의 플레이어가 자금을 분할하는 게임. RL 에이전트(응답자)가 사용자의 선호도에 따라 제안을 수락/거절하도록 학습[1]

**사용자 목표:**
- **Low vs High Percentages:** 특정 비율 이하 제안 거절
- **Low vs High Payoffs:** 특정 금액 이하 제안 거절  
- **Inequity Aversion:** 정확히 50% 분할 아님 거절

**결과:**
- 1개 예제 + 설명: LLM 라벨링 정확도 **85-90%** (SL은 ~30%)
- 10개 예제: LLM과 SL 모두 유사한 성능 (~95%)
- **RL 에이전트 정확도:** LLM 기반 방법이 SL 대비 **평균 35% 향상**[1]

**핵심 발견:** 설명(explanation)의 중요성 - 설명 제거 시 평균 31.67% 정확도 하락[1]

#### 3.2 Matrix Games (영샷 학습, Zero-shot)

**작업 설명:** 2인 정규형 게임에서 잘 알려진 게임이론적 개념을 적용하도록 LLM 유도[1]

**평가 개념:**
- **Total Welfare:** 전체 플레이어 보상 합 최대화
- **Equality:** 플레이어 간 보상 동등
- **Rawlsian Fairness:** 최소 보상 최대화
- **Pareto-optimality:** 한 플레이어 개선 불가능

**결과:**
- LLM은 설명 없이(No Objective) 대비 **평균 48% 성능 향상** (정규 순서)
- 결과 순서 섞기(Scrambled): **36% 향상**
- RL 에이전트 정확도: Total Welfare, Equality는 100%, Rawlsian Fairness는 75%[1]

**발견:** 게임 구조가 LLM 학습 데이터에 포함되어 있어, 순서 변경이 성능 저하 초래[1]

#### 3.3 DEALORNODEAL 협상 작업 (다중 시간 단계, Multi-timestep)

**작업 설명:** 최대 100 시간 단계의 장기 협상. 에이전트 Alice가 Bob과 책, 모자, 공을 분배하는 방식 협상[1]

**협상 스타일:**
- **Versatile:** 동일 제안 반복 금지
- **Push-Over:** Alice가 Bob보다 적은 점수 획득
- **Competitive:** Alice가 Bob보다 많은 점수 획득
- **Stubborn:** Alice가 동일 제안 반복

**결과:**
- **LLM 라벨링 정확도:** Versatile 제외 SL 우수 (Competitive: 98%, Stubborn: 94%)
- **RL 에이전트 정확도:** LLM이 SL 대비 **평균 46% 향상**
- 진정한 보상 함수 대비 **4% 미만 차이**[1]

**인간 연구(Pilot User Study):** 
- N=10 사용자 참여
- 올바른 스타일로 훈련된 에이전트: 평균 평점 3.72/5
- 반대 스타일 에이전트: 평균 평점 1.56/5
- 통계 유의성: $p < 0.001$[1]

***

### 4. 일반화 성능 향상 가능성

#### 4.1 데이터 효율성 분석

논문은 동일 성능 달성을 위해 SL이 필요한 추가 데이터 양을 정량화합니다:[1]

**DEALORNODEAL 도메인에서:**
- LLM: 3개 예제로 충분
- SL: **수백 개** 예제 필요 (약 50-100배 추가)

**수식적 표현:**

```math
\text{Data Efficiency Ratio} = \frac{n_{SL}(p^*)}{n_{LLM}(p^*)}
```

여기서 $n_{method}(p^\*)$는 성능 $p^*$ 달성에 필요한 데이터 개수입니다. 측정 결과 비율은 **50-100:1** 범위입니다.[1]

#### 4.2 프롬프트 변동에 대한 견고성

논문은 DEALORNODEAL의 "Stubborn" 스타일에 대해 프롬프트 변동 효과를 분석했습니다:[1]

| 변수 | LLM 정확도 | SL 정확도 | 성능 격차 |
|-----|----------|---------|----------|
| 키워드 변동 | 0.97±0.16 | 0.58±0.48 | 39% |
| 예제 변동 | 0.91±0.28 | 0.48±0.91 | 43% |
| 설명 품질 | 0.93±0.28 | 0.79±0.33 | 14% |

**발견:** 설명 품질이 LLM 정확도에 가장 영향력 있는 요소 - 좋은 설명 유지 시 LLM의 견고성 향상[1]

#### 4.3 모델 크기의 영향

논문은 GPT-2(1.5B 파라미터)와 GPT-3(175B 파라미터)를 비교합니다:[1]

- **GPT-2 라벨링 정확도:** 기준대비 **15% 하락**
- **GPT-2 RL 에이전트 정확도:** 기준대비 **49% 하락**
- **GPT-2 vs SL:** 라벨링은 LLM 우수, 하지만 RL 에이전트에서는 SL과 비슷

**함의:** 모델 규모 증가가 일반화 성능을 유의미하게 향상시킴[1]

#### 4.4 일반화 병목 및 한계

**인식된 한계:**[1]

1. **프롬프트 엔지니어링 의존성:** 프롬프트 디자인이 여전히 필요하며, 최적 성능 달성에 노력 소요
2. **이진 보상 제약:** 현재 프레임워크는 이진(0/1) 보상만 생성 - 연속 보상 생성 불가
3. **맥락 길이 한계:** 긴 프롬프트에서 LLM의 위치 편향(Recency Bias) 문제
4. **도메인 특화성:** 세 개 도메인 평가로 제한 - 더 광범위한 평가 필요

***

### 5. 기술적 구현 세부사항

#### 5.1 파서(Parser) 설계

**파서 $f: S \times A \rightarrow A$** (에피소드 결과 → 텍스트)
- Ultimatum Game: 제안 분할, 응답자 행동 → 문장
- Matrix Games: 게임 구조, 결과 → 선택지 형식
- DEALORNODEAL: 협상 이력 → 구조화된 텍스트

**파서 $g: A \rightarrow \{0,1\}$** (LLM 출력 → 보상)
- "Yes" 또는 긍정적 응답 → 1
- "No" 또는 부정적 응답 → 0
- 예외 처리: 응답 형식 오류 시 해당 에피소드 스킵

#### 5.2 RL 알고리즘 적용

**Ultimatum Game:**
- 알고리즘: DQN (Deep Q-Network)
- 시간 단계: 10,000
- 학습률: 1e-4

**DEALORNODEAL:**
- 알고리즘: REINFORCE (정책 그래디언트)
- 시간 단계: 1 에포크(250개 맥락)
- 학습률: 0.1
- 정책 네트워크: 4개 GRU 레이어

#### 5.3 프롬프트 엔지니어링 기법

**Chain-of-Thought (CoT) 프롬프팅:**
- "Let's think step by step" 템플릿 사용
- 중간 추론 단계 명시적 유도[1]

**구조화된 프롬프트 구성:**
```
[Task Description]
[User Objective with Examples or Description]  
[Episode Outcome as Structured Text]
[Question: Does the outcome satisfy the objective?]
```

**다중 샷 설계:**
- Few-shot: 긍정/부정 예제 혼합 (Counterbalanced)
- Zero-shot: 개념 정의 포함

***

### 6. 앞으로의 연구에 미치는 영향 및 향후 고려 사항

#### 6.1 최신 연구 기반 영향 분석

논문이 발표된 이후(2023년) 관련 연구의 진전을 보면, 다음과 같은 영향을 미친 것으로 파악됩니다:[2][3][4][5]

**1. LLM 기반 보상 설계의 진화**

논문의 접근법은 이후 연구들에 영감을 주어 더욱 정교한 LLM 보상 설계 방법론이 개발되었습니다:

- **Text2Reward (2024):** 논문의 아이디어를 확장하여 데이터 없이 밀도 높은(Dense) 보상 함수를 자동 생성[3]
- **CARD 프레임워크 (2024):** 동적 피드백 루프를 통한 반복적 보상 설계 개선[2]
- **ERFSL (2024):** LLM을 효율적인 보상 함수 탐색기로 활용하며, 가중치 최적화에 유전 알고리즘 적용[6]

**2. 일반화 성능 향상 연구**

$$P(\text{new task}) = f(P(\text{pretraining}), \text{prompt quality}, \text{few-shot examples})$$

최근 연구들은 다음 요소들을 통해 일반화 성능 향상을 추구합니다:[7]

- **다중 목표 보상 설계(Multi-objective RM):** 단일 가치에서 다중 측면 고려로 확장
- **토큰 수준 보상(Token-level RM):** 응답 수준에서 더 세밀한 토큰 수준 피드백으로 진화
- **하이브리드 보상 모델링:** 규칙 기반과 데이터 기반 접근의 융합

**3. 검증 가능한 보상 프레임워크의 부상**

최근 패러다임 변화로 **Reinforcement Learning with Verifiable Rewards (RLVR)**가 주목받고 있습니다:[8]

$$R_{\text{verifiable}} = \text{Verify}(\text{output}, \text{reference answer})$$

이는 프로그래밍 문제, 수학 등 객관적 검증이 가능한 도메인에서:
- 논문의 LLM 보상 모델보다 더 안정적인 신호 제공
- 수학 추론, 코딩 성능에서 상당한 개선 달성
- 구조화된 환경에서의 일반화 향상[9]

#### 6.2 향후 연구 시 고려할 점

**1. 모달리티 확장(Multimodal Extensions)**

현재의 텍스트 기반 인터페이스를 넘어, 비전 모달리티 통합이 중요해질 것으로 예상됩니다:[1]

- Flamingo 같은 다중 모달 파운데이션 모델 활용
- 로봇 작업에서 이미지/비디오를 통한 직관적 선호도 명시
- 환경 상태의 더 풍부한 표현 가능

$$R_{\text{multimodal}} = g(\text{LLM}(\alpha_{\text{visual}}, \alpha_{\text{text}}, \alpha_{\text{task}}))$$

**2. 연속 보상 설계(Non-Binary Rewards)**

논문의 핵심 한계인 이진 보상의 제약을 극복해야 합니다:[1]

- LLM의 토큰별 우도(Likelihood) 활용
- 확률적 해석을 통한 연속 보상 신호 생성

$$R_{\text{continuous}} = \int_0^1 p(r|x) \cdot r \, dr$$

최근 연구에서 LM의 내부 표현을 통한 목표 조건부 표현 학습(Goal-Conditioned Representation Learning)이 이 방향으로 진전되고 있습니다.[10]

**3. 인간 피드백 통합의 확대**

논문의 초기 인간 연구(N=10)를 넘어, 더욱 포괄적인 평가가 필요합니다:[5]

- 다양한 사용자 그룹(비기술 사용자, 도메인 전문가 등)에서의 평가
- 문화적, 가치관적 차이에 따른 보상 설계 의존성 분석
- 사용자 만족도와 실제 성능의 상관관계 분석

**4. 프롬프트 견고성 및 자동 최적화**

현재 수동 프롬프트 엔지니어링의 의존성을 줄이기 위해:[1]

- **프롬프트 최적화 알고리즘:** 메타 학습을 통한 자동 프롬프트 생성
- **적응형 프롬프팅:** 에이전트 성능 피드백에 따른 프롬프트 동적 조정

$$\alpha^* = \arg\max_{\alpha} J(\pi_{\text{LLM}(\alpha)}|\text{task})$$

최신 연구에서 LLM을 화이트박스 탐색기로 활용하여 가중치 최적화를 상당히 효율화한 사례가 있습니다.[6]

**5. 분포 외 일반화(Out-of-Distribution Generalization)**

논문의 세 도메인 평가에서 발견한 순서 민감성 문제를 해결해야 합니다:[1]

- 도메인 특화적 정보(Matrix Game 구조)에 대한 과적응 방지
- 새로운 도메인, 작업 형식에 대한 견고성 확보
- 메타 학습 접근으로 보상 설계 능력의 일반화

**6. 계산 효율성 및 확장성**

LLM API 호출의 비용 및 레이턴시 문제:[11]

- 더 작은 모델(Llama, Mistral)로의 미세 조정 가능성
- 로컬 배포를 통한 비용 절감
- 배치 쿼리 최적화 및 캐싱 전략

**7. 안전성 및 정렬성(Safety & Alignment) 고려**

LLM의 보상 설계에 대한 신뢰성 평가:[1]

- 보상 해킹 가능성의 재평가
- 목표 왜곡(Goal Misgeneralization) 분석
- 사용자 의도와 LLM 해석의 불일치 감지 메커니즘

**8. 학제 간 협력**

행동 경제학, 윤리학과의 통합:[1]

- 다양한 윤리적 관점의 보상 설계 포함
- 행동 편향이 LLM 보상 신호에 미치는 영향
- 공정성, 투명성 평가 프레임워크 개발

#### 6.3 구체적 고려 사항

**A. 프롬프트 품질과 일반화의 관계**

논문의 분석(Figure 7)에 따르면, 설명 품질이 LLM 정확도에 미치는 영향이 **28% 표준편차**로 가장 큽니다. 이는:[1]

- 설명 작성 지침의 개발 필요
- 자동 설명 품질 평가 메트릭 개발
- 설명-결과 정렬 메커니즘 연구

**B. 도메인 특이성 대 일반성 트레이드오프**

$$\text{Generalization} = f(\text{Domain Knowledge}, \text{Prompt Abstraction}, \text{LLM Scale})$$

Matrix Games에서 구조에 대한 과적응이 관찰되었으므로:
- 추상화 수준 조정 메커니즘
- 도메인 무관 프롬프트 템플릿 개발
- 크로스 도메인 전이 학습 연구

***

### 결론

**"Reward Design with Language Models"**은 강화 학습의 보상 설계 문제를 LLM의 맥락 학습 능력으로 해결하는 혁신적 접근을 제시합니다. 제안된 방법은:

- **효율성:** 수백 개 대신 3-10개 예제로 동일 성능 달성[1]
- **직관성:** 자연 언어를 통한 선호도 명시로 사용자 친화성 극대화[1]
- **적응성:** 다양한 RL 알고리즘과 호환되는 모듈식 설계[1]

그러나 일반화 성능 향상을 위해서는 다중 모달리티, 연속 보상 설계, 자동 프롬프트 최적화, 분포 외 견고성 등의 추가 연구가 필수적입니다. 2024-2025년의 최신 연구 동향은 이러한 한계를 극복하는 방향으로 진전되고 있으며, 특히 검증 가능한 보상과 다중 목표 설계로의 패러다임 전환이 주목할 만합니다.[4][3][5][9][8][7][2]

***

**참고문헌:**

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/6b3b61a0-9755-4b60-ab15-6e456c1065d5/2303.00001v1.pdf)
[2](http://arxiv.org/pdf/2410.14660.pdf)
[3](http://arxiv.org/pdf/2309.11489v3.pdf)
[4](https://arxiv.org/html/2504.07596v1)
[5](http://arxiv.org/pdf/2406.01309.pdf)
[6](https://arxiv.org/abs/2409.02428)
[7](https://arxiv.org/html/2505.02666v1)
[8](https://arxiv.org/html/2509.16679v1)
[9](https://arxiv.org/pdf/2503.23829.pdf)
[10](https://arxiv.org/pdf/2407.13887.pdf)
[11](https://openreview.net/pdf?id=Q0SqJ8rmnP)
[12](http://arxiv.org/pdf/2402.07069.pdf)
[13](https://arxiv.org/pdf/2305.18290.pdf)
[14](https://aclanthology.org/2025.emnlp-main.1085.pdf)
[15](https://www.parloa.com/knowledge-hub/zero-shot-vs-few-shot/)
[16](https://academic.oup.com/bib/article/25/5/bbae354/7739674)
[17](https://arxiv.org/html/2505.02666v2)
[18](https://www.rohan-paul.com/p/zero-shot-and-few-shot-learning-techniques)
[19](https://openreview.net/forum?id=tUM39YTRxH)
