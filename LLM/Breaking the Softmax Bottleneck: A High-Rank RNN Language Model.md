# Breaking the Softmax Bottleneck: A High-Rank RNN Language Model

## 1. 핵심 주장 및 주요 기여
본 논문은 **기존 Softmax 기반 언어 모델이 제한된 임베딩 차원(d)에 의해 모델링 표현력이 병목에 걸린다**는 ‘Softmax 병목(Softmax Bottleneck)’ 문제를 지적한다. 이를 해결하기 위해 연속된 Softmax 분포의 **가중치 혼합(Mixture of Softmaxes, MoS)** 구조를 제안하여, 모델의 로그-확률 행렬의 유효 계수를 높이고(rank를 증가시켜) 자연어의 복잡한 문맥 종속성을 효과적으로 학습할 수 있음을 보였다.

주요 기여:
- Softmax 기반 RNN 언어 모델을 **행렬 분해 관점**으로 재구성하여 표현력 한계를 이론적으로 규명  
- 여러 개의 Softmax를 혼합하는 간단한 구조(MoS)를 통해 **고계수(high-rank)** 모델을 실현  
- Penn Treebank·WikiText-2·1B 단어 데이터셋에서 **최신 최고(perplexity) 성능** 달성  

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 문제 정의: Softmax 병목
- RNN 언어 모델은 문맥 벡터 $$h_c$$와 단어 임베딩 $$w_x$$ 사이의 내적을 Softmax에 통과시켜 다음 단어 분포를 예측  
- 임베딩 차원 $$d$$가 로그-확률 행렬 $$A∈\mathbb{R}^{N×M}$$의 랭크를 상한(bound)하며, 실제 자연어 분포는 **고차원(high-rank)**이므로 저차원 $$d$$로는 정확 근사가 불가능  

### 제안 방법: Mixture of Softmaxes (MoS)
- **수식**  

$$
    P_\theta(x|c) = \sum_{k=1}^{K} \pi_{c,k}\,\mathrm{Softmax}\bigl(h_{c,k}^\top w_x\bigr),
    \quad \sum_{k=1}^K \pi_{c,k}=1
  $$
  
여기서 $$\pi_{c,k}$$는 문맥 $$c$$에 대한 k번째 component의 가중치, $$h_{c,k}$$는 k번째 문맥 벡터  
- **모델 구조**  
  1) RNN(예: LSTM)이 문맥 정보 $$g_t$$ 생성  
  2) $$\pi_{c,k} = \mathrm{Softmax}(w_{\pi,k}^\top g_t)$$, $$h_{c,k}=\tanh(W_{h,k}g_t)$$로 혼합 확률과 개별 문맥 벡터 산출  
  3) K개의 Softmax 분포를 가중합하여 최종 분포 획득  
- **장점**  
  - 로그-확률 행렬에 대한 **비선형 합산(log-sum-exp)** 구조로 인해 랭크 제약 해소  
  - 파라미터 수 증가는 크지 않으면서도 **과적합 없이** 표현력 극대화  

### 성능 향상
- Penn Treebank: 베이스라인(Perplexity 57.3) → MoS (54.4) → 동적 평가(dynamic eval) 적용 시 47.7 기록  
- WikiText-2: 베이스라인(Perplexity 65.8) → MoS (61.5) → 동적 평가 적용 시 40.7 기록  
- 1B 단어 데이터: Softmax(42.8) → MoS(37.1), 약 5.6pt 대규모 개선  
- MoC(문맥 혼합) 대비 **일관된 성능 우위**, 혼합 전 확률 분포 간의 KL 발산 증가로 문맥 구분력 향상  

### 한계
- **계산 비용**: K배의 Softmax 계산으로 학습 속도 2–3배 저하  
- **과적합 위험**: K 너무 클 경우 랭크 완전 증가 후 오히려 성능 저하 관찰  
- **하이퍼파라미터**(K, 드롭아웃 비율) 민감도 존재  

## 3. 일반화 성능 향상 가능성
MoS는 비모수적 N-그램 모델과 달리 임베딩 차원은 낮게 유지하면서도 **모델 수용력(expressiveness)** 만 증대시키므로 과적합 없이 일반화를 유지한다. 특히 1B 단어 실험에서 드롭아웃 등 정규화 기법 없이도 학습·검증 간 성능 격차가 Softmax와 유사하게 유지되어, **우수한 표현력과 견고한 일반화의 균형**을 증명했다.

## 4. 향후 연구 영향 및 고려 사항
- **고계수 모델 설계**: 다른 구조(Transformer, CNN-LM 등)에 MoS 개념 적용 가능  
- **효율적 MoS 변형**: 연산 비용 절감을 위한 **저차원 근사**·스파스 혼합 구조 연구  
- **응용 확장**: 대화 생성·기계 번역·텍스트 생성 등 문맥 민감 태스크에서 일반화 성능 검증  
- **랭크-정규화 트레이드오프**: K 선택·임베딩 차원 조율을 통한 **최적 랭크 제어** 전략 수립  

앞으로의 연구는 Softmax 병목 개념을 바탕으로 **표현력‒일반화 균형**을 더욱 정교하게 다루는 모델 구조와 효율성을 함께 고려해야 할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/2ebc4dd5-4254-4575-8007-12eb2a5e8af9/1711.03953v4.pdf)
