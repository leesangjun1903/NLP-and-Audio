# Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback

**핵심 주장 및 기여**  
Anthropic 연구진은 대규모 언어 모델을 인간의 선호 피드백을 활용한 강화학습(RLHF)으로 파인튜닝하여, 동시에 **도움됨(helpfulness)** 과 **무해함(harmlessness)** 을 달성할 수 있음을 보였다. 이 방법은 모델 성능(예: NLP 벤치마크, 코딩 능력 등)에 큰 손실 없이 안전성을 크게 개선하며, *온라인(iterated online)* 학습을 통해 점진적으로 데이터와 모델을 개선할 수 있음을 입증했다.  

1. 독립적 유저 대화 비교 데이터(‘도움됨’과 ‘레드팀(red-teaming)’ 별도 수집) 구축  
2. 대화 기반 선호도 모델(preference model, PM)을 훈련  
3. PM 보상을 활용한 PPO 기반 RLHF로 최종 어시스턴트 정책 학습  
4. 모델·데이터 규모 스케일링, 안정성·강인성 분석  
5. 요약·코딩과 통합해도 성능 저하 없음  

***

## 1. 문제 정의 및 제안 기법

### 1.1 해결하고자 하는 문제  
- 순수 사전학습 언어 모델은 유해·편향 답변을 생성할 수 있음  
- 인스트럭션 파인튜닝만으로는 안전성(무해함)을 충분히 보장하기 어려움  
- 반면, 과도한 안전성 강화는 유용성(도움됨)을 저해  

### 1.2 제안 방법: RLHF 파이프라인  
1) **비교 데이터 수집**  
   - ‘도움됨’ 대화: 사용자가 두 모델 반응 중 더 도움이 되는 답 선택  
   - ‘레드팀’ 대화: 더 유해한 반응 선택  
   - 총 160k+ 비교 샘플 수집  
2) **선호도 모델(Preference Model, PM) 훈련**  
   - 언어 모델에 PMP(preference-model pretraining) 후 HF 비교 데이터로 파인튜닝  
   - 손실:  

$$ L_{\text{total}} = -\sum \log \sigma(r(\text{better}) - r(\text{worse})) $$  

3) **강화학습 정책 학습 (PPO)**  
   - 행동(대화 생성)에 대한 보상: PM 점수 r_PM  
   - KL 페널티를 추가한 보상:  

$$ r_{\text{total}} = r_{\text{PM}} - \lambda_{\text{KL}} D_{\mathrm{KL}}(\pi \,\|\, \pi_0) $$  
   
   - PPO 최적화로 최종 정책 파인튜닝  

***

## 2. 성능 향상 및 한계

### 2.1 성능 향상  
- **Elo 평가**: Context-distilled → Static RLHF → Online RLHF 순으로 인간 선호도 급상승  
- **NLP 벤치마크**:  
  - Zero-shot/N-shot 과제(MMLU, ARC, HellaSwag 등)에서 13B 이상 모델은 RLHF 후 성능↑  
  - 소형 모델은 ‘정렬 세금(alignment tax)’ 경험  
- **코딩 과제**: Python HumanEval에서도 52B 기준 RLHF 후 pass@k 성능↑  
- **편향·감정 분석**:  
  - 인종·종교 감정 표현 긍정성 전반 ↑  
  - 성별 편향 점수는 RLHF 후에도 원언어모델과 강하게 상관(온도 효과)  

### 2.2 한계 및 주목할 점  
- **도움됨 vs 무해함 충돌**:  
  - 순수 무해성 최적화 시 단순 회피 답변(“답변 불가”) 과잉 학습  
  - 상충 완화 위해 데이터 비율 조정 필요  
- **PM·RLHF 강인성 한계**:  
  - 고점수 구간 데이터 희소로 과최적화 위험  
  - 분포 변화로 발생하는 평가 불일치 문제  
- **정직성 개선 한계**: 거짓 정보 탐지 미흡, Human-in-the-loop 한계  

***

## 3. 일반화 성능 향상 가능성

- 대규모 모델일수록 PM·RLHF가 **다양한 스킬(요약·코딩·정렬 안전성)** 을 동시에 학습  
- 스케일 확장에 따라 **√DKL vs PM 보상** 선형 관계 관찰  
  - 적은 정책 변화로도 예측 가능한 성능 향상  
- “Online RLHF”로 **고품질 희귀 사례** 보강 → 분포 상위 꼬리 데이터 확보  
- OOD 탐지(거리 기반 Mahalanobis 변형)로 새로운 유해 요청 자동 차단 가능  

***

## 4. 향후 연구 방향 및 고려 사항

- **최악의 경우 안전성**: 평균 성능뿐 아니라 희귀·악의적 시나리오 대응 강화  
- **정직성(factuality)**: RLHF 외 자체 검증·검색 기반 진위확인 모듈 통합  
- **데이터 공유·표준화**: 가치 지향적 정렬 데이터 공개→커뮤니티 검증·공통 정책 수립  
- **인간-알고리즘 상호작용**: 대화 연속 생성에 대한 정책 학습, 사용자 의도 예측  
- **조화로운 멀티오브젝티브**: 도메인별·문화별 규범 반영한 유연한 보상 설계  

Anthropic의 연구는 **정렬(alignment)·정책 학습·스케일링** 의 조화 가능성을 제시하며, **대규모 AI 어시스턴트** 의 안전·유용성 확보를 위한 실질적 로드맵을 제시했다. 앞으로는 **분포 shift·악용 방지·정직성** 과제에 대한 기법 정교화가 핵심 이슈가 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/71590326-8850-4295-9e58-41a86f3b924c/2204.05862v1.pdf)
