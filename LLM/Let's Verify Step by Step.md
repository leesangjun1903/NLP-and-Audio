# Let's Verify Step by Step

### 1. 논문의 핵심 주장과 주요 기여

**핵심 주장**

이 논문은 **프로세스 감독(Process Supervision, PS)이 결과 감독(Outcome Supervision, OS)보다 대수학적 추론 작업에서 신뢰성 있는 보상 모델 학습에 현저히 우월**함을 주장합니다. 전통적으로 보상 모델은 최종 답변의 정확성만을 평가하는 OS 방식으로 훈련되어 왔으나, PS 방식은 각 중간 단계의 정확성을 평가함으로써 보다 정밀한 피드백을 제공합니다.[1]

**주요 기여**

1. **PS의 우월성 입증**: 프로세스 감독 기반 보상 모델(PRM)이 결과 감독 기반 모델(ORM)을 능가하며, MATH 데이터셋의 대표 부분집합에서 78.2%의 문제 해결률을 달성했습니다.[1]

2. **대규모 보상 모델의 감독 능력**: 대규모 PRM이 소규모 모델 훈련을 위한 감독 신호로 신뢰할 수 있게 작동하며, 이를 통해 대규모 인간 피드백 없이도 데이터 수집 절충을 효율적으로 수행할 수 있음을 보였습니다.[1]

3. **능동 학습의 효과**: 능동 학습이 프로세스 감독의 데이터 효율성을 **2.6배** 향상시킬 수 있음을 입증했습니다.[1]

4. **공개 데이터셋 공개**: 80만 개의 단계 수준 인간 피드백 라벨을 포함한 PRM800K 데이터셋을 공개하여 후속 연구를 촉진했습니다.[1]

***

### 2. 문제 정의, 제안 방법, 성능 분석 및 한계

#### 2.1 문제 정의

**근본적인 문제**: 대규모 언어 모델(LLM)은 복잡한 다단계 추론을 수행할 수 있지만, 논리적 오류를 빈번히 생성합니다. 특히 수학 문제 풀이와 같은 다단계 추론 작업에서 한 단계의 오류가 전체 해를 무효화할 수 있다는 점이 핵심 문제입니다.[1]

**기존 방식의 한계**: 
- **결과 감독 기반 ORM**: 최종 답변만을 평가하므로, 잘못된 추론으로 올바른 답에 도달한 경우를 감지하지 못합니다(거짓 양성).
- **신용 배분 문제**: 어느 단계에서 오류가 발생했는지 파악하기 어려워 모델이 학습하기 어렵습니다.

**필요성**: 각 중간 단계의 정확성을 평가하는 프로세스 감독을 통해 보다 정밀한 피드백과 개선된 신용 배분이 필요합니다.[1]

#### 2.2 제안 방법 및 수식

**프로세스 감독 기반 보상 모델(PRM) 구조**

PRM은 언어 모델 기반의 분류기로, 각 단계 이후의 토큰에서 해당 단계의 정확성을 예측합니다.

$$P_{\text{solution}} = \prod_{i=1}^{n} P(\text{step}_i \text{ is correct})$$

여기서:
- $P_{\text{solution}}$: 전체 해의 정확성 확률
- $P(\text{step}_i \text{ is correct})$: $i$번째 단계가 정확할 확률
- $n$: 총 단계 수

**훈련 목표**: 각 단계 $s_i$ 다음에 긍정(positive), 부정(negative), 중립(neutral) 레이블에 대한 로그 우도를 최대화합니다.

$$\mathcal{L}_{\text{PRM}} = -\sum_{i=1}^{n} \log P(y_i | \text{solution prefix ending in step } s_i)$$

여기서 $y_i$는 단계 $i$의 레이블입니다.[1]

**결과 감독 기반 보상 모델(ORM) 구조**

$$\mathcal{L}_{\text{ORM}} = -\log P(y_{\text{final}} | \text{full solution})$$

ORM은 최종 토큰의 예측만을 사용하여 전체 해의 정확성을 판단합니다.[1]

**능동 학습 전략**

$$\text{Selection Strategy} = 0.8 \times \text{Top-K wrong-answer} + 0.2 \times \text{Top-K remaining}$$

이 전략은 현재 최고 성능의 PRM이 잘못 평가한 '설득력 있는 오답' 해를 80% 선택하고, 나머지 20%는 다른 설득력 있는 해를 선택합니다.[1]

**합성 감독 방법**

대규모 PRM($\text{PRM}_{\text{large}}$)이 소규모 모델 훈련에 감독 신호를 제공합니다. 단계 정확성 결정을 위한 임계값:

$$\text{Step}_i \text{ is incorrect if } P_{\text{PRM}_{\text{large}}}(\text{negative}_i) > 0.2$$

#### 2.3 모델 구조

**데이터 수집 파이프라인**:
1. **생성기(Generator)**: GPT-4 기반 모델이 줄바꿈으로 구분된 단계별 해를 생성
2. **인간 라벨링**: 각 단계를 긍정/부정/중립으로 분류
3. **반복적 정제**: 현재 최고 성능 PRM을 사용하여 정보가 많은 해를 선별
4. **데이터셋 구성**: PRM800K = 75,000개 해, 800,000개 단계 레이블 (12,000개 문제)[1]

**모델 크기 및 구성**:
- **대규모 실험**: GPT-4 기반 미세조정 모델
- **소규모 실험**: GPT-4 대비 약 200배 적은 계산량의 모델
- **사전훈련 데이터**: MathMix (15억 토큰)에서 미세조정[1]

#### 2.4 성능 향상

**주요 성과**

| 방법 | Best-of-1860 성능 | 향상도 |
|------|-----------------|-------|
| 다수결 투표(Majority Voting) | 69.6% | 기준선 |
| ORM(결과 감독) | 72.4% | +2.8%p |
| **PRM(프로세스 감독)** | **78.2%** | **+8.6%p** |

PRM은 모든 N 값에서 ORM을 능가하며, **N이 증가할수록 성능 격차가 벌어집니다**.[1]

**능동 학습의 효과** (소규모 실험)

- **균등 샘플링**: 200개 샘플/문제로 ~55% 성능
- **능동 학습**: 40개 샘플/문제로 ~48% 성능 달성
- **효율성 향상**: **2.6배 데이터 효율 개선**[1]

**분포 외 일반화** (최근 STEM 시험)

| 데이터셋 | 문제 수 | ORM | PRM | 다수결 |
|---------|--------|-----|-----|-------|
| AP 미적분 | 45 | 68.9% | **86.7%** | 80.0% |
| AP 화학 | 60 | 68.9% | **80.0%** | 71.7% |
| AP 물리 | 45 | 77.8% | **86.7%** | 82.2% |
| AMC 10/12 | 84 | 49.1% | **53.2%** | 32.8% |
| **합계** | **234** | **63.8%** | **72.9%** | **61.3%** |

PRM은 훈련 분포 밖의 문제에서도 일관되게 우월한 성능을 보입니다.[1]

#### 2.5 모델의 한계

**신용 배분의 한계**: 
PRM은 각 단계의 정확성을 독립적으로 평가하지만, 실제로는 단계들 간의 복잡한 의존성이 존재합니다. 현재 설계는 단방향 피드백만 제공하므로 **향후 성공 가능성을 고려하지 않습니다**.[1]

**거짓 양성/음성**:
- PRM이 미묘한 세기 오류(Problem 9)나 대수적 오류(Problem 10)를 간과하는 경우가 있습니다.
- PRM의 보정(calibration) 문제로 인해 음성 레이블을 양성으로 해석할 수 있습니다.[1]

**데이터 불균형**:
능동 학습 전략의 부작용으로 훈련 데이터가 오답 해에 편향됩니다(Phase 2: 정답률 13.2%).[1]

**테스트 세트 오염**:
MathMix 데이터셋과 MATH 테스트 세트 간의 중복 가능성이 완전히 배제될 수 없습니다. 그러나 생성기의 낮은 해결률(많은 문제에서 단일 자릿수%)이 이를 일부 완화합니다.[1]

**제한된 도메인 평가**:
현재 연구는 MATH 데이터셋의 수학 문제에 제한되어 있으며, 다른 추론 작업(물리, 화학, 코딩 등)으로의 일반화는 미검증입니다.[1]

***

### 3. 일반화 성능 향상 가능성

#### 3.1 일반화 성능의 이론적 기초

**신용 배분 개선**: 
프로세스 감독은 오류의 정확한 위치를 특정함으로써 신용 배분 문제를 완화합니다.

$$\text{Information Gain}_{\text{PRM}} = \text{Entropy}(y_{\text{final}}) - \text{Entropy}(y_1, y_2, \ldots, y_n)$$

단계별 피드백은 훨씬 높은 정보 이득을 제공하므로 모델이 일반화하기 쉬워집니다.[1]

**해석 가능성**:
프로세스 감독은 인간이 검증하기 쉬운 중간 단계를 강조하므로, 인간 평가자의 피드백이 더 정확해집니다.

#### 3.2 분포 외 성능

**테스트 결과** (Section 5):
최근 AP 시험과 AMC 문제에서 PRM이 일관되게 ORM을 능가합니다.

- **일반화의 증거**: 훈련되지 않은 최신 문제에서도 PRM이 72.9% 달성
- **난이도 안정성**: 모든 난이도 범위에서 성능 향상 (Appendix G)[1]

#### 3.3 능동 학습의 일반화 효과

능동 학습은 **정보 밀도가 높은 샘플을 선택**하여 일반화 성능을 향상시킵니다.

$$\text{Information Density} = \frac{\text{P}(\text{Model disagrees with true label})}{\text{Labeling cost}}$$

설득력 있는 오답을 선택하면 모델이 구별하기 어려운 경계 사례를 학습합니다.[1]

#### 3.4 현재 연구의 한계와 미래 가능성

**현재 한계**:
1. **단방향 보상**: 기존 PRM은 과거 단계만 평가하고 미래 성공 가능성을 고려하지 않음
2. **도메인 특이성**: 수학 영역에서만 검증, 일반화 도메인 성능 미불명
3. **복잡한 추론 구조**: 의존성이 있는 다단계 작업에 적용 미검증[1]

***

### 4. 최신 연구 기반 앞으로의 연구 영향 및 고려 사항

#### 4.1 논문이 AI 연구에 미친 영향

**즉각적 영향**:

1. **프로세스 감독 방법론의 확산** (2024-2025): 
   - OpenAI의 PRM800K 공개 이후, 프로세스 감독이 LLM 추론 연구의 표준 패러다임으로 확립되었습니다.[2][3][4]
   - 수학(Deepseek-R1), 코딩, 임상 기록 생성 등 다양한 분야로 확대 적용되고 있습니다.[5][2]

2. **양극 보상 신호로의 진화** (2025):
   - BiRM(쌍방향 보상 모델)이 단방향 PRM의 한계를 해결하려는 시도로 등장했습니다.[6]
   - 기존 PRM의 일방향 신호 외에도 미래 성공 확률을 모델링하여 MATH-500에서 3.1% 향상 달성[6]

3. **차원 수준 감독으로의 확장** (2025):
   - Dimension-level Reward Model(DRM)이 신뢰성(Confidence), 관련성(Relevance), 응집성(Coherence) 세 차원으로 일반화를 촉진했습니다.[7]
   - 열린 도메인 작업(QA, 퍼즐)에서 기존 OS/PS 방식 능가[7]

#### 4.2 도메인 확장 연구

**1. 의료/임상 도메인** (2024-2025):
임상 기록 생성에 PRM을 적용한 연구에서는 다음을 입증했습니다.[2]
- 도메인 전문성 기반 오류 주입으로 "스텝" 정의 도메인 화
- 의사 평가에서 Best-of-N 성능 56.2% 달성
- **영향**: 건강-정책 대화에서 안전성 중심 감독의 중요성 강조[2]

**2. 코드 생성 도메인** (2024-2025):
프로세스 감독을 코드 생성에 적용한 결과:[8][5]
- 라인별 피드백으로 테스트 실패 코드에도 학습 신호 제공
- LiveCodeBench: 28.2% → 29.8%, 내부 벤치: 31.8% → 35.8%
- **영향**: 긴 시계열 작업에서 PS의 우월성 재확인[5][8]

**3. 운영 연구(OR) 도메인** (2025):
StepORLM은 생성형 프로세스 감독(GenPRM)을 도입하여:[9]
- 기존 판별형 PRM과 달리 전체 해 구조를 통합 평가
- 외부 솔버와의 이중 피드백 루프로 일반화 촉진
- 6개 벤치마크에서 SOTA 달성[9]

#### 4.3 알고리즘 혁신

**1. 자동화된 프로세스 감독** (2024):
OmegaPRM이 이진 검색과 MCTS를 통해 자동 프로세스 감독을 구현했습니다.[3]
- 수동 아노테이션 없이 150만 개 이상의 프로세스 감독 데이터 수집
- **의의**: 인간 피드백의 병목 해소 및 대규모 학습 가능[3]

**2. RL 훈련 시 보상 해킹 완화** (2024):
논문에서 식별하지 못한 보상 해킹 문제가 2024년 후속 연구에서 대두되었습니다.[4]
- 보상 모델이 불필요한 정확한 단계 반복으로 높은 점수 획득
- **해결책**: Clipping과 Delta 정규화로 누적 보상 상한선 설정[4]
- **교훈**: OS/PS 모두 RL 훈련에서 직접 사용 시 신중 필요[4]

**3. 반대 방향 강화학습** (2025):
역 프로세스 감독(InversePRM)이 명시적 결과 감독 없이 데모에서 직접 학습합니다.[10]
- 에이전트 상호작용을 통한 온라인 학습 지원
- ALFWorld 벤치: 3B 모델이 GPT-4o 능가[10]

#### 4.4 일반화 성능 개선을 위한 향후 연구 고려사항

**1. 도메인 이동 강건성**:
현재 연구에서 MATH와 최신 시험 간 약간의 성능 하락이 관찰됩니다. 향후 연구는:
- 다양한 도메인에서의 교차 도메인 미세조정 전략 개발
- 도메인 불변 표현 학습을 통한 일반화 개선[7]

**2. 단계 정의의 일관성**:
현재 PRM은 문제-해 쌍에서 자동으로 단계를 파싱합니다. 향후 개선:
- 계층적 단계 정의(추상 → 구체적)로 도메인 간 일반성 확보
- LLM 기반 동적 단계 분할 전략[9]

**3. 보상 모델의 보정**:
PRM의 거짓 양성/음성 문제를 해결하기 위해:
- 온도 스케일링(Temperature Scaling) 등 보정 기법 적용
- 불확실성 추정을 통한 신뢰도 높은 평가[6]

**4. 장기 의존성 모델링**:
장문의 추론(예: 과학 논문, 법률 문서)에 대해:
- BiRM 방식의 양방향 신호가 더 효과적임을 시사[6]
- 어텐션 메커니즘으로 전역 맥락 통합[9]

**5. RL 훈련 안정성**:
보상 해킹 문제를 근본적으로 해결하기 위해:
- 다목적 최적화로 OS와 PS를 결합한 안정적 신호[7]
- 정책 그래디언트 정규화로 보상 모델 오버피팅 방지[4]

**6. 인간-AI 협업 강화**:
프로세스 감독의 해석 가능성 장점을 활용하여:
- 중간 단계의 인간 검증을 통한 신뢰도 향상
- 설명 가능성 기반의 적응형 피드백 루프 구축[2]

***

### 5. 결론

"Let's Verify Step by Step"는 **프로세스 감독의 우월성을 체계적으로 입증**함으로써 LLM 추론 분야에 패러다임 전환을 가져왔습니다. 특히 능동 학습과 합성 감독을 결합한 데이터 수집 방법론은 인간 피드백의 비용을 획기적으로 절감하면서도 성능을 향상시키는 경로를 제시했습니다.[1]

2024-2025년 후속 연구들은 이 논문의 기초 위에서:
- **도메인 확장** (의료, 코딩, OR)으로 일반성 검증
- **알고리즘 혁신** (양극 신호, 다차원 감독, 자동화)으로 한계 극복
- **RL 안정성** 개선으로 실제 적용 가능성 강화

를 추진하고 있습니다. 다만 **도메인 간 전이 학습**, **보상 모델 안정성**, **장기 추론 구조 모델링** 등은 여전히 해결해야 할 중요 과제이며, 이들을 해결할 때 프로세스 감독 기반 접근의 진정한 가치가 실현될 것으로 전망됩니다.[4][7][6][9]

***

### 참고문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/6613818f-3868-44fc-a4c7-e7a3b660e2d7/2305.20050v1.pdf)
[2](https://aclanthology.org/2025.emnlp-main.967)
[3](https://www.semanticscholar.org/paper/f32bcc2155997110a7905da050df4c8404867b24)
[4](https://arxiv.org/abs/2410.15115)
[5](https://arxiv.org/abs/2410.17621)
[6](https://arxiv.org/abs/2503.04618)
[7](https://arxiv.org/abs/2510.11457)
[8](https://openreview.net/forum?id=Cn5Z0MUPZT)
[9](https://arxiv.org/abs/2509.22558)
[10](https://arxiv.org/abs/2502.10325)
[11](https://ieeexplore.ieee.org/document/11166626/)
[12](https://arxiv.org/abs/2402.00658)
[13](http://arxiv.org/pdf/2502.10325.pdf)
[14](https://arxiv.org/pdf/2503.04618.pdf)
[15](http://arxiv.org/pdf/2410.17621.pdf)
[16](http://arxiv.org/pdf/2407.04185.pdf)
[17](https://arxiv.org/html/2503.21295v1)
[18](http://arxiv.org/pdf/2502.01456.pdf)
[19](https://arxiv.org/html/2412.15118)
[20](https://arxiv.org/pdf/2410.08146.pdf)
[21](https://www.emergentmind.com/topics/process-supervised-reward-models-prm)
[22](https://www.emergentmind.com/topics/verification-chain-of-thought-cot)
[23](https://tiger-ai-lab.github.io/General-Reasoner/)
[24](https://aclanthology.org/2025.findings-acl.747.pdf)
[25](https://arxiv.org/html/2510.09312v1)
[26](https://arxiv.org/abs/2505.14652)
[27](https://research.google/pubs/a-chain-of-thought-is-as-strong-as-its-weakest-link-a-benchmark-for-verifiers-of-reasoning-chains/)
[28](https://arxiv.org/html/2507.04391v1)
[29](https://aclanthology.org/2025.findings-emnlp.253.pdf)
