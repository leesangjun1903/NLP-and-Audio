# Mistral 7B

**핵심 주장**  
Mistral 7B는 7억 개 매개변수를 가진 경량 언어 모델로, 기존의 13B 및 34B 모델을 능가하는 성능을 유지하면서도 효율적인 추론 속도와 메모리 사용을 달성한다.[1]

**주요 기여**  
- 그룹화된 쿼리 어텐션(GQA)을 통한 추론 가속화  
- 슬라이딩 윈도우 어텐션(SWA)으로 긴 문맥 처리 비용 절감  
- 롤링 버퍼 캐시 및 청크 전처리로 메모리 사용 최적화  
- 지침 기반 파인튜닝(Instruct) 버전으로 챗 성능 강화[1]

***

## 1. 해결하고자 하는 문제  
기존 대형 언어 모델들은  
1. **높은 연산 비용**  
2. **긴 문서 처리 시 지수적 비용 증가**  
3. **메모리 폭발 문제**  
로 인해 실시간·저비용 배포에 제약이 있었다. Mistral 7B는 이 세 가지 병목을 동시에 해소하는 것을 목표로 한다.[1]

***

## 2. 제안 방법 및 수식

### 2.1 그룹화된 쿼리 어텐션(GQA)  
입력 토큰 $$Q,K,V$$에서 헤드 수를 줄이지 않고도 쿼리 프로젝션만 그룹화하여 계산량을 $$\tfrac{1}{g}$$로 감소시킨다.  

$$
\text{GQA}(Q,K,V) = \mathrm{softmax}\Bigl(\tfrac{(QW_Q) (KW_K)^\top}{\sqrt{d}}\Bigr)\,(VW_V)
$$  

단, $$W_Q$$를 $$g$$그룹으로 분할해 연산을 가속화한다.[1]

### 2.2 슬라이딩 윈도우 어텐션(SWA)  
각 계층 $$k$$에서 토큰 $$i$$가 직전 $$W$$개 토큰만 참조하도록 제한한다.  

$$
h_i^{(k)} = \sum_{j=\max(0,i-W)}^i \alpha_{ij} \, h_j^{(k-1)},\quad \alpha_{ij}=\mathrm{softmax}\bigl(h_i^{(k-1)}W_Q (h_j^{(k-1)}W_K)^\top\bigr)
$$  

이로써 이론적 최대 문맥 길이는 $$W\times n_{\text{layers}}$$에 도달한다.[1]

***

## 3. 모델 구조  
| 구성 요소        | 값               |
|-----------------|------------------|
| 차원 (dim)     | 4096             |
| 레이어 수       | 32               |
| 헤드 수         | 32               |
| 윈도우 크기 $$W$$ | 4096             |
| 최대 문맥 길이  | 8192             |
| 어휘 크기       | 32000            |

롤링 버퍼 캐시로 키·값 텐서를 크기 $$W$$의 순환 버퍼에 저장해 메모리 사용량을 $$W$$ 이상으로 늘어나지 않게 한다.[1]

***

## 4. 성능 향상 및 한계

### 4.1 성능 비교  
Mistral 7B는 Llama 2 13B 대비  
- **MMLU**: 55.6% → 60.1%  
- **공통상식 추론**(HellaSwag 등) 전 범주 우위  
- **수학·코드 생성**에서 34B 수준 성능 달성[1]

### 4.2 효율성  
- GQA+SWA를 통해 동일 수준 성능을 3× 모델 크기로 달성 가능  
- 메모리 사용 8× 감소[1]

### 4.3 한계  
- **지식 압축 한계**: 세계 지식 벤치마크에서는 1.9× 압축률로 다소 성능 저하  
- **학습 데이터 규모**: 추가 대규모 지식 학습 시 성능 향상 여부 불확실  
- **안전성·윤리적 가드레일**: 시스템 프롬프트 활용에도 일부 케이스서 답변 거부 편향 존재 가능[1]

***

## 5. 일반화 성능 향상 가능성

Mistral 7B 아키텍처가 모델 크기를 늘리지 않고도 고차원 표현 학습을 가능케 하므로,  
- **다양한 태스크 전이 능력**이 향상될 수 있으며  
- 슬라이딩 윈도우 구조는 문맥 흐름 학습을 장기 종속성에까지 확장시켜  
- **제로샷/소샷** 일반화가 더욱 강화될 여지를 제공한다.  

특히 파인튜닝 없이도 In-Context Learning 효율이 향상되어, 새로운 도메인 적응 시 빠른 수렴을 기대할 수 있다.

***

## 6. 향후 연구 영향 및 고려사항

Mistral 7B는 **비용·추론·성능**의 삼차원 스펙트럼 최적화 방향을 제시한다.  
**향후 과제**  
- GQA/SWA를 더 경량화해 저전력 엣지 디바이스 적용  
- 지식 증류 및 외부 메모리 통합으로 압축 한계 극복  
- 안전성·윤리성 평가 지표 고도화  

이를 통해 연구자들은 **규모 확장 대신 구조 혁신**으로 효율적 고성능 모델 개발 전략을 재고하게 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/8d24bea5-b715-4a39-9604-f6510ba31246/2310.06825v1.pdf)
