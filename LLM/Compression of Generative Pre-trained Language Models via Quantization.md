# Compression of Generative Pre-trained Language Models via Quantization

## 1. 핵심 주장 및 주요 기여
- **핵심 주장**: 기존 양자화 기법은 생성형 언어 모델(예: GPT-2, BART)에 직접 적용할 경우 성능이 급격히 저하된다. 그 원인은 (1) 임베딩 벡터의 동질성(homogeneous embeddings) 증가, (2) 서로 다른 모듈별 가중치 분포 변화가 크기 때문이다.
- **주요 기여**:
  1. **문제 발견**: 생성형 PLM을 양자화할 때 임베딩 구분력 저하와 모듈별 가중치 분포 변화가 성능 저하를 초래함을 규명.
  2. **토큰 단위 대조 증류(Token-level Contrastive Distillation)**: 양자화된 학생 모델과 풀-프리시전 교사 모델의 각 토큰 표현을 대조 학습함으로써 임베딩 구분력을 회복.
  3. **모듈별 동적 스케일링(Module-wise Dynamic Scaling)**: 각 모듈 가중치 ℓ₁ 평균값에 비례하여 클리핑 팩터 α를 γ·(∥w∥₁/n)로 학습, 내부·외부 구간 모든 가중치를 고려한 그라디언트 추정식(식 7) 제안.
  4. **경량 모델 달성**: GPT-2에서 14.4×, BART에서 13.4× 압축하면서 원-정밀도 성능과 유사한 퍼플렉서티 및 ROUGE 성능 달성.

## 2. 문제 정의, 제안 기법 및 모델 구조

### 2.1 해결하고자 하는 문제
- **양자화 실패 원인**  
  -  **임베딩 동질성 증가**: 비트 수 감소 시 단어 임베딩이 군집화되어 구분이 어려워지고, 순차 생성 과정에서 누적된 양자화 오차가 더욱 심화됨.  
  -  **모듈별 가중치 분포 편차**: Transformer 내부 레이어·모듈 간 분포 차이가 커, 단일 클리핑 팩터로는 최적 양자화 불가.

### 2.2 제안 방법

1. **토큰 단위 대조 증류**  
   – 학생(s)·교사(t) 네트워크의 i번째 토큰 표현 $$h^{s}_i, h^{t}_i$$를 양성(positive)으로, 동일 문장 내 타 토큰 표현을 음성(negative)으로 사용  
   – 메모리 뱅크에 모멘텀 업데이트된 표현 $$q^{s}_i$$ 저장  
   – 학생→교사, 교사→학생 대조 손실:  

$$
       \ell_{s2t} = -\sum_{i=1}^n \log \frac{\exp\big(\mathrm{cos}(q^s_i, h^t_i)/\tau\big)}{\sum_{j\in S_i}\exp\big(\mathrm{cos}(q^s_i, h^t_j)/\tau\big)}
     $$  
   
   – 최종 손실: $$\ell = \lambda\ell_{cont} + \ell_{dist}$$, $$\ell_{dist}=-\sum_i z^t_i\log z^s_i$$, $$\lambda=0.1$$

2. **모듈별 동적 스케일링**  
   – 각 가중치 벡터 $$w$$에 대해 클리핑 팩터를  

$$
       \alpha = \gamma \cdot \frac{\|w\|_1}{n}
     $$  
     
  으로 정의 (γ 학습 파라미터)  
   – 그라디언트 추정(식 7)에서 클리핑 경계 내부·외부 모두 고려하여 γ 업데이트  

### 2.3 모델 구조
- 기존 GPT-2/BART 백본에 **전층(임베딩 및 Transformer 레이어)** 양자화 적용  
- 토큰 단위 대조 증류를 위해 메모리 뱅크·모멘텀 인코더 추가  
- BART는 디코더 마지막 레이어 표현에 대조 증류, 양자화 후 요약 생성  

## 3. 성능 향상 및 일반화 역량
- **언어 모델링(PPL)**  
  – GPT-2 8/4/2비트 모델에서 원-정밀도 대비 각각 미미한(≤1–2PPL) 성능 하락, 14.4× 모델 크기 축소  
- **추론 과제(Acc)**  
  – Persona-Chat 다음 발화 예측에서 2비트까지도 정확도 74.8% 유지 (원-정밀도 77.0%)  
- **추상 요약(ROUGE)**  
  – BART 2비트 모델이 원-정밀도 대비 R1/R2/RL 각각 약 1.6/1.4/1.3포인트 하락  
- **일반화 성능**  
  – 토큰 대조 증류가 표현의 판별력을 강화하여 소규모 데이터셋(PTB 등)과 대규모( WikiText103, XSum) 모두에서 안정적 성능 유지  
  – 메모리 뱅크 기반 대조 손실이 **오버피팅 방지** 및 **분석되지 않은 입력 분포**에서도 견고한 표현 학습에 기여  

## 4. 한계 및 향후 고려 사항
- **메모리·연산 오버헤드**: 대조 증류 메모리 뱅크 저장으로 GPU 메모리 소모 증가  
- **음성 샘플링 민감도**: 부적절한 negative 샘플링 또는 λ 과대 설정 시 성능 저하  
- **실제 속도 검증 필요**: 하드웨어별 저비트 연산 최적화 미구현 상태

## 5. 향후 연구 영향 및 고려점
- **차세대 초거대 모델 압축**: GPT-3, PaLM 등 수십억 파라미터급 모델 양자화 전략으로 확장 가능  
- **하드웨어 연동 최적화**: ASIC/FPGA 등 저비트 연산 가속기와 연계해 실제 추론 속도·전력 효율 검증  
- **대조 학습 일반화**: 소수 샘플 환경·다중 언어·멀티모달 모델 양자화 시 토큰 단위 대조 증류 적용 연구  
- **동적 스케일링 개선**: 클리핑 초기화 민감도 완화, 적응적 스케일링 스케줄링 기법 탐색

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b3f20c59-d3c3-4b8b-a579-5c688928245f/2203.10705v2.pdf
