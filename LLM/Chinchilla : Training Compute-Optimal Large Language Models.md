# Chinchilla : Training Compute-Optimal Large Language Models

# 핵심 요약

**최적의 계산 자원 활용을 통해 모델 크기와 학습 토큰 수를 균형 있게 확장할 때 언어 모델의 성능이 최대화된다.** 구체적으로, 계산 예산을 두 배로 늘리면 모델 파라미터 수와 학습 토큰 수도 각각 두 배로 늘려야 하며, 이 전략으로 훈련한 ‘Chinchilla(70B)’는 동일 계산 예산의 ‘Gopher(280B)’보다 일관되게 뛰어난 성능을 보였다.  

***

# 1. 문제 정의  
대규모 언어 모델 훈련 시 주어지는 계산(FLOPs) 예산 내에서  
- 모델 파라미터 수(N)와 학습에 사용되는 토큰 수(D)를 어떻게 **최적 배분**해야 최종 언어 모델 손실(L)을 최소화할 수 있는가?  

기존 연구(Kaplan et al., 2020)는 계산 예산이 10배 증가하면 N은 5.5배, D는 1.8배만 늘리면 된다고 제안했으나, 실제 최적 전략은 이와 달리 **N과 D를 동일 비율로 확장**하는 것이다.  

***

# 2. 제안 방법  
세 가지 상이한 실험·추정 기법을 통해 최적 전산경계(compute-optimal frontier)를 도출하였다.  

## 2.1. 훈련 곡선 최소화 방식  
- 서로 다른 크기(70M–10B) 모델을 다양한 학습 토큰 수(5B–400B)로 다수 훈련  
- 각 실험에서 FLOPs 대비 최저 손실 지점을 연결하여 (C→L) 함수 구축  
- 지수 함수 $$N_{\mathrm{opt}} \propto C^a,\; D_{\mathrm{opt}} \propto C^b$$를 피팅  
  → $$a\approx0.50,\; b\approx0.50$$  

## 2.2. IsoFLOP 프로파일  
- 고정 FLOPs(6×10¹⁸–3×10²¹)마다 다양한 모델 크기로 학습  
- 같은 계산량 하에서 파라미터 수별 최종 손실의 최소값 지점으로 최적 N 추정  
- 1.과 유사하게 $$a\approx0.49,\; b\approx0.51$$  

## 2.3. 파라메트릭 손실 함수 적합  
- 손실을  

$$
    \widehat{L}(N,D) = E + A\,N^{-\alpha} + B\,D^{-\beta}
  $$  
  
형태로 가정하고, 실험 데이터에 Huber 손실로 최적 파라미터($$E,A,B,\alpha,\beta$$) 피팅  

- 계산 예산 제약(FLOPs ≈6ND) 하에서 라그랑주법으로 $$N_{\mathrm{opt}},D_{\mathrm{opt}}$$ 유도  
  
→ $$a=\beta/(\alpha+\beta)\approx0.46,\; b\approx0.54$$  

세 접근 모두 **파라미터 수와 학습 토큰 수를 동일 비율로(≈50% 대 50%) 증대**해야 함을 일관되게 나타냈다.  

***

# 3. 모델 구조와 실험 설정  
- **Chinchilla(70B)**: Gopher(280B)와 동일한 Transformer 구조  
  - 레이어 수 80, 어텐션 헤드 64, 모델 차원 8,192, FFN 차원 32,768  
  - 학습 토큰 1.4조(4× Gopher), 계산 예산 동일  
  - 옵티마이저: AdamW, 학습률 코사인 스케줄(10× 감쇄)  
  - 데이터: MassiveText(중량 웹, 책, C4, 뉴스, GitHub, 위키피디아)  

***

# 4. 주요 성능 향상  
Chinchilla는 Gopher 대비 거의 모든 벤치마크에서 우수성을 입증했다.  

- **언어 모델링**(The Pile, Wikitext-103, C4 등): 비트 당 바이트(bpb) 및 퍼플렉서티 일관 하락  
- **MMLU**(57개 과목): 5-shot 평균 정확도 67.6%로 Gopher 대비 +7.6%p, 인간 예측(2023년)보다 우수  
- **BIG-bench**: 62개 과제 중 58개에서 상향, 평균 +10.7%p  
- **독해(Reading Comprehension)**: RACE-m +11.7%p, RACE-h +10.7%p  
- **클로즈드북 QA**: NaturalQuestions 5-shot 31.5%(Gopher 24.5%), TriviaQA 0-shot 67.0%(Gopher 52.8%)  
- **공통 상식(Common Sense)**: HellaSwag, PIQA, Winogrande, BoolQ 모두에서 Gopher 상회  

이처럼 동일 계산 예산 하에 **모델 크기를 줄이고(4× 작음) 데이터 양을 늘릴수록**(4× 많음) 전반적 성능이 크게 향상됨을 실험적으로 확인했다.  

***

# 5. 한계 및 고려 사항  
- 대규모 재훈련 런 수 제한(주요 비교는 Gopher 단일 대비)  
- 파워로 관계만 고려, 고계산 예산 영역의 곡률(비선형성) 반영 미흡  
- 데이터·모델 크기 확장에 따른 **데이터 품질, 윤리·프라이버시 이슈** 심화  

특히 **훈련 토큰 수가 수조 단위를 넘어설수록** 학습-테스트 중복, 민감 정보 유출, 편향·독성 데이터 증폭 등 윤리적 위험이 커지므로 세심한 데이터 큐레이션과 위험 완화가 필요하다.  

***

# 6. 향후 영향 및 연구 시 고려 사항  
- **데이터 스케일링의 중요성 강조**: 모델 성능 향상을 위해서는 하드웨어 확장과 더불어 대규모·고품질 데이터 확보가 필수  
- **최적 계산 프론티어 적용 범위 확대**: 비자연어(LM 외)·다른 아키텍처(MoE, Retrieval-Augmented 등)에도 동일 원리 적용 가능성 탐색  
- **비선형 스케일링 탐구**: 고계산 예산 구간에서의 곡률 효과, 다중 에폭 학습(regime) 영향 분석  
- **윤리·안전성**: 수조 토큰 규모 훈련에 따른 편향·독성·개인정보 유출 위험 평가 및 완화 전략 병행 연구  

이 논문은 “계산 예산 내 모델과 데이터 균형”이라는 명확한 설계를 통해 대규모 언어 모델 연구의 방향을 재정립하며, 향후 **효율적·책임감 있는 AI 시스템 개발**을 위한 핵심 지침을 제시한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/4f068a16-08a1-4989-b83b-1e0d311127af/2203.15556v1.pdf)
