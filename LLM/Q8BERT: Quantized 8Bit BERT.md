# Q8BERT: Quantized 8Bit BERT

## 핵심 주장 및 주요 기여
Q8BERT는 BERT 모델의 파라미터와 활성화 모두를 8비트 정수(Int8)로 양자화하면서, 원본 FP32 모델 대비 약 4×의 메모리 절감을 달성하고, 대부분의 NLP 과제에서 1% 미만의 성능 저하만으로 정확도를 유지할 수 있음을 보인다.[1]
- **양자화 인식 학습(Quantization-Aware Training, QAT)**을 통해 양자화 오차를 모델 학습 과정에서 반영하여, 8비트 정밀도로도 FP32 성능에 근접한 결과를 얻는다.[1]
- 8비트 하드웨어 가속을 활용하면 GEMM 연산 속도가 최대 3.7×까지 향상될 수 있음을 확인하였다.  

## 해결하고자 하는 문제
대규모 Transformer 기반 언어 모델(BERT-Base: 110M 파라미터, BERT-Large: 334M 파라미터)은 추론 시 높은 메모리 대역폭과 연산량 때문에 실시간 응용에 적합하지 않다.  
- **메모리 풋프린트**와 **지연 시간** 문제로, 모바일·엣지·데이터센터 환경에서 배포 및 확장에 제약이 크다.  

## 제안하는 방법
### 양자화 스킴 (Linear Symmetric Quantization)
x를 b비트 정수로 변환할 때 다음 수식을 사용한다:  

$$
\mathrm{Quantize}(x \mid S_x, M) := \mathrm{Clamp}\bigl(\lfloor x \times S_x\rceil,\,-M,\,M\bigr)
$$  

$$
M = 2^{b-1}-1,\quad S_x=\frac{M}{\max(|x|)}
$$  

활성화의 스케일링 인자 $$S_x$$는 지수이동평균(EMA)을 이용해 학습 중 통계량으로 계산한다.  

### 양자화 인식 학습 (QAT)
- **페이크 양자화(fake quantization)** 연산을 순전파 단계에 삽입하여, 학습 중에 양자화 오차가 모델에 반영되도록 함.  
- 역전파 시에는 Straight-Through Estimator(STE)를 사용하여 비도함수 연산의 그라디언트를 근사한다:  

$$
  \frac{\partial x_q}{\partial x} = \overrightarrow{1}
  $$  
- 임베딩과 FC(fully connected) 계층의 가중치 및 입력을 Int8로 양자화하고, 바이어스만 Int32로 유지하여 누적 연산의 정밀도를 확보한다.[1]

### 모델 구조
- 기존의 BERT 임베딩 및 FC 계층을 양자화된 버전으로 대체.  
- Softmax, LayerNorm, GELU 등 정밀도가 중요한 연산은 FP32로 유지.  
- PyTorch-Transformers 기반 구현체를 수정하여 모든 주요 행렬곱 연산을 양자화된 버전으로 수행.  

## 성능 향상 및 한계
| 과제           | FP32 정확도  | Q8BERT(Int8) 정확도 | 상대 오차율  |
|---------------|--------------|----------------------|------------|
| GLUE 평균     | –            | –                    | <1%       |
| SQuAD v1.1 F1 | 88.46        | 87.74                | 0.81%      |

- **메모리 절감**: 모델 크기 4× 감소.  
- **추론 속도**: 8비트 하드웨어 지원 시 최대 3.7× 가속.  
- **한계**:  
  - RTE(Task) 등 일부 극단적 소규모 데이터 과제에서 1% 이상의 성능 저하 발생.  
  - 완전한 8비트 연산 지원 하드웨어에 의존적이며, 비가속 환경에서는 속도 이점이 제한적일 수 있음.  

## 모델 일반화 성능 향상 가능성
- 학습 과정에 양자화 오차를 반영하므로, 도메인 변화에 강인해질 가능성이 있다.  
- 페이크 양자화와 EMA 기반 스케일링은 드롭아웃, 배치 차원 다양성 등에 대한 내성을 높여, 다양한 입력 분포에서도 성능 안정성을 확보할 수 있다.  

## 향후 연구에 미치는 영향 및 고려점
- **영향**: 대규모 언어 모델의 배포 효율화를 가속하여, 엣지·모바일 장치에서도 실시간 NLP 응용을 현실화할 기반을 제공한다.  
- **고려점**:  
  - 양자화 외에 프루닝(pruning), 지식 증류(distillation) 등 다른 압축 기법과의 결합 연구.  
  - 4비트 이하 초저비트(ultra-low-bit) 양자화 적용 시 추가 정밀도 보전 전략.  
  - 비Intel 비전용 하드웨어(GPU·TPU)에서의 최적화 가능성 및 라이브러리 지원.  

***

 Abstract 및 도입부에서 정리된 주요 기여[1]
 Section 2 “Quantization Scheme”에서 제안된 스케일링 및 가속 성능 근거

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6b63f1ee-19ad-435b-97cf-8c6e68e6f284/1910.06188v2.pdf
