# Evaluating Quantized Large Language Models

## 1. 핵심 주장 및 주요 기여  
이 논문은 **포스트 트레이닝 양자화(Post-Training Quantization, PTQ)가 대형 언어 모델(LLM)에 미치는 영향**을 체계적으로 평가하고,  
모델 크기·테스크 유형·텐서 종류(가중치·활성화·KV 캐시)·양자화 방법별로 최적 비트폭(bit-width)을 제시함.  
주요 기여는 다음과 같다:  
- 11개 모델 계열(125M–180B)·5개 테스크(기초 NLP, 응급 능력, 신뢰성, 대화, 장문 처리) 전반에 걸친 종합 평가  
- Weight-only, Weight-Activation, KV Cache 양자화별 성능 저하 분석 및 모델 크기 의존성 규명  
- 비트폭별 추천 설정 제안(예: 대부분 테스크에서 W4, W4A8, KV4는 2% 이내 성능 저하)  
- AWQ·SmoothQuant 등 SOTA 복원 기법 적용성 검증  

## 2. 문제 정의  
- **문제**: LLM은 FP16 정밀도의 큰 모델이지만, 추론 시 메모리·연산 비용이 매우 큼. PTQ는 2–8비트로 양자화해 비용을 줄이나, 성능 저하 우려가 있음.  
- 현황: 기존 연구는 일부 모델·테스크·텐서에 한정된 실험만 수행했고, 장문 처리·대화·신뢰성 등 중요 테스크 종합 평가는 부족함.  

## 3. 제안 방법 및 실험 설계  
1) **양자화 유형**  
   - Weight-only: 가중치만 그룹별 비대칭 양자화  
   - Weight-Activation: 가중치 그룹별 + 활성화 per-token 대칭 양자화  
   - KV Cache: 키·값 텐서에 그룹별 비대칭 양자화  

2) **수식**  
   - 양자화:  

$$
       X_{\text{INT}} = \left\lfloor \frac{X_{\text{FP16}} - Z}{S} \right\rceil,\quad
       S = \frac{\max(X_{\text{FP16}})-\min(X_{\text{FP16}})}{2^{N-1}-1},\quad
       Z=\min(X_{\text{FP16}})
     $$  
   
  - N: 비트폭, S: 스케일, Z: 제로포인트  

3) **모델·테스크**  
   - 11개 모델 계열(OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, Mamba)  
   - 5개 테스크:  
     -  기본 NLP(언어모델링·이해·추론)  
     -  응급 능력(인컨텍스트 학습·지침 수행·다단계 추론·자기캘리브레이션)  
     -  신뢰성(윤리·환각·강건성)  
     -  대화(MT-bench)  
     -  장문 처리(Longeval·Multi-Doc QA)  

4) **성능 측정**  
   - 정확도·GPT-4 점수·정답률·PPL 등  
   - 2% 이내 성능 저하 기준  

## 4. 성능 향상 결과 및 한계  
- **모델 크기와 양자화 내성**  
  - 대형 모델일수록 Weight-only·KV Cache 양자화에 강하나, 활성화 양자화는 취약[표3]  
- **비트폭별 권장 설정**  
  - 대부분 테스크: W4, W4A8, KV4 (2% 이내 손실)  
  - 장문(≥4K 토큰): W4, W4A8, KV8  
  - 응급 능력: 다단계·자기캘리브레이션은 W8 이상 권장  
- **SOTA 복원 기법**  
  - AWQ: W3 성능 대폭 향상, W2는 여전히 붕괴  
  - SmoothQuant: W4A4 일부 회복하나 FP16 수준 못 미침[표5]  
- **한계**  
  - PTQ만 다룸(QAT 미적용)  
  - 일부 소형 모델·장문 파라미터 조합 미평가  
  - 그룹 크기·그 외 하이퍼파라미터 민감도 미분석  

## 5. 일반화 성능 향상 관점  
- **텐서별 통계 분석**(Kurtosis·Std): 일부 층별로 비트폭 다르게 설정하는 **혼합 정밀도** 가능성 제시[표4,10]  
- **스케일링 방식**: 정적 vs 동적 활성화 양자화 비교, 다운 프로젝션만 동적 적용 시 static 대비 PPL 근접[표12]  
→ **실제 응용 시 모델·레이어·테스크 맞춤 혼합·하이브리드 양자화** 연구 필요  

## 6. 향후 영향 및 고려 사항  
- **영향**  
  - 대형 언어모델 경량화 가이드라인 제시로 실서비스·온디바이스 응용 촉진  
  - 양자화 연구·하드웨어 설계·혼합 정밀도 스케줄링 연구에 기초 자료 제공  
- **고려 사항**  
  - QAT 병행, 양자화 민감도 예측 메커니즘 개발  
  - 새로운 LLM·장문 테스크·하드웨어별 최적 스케줄 자동화  
  - 지속적 통계 분석으로 동적 하이퍼파라미터 조정  

> 이 논문은 LLM 추론 효율화 연구의 토대를 다지며, 양자화 적용 범위와 한계를 명확히 제시했다. 이후 연구에서는 정밀도 혼합, 하이퍼파라미터 자동화, QAT 결합 등으로 **모델 일반화 성능**을 더욱 개선할 필요가 있다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/5895b682-5bb8-4521-b821-05b9f62906b9/2402.18158v2.pdf
