# Decomposed Prompting: A Modular Approach for Solving Complex Tasks

## 1. 핵심 주장 및 주요 기여  
**Decomposed Prompting**은 복잡한 작업을 단일 거대 LLM 프롬프트로 해결하는 대신, 작업을 일련의 간단한 하위 과제로 분해(decomposition)하고, 각 하위 과제에 최적화된 프롬프트 또는 심볼릭 함수로 처리함으로써  
- 복잡도 증가에 따른 성능 저하 완화  
- 하위 과제별 모듈 교체·최적화 용이  
- 재사용 가능한 프롬프트 라이브러리 구축  
를 가능케 한다.[1]

***

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 2.1 해결하고자 하는 문제  
- Few-shot CoT(Chain-of-Thought) 프롬프트는 작업 복잡도나 중간 추론 단계가 어려워질수록 성능이 급락  
- 긴 입력(문맥)에서의 정보 통합, 특정 연산(예: k번째 문자 추출) 학습 한계  

### 2.2 제안 방법  
1. **분해기(Decomposer LLM)**  
   - 복잡한 질의 $$Q$$에 대해 “어떤 하위 함수 $$f_i$$를 호출해 어떤 질문 $$Q_i$$을 던질지”를 순차 생성  
   - Few-shot 예시:  

$$
       E_j = \bigl(Q_j,\,(f_{j,1},Q_{j,1},A_{j,1}),\dots,(f_{j,k_j},Q_{j,k_j},A_{j,k_j})\bigr)
     $$

2. **하위 함수 집합 $$\mathcal{F}$$**  
   - 문자열 분할(“split”), 위치 기반 문자 추출(“str_pos”), 병합(“merge”) 등  
   - 필요 시 재귀 호출 또는 심볼릭 모듈(예: Elasticsearch 기반 Retrieval)로 확장  
3. **프롬프트 프로그램 실행**  
   - 분해기가 생성한 $$(f_i,Q_i)$$ 쌍을 컨트롤러가 각 하위 핸들러에 전달·수행  
   - “[EOQ]” 토큰 생성 시 최종 답안 반환  

### 2.3 모델 구조  
```
Input Q → Decomposer Prompt → (f1,Q1,A1)
               ↓                  │
      Controller              Sub-Task Handler
               ↓                  │
         … 반복 …            (Prompted LLM or Symbolic)
               ↓
       [EOQ] + 최종 답안 Aₖ
```

### 2.4 성능 향상  
- **k번째 문자 추출**: OOD 일반화 성능 기존 CoT 20~70% → DECOMP ≈100%[1]
- **리스트 뒤집기**: CoT 무력(≈5%) → DECOMP 재귀 분해로 76~86% 달성[1]
- **긴 문맥 QA(CommaQA-E)**: CoT 42% → DECOMP 55~64%[1]
- **Open-Domain 멀티홉 QA**: Retrieval 없이 18~38% → DECOMP 25~64% (Flan-T5-XXL 기준)[1]
- **수학 문제 풀이**: CoT 36~78% → DECOMP 50~95% (정답 추출 모듈 분리)[1]

### 2.5 한계  
- 분해기와 하위 핸들러에 대한 **프롬프트 설계 비용**  
- 분해 프로그램 실행 시 **API 호출 횟수 증가**  
- LLM의 생성 불안정성에 따른 **분해 오류 발생**  

***

## 3. 일반화 성능 향상 관점  
- **계층적 분해**: 어려운 하위 과제(str_pos)도 재분해(arr_pos+split) 가능→세분화된 학습  
- **재귀 분해**: 동일 과제 재호출로 입력 크기 무관한 일반화(길이(OOD) 확장)  
- **모듈화 학습**: 각 하위 프롬프트에 풍부한 예시 제공 가능→복합 구조 학습보다 단일 과제 학습 효율적  
이로써 복잡도·문맥 길이에 따른 성능 저하를 근본적으로 완화하고, 신속한 모듈 교체로 지속적 개선도 가능해진다.[1]

***

## 4. 향후 영향 및 고려 사항  
- **영향**:  
  - 모듈형 LLM 어플리케이션 설계 패러다임 제시  
  - LLM+심볼릭 결합 연구 가속  
  - 복잡한 체인오브소트(Chain-of-Thought)의 대안으로 주목  

- **고려점**:  
  1. 분해기·핸들러 **프롬프트 자동화 및 최적화**  
  2. **오류 전파 분석**을 통한 안정성 강화  
  3. **분해 방식 학습**을 위한 감독 데이터 확보  
  4. API 호출 지연·비용 문제 해결을 위한 **효율적 분해 전략** 개발  

이러한 방향이 모듈형 LLM 시스템의 실용화와 신뢰성 향상에 기여할 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/4793b9fd-e882-473b-afae-f3bb1d32ed2f/2210.02406v2.pdf)
