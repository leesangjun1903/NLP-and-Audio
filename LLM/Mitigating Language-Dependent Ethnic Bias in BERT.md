# Mitigating Language-Dependent Ethnic Bias in BERT

## 1. 핵심 주장 및 주요 기여  
**핵심 주장:**  
- BERT 등 대형 언어 모델에도 언어별·문화별로 상이한 **민족(국적) 편향**이 존재하며, 이를 양적 지표로 측정하고 효과적으로 완화하는 방법을 제안한다.[1]

**주요 기여:**  
- **Categorical Bias (CB) Score** 제안: 다중 클래스 민족 편향을 정량화하는 분산 기반 지표 개발.  
- **언어 종속적 편향 분석**: 영어, 독일어, 스페인어, 한국어, 터키어, 중국어 등 6개 언어에서 민족 편향 분포 차이를 실험적으로 규명.[1]
- **편향 완화 기법 두 가지**  
  1. **Multilingual BERT 활용**: 자원 풍부 언어에 대해 다국어 모델의 상호 보완 효과 이용.  
  2. **Contextual Word Alignment**: 편향이 낮은 영어 BERT와의 문맥 임베딩 정렬 후 미세조정으로 저자원 언어의 편향 완화.[1]

***

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 개선 및 한계

### 문제 정의  
- 언어 모델이 “A person from [MASK] is an enemy” 등의 문장 내에서 특정 국가명을 과도하게 연관 짓는 **민족 편향**을 보임.  
- 편향 정도를 단순 발생 빈도가 아닌, 속성 단어 존재 여부에 따른 **정규화된 확률 변화**로 측정할 필요가 있음.[1]

### Categorical Bias Score  
- 다중 클래스 대상 민족 단어 $$N=\{n_1,\dots,n_k\}$$와 속성 단어 $$A=\{a_1,\dots,a_m\}$$, 템플릿 $$T$$를 사용.  
- 각 템플릿–속성 조합마다 정규화된 확률 $$P' = \frac{p_\text{target}}{p_\text{prior}}$$의 로그 값을 구하고,  
- CB Score = $$\frac{1}{|T|\cdot|A|}\sum_{t\in T}\sum_{a\in A}\mathrm{Var}_{n\in N}\bigl(\log P'\bigr)$$  
  ($$CB=0$$일수록 균일한 분포, 높을수록 편향 심각).[1]

### 제안 모델 구조 및 방법  
1. **Multilingual BERT (M-BERT)**  
   - 여려 언어가 공유된 임베딩 공간에서 상호 완충 효과 기대.  
   - 자원 풍부(영어, 독일어, 스페인어) 언어에 효과적이나, 저자원(한국어·터키어) 언어는 미세조정 필요.[1]
2. **Contextual Word Alignment**  
   - 영어↔타 언어 monolingual BERT 간 병렬 코퍼스 기반 fast_align으로 anchor 단어 추출.  
   - Procrustes 분석으로 변환 행렬 $$W^* = \underset{W}{\arg\min}\|WX - Y\|^2$$ (SVD: $$Y X^\top = U\Sigma V^\top$$)  
   - 원소 개선 후, BERT와 $$W$$ 고정(freeze)한 채 MLM 헤드만 해당 언어 코퍼스로 미세조정(fine-tune).[1]

### 성능 향상  
- **CB Score 감소**:  
  - 자원 풍부 언어: M-BERT 미세조정 후 CB 최대 70% 감소.  
  - 저자원 언어: 영어 정렬(alignment) + 미세조정으로 CB 20–60% 감소, 랜덤 정렬 대비 유의미 개선.[1]
- **일반화 성능(Downstream NER)**  
  - BERT 고정(frozen)·언어별 NER 태스크에서도 F1 차이 1% 내외로 실사용 성능 유지.[1]

### 한계  
- **민족 범위**: 국가 단위의 거친 분류만 적용, 세부 민족·문화군 반영 한계.  
- **부작용**: 편향 감소 과정에서 가장 확률 높은 민족이 다른 민족으로 이동하는 “편향 역전” 발생 가능.  
- **언어 수 제한**: 6개 주요 언어 실험에 한정, 더 저자원·다양한 언어로 확장 필요.[1]

***

## 3. 모델의 일반화 성능 향상 관점

- **Alignment 기반 미세조정**은 민족 편향을 줄이면서도 BERT의 downstream 일반화 능력을 유지 또는 소폭 개선함.  
- 특히, NER 태스크에서 BERT 임베딩 고정을 전제하면, 정렬–미세조정 후에도 기존 BERT 대비 F1 드롭폭이 거의 없어 실용적 이점 강조.[1]
- 이는 편향 완화가 모델의 표현력 감소 없이 가능함을 시사하며, 타 태스크(문장 분류·기계 독해 등)에도 적용 잠재력 보유.

***

## 4. 향후 연구에 미치는 영향 및 고려 사항

- **영향**:  
  - **언어 다양성**을 반영한 편향 연구 패러다임 제시.  
  - **Alignment 기법**을 통한 편향 완화 접근은 다국어·저자원 환경의 공정 NLP 발전에 기여.

- **고려점**:  
  1. 민족·문화 정체성의 세밀한 정의 및 언어별 대상군 재설계 필요.  
  2. 편향 역전 등 부작용 방지를 위한 균형 지표 및 제약 조건 연구 필수.  
  3. 다중 태스크·다중 언어 동시 학습 시 편향 완화·성능 유지 기법 융합 검토.  
  4. 실제 서비스 배포 단계에서 사용자·사회적 맥락 고려한 평가 체계 마련.

***

**참고 문헌**  
 Ahn, J., & Oh, A. (2021). Mitigating Language-Dependent Ethnic Bias in BERT. arXiv:2109.05704v2.[1]

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/29efa28a-b487-4a70-9443-780b4fbde56b/2109.05704v2.pdf
