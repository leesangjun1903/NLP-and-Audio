# The Flan Collection: Designing Data and Methods for Eﬀective Instruction Tuning

## 1. 핵심 주장
**Flan 2022**는 공개된 다양한 지시문 튜닝(instruction tuning) 컬렉션을 통합·확장하고, 간단하지만 효과적인 방법을 통해 모델의 일반화 능력을 크게 향상시킨다.[1]
- **혼합 프롬프트 설정**(zero-shot, few-shot, chain-of-thought)을 훈련에 동시에 활용하여 모든 평가 설정에서 일관된 성능 향상을 달성한다.  
- **태스크 균형화**와 **입력-출력 반전(input inversion)** 기법을 통해 태스크 다양성과 데이터 품질을 높인다.  
- 결과적으로 **Flan-T5 XL(3B)**은 동형 모델 대비 3–17%p 이상의 성능 향상을 보이며, 심지어 대규모 비공개 모델(OPT-IML-Max 175B)보다도 우수하다.[1]

## 2. 문제 정의 및 제안 기법

### 2.1 해결하고자 하는 문제
- 기존 공개 지시문 튜닝 컬렉션(Flan-2021, P3++, Super-Natural Instructions 등)은 **프롬프트 설정이 단일**이거나 **태스크 균형**이 부족하여 다양한 평가 환경에서 충분한 일반화를 보장하지 못함.  
- **대규모 모델**이 아닌 중형(3B) 모델에서도 **효율적**이고 **범용적인** 지시문 튜닝 기법을 찾는 것이 목표.

### 2.2 제안 방법
1. **혼합 프롬프트 설정**  
   - 훈련 데이터의 일정 비율을 zero-shot, few-shot, chain-of-thought(CoT)로 섞어 사용.  
   - few-shot 비율을 $$p\%$$로 조정했을 때, zero-shot과 few-shot 평가 모두에서 성능이 최고점을 기록함.[1]
2. **입력-출력 반전(Input Inversion)**  
   - 기존 $$(x,y)$$ 데이터 쌍에서 $$(y \rightarrow x)$$ 쌍도 추가로 학습시켜 태스크 다양성을 확대.  
3. **데이터 소스 균형화(Mixture Balancing)**  
   - Flan-2021, T0-SF, Super-Natural Instructions, CoT, 대화(dialog), 프로그램 합성(prog. synth.) 등 다양한 소스를 적절한 비율로 배정하여 최적 혼합비 탐색.[1]

### 2.3 모델 구조
- **T5-LM Adapted** 프리픽스 언어 모델 활용  
- 일관된 비교를 위해 **XL(3B)** 크기 사용  
- Few-shot 및 CoT 예제는 2, 3, 5개의 사례(exemplars)로 구성  

## 3. 성능 향상 및 한계

### 3.1 성능 향상
- **Held-In(8개 QA/NLI 태스크)**: +3.3%p(Zero-Shot), +12%p(Few-Shot)  
- **Chain-of-Thought 평가(5개 데이터셋)**: +10.2%p(Zero-Shot)  
- **Held-Out(MMLU, BBH)**: +4.2%p 및 +8.5%p 개선.[1]
- **단일 태스크 파인튜닝** 시작점으로서도 T5 대비 빠른 수렴 및 높은 최종 성능 달성.[1]

### 3.2 한계
- **태스크 정의 및 품질**: 서로 다른 컬렉션 간 태스크 정의 일관성 부족  
- **혼합비 탐색**: 단순 균등 또는 직관 기반으로 결정, 자동 최적화 여지  
- **스케일 확대**: 1.8k 태스크 이상에서는 모델별 성능 포화 가능성 존재  

## 4. 일반화 성능 향상 관련 심층 분석
- **혼합 프롬프트**: 5–10% few-shot 추가만으로 zero-shot 성능 +2%p 이상, 역도 성립.[1]
- **태스크 스케일 법칙**: 모델 크기가 클수록 더 많은 태스크가 성능 향상에 기여하며, Held-Out는 로그-선형 증가 추세.[1]
- **입력 반전**: Held-Out 평가에서 특히 효과(최대 +8.3%p).[1]
- **소스 균형화**: Flan-2021과 T0-SF 제거 시 MMLU 성능 가장 큰 하락, CoT 제거 시 CoT 평가 큰 영향.[1]

## 5. 향후 연구 방향 및 고려 사항
- **태스크 자동 혼합비 최적화**: 베이지안 최적화나 메타러닝 도입  
- **입력 반전 기법 확장**: 더 다양한 데이터 증강 및 질적 분석  
- **다국어·다양한 모달리티**로의 일반화: xP3, 멀티모달 지시문 튜닝 실험  
- **인간 피드백 결합**: 지시문 튜닝과 RLHF(강화학습 기반 인간 피드백) 균형 탐색  
- **환경적 영향**: Green AI 관점에서 지시문 튜닝의 계산 비용 대비 편익 분석  

 2301.13688v2.pdf[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b121f47c-4bdd-443f-bdc3-605939354b6f/2301.13688v2.pdf)
