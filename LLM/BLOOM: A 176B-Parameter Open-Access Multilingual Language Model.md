# BLOOM: A 176B-Parameter Open-Access Multilingual Language Model

**주요 주장**  
- **개방형 대규모 다국어 언어 모델**을 공개하여 자원 부족 연구자도 활용할 수 있도록 함.  
- 1760억 개 매개변수(decoder-only Transformer)로 GPT-3 유사 성능을 다국어에 걸쳐 구현.  
- **ROOTS**라는 46개 자연어·13개 프로그래밍 언어로 구성된 1.6TB 전처리·문서화된 코퍼스를 활용.  
- **BLOOMZ**라는 다국어 다태스크(prompted) 미세조정 모델을 통해 제로샷·소수샷 일반화 성능 대폭 향상.

**주요 기여**  
1. **데이터 거버넌스 및 다국어 코퍼스 제작**  
   - 데이터 제공권자와 협의, 출처 유지, PII(개인정보) 마스킹 등 윤리·법적 고려 반영.  
   - 498개 소스 분리·추적 가능하게 보존, 223개 공개 가능한 구성요소 공개.  
2. **대규모 분산 학습 인프라 구축**  
   - 384개의 A100 GPU(Jean Zay 슈퍼컴)에서 3.5개월간 1.08M GPU시간 소요.  
   - Megatron-DeepSpeed 기반 3D 병렬화(DP+TP+PP) 및 bfloat16 혼합정밀도 활용.  
3. **아키텍처 및 학습 기법**  
   - **Decoder-only + 인과적 언어 모델링**: 제로샷 성능 우수 검증.  
   - **ALiBi 위치 인코딩**: 더 나은 긴 거리 의존성 적용 및 안정적 학습.  
   - 임베딩 직후 LayerNorm 도입으로 bfloat16에서 학습 안정화.  
4. **토크나이저 설계**  
   - 바이트 레벨 BPE, 250,680개 어휘, 다국어 모놀링구얼 토크나이저 대비 평균 ~±10% 펄틸리티 보장.  
5. ** 다태스크 프롬프트 미세조정(BLOOMZ)**  
   - P3 확장판인 **xP3**(83개 데이터셋·46개 언어·16개 태스크)로 Instruction tuning.  
   - 제로샷 NLI·코어퍼런스·요약·번역 성능 평균 +10–30%p 대폭 향상.  

***

## 논문 상세 설명

### 1. 해결하고자 하는 문제  
- 소수의 자원 풍부 기관에만 편중된 대규모 LLM 개발 관행  
- 비(非)영어권·저자원 언어 배제  
- 데이터 출처 불투명·윤리·법적 이슈 미고려  

### 2. 제안 방법  
1) **ROOTS 코퍼스 구축**  
   - 46개 자연어, 13개 프로그래밍 언어, 498개 소스 → 1.61 TB 텍스트  
   - 휴먼 큐레이션＋해당 언어 전문가가 필터링·전처리  
   - 출처 메타데이터 유지, 중복 제거, 개인정보 레드액션     
2) **모델 아키텍처**  
   - Decoder-only Transformer 70층, hidden 14,336, 112 heads  
   - **ALiBi**: Attention score에 거리 가중치 직접 적용  
   - 임베딩 후 LayerNorm 추가로 bfloat16 학습 안정화  
3) **학습 하이퍼파라미터**  
   - 2048 길이 시퀀스, 배치 2048, 학습률 cosine decay, 410B 토큰 스케줄  
   - 총 366B+25B 토큰 학습  
4) **다태스크 미세조정 (BLOOMZ)**  
   - xP3 데이터셋(83개) 프롬프트 튜닝 → 제로샷 일반화 강화  
   - 기여: 언어간 번역·요약·QA·NLI 등 16개 태스크  

### 3. 성능 향상  
- **제로샷 SuperGLUE**: GPT-3(175B) 대비 근접, 1-shot 시 BLOOM 우세  
- **다국어 번역**  
  - 1-shot FLORES-101: 고·중·저자원 언어 쌍 전반에서 M2M-100(615M) 유사 성능  
  - 영어↔프랑스어 등 고자원 언어 번역 BLEU 34–45 달성  
- **요약**: WikiLingua 1-shot ROUGE-2 평균 26 → BLOOM-176B 40 이상  
- **코드 생성**: HumanEval pass@1 ≈ 15% (Codex-12B ≈ 29% 대비 경쟁력 확보)  
- **BLOOMZ 미세조정**: 제로샷 NLI·코어퍼런스·문장완성 +20–30%p 폭발적 향상  

### 4. 한계  
- 학습 데이터 편중: 중·고자원 언어에 비해 적게 대표된 언어 성능 저하  
- 학습 비용·인프라 의존: 대중적 재현 어려움  
- 편향·윤리적 위험: CrowS-Pairs 테스트 상 무작위 수준이지만 다변량 평가 필요  

***

## 일반화 성능 향상 가능성

- **Instruction tuning**(xP3)으로 모델이 자연어 명령에 더 잘 반응  
- **더 다양한 언어·도메인** 포함 시 저자원 언어 일반화 강화 기대  
- **모델 크기 확장**과 더 풍부한 미세조정 데이터 결합 시 극적 성능 상승 여지  
- **대체 아키텍처**(MoE, SSM) 통합으로 자원 효율적 확장 가능  

***

## 향후 연구 영향 및 고려사항

- **공동체 참여·거버넌스 모델**: 대규모 연구 협업의 교본 제시  
- **TLS 기반 데이터 윤리·법적 가이드**: LLM 개발의 표준화 동력  
- **다국어·도메인 확장**: 최적 토크나이저·데이터 증강 기술 연구  
- **효율적 미세조정**(LoRA, QLoRA)과 **저비용 배포** 방안 모색  
- **편향·재현성 평가**: 다언어·다문화 관점의 자동화된 측정법 개발 필요  

BLOOM은 “LLM 민주화”와 **지속 가능한 대규모 모델 개발**을 위한 종합 가이드라인을 제시하며, 후속 연구자들에게 데이터 거버넌스·분산 학습·Instruction tuning의 실용적 청사진을 제공한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/23dc04a2-d5ba-40e2-a8ed-22437255f947/2211.05100v4.pdf)
