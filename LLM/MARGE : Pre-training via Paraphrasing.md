# MARGE : Pre-training via Paraphrasing

## 핵심 주장 및 주요 기여  
**Pre-training via Paraphrasing**, 또는 **MARGE**는 기존의 마스킹 기반 언어 모델링(MLM)을 대체하는 새로운 사전학습 패러다임을 제시한다.  
- **핵심 주장**: 대상 문서(`x`)를 재구성하기 위해 언어 간·문서 간 유사한 텍스트(`z₁…zₘ`)를 검색(retrieval)하고, 이를 조건으로 Seq2Seq 모델이 원본 문서를 생성하도록 학습함으로써 마스킹보다 풍부한 노이즈를 제공하는 *무감독 다국어·다문서 패러프레이징* 목표를 실현한다.[1]
- **주요 기여**:  
  1. 랜덤 초기화만으로 검색 모델과 재구성 모델을 공동 학습하는 **다중 소스 자동인코더** 구조를 제안.  
  2. 검색된 문서와 유사도 점수를 디코더의 cross-attention에 가중치로 활용하는 **Retrieval–Reconstruction** 메커니즘을 도입(식 (1)–(4)).  
  3. 마스킹 없이 전체 문장을 입력으로 사용하여 패러프레이징, 번역, 요약, 정보 검색을 통합하는 새로운 사전학습 목표 수립.  
  4. **제로샷** 번역(BLEU 35.8), 요약, 문장 검색, QA, 패러프레이즈 검출 등 다양한 태스크에서 강력한 성능을 달성.

## 해결 문제 및 제안 방법  
### 문제 정의  
마스킹 기반 모델은 국소적 토큰 복원을 목표로 하며, 모델 파라미터에 백과사전적 지식을 과도하게 암기시킨다. 이에 반해 MARGE는:
- 전 범위 문장을 입력으로
- 여러 언어·문서 간 패러프레이징 신호를 활용  
하여 더 일반화 가능하고 다양한 태스크에 적응 가능한 사전학습을 설계한다.[1]

### 모델 구조 및 수식  
1. **문서 검색 (Relevance)**  
   - 문서 인코더 $$g$$로 문서 $$x_i, z_j$$를 임베딩 후 코사인 유사도를 계산:  

$$
       f(x_i, z_j) = \frac{g(x_i) \cdot g(z_j)}{\|g(x_i)\|\|g(z_j)\|}
     $$  

2. **재구성 (Reconstruction)**  
   - 재구성 확률:

$$
       \mathcal{L}_i = -\log p\bigl(x_i \mid z_{1:M}, f(x_i, z_{1:M})\bigr)
     $$
   
   - Cross-attention 가중치에 유사도 점수 반영(식 (4)):

$$
       \mathrm{Attention} \propto \mathrm{softmax}\bigl(QK + \alpha \cdot f(x_i, z_j)\bigr)
     $$
   
   - $$\alpha$$는 학습 가능한 스칼라로, 유사도 가중치 영향력을 조절한다.  
3. **배치 구성**  
   - 문서 셰어딩, 유사도 기반 임계값 필터링, 가중치 그래프 탐색으로 관련 문서 클러스터를 구성하여 효율적 학습을 보장한다.[1]

### 사전학습 아키텍처  
- **인코더**: 12-layer Transformer (1024 dim)  
- **디코더**: 16-layer Transformer + 4 local self-attention layers, 총 약 963M 파라미터.[1]
- MARGE-NEWS(뉴스)→MARGE(위키) 순으로 450k+650k steps 학습, 26일 GPU 훈련.[1]

## 성능 향상 및 한계  
### 성능  
- **제로샷 문서 번역**: BLEU 최대 35.8 (de→en).  
- **문서 요약(MLSum)**: Rouge-L 42.7 (다국어 통합 학습).  
- **크로스링구얼 검색(BUCC)**: F1 78.8, Tatoeba 평균 80.9.  
- **QA(MLQA)**: Chinese 최고 90.4.  
- **패러프레이즈 검출(PAWS-X)**: zero-shot 전 언어 평균 86.5.  

### 한계  
- **언어별 편차**: 드문 언어·비라틴 알파벳에서 성능 저하.  
- **도메인 제약**: 뉴스·위키 기반, 일반 도메인 확장 시 근사 검색 필요.  
- **학습 비용**: 대규모 배치 및 모델-병렬 학습 요구.  
- **피드백 루프**: 낮은 성능 언어는 덜 검색되어 추가 학습 기회 감소.

## 일반화 성능 향상 가능성  
- **다양한 노이즈**: 마스킹보다 풍부한 패러프레이징 신호로 문장 단위 추론 능력 강화.  
- **Cross-Attention 기반 검색**: 유사도 가중치로 모델이 관련 정보를 동적으로 조절, 문서 이해력 및 적응력 향상.  
- **제로샷·로우샷**: fine-tuning 없이도 다국어·다태스크 generalization 보장.  

## 향후 연구 영향 및 고려사항  
MARGE는 사전학습 목표의 새로운 방향을 제시하며,  
- **다양한 도메인**(의료, 법률 등)과 언어 확장  
- **근사 최근접 검색** 연동  
- **모델-태스크 정렬**: 특정 downstream 목적에 맞춘 retrieval–reconstruction 변형  
- **효율화**: 더 작은 모델에서도 유사 성능을 내는 압축 기법 적용  
을 고려해야 한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/88148f85-dd94-4b73-ab04-45274818e707/2006.15020v1.pdf)
