
# Eureka: Human-Level Reward Design via Coding Large Language Models

## 1. 핵심 주장 및 주요 기여

EUREKA는 **대규모 언어 모델(LLM)의 코딩 능력**을 활용하여 강화학습(RL)에서 가장 어려운 문제 중 하나인 **보상함수 설계를 자동화**하는 알고리즘입니다. 논문의 핵심 주장은 다음과 같습니다:[1]

**주요 기여:**

1. **인간 수준의 성능 달성**: 29개의 다양한 RL 환경에서 83%의 작업에서 인간 전문가가 설계한 보상함수를 능가하며, 평균적으로 **52% 향상** 달성[1]

2. **선행 작업(L2R) 대비 우월성**: 과제별 프롬프트나 미리 정의된 템플릿 없이도 자유로운 형태의 보상함수 생성으로 템플릿 기반 방식인 L2R보다 현저히 뛰어난 성능 발휘[1]

3. **새로운 복잡 태스크 해결**: 최초로 5개 손가락을 가진 Shadow Hand로 고속 펜 회전(pen spinning) 수행 달성 - 이는 기존 수동 보상 설계로는 불가능했던 작업[1]

4. **인간 피드백 통합**: 그래디언트 업데이트 없이 인-콘텍스트 학습을 통해 인간 입력을 점진적으로 반영하는 새로운 **RLHF 접근법** 제시[1]

***

## 2. 해결 문제 및 제안 방법

### 2.1 핵심 문제 정의

강화학습에서 보상함수 설계는 **핵심적이면서도 극도로 어려운 문제**입니다:[1]

- 최근 설문조사에 따르면, RL 연구자의 **92%가 수동 시행착오** 방식을 사용하며, **89%가 자신의 보상함수가 부최적**이라고 보고[1]
- 희소 보상(sparse reward)은 학습 신호가 부족하여 **보상 설계(reward shaping)**가 필수적[1]
- 오설계된 보상은 의도하지 않은 행동(reward hacking)으로 이어질 수 있음[1]

**정식화 (Definition 2.1 - Reward Design Problem):**

보상 설계 문제는 다음과 같이 정의됩니다:[1]

$$P = \langle M, R, \pi_M, F \rangle$$

여기서:
- $M = (S, A, T)$: 상태공간 $S$, 행동공간 $A$, 전이함수 $T$를 가진 세계 모델
- $R$: 보상함수의 공간
- $\pi_M(\cdot) : R \rightarrow \Pi$: 학습 알고리즘으로, 보상 $R$에 대한 정책 $\pi : S \rightarrow \Delta(A)$ 생성
- $F : \Pi \rightarrow \mathbb{R}$: 정책 평가를 위한 적합도 함수

**목표**: $F(\pi_M(R))$를 최대화하는 $R$를 찾기[1]

### 2.2 제안 방법: EUREKA 알고리즘

EUREKA는 세 가지 핵심 알고리즘 요소로 구성됩니다:[1]

#### (1) **환경을 컨텍스트로 활용 (Environment as Context)**

LLM에 직접 **환경의 원본 소스 코드**를 제공하여 zero-shot 생성을 가능하게 합니다:[1]

```python
# 예시: Humanoid 환경의 관측 코드
class Humanoid(VecTask):
    def compute_observations(self):
        torso_position = root_states[:, 0:3]
        torso_rotation = root_states[:, 3:7]
        velocity = root_states[:, 7:10]
        # ... 추가 상태 변수들
```

이를 통해 LLM은 실제 사용 가능한 상태 변수들을 정확히 파악하고 보상함수를 직접 생성할 수 있습니다.[1]

#### (2) **진화적 탐색 (Evolutionary Search)**

**Algorithm 1: EUREKA**[1]

```
입력: 작업 설명 l, 환경 코드 M, 코딩 LLM, 적합도 함수 F, 초기 프롬프트
초매개변수: 탐색 반복 N, 배치 크기 K

for N 반복:
    // K개의 보상 코드를 LLM에서 샘플링
    R₁, ..., Rₖ ~ LLM(l, M, prompt)
    
    // 보상 후보 평가
    s₁ = F(R₁), ..., sₖ = F(Rₖ)
    
    // 보상 반영
    prompt := prompt : Reflection(R_best, s_best),
    여기서 best = argmax(s₁, ..., sₖ)
    
    // EUREKA 보상 업데이트
    R_EUREKA, s_EUREKA = (R_best, s_best), if s_best > s_EUREKA
    
반환: R_EUREKA
```

각 반복에서:[1]
- **샘플링**: K=16개의 보상 함수 i.i.d 생성
- **평가**: GPU 가속 분산 RL (IsaacGym 활용)
- **개선**: 최고 성능 보상 기반 돌연변이 및 진화
- **종료**: N=5 반복, 5번의 독립 실행

#### (3) **보상 반영 (Reward Reflection)**

정책 학습 과정에서의 **자동화된 피드백** 요약:[1]

보상 반영은 다음을 추적합니다:
- 각 보상 컴포넌트의 스칼라 값 (체크포인트별)
- 작업 적합도 함수 값
- 정책의 전체적 성능 지표

**예시 반영 (Iteration 1 - ShadowHand 태스크)**:[1]

```
rotation_reward: [0.03, 0.31, 0.30, ..., 0.32]
                Max: 0.36, Mean: 0.32, Min: 0.03

success_rate: [0.00, 0.83, 1.85, ..., 8.83]
              Max: 9.29, Mean: 4.81, Min: 0.00

episode_lengths: [7.07, 384.30, ..., 434.24]
                 Max: 482.35, Mean: 396.02, Min: 7.07
```

이를 통해 LLM은 다음을 자동으로 판단:[1]
- 보상 컴포넌트가 정체된 경우 (값이 거의 변하지 않음)
- 스케일 조정이 필요한 경우
- 새로운 컴포넌트 추가 필요 여부

***

## 3. 모델 구조 및 성능 분석

### 3.1 모델 구조

EUREKA의 구조는 다음과 같습니다:[1]

```
┌─────────────────────────────────────────┐
│      작업 설명 + 환경 소스 코드          │
└────────────────┬────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────┐
│    초기 보상 함수 생성 (Zero-shot)      │
│         (GPT-4 코딩 LLM)               │
└────────────────┬────────────────────────┘
                 │
                 ▼
    ┌────────────────────────────┐
    │   보상 후보 샘플링 (K=16)  │
    └────────────┬───────────────┘
                 │
                 ▼
    ┌────────────────────────────┐
    │ GPU 가속 PPO 평가          │
    │ (IsaacGym)                 │
    └────────────┬───────────────┘
                 │
                 ▼
    ┌────────────────────────────┐
    │ 최고 성능 보상 선택        │
    │ + 보상 반영 생성           │
    └────────────┬───────────────┘
                 │
    진화적 개선 (5반복) ◄─────┘
                 │
                 ▼
    ┌────────────────────────────┐
    │   최종 EUREKA 보상         │
    │   (5번 독립 실행 중 최고)  │
    └────────────────────────────┘
```

### 3.2 핵심 성능 지표

**(1) IsaacGym 환경에서의 성능**[1]

| 메트릭 | EUREKA | Human | L2R |
|--------|--------|-------|-----|
| 평균 정규화 점수 | **1.52** | 1.0 | 0.41 |
| 인간 상위 달성률 | 83% | - | 12% |
| 평균 향상도 | **52%** | - | -59% |

**(2) Dexterity 벤치마크 (20개 양손 조작 작업)**[1]

- 성공률 평균: EUREKA 72% vs Human 61%
- 통계적 유의성: 95% 신뢰도로 모든 작업에서 EUREKA 우월

**(3) 펜 회전 (Pen Spinning) - 최초 달성**[1]

커리큘럼 학습과 결합하여:
- **사전학습 단계**: 무작위 목표 방향으로 펜 재정렬
- **미세조정 단계**: 사전학습된 정책으로 연속 펜 회전 패턴 수행
- **결과**: 초당 여러 회전 주기 달성 (기존 수동 방식으로는 단일 사이클도 불가능)

### 3.3 일반화 성능 분석

#### (1) **보상 반영의 중요성**[1]

보상 반영을 제거한 경우의 성능 저하:

$$\text{성능 저하} = 28.6\% \text{ (IsaacGym 평균)}$$

특히 고차원 작업에서 더 큰 저하 (최대 60% 이상):
- 저차원 작업: 10-20% 저하
- 중차원 작업: 30-40% 저하  
- 고차원 작업: 50-60% 저하

이는 보상 반영이 **목표된 보상 편집**을 가능하게 하는 메커니즘임을 입증[1]

#### (2) **진화적 탐색의 효과**[1]

```
성능 궤적:
반복 0 (초기): 0.6 (Human 기준)
반복 1: 1.1
반복 2: 1.25
반복 3: 1.35
반복 4: 1.40
반복 5: 1.45 (최종)
```

**중요한 발견**: 첫 32개 샘플만 사용한 경우보다 진화적 탐색이 더 효과적[1]
- 단순 샘플링: 0.95 (32개)
- 진화적 탐색: 1.45 (32개 사용, 5 반복)

#### (3) **모델 간 비교 (GPT-4 vs GPT-3.5)**[1]

GPT-3.5 사용 시:
- IsaacGym: 여전히 대부분 인간 수준 이상
- Dexterity: 성능 저하 (70% → 55%)
- 결론: EUREKA 원리는 일반적이며 코딩 LLM의 품질에 따라 확장 가능

#### (4) **보상 상관성 분석**[1]

```
상관성 vs 작업 어려움:
낮은 차원 작업:   상관성 0.6~0.8 (Human 보상과 유사)
중간 차원 작업:   상관성 0.2~0.4 (부분적 상이)
고차원 작업:      상관성 -0.2~0.2 (완전히 다른 설계)

통찰: 어려운 작업일수록 인간 보상이 부최적이므로,
      EUREKA는 근본적으로 다른 혁신적 설계 발견
```

일부 경우 **음의 상관성**도 관찰됨 - 즉, EUREKA는 인간 직관과 반대되는 보상을 설계하면서도 더 나은 성능 달성[1]

***

## 4. 한계와 과제

### 4.1 명시적 한계

**1. 계산 비용**[1]
- 각 환경당 5회 독립 실행 × 5 반복 × 16 샘플 = 400회 정책 학습 필요
- GPU 가속 시에도 각 환경당 몇 시간 소요
- 고차원 환경에서는 수십 시간 가능

**2. 보상의 해석 가능성**[1]
- 생성된 보상 코드가 직관적이지 않을 수 있음
- 도메인 전문가의 이해와 검증 어려움
- 안전 크리티컬 애플리케이션에서 신뢰성 문제

**3. 시뮬레이션 정확도 의존성**[1]
- sim-to-real 전이 시 시뮬레이션 왜곡(simulation bias)에 영향받을 수 있음
- 물리 파라미터의 부정확성이 학습된 정책에 영향

**4. 인간 피드백 통합의 확장성**[1]
- 현재 사용자 연구는 20명에 국한 (충분하지 않을 수 있음)
- 대규모 배포 시 일관된 인간 선호도 수집의 어려움

### 4.2 기술적 한계

**1. 마르코프 가정 (Markovian Assumption)**[1]
- 컨텍스트 길이 제약으로 최근 반복만 유지
- 장기적 진화 궤적 활용 불가능
- 최적이 아닌 로컬 수렴 가능성

**2. 평가 편향**[1]
- 중간 보상 평가는 1회 PPO 실행만 수행 (최종은 5회)
- 분산 감소와 신뢰성 향상의 트레이드오프

**3. 과제 다양성 한계**[1]
- 29개 환경 모두 IsaacGym 기반 로봇 조작 태스크
- 게임, 언어 모델 정렬 등 다른 도메인 미평가

**4. 보상 설계의 기본 한계**[1]
- 사양 게임(specification gaming): 보상을 최대화하지만 원래 의도는 달성하지 못하는 행동 (근본적 해결 불가)
- 값 정렬(value alignment) 문제 지속

***

## 5. 최신 연구 기반 연구 영향 및 고려사항

### 5.1 EUREKA의 영향과 후속 연구

**1. DrEureka (2024년 6월)**[2]
- EUREKA의 sim-to-real 확장
- 보상함수뿐 아니라 도메인 랜더마이제이션도 LLM으로 자동 설계
- 실제 사족 로봇 보행 및 조작 성공

**2. CARD (2024년 10월)**[2]
- 역동적 피드백 기반의 LLM 보상 설계 프레임워크
- 코더(생성)와 평가자(검증) 분리
- 중간 보상 검증으로 실행 불가능 코드 감소

**3. 보상 모델의 일반화 개선 (2024-2025)**[2]
- 정규화를 통한 일반화 성능 향상
- 적대적 프로세스 활용한 더 나은 선호도 데이터 생성
- 보상 모델이 일관성만 학습하고 인과성은 놓치는 문제 지적

### 5.2 미래 연구 시 고려사항

#### **(1) 보상 함수의 안전성과 정렬**

**과제**: 자동 생성 보상의 안전성 검증
- 보상 오버옵티마이제이션(reward over-optimization) 감시
- 다중목표 정렬(multi-objective alignment) 필요
- 적대적 검증(adversarial validation) 포함

**최신 접근법**:[2]
```
인간 선호도 + 검증 가능한 신호 (사실성, 지시 따르기)
= 더 신뢰할 수 있는 보상 모델
```

#### **(2) 도메인 외 일반화 (Out-of-Domain Generalization)**

**문제점**:[1]
- IsaacGym 환경에 최적화된 EUREKA
- 다른 시뮬레이터나 실제 환경 성능 미지

**향후 방향**:
1. 다양한 물리 엔진(MuJoCo, PyBullet, V-REP) 테스트
2. 비로봇 도메인 확대 (게임 AI, 자율주행, 금융)
3. 도메인 일반화 메커니즘 개발

#### **(3) 계산 효율성 개선**

**현재 병목**:
- 각 환경당 수십 시간 GPU 연산
- 400회 이상 정책 학습

**개선 방안**:
1. **메타 학습 활용**: 유사 작업의 학습 결과 전이
2. **조건부 생성**: 과제 유사도 기반 초기 보상 선택
3. **병렬 평가**: 여러 보상 동시 평가로 성능 향상

#### **(4) 인간-AI 협업의 확장**

**현재 수준**:
- 텍스트 피드백 기반 미세조정 (20명 사용자, 개념 증명)
- 제한적 RLHF 통합

**향후 연구 방향**:[2]
1. **대규모 인간 피드백**: 수천 명 참여 연구
2. **멀티모달 피드백**: 텍스트 + 비디오 + 선호도 결합
3. **적응적 질의**: 활성 학습으로 핵심 피드백만 수집
4. **설명 가능성**: 생성 보상 설계 원칙의 명확화

#### **(5) 이론적 이해 심화**

**필요한 이론**:
1. **수렴성 분석**: EUREKA 진화 탐색의 수렴 보장
2. **샘플 복잡성**: 얼마나 많은 정책 학습이 필요한가?
3. **일반화 한계**: 훈련-테스트 간 보상 성능 격차

$$\text{일반화 오차} = E_{\text{test}}[L(R)] - E_{\text{train}}[L(R)]$$

#### **(6) LLM과 진화 알고리즘의 결합**

**최신 연구 트렌드** (2025):[2]
- LLM과 진화 계산(EA)의 개념적 유사성 발견
- LLM 강화 진화 알고리즘의 우월성 입증
- 상호 강화를 통한 성능 개선

**EUREKA와의 연관성**:
```
LLM 능력           ↔  진화 알고리즘 특성
---------------------------------------
코드 생성          ↔  세대 간 돌연변이
맥락 이해          ↔  적응도 평가
메모리 활용        ↔  개체군 유지
```

### 5.3 실제 적용 시 체크리스트

**1. 도메인 준비**
- [ ] 환경 소스 코드 준비 (또는 상태 API 정의)
- [ ] 적합도 함수 명확화
- [ ] 연산 리소스 확보 (GPU 몇 시간 이상)

**2. 안전성 검증**
- [ ] 생성된 보상 코드의 문법 검증
- [ ] 보상 범위 체크 (무한/NaN 방지)
- [ ] 정책의 이상 행동 모니터링

**3. 비용-편익 분석**
- [ ] 수동 설계 시간 vs EUREKA 실행 시간
- [ ] 최종 성능 향상 vs 개발 비용

**4. 장기 유지**
- [ ] 생성된 보상 코드의 문서화
- [ ] 모델 버전 관리 (GPT-4 업데이트 시 재실행)
- [ ] 새로운 작업으로의 전이 전략

***

## 6. 결론

EUREKA는 **LLM의 코딩 능력과 진화 알고리즘의 탐색 강점**을 결합하여 보상함수 설계를 획기적으로 자동화했습니다. 83%의 작업에서 인간 전문가를 능가하고, 기존 방식으로 불가능했던 고난도 조작 작업(펜 회전)을 최초로 달성한 것은 강화학습의 실질적 적용 가능성을 크게 확대했습니다.[1]

다만 계산 비용, 해석 가능성, 안전성 등의 한계가 있으며, 향후 연구는 **다양한 도메인으로의 확장, 인간-AI 협업 심화, 이론적 기반 강화**에 초점을 맞춰야 할 것으로 보입니다. 특히 DrEureka와 같은 실제 로봇 적용 사례와 CARD 같은 동적 피드백 메커니즘의 발전은 EUREKA의 실용성을 크게 향상시키고 있습니다.[2]

***

## 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8b58b157-14a5-4fba-af1b-fca2bd58d33f/2310.12931v2.pdf)
[2](http://arxiv.org/pdf/2410.14660.pdf)
[3](https://arxiv.org/html/2503.06358v1)
[4](https://arxiv.org/html/2406.01967v1)
[5](https://arxiv.org/pdf/2502.19328.pdf)
[6](https://arxiv.org/pdf/2406.10216.pdf)
[7](https://arxiv.org/html/2502.14619v1)
[8](https://arxiv.org/html/2504.07596v1)
[9](https://arxiv.org/pdf/2303.00001.pdf)
[10](https://huggingface.co/papers/2310.12931)
[11](https://arxiv.org/abs/2504.13958)
[12](https://spj.science.org/doi/10.34133/research.0646)
[13](https://arxiv.org/pdf/2310.12931.pdf)
[14](https://openreview.net/forum?id=2FGpL5Nd4C&noteId=osaQqKhbep)
[15](https://gecco-2025.sigevo.org/Workshop?itemId=2840)
[16](https://github.com/eureka-research/Eureka)
[17](https://arxiv.org/html/2506.15421v1)
[18](https://www.sciencedirect.com/science/article/abs/pii/S0925231224020435)
[19](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/eureka/)
