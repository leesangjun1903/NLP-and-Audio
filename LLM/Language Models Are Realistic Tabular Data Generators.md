# Language Models Are Realistic Tabular Data Generators

**핵심 주장 및 기여**  
이 논문은 **대규모 언어 모델(LLM)을 이종(heterogeneous) Tabular 데이터 생성기로 전환**하는 새로운 패러다임인 **GReaT(Generation of Realistic Tabular data)**를 제안한다. 주요 기여는 다음과 같다.  
- **텍스트 인코딩**: 각 테이블 행(row)을 “속성 이름은 값이다” 형태의 문장으로 변환하여 수치·범주 데이터의 정보 손실 없이 표현.  
- **무작위 특성 순열**: 특성(feature) 순서를 무작위로 섞어 학습함으로써 순서에 독립적인 임의 조건부 생성(arbitrary conditioning) 지원.  
- **사전학습 활용**: 방대한 언어 데이터로 사전학습된 트랜스포머 디코더를 미세조정(fine-tuning)하여 문맥(contextual) 지식 활용.  
- **높은 생성 품질**: 다양한 실제·합성 데이터셋에서 최첨단(tabular GAN/VAE) 대비 분류기 성능, 판별자 식별 곤란도, 분포 일치도 등 다중 지표에서 우수함을 입증.  

***

## 1. 해결하고자 하는 문제  
1. **정보 손실적 전처리**: 기존 GAN/VAE 기반 탭ular 생성기는 범주형 인코딩·정규화 과정에서 순서 인위 부여 및 정보 손실 발생.  
2. **문맥 지식 부족**: 나이·학력·결혼 상태 등 변수들 사이의 상호 연관성(contextual coherence)을 모델링 어려움.  
3. **조건부 생성의 비유연성**: 특정 특성 조합으로 샘플링하려면 모델 재학습이 필요해 실용성 제약.  

***

## 2. 제안 방법: GReaT  
### 2.1 텍스트 인코딩  
각 행 $$s_i$$의 $$m$$개 특성 $$\{f_j\}$$과 값 $$v_{i,j}$$을 다음과 같은 절(subject–predicate–object) 문장으로 변환:  

$$
t_{i,j} = [\,f_j,\ \text{"is"},\ v_{i,j},\ ","\,],\quad
t_i = \bigl[t_{i,1},t_{i,2},\dots,t_{i,m}\bigr].
$$  

### 2.2 무작위 특성 순열  
문장 순서에 의존하지 않도록 $$t_i$$의 조각 $$\{t_{i,j}\}$$을 무작위 순열 $$k$$로 섞어  

$$
t_i^{(k)} = [\,t_{i,k_1},\,t_{i,k_2},\dots,t_{i,k_m}\,].
$$  

### 2.3 사전학습 LLM 미세조정  
- **모델**: GPT-2 계열 트랜스포머 디코더  
- **목적함수**: 문장 토큰열 $$(w_1,\dots,w_L)$$의 **자동회귀 확률**  
$$
p(t) = \prod_{k=1}^L p(w_k \mid w_1,\dots,w_{k-1})
$$  
- **학습**: 텍스트 인코딩된 데이터 $$\{t_i^{(k)}\}$$에 대해 교차엔트로피 최소화  

***

## 3. 샘플링 및 성능 향상  
### 3.1 임의 조건부 샘플링  
- **특성명만 사전조건**: 조합 전(無값) 상태에서 전체 분포 $$p(V_1,\dots,V_m)$$ 샘플링  
- **단일 특성명–값 쌍**: $$p(V_{\setminus i}\mid V_i=v_i)$$  
- **다중 특성명–값 쌍**: $$p(V_{\setminus \{i_1,\dots,i_k\}}\mid V_{i_1}=v_{i_1},\dots)$$  

### 3.2 성능 개선 요인  
- **최소 전처리**: 원본 특성명·값 보존으로 정보 손실 방지  
- **문맥 지식 활용**: 사전학습된 언어 표현이 연관성 파악 지원  
- **조건부 유연성**: 특성 순열 덕분에 재학습 없이 다양한 조건부 생성 가능  

***

## 4. 실험 결과  
- **ML 효율성**: 합성 데이터로 훈련된 분류·회귀 모델 성능이 실제 데이터 대비 동등 또는 우수  
- **판별자 정확도**: Real vs. Fake 판별 정확도가 50%에 근접해 구분 어려움  
- **분포 일치도**: 주요 변수 쌍의 공동분포가 원본 데이터와 유사  

***

## 5. 한계 및 고려사항  
- **연산 비용**: GPT-2 기반 미세조정 및 샘플링 시간이 GAN/VAE 대비 다소 증가  
- **숫자 처리**: 문자열 형태 수치 표현에 의존하므로 분포 특성에 따른 추가 인코딩 연구 필요  
- **데이터 규모**: 극히 대규모 테이블 또는 고차원 범주형 변수에 대한 확장성 검증 필요  

***

## 6. 향후 영향 및 연구 방향  
- **멀티모달 통합**: 텍스트·수치·이미지 등 다양한 데이터 결합 가능성  
- **컨텍스트 강화**: 의학·금융 등 민감 영역에서 전문가 지식 주입 기법 연구  
- **효율적 경량화**: 보다 작은 모델로도 동등 성능 확보를 위한 지식 증류(distillation) 기법 발전  
- **조건부 생성 응용**: 누락값 보간, 반사실적 설명(counterfactual explanation) 생성 등 실무 활용 확장  

GReaT는 **텍스트 기반 탭ular 생성**의 새 장을 열며, **언어 모델의 문맥 이해 능력**을 표 형식 데이터에 도입함으로써 **유연하고 정보 손실 없는 합성 데이터** 구축을 가능케 한다. 앞으로 고급 전처리 대체와 실세계 애플리케이션 적용에서 중요한 연구 기반이 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/3e337be7-ce5d-4b4d-9952-8c5ed37252ce/2210.06280v2.pdf)
