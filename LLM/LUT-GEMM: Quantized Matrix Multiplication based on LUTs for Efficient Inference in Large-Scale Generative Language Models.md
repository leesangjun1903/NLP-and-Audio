# LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models

**핵심 주장**  
LUT-GEMM은 대규모 생성형 언어 모델의 추론(inference) 단계에서, 4비트 이하로 양자화된 가중치를 그대로 사용하면서 풀 프리시전 활성화(activations)와 곱셈을 수행할 수 있는 LUT 기반 커널을 제안한다. 이를 통해 기존의 dequantization 오버헤드를 제거하고, 메모리 이동 및 계산량을 모두 줄여 단일 GPU 환경에서 최대 2.1×의 토큰 생성 속도 향상을 달성할 수 있다.

**주요 기여**  
1. **BCQ 포맷을 이용한 양자화 표현 확장**  
   - 비균등(non-uniform)·균등(uniform) 양자화를 모두 다룰 수 있도록 레벨별 스케일링과 바이어스 항을 도입.  
2. **LUT 기반 매트릭스 곱셈 커널 설계**  
   - μ-길이 서브벡터별로 가능한 모든 조합을 사전 계산해 저장함으로써, 런타임에 LUT 인덱싱만으로 부분 내적을 대체.  
3. **그룹 단위 양자화(group-wise quantization)**  
   - 가중치를 행(row)이 아닌 그룹(g)에 따라 묶어 스케일링 팩터 공유 비율을 조정, 압축 비율과 정확도 간 섬세한 트레이드오프 제공.  
4. **단일-배치(single-batch) 추론 최적화**  
   - 높은 GPU 공유 메모리 활용과 스레드 블록 구성 최적화를 통해, 기존 INT8/FP16 커널 대비 메모리 대역폭 활용률과 전력 효율 개선.  

# 상세 설명

## 1. 문제 정의  
- 대규모 언어 모델(LLM)은 수십억~수백억 파라미터로 인해 GPU 단일 장비에 모델 전체를 적재하기 어렵고, 토큰 단위 생성(generation) 단계는 메모리 대역폭 제약 때문에 병목이 심함.  
- 기존 W4/A16(weight-only 4bit, activation full-precision) 양자화 기법(OPTQ, AWQ 등)은 낮은 메모리 이동을 얻지만, 빠른 dequantization 처리에 많은 리소스를 소모해 실제 계산 가속 효과가 제한됨.

## 2. 제안 방법  
### 2.1 확장된 BCQ(Binary-Coding Quantization) 포맷  
양자화된 가중치 $$ \hat w $$ 를 비트별로 이진 벡터 $$b_i \in \{\pm1\}^n $$ 와 스케일 팩터 $$\alpha_i $$, 그리고 바이어스 $$z $$ 로 표현:  

$$
\hat w = \sum_{i=0}^{q-1} (\alpha_i \cdot b_i) + z
$$

- 비균등 양자화 시 각 비트별 $$\alpha_i$$ 를 독립 설정  
- 균등 양자화는 $$\alpha_i = 2^{i-1}s,\; z = \sum_i\alpha_i + \hat z$$ 로 변환 가능  

### 2.2 LUT 기반 행렬 곱셈  
- 입력 벡터 $$x \in \mathbb R^n $$ 를 $$\mu $$ 길이씩 분할  
- 각 분할된 부분벡터와 $$2^\mu$$ 개의 이진 패턴 조합에 대한 내적을 사전 계산해 LUT에 저장  
- 런타임: 이진 가중치 블록에서 키(key)를 추출해 LUT 조회 후 결과를 스케일 팩터와 곱하고 합산  
- 계산 복잡도: $$O(mnq/\mu)$$ 로, 일반 GEMM $$O(mn)$$ 대비 $$q/\mu$$ 배 절감  

### 2.3 GPU 최적화  
- 스레드 블록마다 $$l$$ 개의 LUT을 공유 메모리에 로드  
- 각 스레드가 독립적으로 LUT 읽기 및 스케일 팩터 곱셈 수행  
- 그룹 크기 $$g$$ 와 양자화 비트 $$q$$ 를 조합해 압축 비율 및 정확도 최적화  

## 3. 성능 향상  
- OPT-175B 모델, 3비트 양자화 시 단일 A100 GPU에서 OPTQ 대비 **2.1× 토큰 생성 속도**↑  
- 메모리 이동 및 계산량 동시 감소로 GPU 활용률·전력 효율 대폭 개선  
- 그룹 크기 $$g=128$$ ~ 256 설정으로 압축 비율과 언어 모델 성능(perplexity) 간 우수한 균형 확보  

## 4. 한계  
- **단일-배치 추론에 최적화**되어 배치 크기 증가 시 공유 메모리 대역폭 병목 발생  
- 현재는 GPU 공유 메모리 크기에 의존, 하드웨어 아키텍처 변화에 민감  

## 5. 모델 일반화 성능  
- 그룹 단위 양자화로 스케일 팩터 공유 범위 조정 시, **모델 로버스트니스(robustness)** 및 미세 조정(fine-tuning) 후 일반화 성능 손실 최소화  
- 바이어스 항 도입으로 0 근처 값 표현력 증가 → 작은 비트수에서도 정보 손실 억제  
- AlphaTuning 등 양자화-인식(fine-tune) 기법과 결합 시 2비트까지 낮춰도 자연어 이해 과제(MNLI)에서 원본 대비 **1~2% 이내 정확도** 유지  

# 향후 연구 및 고려 사항

- **하드웨어 친화적 설계**: 공유 메모리 대역폭 한계 극복을 위한 GPU-특화 아키텍처 또는 TPU 구성 연구  
- **대규모 배치 최적화**: 배치 처리용 LUT 관리 및 스트리밍 기법 도입으로 실시간 서비스 워크로드 대응  
- **양자화-인식 학습**: AlphaTuning, QAT(Quantization-Aware Training)과 결합해 2비트 이하 극저비트 양자화 시 더 강한 일반화 보장  
- **추론 프레임워크 통합**: TensorRT, ONNX Runtime 등 상용 엔진과의 호환성 및 자동화된 $$q$$, $$g$$ 탐색(algo tuning) 모듈 개발  

LUT-GEMM은 양자화 가중치로부터 dequantization 오버헤드를 제거함으로써 메모리 및 연산 효율을 동시에 개선하는 새로운 추론 엔진 설계 방향을 제시하며, 하드웨어 발전과 양자화 학습 기법의 발전을 이어받아 향후 초대규모 언어 모델 서비스의 핵심 컴포넌트로 자리잡을 가능성이 크다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d9c9b0ed-d792-4ef8-a64c-de6d135ec4b3/2206.09557v4.pdf
