# Red Teaming Language Models to Reduce Harms

## 1. 핵심 주장과 주요 기여

이 논문은 대규모 언어모델의 안전성을 향상시키기 위한 **적대적 테스팅(Red Teaming)** 방법론을 체계적으로 연구한 중요한 연구입니다. 논문의 핵심 기여는 다음 세 가지로 요약됩니다:

**첫 번째 기여: 모델 규모와 안전 기법에 따른 확장성 분석**
- 2.7B, 13B, 52B 파라미터 규모의 4가지 모델 유형에 대한 체계적 분석
- RLHF(인간 피드백 강화학습) 모델이 규모가 커질수록 레드팀 공격에 대한 저항성이 크게 향상됨을 발견
- 일반 언어모델과 프롬프팅된 모델은 규모에 따른 뚜렷한 개선 패턴을 보이지 않음

**두 번째 기여: 대규모 레드팀 데이터셋 공개**
- 38,961개의 레드팀 공격 시도로 구성된 데이터셋 공개 (기존 BAD 데이터셋의 10배 규모)
- RLHF로 훈련된 모델에 대한 최초의 공개 레드팀 데이터셋
- 다양한 유해성 카테고리에 대한 정량적/정성적 분석 결과 포함

**세 번째 기여: 투명한 방법론 제시**
- 레드팀 실험의 모든 과정, 통계적 방법론, 불확실성 요소에 대한 상세한 기술
- 연구 참여자 보호를 위한 안전 고려사항 및 윤리적 지침 제공

## 2. 문제 정의 및 제안 방법

### 해결하고자 하는 문제

대규모 언어모델은 다음과 같은 다양한 유해한 행동을 보입니다:
- 사회적 편견 강화
- 공격적이거나 독성 있는 출력 생성
- 개인식별정보 노출
- 허위정보 확산
- 극단주의적 텍스트 생성

### 제안하는 방법론

**1. 다층적 안전 개입 접근법**

논문에서는 4가지 모델 유형을 비교 분석합니다:

- **Plain LM**: 기본 언어모델 (1-shot 학습으로 대화 형태 변환)
- **Prompted LM**: HHH(도움이 되고, 정직하고, 무해한) 프롬프팅 적용 (14-shot 학습)
- **Rejection Sampling (RS)**: 16개 샘플 생성 후 무해성 선호 모델로 상위 2개 선택
- **RLHF**: 인간 피드백 강화학습으로 훈련된 모델

**2. 무해성 점수 계산**

무해성 선호 모델을 사용하여 AI 응답의 무해성을 정량적으로 측정:
- 낮은 점수 = 더 유해한 응답
- 높은 점수 = 더 무해한 응답
- 대화의 최소 무해성 점수를 전체 대화의 무해성 지표로 사용

**3. 레드팀 실험 설계**

- 324명의 크라우드워커로 구성된 레드팀
- 개방형 대화를 통한 적대적 공격 시도
- 5점 리커트 척도로 공격 성공도 자가 평가
- 각 턴마다 2개의 모델 응답 중 더 유해한 것 선택

### 모델 구조

**기본 아키텍처**: 디코더 전용 트랜스포머 모델
- 2.7B, 13B, 52B 파라미터 규모
- 일반적인 언어모델 사전훈련 후 각각의 안전 기법 적용

**무해성 선호 모델**: 
- 동일한 아키텍처의 2.7B, 13B, 52B 모델
- 레드팀 비교 데이터로 훈련하여 응답의 무해성 예측

## 3. 성능 향상 및 일반화 성능

### 주요 성능 결과

**RLHF 모델의 뛰어난 확장성**:
- 모델 규모가 커질수록 레드팀 공격에 대한 저항성 현저히 향상
- 52B RLHF 모델이 가장 우수한 안전성 성능 달성

**Rejection Sampling의 효과성**:
- 모든 규모에서 가장 공격하기 어려운 모델
- 하지만 회피적인 답변으로 무해성을 달성하는 경향

**프롬프팅의 한계**:
- HHH 프롬프팅이 정적 평가에서는 효과적이었으나, 적대적 대화 상황에서는 일반 모델과 차이 없음

### 일반화 성능 향상 가능성

**1. 다양한 유해성 카테고리에 대한 강건성**
- 차별과 불공정성, 혐오 발언, 폭력 선동, 비폭력적 비윤리적 행동 등 광범위한 공격 유형에 대한 저항성
- UMAP 시각화를 통해 다양한 공격 클러스터 식별 및 분석

**2. 모델 규모에 따른 일반화 개선**
- RLHF 모델의 경우 규모가 커질수록 일반화 성능이 체계적으로 향상
- 52B RLHF 모델에서 최고의 안전성 달성

**3. 실제 배포 환경에서의 적용 가능성**
- 오픈엔드 대화 시스템에서의 실증적 검증
- 다양한 사용자 공격 시나리오에 대한 강건성 입증

## 4. 연구의 한계

**1. 범위의 제한성**
- AI 어시스턴트 형태의 대화 시스템에만 초점
- 추천 시스템, 자동완성, 분류기 등 다른 응용 분야 미고려

**2. 도메인 전문성 부족**
- 크라우드워커의 도메인 전문 지식 한계 (예: 폭탄 제조, 화학 물질 제조 관련 평가)

**3. 데이터의 불완전성**
- 언어모델의 일반적 특성상 가능한 모든 유해성을 포괄할 수 없음
- 코드 생성 능력 관련 공격 등 일부 공격 유형 누락

**4. 수동 레드팀의 확장성 한계**
- 비용과 시간 소모가 큰 수동 방식
- 자동화된 레드팀 기법과의 비교 연구 필요

## 5. 향후 연구에 미치는 영향 및 고려사항

### 연구에 미치는 영향

**1. 안전한 AI 개발을 위한 새로운 표준 제시**
- 체계적인 레드팀 방법론이 AI 안전성 연구의 새로운 벤치마크 역할
- RLHF의 효과성 입증으로 인간 피드백 기반 훈련법의 중요성 부각

**2. 투명성과 재현성 향상**
- 대규모 레드팀 데이터셋 공개로 연구 커뮤니티의 후속 연구 촉진
- 상세한 방법론 공개로 재현 가능한 연구 환경 조성

**3. 다학제적 접근법의 중요성 강조**
- Trust & Safety 전문가와의 협력을 통한 연구 참여자 보호
- 심리학, 사회학적 관점의 AI 안전성 연구 필요성 제기

### 향후 연구 시 고려사항

**1. 자동화된 레드팀 기법 개발**
- 수동 방식의 한계 극복을 위한 AI 기반 자동 공격 생성 연구
- 수동과 자동 방법의 효과성 비교 분석 필요

**2. 도메인별 특화 연구**
- 의료, 법률, 금융 등 특정 도메인의 전문적 유해성 분석
- 도메인 전문가를 포함한 레드팀 구성 필요

**3. 윤리적 데이터 공유 프레임워크**
- 유해 콘텐츠 포함 데이터셋의 안전한 공유 방안
- 연구 목적과 악용 가능성 간의 균형점 모색

**4. 실시간 안전성 모니터링**
- 배포된 시스템의 지속적인 안전성 검증 체계
- 새로운 공격 패턴에 대한 적응적 방어 메커니즘 개발

**5. 다국가, 다문화적 관점 확장**
- 서구 중심의 안전성 기준을 넘어선 글로벌 표준 개발
- 문화적 맥락을 고려한 유해성 정의 및 평가 방법 연구

이 연구는 AI 안전성 분야의 중요한 이정표로서, 앞으로 더욱 안전하고 신뢰할 수 있는 AI 시스템 개발을 위한 기초를 마련했습니다. 특히 RLHF의 확장성과 효과성을 입증함으로써, 인간 가치와 일치하는 AI 개발의 중요한 방향을 제시했다고 할 수 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/4b0a6266-b418-4060-a99d-05108f3e3d24/2209.07858v2.pdf)
