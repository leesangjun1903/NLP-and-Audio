# CPM: A Large-scale Generative Chinese Pre-trained Language Model

**핵심 요약**  
CPM은 중국어에 특화된 2.6억 개의 매개변수를 가진 **대규모 자가회귀(autoregressive) 언어 모델**로, 100GB의 중국어 말뭉치로 사전학습되어 다양한 다운스트림 과제에서 **few-shot 및 zero-shot** 상황에서도 뛰어난 성능을 보인다. 주요 기여는 (1) 중국어 서브워드 어휘 구축 (2) 최대 3백만 토큰의 대형 배치 학습 전략 (3) 2.6B 매개변수 모델 공개 및 벤치마크 상위권 성능 입증이다.[1]

***

## 1. 해결하고자 하는 문제  
기존 GPT-3 등 대규모 언어 모델은 영어 말뭉치에 편중되어 있어 중국어 자연어처리 과제에 직접 활용하기 어려웠다.  
- 중국어 어휘 분할 문제: BERT-Chinese의 문자 기반 토크나이저는 단어 의미 손실 발생  
- 대규모 중국어 데이터와 모델 학습 인프라 부족  
- few-shot/zero-shot 학습 성능 미확인  

***

## 2. 제안 방법  
### 2.1 모델 구조  
- **아키텍처**: Transformer 디코더(자가회귀)  
- **모델 스케일**:  
  - CPM-Small: 109M parameters  
  - CPM-Medium: 334M parameters  
  - CPM-Large: 2.6B parameters  
- **하이퍼파라미터**:  
  - `d_model`, `n_heads`, `d_head` 등은 Table 1 참조  
  - 최대 시퀀스 길이 1024  

### 2.2 어휘 구축  
- 단어 및 문자 기반 **Sub-word vocabulary** 생성 (Unigram LM)  
- 스페셜 토큰으로 단어 경계를 표시하여 토크나이징 가역성 보장  

### 2.3 학습 전략  
- **대형 배치**: 3,072 샘플(≈3백만 토큰)  
- **분산 학습**: 파라미터 병렬화로 대형 모델 GPU 간 분산  
- **학습 스케줄**:  
  - 총 20,000 steps, 처음 5,000 steps warm-up  
  - 학습률 $$1.5 \times 10^{-4}$$  
  - Optimizer: Adam  

***

## 3. 성능 향상 및 한계  
### 3.1 성능 향상  
- **Zero-shot 텍스트 분류**: CPM-Large가 TNEWS 70.3%, IFLYTEK 70.8%, OCNLI 44.2% 달성, 랜덤 대비 대폭 개선  
- **Unsupervised 클로즈 테스트**: CPM-Large가 68.5% 정확도, supervised CPM-Medium 수준 달성  
- **Few-shot 대화 생성**: CDial-GPT 대비 다양성 및 임베딩 유사도 지표 상회  
- **QA (CMRC2018, DuReader)**: 1회 예시(one-shot)로 F1·EM 상승, 모델 크기 증가 시 성능 선형 상승  

### 3.2 모델 일반화 및 한계  
- **규모 의존성**: 2.6B 파라미터 이상 모델에서 “매직” 성능 향상 관찰  
- **과잉 생성**: QA에서 불필요하게 장황하고 반복적 문장 경향  
- **데이터 편향**: 문서 길이, 도메인 다양성 한계  
- **효율성**: 학습 비용 및 추론 시 비용 과다  

***

## 4. 일반화 성능 향상 관점  
- **대규모 배치**와 **가역적 서브워드 어휘**가 희소한 중국어 분포 학습 안정화에 기여  
- **Few-shot prompting**이 간단한 템플릿만으로도 다양한 과제 일반화 능력을 이끌어냄  
- **파라미터 확장**이 모델의 언어 생성 및 이해 범용성에 핵심적 역할 수행  
- 추가로 **multi-lingual** 및 **지식 그래프 결합** 학습이 일반화 성능을 더욱 강화할 여지가 있음  

***

## 5. 향후 영향 및 고려사항  
CPM 공개는 중국어 NLP 연구에 큰 전환점을 제공하며 다음 연구에 시사점을 준다:  
- **모델 압축**(pruning, distillation)을 통한 경량화 및 배포 가능성 확대  
- **희소 어텐션** 도입으로 추론 효율 및 맥락 확장  
- **다중 모달·지식 통합**로 일반 지능 향상  
- **데이터 다양성**(도메인·어투·멀티언어) 확보로 과제 간 일반화 연구 강화  
- **응용 과제**: 기계 번역, 요약, 코딩 지원 등 상용화 연구  

앞으로 CPM 구조와 학습 전략을 기반으로 효율성 및 도메인 일반화를 중심으로 한 추가 연구가 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/13cb811b-bbb7-4d1e-8eab-0a7ed2b6a2ce/2012.00413v1.pdf)
