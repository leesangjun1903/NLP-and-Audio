# What learning algorithm is in-context learning? Investigations with linear models

# 핵심 요약

**주장:** 트랜스포머 기반의 인-컨텍스트 학습(In-Context Learning, ICL)은 입력된 예제들만으로 새로운 예측기를 구성하는데, 이는 네트워크 파라미터 업데이트 없이 내부 활성화의 형태로 표준 학습 알고리즘(그래디언트 디센트, 능형 회귀, 최소자승 해 등)을 암묵적으로 구현한 결과라는 가설을 제시한다.

**주요 기여:**
1. **이론적 구현 가능성 증명:**  
   - d차원 선형회귀 문제에 대해, 트랜스포머가 O(d) 은닉 크기와 상수 깊이로 한 스텝의 그래디언트 디센트를, O(d²) 은닉 크기로 능형 회귀 업데이트를 각각 구현할 수 있음을 구성적으로 증명.  
2. **경험적 행태 분석:**  
   - 학습된 ICL 모델이 예제 수·노이즈·모델 용량 조건에 따라 그래디언트 디센트, 능형 회귀, 최소자승 해, 베이지안 추정기 사이를 전형적인 방식으로 전이함을 관찰.  
3. **알고리즘적 특징 탐색:**  
   - 중간층 활성화에서 모멘트 벡터 $$X^\top y$$와 최소자승 해 $$w_{\text{OLS}}$$가 비선형적으로 인코딩됨을 프로빙(probing) 기법으로 확인.  

***

# 1. 해결하고자 하는 문제

대규모 트랜스포머가 주어진 예제 시퀀스 $$[x_1, y_1, ..., x_n, y_n]$$만으로 새로운 입력 $$x'$$에 대한 예측 $$y'$$을 수행하는 *인-컨텍스트 학습* 메커니즘은, 네트워크 파라미터 고정 상태에서 어떻게 작동하는가?  

기존 연구는 ICL의 *무엇*(어떤 함수 클래스 학습 가능한지)에 주목했으나, 본 논문은 ICL의 *어떻게*(내부적으로 어떤 학습 알고리즘을 구현하는지) 해명에 집중한다.

***

# 2. 제안 방법

## 2.1 트랜스포머로의 구현 가능성

트랜스포머 디코더 레이어를 통해 기본 연산 프리미티브(primitives)를 구성하고, 이를 쌓아 다음 두 학습 알고리즘의 한 스텝을 구현 가능함을 보인다.

1. **한 스텝 그래디언트 디센트**  

$$
     w' = w - 2\alpha\bigl(x(w^\top x - y) + \lambda w\bigr)
   $$
   
   - 은닉 크기 $$O(d)$$, 깊이 상수.

2. **셰르만–모리슨(Sherman–Morrison) 능형 회귀 업데이트**  

$$
     w' = (\lambda I + x x^\top)^{-1} \, x y
         = \Bigl(I/\lambda - \tfrac{1}{\lambda^2}\tfrac{xx^\top}{1 + x^\top x/\lambda}\Bigr)\,xy
   $$
  
   - 은닉 크기 $$O(d^2)$$, 깊이 상수.

이를 통해 트랜스포머가 표준 선형 학습 알고리즘을 *암묵적으로* 구현할 수 있는 충분한 표현력을 지님을 수학적으로 증명한다.

## 2.2 경험적 분석

### 2.2.1 평가 지표

- **SPD (Squared Prediction Difference):** 두 예측기 $$A_1,A_2$$ 출력 차이의 제곱 평균.  
- **ILWD (Implicit Linear Weight Difference):** 예측 결과로부터 가장 가까운 선형 파라미터를 추정하고, 이들 간 유클리드 거리 제곱의 평균.

### 2.2.2 주요 관찰

1. **무잡음, 저차원 과소결정 문제:**  
   - ICL 예측은 최소노름 최소자승(OLS) 예측기와 거의 일치.  
2. **노이즈 데이터하에서 베이지안 추정기 매칭:**  
   - 데이터 노이즈 분산 $$\sigma^2$$와 가중치 사전 분산 $$\tau^2$$에 따라, 최적 능형 회귀 정규화 항 $$\lambda=\sigma^2/\tau^2$$를 사용하는 베이지안 최소위험 추정기와 일치.  
3. **모델 용량에 따른 알고리즘 전이(Phase Transition):**  
   - 매우 얕은/소형 모델: 단일 그래디언트 디센트 스텝 유사.  
   - 중간 용량 모델: 능형 회귀 유사.  
   - 고용량 모델: OLS(무정규화 최소자승) 유사.

***

# 3. 모델 구조 및 프로빙

- **모델:** Transformer 디코더 (깊이 $$L$$, 은닉 크기 $$H$$, 헤드 수 $$M$$).  
- **프로빙 기법:** 특정 층 활성화 $$h_i^{(\ell)}$$에서  
  1. 위치-가중합(attention)으로 관심 위치 정보 추출  
  2. 선형/MLP 레이어로 $$\,X^\top y$$ 및 $$w_{\text{OLS}}$$ 예측  
- **결과:**  
  - $$\,X^\top y$$는 중간층(약 7층)에서, $$w_{\text{OLS}}$$는 후반층(약 12층)에서 비선형 인코딩이 회수됨.  

***

# 4. 성능 향상 및 한계

- **성능 향상:** ICL은 훈련 집합 노이즈 수준과 모델 용량에 맞춰 **최소위험 추정**(Bayes-optimal) 알고리즘을 암묵 구현, 불확실성 하에서 일반화 성능을 극대화.  
- **제한점:**  
  1. **비선형 함수 추정 미확인:** 선형 회귀를 넘어 비선형 회귀나 분류 등 다른 함수 클래스에서 동일한 발견이 재현될지 불분명.  
  2. **이론적 하한 미제공:** 실제 학습을 통해 어떤 알고리즘이 *어떻게 선택*되는지에 대한 이론적 설명·최적화는 미흡.  

***

# 5. 일반화 성능 향상 관점

- **베이지안 최소위험 추정 구현:** 노이즈·사전 분산 정보만으로 정규화 강도를 동적으로 조절, 과소적합·과적합 간 **균형**을 맞춰 일반화 성능을 극대화.  
- **용량 제약하 대체 알고리즘:** 모델 용량 부족 시에도 능형 회귀나 부분 그래디언트 디센트를 구현해 최소위험 솔루션에 근접, 제한된 자원 환경에서도 안정적 일반화 가능.

***

# 6. 향후 연구 및 고려사항

- **확장성 검증:** 자연어·비선형 회귀·강화학습 등 **다양한 도메인**에서 ICL 알고리즘 전이가 일관되게 관찰되는지 실험 필요.  
- **이론적 해석 강화:** 왜·어떤 조건에서 특정 알고리즘(GD, 능형, OLS)이 선택되는지 **메타러닝 관점**에서 원리 규명.  
- **모델 효율화:** O(d²) 은닉 크기 대신 실무적 용량으로 베이지안 추정기에 근접하는 **경량화 구조** 설계.  
- **안정적 프로빙 기법:** 대규모 언어 모델 중간층에서 복잡한 추론 절차를 식별할 수 있는 **일반화된 프로빙** 프레임워크 개발.

이 논문은 ICL의 “흑상자” 메커니즘을 표준 학습 알고리즘 관점에서 처음으로 명시화했으며, 대규모 모델의 **내부 추론 과정**을 해명하고 효율적 설계 방향을 제시하는 기폭제가 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/33eed3db-4ef2-422a-bd31-0cbe3f17687e/2211.15661v3.pdf)
