# Program Synthesis with Large Language Models

**주요 주장 요약**  
대규모 언어 모델(LLM)이 일반 목적 프로그래밍 언어(예: Python)에서 자연어로 제시된 과제 설명을 바탕으로 짧은 프로그램을 합성할 수 있는 능력을 보유하며, 모델 파라미터 수가 증가할수록 합성 성능이 로그-선형적으로 향상된다는 점을 입증한다.[1]

## 1. 해결하고자 하는 문제  
기존의 프로그램 합성 연구는 주로 제약 논리, 도메인 특화 언어(DSL), 또는 입력-출력 예시 기반의 방식에 의존해 왔으나,  
-  일반 프로그래밍 언어(예: Python)에서 작동하는 범용적 합성 방법은 부족했다.  
-  자연어 설명을 보다 유연한 사양(specification)으로 활용하는 연구가 미흡했다.[1]

## 2. 제안 방법 및 모델 구조  
### 2.1 데이터셋  
-  MBPP (Mostly Basic Programming Problems): 974개의 짧은 Python 과제 + 3개의 테스트 케이스.[1]
-  MathQA-Python: 23,914개의 수학 문제를 Python 코드로 변환한 데이터셋.[1]

### 2.2 모델  
-  디코더 전용 Transformer 아키텍처  
-  파라미터 수: 244M, 422M, 1B, 4B, 8B, 68B, 137B  
-  사전학습: 웹 문서(코드 포함) 2.81T 토큰  
-  토크나이저: 32K BPE  

### 2.3 학습 및 평가  
-  Few-shot prompting: 각 과제별 80개 샘플 생성, 온도 0.5  
-  Fine-tuning: MBPP 374개 예시에 대해 100 스텝, MathQA-Python 19,209개 예시에 대해 에폭 학습  
-  정답 판정: 테스트 케이스 기반 실행 통과 여부  

## 3. 성능 향상 및 한계  
### 3.1 모델 크기와 성능  
-  MBPP Few-shot: 244M→137B로 증가 시, “어떤 샘플이든” 통과 비율이 15%→59.6%로 상승.[1]
-  Fine-tuning 시 전 모델 크기에서 약 +10%p 일관된 성능 향상 관찰.[1]
-  MathQA-Python Few-shot(137B): 33.4%, Fine-tuned: 81.2%.[1]

### 3.2 일반화 성능  
-  테스트 케이스 수(1~3개)에 민감도 낮음: 1개만 제시해도 3개 제시 대비 +0.6%p 차이.[1]
-  Prompt 예시 선택에 민감: Seed별 최대 60% vs 최소 40% 편차, 앙상블로 66.4% 달성.[1]
-  적대적 테스트 생성 시 약 12%의 해결 사례가 일반 테스트만으로는 판별 실패.[1]

### 3.3 한계  
-  실행 능력 부족: 코드 실행 결과 예측 정확도 최대 29% 불과.[1]
-  단일 샘플 성공률 낮음: 대부분 과제에서 80샘플 중 1~2개만 정답 통과.[1]
-  복합 과제(여러 제약·하위 문제)에서 부분 해결에 그침.[1]

## 4. 모델의 일반화 성능 향상 가능성  
1. **Prompt 설계 최적화**: 예시 수보다 “대표성 있는” 예시 선택이 핵심. Prompt-tuning, Retrieval-augmented prompting 연구 필요.[1]
2. **Execution-guided 학습**: 코드 실행 작업에 대한 Fine-tuning이 합성 성능에도 소폭 긍정적 전이 효과 관측(137B 모델 기준 +3.6%p).[1]
3. **대화형 피드백**: 최대 4회 인간 피드백으로 Few-shot 성능 30%→65% 향상, 피드백 전략 학습이 유망.[1]

## 5. 향후 연구 방향 및 고려사항  
- **정밀 실행 역량 강화**: 코드 의미론 이해를 위한 Multi-modal 학습 또는 기호적 해석 기법 결합  
- **대규모 Execution 데이터 활용**: Zaremba & Sutskever(2014) 방식처럼 실행 데이터 대량 수집 후 Fine-tuning  
- **Prompt-centric 학습**: 메타 학습 기반 Prompt-tuning 및 Retrieval 기법 통합 연구  
- **안전성·윤리성 검토**: 생성 코드의 편향·보안 취약점·저작권 이슈 분석  
- **복합 과제 확장**: 여러 하위 모듈, 외부 라이브러리 호출, 비정형 처리 과제로 확장 검증  

위 연구는 LLM을 일반 프로그래밍 언어 합성에 적용한 첫 대규모 실험이라는 점에서, 프로그래밍 지원 도구(Pair-programming), 자동 코드 리뷰, 교육용 튜터 등 광범위한 응용 가능성을 열어준다. 향후 합성 모델의 의미론적 이해와 실행 정확도를 높이는 연구에 주력할 필요가 있다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/91372788-6659-499f-b56b-dcbaa40c57e1/2108.07732v1.pdf)
