# Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM

## 핵심 주장 및 주요 기여  
본 논문은 **파이프라인, 텐서, 데이터 병렬성**을 결합한 PTD-P(파이프라인-텐서-데이터 병렬화)를 통해 최대 **1조(10¹²) 매개변수** 규모 언어 모델을 **3072 A100 GPU** 클러스터에서 **초당 502 PFLOP/s**로 학습하여, 3개월 이내에 훈련을 완료할 수 있음을 보였다. 주요 기여는 다음과 같다:[1]
- 텐서·파이프라인·데이터 병렬성의 **합성 전략** 제안 및 **이론·실험 분석**  
- **인터리브드(Interleaved) 1F1B** 스케줄링으로 파이프라인 버블을 최대 𝑣배 감소  
- **Scatter/Gather** 통신 최적화로 노드 간 통신량 1/𝑡로 감소  
- 연산 집약도를 높이는 **커널 퓨전**과 메모리·연산 최적화 기법

## 문제 정의  
- 단일 GPU 메모리 한계로 **1조 파라미터** 모델 수용 불가  
- 단일 GPU로 학습 시 **연산량** 과다(예: GPT-3 학습에 288년 소요)  
- 기존 데이터 병렬화는 배치 크기에 종속, 확장성 한계  
- 파이프라인·텐서 병렬화 단독 사용 시 통신 병목 및 파이프라인 버블 발생  

## 제안 방법  
### PTD-P 구성  
- p: 파이프라인 병렬도, t: 텐서 병렬도, d: 데이터 병렬도 (p·t·d = 총 GPU 수)  
- 파이프라인 단계별로 **m = B/(b·d)** 개의 마이크로배치를 처리  
- **인터리브드 1F1B** 스케줄: 각 GPU에 v개 청크(chunk) 할당해 버블 크기를  

$$ \frac{(p-1)(t_f + t_b)}{v} $$  
  
  로 줄임 (기존 대비 v배 감소)  
- **Scatter/Gather**: 텐서 병렬도 t만큼 중복 전송되는 인터–노드 텐서를 1/t 크기로 분할 전송 후 NVLink로 재조립

### 수식 요약  
- 모델 파라미터 수:  

$$ P = 12\,\ell\,h^2\bigg(1 + \frac{13}{12h} + \frac{V + s}{12\ell h}\bigg) $$  

- FLOP/s 계산(활성 재계산 포함):  

$$ F = 96\,B\,s\,\ell\,h^2\Bigl(1 + \frac{s}{6h} + \frac{V}{16\ell h}\Bigr) $$  

## 모델 구조  
- **Transformer** 기반 GPT 계열  
- 토큰당 시퀀스 길이 s=2048, 어휘수 V=51,200  
- ℓ 레이어, 헤드 수 및 히든 차원 h를 모델 크기 따라 조율  
- 각 DGX-A100 노드당 8 GPU, 파이프라인은 노드 간, 텐서는 노드 내로 분할  

## 성능 향상 및 한계  
- **1B→1T 파라미터** 모델 약 32→3072 GPU 약 **44%→52%** 피크 달성[1]
- **혼합 정밀도**(FP16) 활용, 커널 퓨전으로 연산 집약화, 통신 최적화로 효율 극대화  
- **버블 크기**: 배치 크기 증가 시 감소, 마이크로배치 크기 선택(𝑏)는 v·p 및 h에 의존  
- **제한점**:  
  - 통신 지연 여전(IB 링크), 매우 작은 배치에선 파이프라인 버블 비용 큼  
  - 활성 재계산 시 연산량 과다(소규모 배치에서 최대 33% 성능 저하)  
  - ZeRO-3 대비 모델 단독 확장성 부족  

## 일반화 성능 향상 가능성  
- **배치 크기**와 **마이크로배치 크기**(b) 조절이 최적화되어야 일반화 성능 극대화  
- 파이프라인·텐서·데이터 병렬화는 **동기적 최적화**(strict optimizer semantics) 보장  
- 인터리브드 스케줄 및 활성 재계산은 **메모리-연산 트레이드오프** 최적화로 **과적합 억제** 잠재  
- 그러나, 거대한 배치 크기 증가가 일반화에 미치는 영향 연구 필요  

## 향후 연구에의 영향 및 고려사항  
- 자동화된 병렬화 전략 탐색 강화: PTD-P 조합을 자동 선택하는 **AutoML** 기법  
- **하드웨어 중립성** 확보: TPU 등 가속기에도 동일 최적화 적용  
- **통신 비용 모델링** 개선: 계층적 네트워크 토폴로지에 대한 정밀 분석  
- **일반화 연구**: 대규모 배치·활성 재계산이 downstream 태스크 성능에 미치는 영향  
- **비동기·약결합 파이프라인** 적용으로 수렴 속도 대 communication cost 간 상관관계 탐구

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/4cee0920-597c-4f2d-a183-e72cf9c802a5/2104.04473v5.pdf)
