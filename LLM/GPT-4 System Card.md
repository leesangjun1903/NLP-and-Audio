# GPT-4 System Card

**주요 주장 및 기여**  
GPT-4 System Card 는 GPT-4의 잠재적 위험과 이를 완화하기 위한 OpenAI의 다층적 안전 전략을 제시한다. 핵심 기여는 다음과 같다:[1]

- GPT-4의 안전 도전 과제(환각, 유해 콘텐츠, 편향, 프라이버시 침해, 생화학 무기 등) 식별  
- RLHF(Reinforcement Learning from Human Feedback) 및 규칙 기반 보상 모델(RBRM)을 통한 모델 수준의 거부·제어 메커니즘 설계  
- 모니터링·정책·분류기 통합 등의 시스템 수준 안전망 구축  
- 전문가 레드팀 평가와 사용자 피드백을 통한 반복적 개선 프로세스  

***

## 1. 해결 문제 및 제안 방법

### 1.1 해결하고자 하는 문제  
GPT-4는 높은 언어 능력과 확장성으로 인해 다음과 같은 문제를 발생시킨다:[1]
- 환각(hallucinations): 사실과 다른 정보 생성  
- 유해 콘텐츠 생성: 폭력·혐오·불법 행위 조장  
- 편향 및 대표성 문제: 사회적 고정관념 강화  
- 프라이버시 침해: 개인 정보 노출 가능성  
- 이중 용도 악용: 생화학·사이버 공격 등  
- 모델 남용 감지 및 제재 어려움  

### 1.2 제안하는 방법  
OpenAI는 **모델 수준**과 **시스템 수준** 양축에서 다음을 제안한다:  

1) 모델 수준 안전화  
  - 사전학습 데이터 필터링: 부적절 에로틱 콘텐츠 제거  
  - RLHF: 인간 피드백을 통한 거부·허용 행동 학습  
  - RBRM(Rule-Based Reward Model): GPT-4 분류기를 이용한 거부 스타일 세부 제어  
  - 환각 감소용 비교 학습: 모델 자체 생성 데이터로 환각 교정  
  - 다국어·다도메인 거부 경계 학습  

2) 시스템 수준 안전망  
  - **사용 정책·모니터링**: 자동·수동 분류기, 이상 트래픽 탐지, 사용자 제재  
  - **콘텐츠 분류기 개발**: GPT-4 기반 자동 분류기로 정책 위반 감지 가속화  
  - **제품 디자인**: 명확한 에러 메시지·거부 안내, 사용자 교육 문서 제공  

***

## 2. 모델 구조 및 수식 개요

GPT-4 자체 구조는 Transformer 기반이며, RLHF 및 RBRM 학습 절차는 다음 수식으로 요약된다:

1. Supervised Fine-Tuning (SFT):  

```math
     \mathcal{L}_{\mathrm{SFT}} = -\mathbb{E}_{(x,y^*)\sim D_{\mathrm{demo}}} \sum_t \log p_\theta(y^*_t \mid y^*_{ < t }, x)
```

2. Reward Model (RM) 학습:  

```math
     \mathcal{L}_{\mathrm{RM}} = \mathbb{E}_{(x,y_1,y_2)\sim D_{\mathrm{rank}}} \bigl[\max(0, -s_\phi(x,y_1) + s_\phi(x,y_2) + \delta)\bigr]
```

3. PPO 기반 강화학습:  

```math
     \max_\theta \; \mathbb{E}_{y\sim p_\theta(\cdot\mid x)} \left[ \sum_t r_\phi(x,y)\right] - \beta\,\mathrm{KL}\bigl[p_\theta(\cdot\mid x)\,\|\,p_{\mathrm{SFT}}(\cdot\mid x)\bigr]
```

4. RBRM을 통한 세부 보상 스칼라 조절  

이로써 GPT-4는 정책 위반 거부와 안전한 응답 생성 간 균형을 학습한다.

***

## 3. 성능 향상 및 한계

### 3.1 성능 향상  
- **유해 콘텐츠 생성률**: GPT-4는 GPT-3.5 대비 유해 생성 82% 감소  
- **민감 요청 대응**: 의료·법률 조언 등 민감 콘텐츠 처리 정확도 29% 향상  
- **사실성**: TruthfulQA 기준 정확도 60% 수준으로 대폭 상승  
- **사용자 선호도**: 기존 모델 대비 응답 선호도 70% 이상 달성  

### 3.2 한계  
- **거부 불일관성**: 언어·코드·모스 부호 간 거부 경계 차이  
- **과도한 거부(overrefusal)**: 무해 요청도 거부할 가능성  
- **다국어·문화적 편향**: 영어 중심 데이터 한계로 다른 언어·문화에서 오분류  
- **에이전시 공격(jailbreak)**: 시스템 메시지·재프롬프트를 통한 우회 여전  
- **급증적 능력 위험**: 외부 파인튜닝·체인 오브 사고로 예측 불가 성능 도약 가능  

***

## 4. 일반화 성능 향상 가능성

- **RBRM**: 사용자 정의 규칙 기반으로 다중 도메인·다국적 편향 완화 가능  
- **환각 감소 학습**: 모델 자체 생성 데이터 재교정 기법이 다양한 태스크로 전이 효율성 시사  
- **도구 연동(tool use)**: 외부 지식·검색 도구와의 결합이 복합 과제 일반화 촉진  
- **지속적 레드팀**: 전문가 평가 데이터 축적으로 예측 불가 도메인 대처력 강화  

***

## 5. 향후 연구 영향 및 고려사항

- **안전 평가 다양화**: 비영어권·문화권 거버넌스 반영한 멀티언어 평가 필요  
- **에이전시 대응 전략**: 체인 오브 사고·에이전트 음성 공격에 대한 보완책 연구  
- **거버넌스·정책 통합**: 기술적·제도적·윤리적 프레임워크 동시 발전  
- **일반화 연구**: RBRM·환각 교정 방법을 다른 대형 모델·미세조정 기법으로 확장  
- **사용자 교육 및 문서화**: 과신 방지 위한 인터페이스·가이드라인 설계 강화  

앞으로 GPT-4와 차세대 모델의 **안전·정책·성능**을 통합한 종합적 연구가 중요하다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/ea8823b2-2936-4c62-b98b-949d3387a7c9/gpt-4-system-card.pdf)
