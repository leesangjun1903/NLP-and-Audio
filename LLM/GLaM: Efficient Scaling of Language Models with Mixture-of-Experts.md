# GLaM: Efficient Scaling of Language Models with Mixture-of-Experts

## 핵심 주장 및 주요 기여  
**핵심 주장**  
- **밀집(dense) 아키텍처를 희소 활성화(sparse activation) Mixture-of-Experts(MoE)로 대체**함으로써, 동일한 또는 더 우수한 제로/원/소수 샷 성능을 유지하면서 훈련 시간 에너지 소모를 대략 1/3으로 절감할 수 있다.

**주요 기여**  
1. 1.2조 파라미터의 대형 MoE 언어 모델(GLaM)을 제안하고, 토큰당 활성화 파라미터를 96.6B로 제한하여 연산량 절감  
2. GLaM (64B/64E)이 29개 NLP 벤치마크에서 GPT-3(175B)를 제로/원/소수샷 모두 상회  
3. 동일 수준 성능 달성 시 GPT-3 대비 훈련 에너지 64.6% 절감, 추론 FLOPs 48.6% 절감  
4. 고품질 데이터 필터링의 중요성을 대규모 모델에도 재확인  

***

## 문제 정의  
- **대형 밀집 언어 모델**(예: GPT-3)은 뛰어난 in-context 학습 성능에도 불구하고  
  -  훈련 에너지·연산 비용 과다  
  -  추론 FLOPs 선형 증가  
  -  탄소 배출량 높음  

이로 인해 모델 규모 추가 확장이 점점 현실적 한계에 봉착함.

***

## 제안 방법  
### Mixture-of-Experts(MoE) 기반 희소 활성화  
- 각 MoE 레이어에 E개의 전문가(expert)가 존재  
- **각 토큰당 상위 2개 전문가만 활성화**  
- 입력 토큰 $$x$$에 대해 gating 네트워크가 각 전문가의 확률 $$g_i(x)$$ 계산  
- 출력 $$y = \sum_{i\in\text{Top-2}}g_i(x)\,f_i(x)$$  

### 모델 구조  
- Transformer 디코더 기반  
- 전통적 FFN 레이어 절반을 MoE 레이어로 대체  
- 비-MoE 레이어에 GLU+$$\text{GELU}$$ 활성화, relative positional bias 적용  
- 2D 텐서셔딩을 통해 TPU-V4 클러스터에 효율적 분산 학습

### 수식적 요약  
1. Gating: $$\mathbf{g}(x) = \text{softmax}(W_g\,x + b_g)$$  
2. 전문가 출력: $$f_i(x) = \text{FFN}_i(x)$$  
3. 혼합: $$y = \sum_{i=1}^E g_i(x)\,f_i(x)$$, 단 $$\{i\mid g_i(x)\text{ 상위2}\}$$만 사용  
4. 부가 손실: $$L_\text{aux} = \lambda\sum_{i=1}^E \bar{g}_i \ln \bar{g}_i$$ (load balancing)

***

## 성능 분석  
| 범주 | 평가 방식 | GLaM (64B/64E) | GPT-3 (175B) | 개선량 |
|------|-----------|---------------:|-------------:|-------:|
| 제로샷 평균 | NLU+NLG | 62.7 → 56.9 | +10.2% |
| 원샷 평균 |   “  | 65.5 → 61.6 | +6.3% |
| 소수샷 평균 |   “  | 68.1 → 65.2 | +4.4% |
| 훈련 에너지 | MWh | 456 ← 1,287 | –64.6% |
| 추론 FLOPs | GFLOPs/token | 180 ← 350 | –48.6% |

- 29개 벤치마크 → GLaM이 6/7 범주 우수  
- TriviaQA 원샷 $$75\%$$ 달성, GPT-3 대비 +7pp  

***

## 일반화 성능 향상 가능성  
1. **모델 용량 증대와 계산량 분리**  
   - MoE로 파라미터는 1.2T 유지, 활성화는 96.6B로 억제  
   - 다양한 전문가 조합($$O(E^2)$$)을 통한 입력별 서브네트워크 형성  
2. **데이터 필터링 효과**  
   - 고품질 웹 데이터 선별 시 소수샷 NLG 성능 +4–5pt 향상  
   - 데이터 품질이 모델 일반화에 결정적 기여  
3. **스케일링 연구**  
   - 동일 FLOPs budget 내 전문가 수 증가 시 전 범주 성능 상승  
   - 토큰당 활성화 파라미터 고정 → 더 많은 전문가 배치로 모델 다양성 및 표현력 강화

***

## 한계 및 고려 사항  
- **서빙 비용**: 파라미터 수 증가는 디바이스 수 요구를 증가시켜 저부하 트래픽 시 비효율  
- **MoE 불안정성**: 적은 샷·소규모 전문가 설정에서 게이팅 네트워크 훈련 불안정  
- **편향·안전성**: 대규모 MoE도 기존 대형 모델과 유사한 편향·독성 위험 잔존  

***

## 향후 연구에 미치는 영향 및 고려점  
- **MoE 아키텍처 확장**: 희소 활성화 모델이 추론·훈련 비용 효율적 대규모 확장의 핵심 방향  
- **데이터 품질 중심**: 양보다 질 우선 데이터 샘플링 및 필터링 전략 연구 강화  
- **게이팅 안정화**: 전문가 균등 활용 및 고정밀 로드 밸런싱 기법 개발  
- **편향·독성 완화**: MoE 구성원별 편향 모니터링·교정 및 안전 강화 메커니즘 통합  
- **서빙 최적화**: 동적 전문가 활성화·경량화 서빙 파이프라인 설계로 운영 비용 절감

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6e1dcb76-37f1-4085-9b16-38353f21830b/2112.06905v2.pdf)
