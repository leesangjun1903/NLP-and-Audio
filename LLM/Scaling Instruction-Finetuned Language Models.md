# Scaling Instruction-Finetuned Language Models

## 1. 핵심 주장 및 주요 기여  
본 논문은 언어 모델을 “지시(instruction)” 형태의 다중 과제 데이터로 미세조정함으로써, 모델의 **일반화 능력**과 **추론 성능**을 크게 향상시킬 수 있음을 실증한다.  
- 약 1,836개의 다양한 과제를 이용해 PaLM·T5 등 모델을 미세조정한 **Flan** 방법론 제안  
- 체인오브쏘트(CoT) 데이터를 일부 포함시켜 복잡한 추론 과제 성능 유지 및 개선  
- PaLM 540B 기준, 5-shot MMLU 정확도 69.3%→75.2%로 SOTA 달성  

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계  
### 2.1 해결 과제  
- 프리트레인된 대규모 언어 모델이 미지 과제에 직접 잘 일반화하지 못함  
- 체인오브쏘트 평가에서 순수 지시 미세조정된 모델이 오히려 성능 저하  

### 2.2 제안 방법: Flan 미세조정  
- Mixture of Tasks:  
  -  CoT(9개 추론 과제)  
  -  Muffin(80개 대화·코드·QA 과제)  
  -  T0-SF(193개 기존 T0 과제)  
  -  NIV2(1,554개 자연어 지시 과제)  
- 학습 목표: 표준 스팬 마스킹(T5), 인과 LM(PaLM)  
- 손실 함수: 교차엔트로피  
- 최종 목표:  

$$ L = -\sum_{i}\log P_\theta(y_i \mid x_i,\text{instruction}) $$  

### 2.3 모델 구조  
- Flan-T5: 80M–11B 파라미터, encoder–decoder  
- Flan-PaLM: 8B–540B 파라미터, decoder-only  
- Flan-U-PaLM: PaLM-540B 기반 UL2 objective 추가 학습  

### 2.4 성능 향상  
- **스케일링 효과**: 모델 크기·과제 수 증가에 따라 normalized average 점수 +9.4~15.5% 개선  
- **CoT 데이터 포함**: CoT 없는 미세조정은 추론 과제 성능 저하, CoT 포함 시 전 과제 성능 유지·개선  
- **Zero-shot CoT 활성화**: “let’s think step-by-step” 프롬프트로 플랜-PaLM만이 복잡 과제 해결  
- **Usability**: 인간 평가에서 Flan-PaLM 응답 선호도 79%  

### 2.5 한계  
- 미세조정 과제 편향·중복 위험  
- 데이터 기반 편향·유해 발화 완전 제거 불가  
- 거대 모델 의존성, 추론 비용  

## 3. 일반화 성능 향상 관점  
- **다중 과제 학습**: 282개 과제까지만으로도 대부분 이득 획득, 1.8K 과제 추가 시 여전히 개선 지속  
- **모델 크기**: 8B→62B→540B 순으로 절대 성능 크게 상승  
- **CoT 결합**: 지시 미세조정만으로는 비추론 과제에 국한, CoT 병합으로 추론·비추론 과제 모두 강건  

## 4. 향후 연구 영향 및 고려사항  
- **과제 확장**: 더욱 다양한·도메인 특화 과제로 미세조정 범위 확대 가능  
- **효율화**: 파라미터 효율적 미세조정(prompt tuning, LoRA)과 결합한 연구  
- **책임 있는 AI**: 편향 완화·안전성 평가를 위한 지속적 실험 및 거버넌스 필요  
- **추론 메커니즘 분석**: CoT 미세조정이 내부 표현에 미치는 영향 심층 탐색  

본 연구는 지시형 미세조정이 대규모 언어 모델의 범용성·추론 능력을 강화한다는 강력한 설계 지표를 제시하며, 향후 AI 모델의 실용적·책임 있는 적용에 중요한 토대를 제공한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/3a8c4904-23c5-4106-9ce3-e9b63bc4f9a5/2210.11416v5.pdf)
