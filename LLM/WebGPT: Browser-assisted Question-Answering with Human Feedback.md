# WebGPT: Browser-assisted Question-Answering with Human Feedback

**핵심 주장 및 주요 기여**  
WebGPT 논문은 대형 언어 모델(GPT-3)을 **텍스트 기반 웹 브라우징 환경**에 파인튜닝함으로써, 검색과 문서 인용을 통해 긴 형식의 질문에 답변하는 새로운 접근을 제안한다. 인간 시연과 인간 피드백(비교 평가)을 활용하여 모델을 행동 복제, 보상 모델 학습, 강화학습, 그리고 리젝션 샘플링(best-of-n) 단계로 연속적으로 훈련시켜, ELI5 데이터셋 질문에 대해 인간 수준 이상의 답변 품질을 달성한다.  

***

## 1. 문제 정의  
일반 언어 모델은 외부 지식에 기반한 긴 형식 질문응답(Long-Form QA)에서  
-  문서 검색과 사실 검증 능력이 부족  
-  생성된 답변의 **사실 정확도(factual accuracy)**와 **투명성(transparency)** 결여  

이를 해결하기 위해 WebGPT는 **실시간 웹 검색**과 **텍스트 기반 브라우징**을 통해 근거(reference)를 수집하고, 인간 평가자와의 비교 데이터로 직접 품질을 최적화한다.  

***

## 2. 제안 방법  

### 2.1 텍스트 기반 웹 브라우징 환경  
-  **명령어 인터페이스**: Search, Click link, Find in page, Quote, Scroll, End:Answer 등  
-  **트리거된 웹 검색**: Bing Web Search API 활용  
-  **문서 인용**: 모델이 “Quote: <텍스트>” 명령어로 페이지에서 인용문 추출  
-  **상태 요약**: 각 단계마다 질문, 현재 페이지 요약, 이전 행동 기록, 수집된 인용문 제공  

### 2.2 학습 파이프라인  
1) **행동 복제(Behavior Cloning, BC)**  
   – 인간이 브라우저를 사용하는 시연 데이터(≈6,000회)로 GPT-3 모델을 지도학습  
2) **보상 모델(Reward Modeling, RM)**  
   – 모델 응답 쌍(≈21,500건)을 인간 비교 평가(‘A가 더 좋음’ 등)로 학습해 스칼라 보상 예측  
   – 크로스엔트로피 손실, 타이 평가는 50% 레이블  
3) **강화 학습(Reinforcement Learning, PPO)**  
   – 에피소드 종료 시 RM 점수 + BC 정책과의 KL 벌점을 보상으로 사용  
   – KL 벌점 계수(λ)로 과도한 최적화 억제  
4) **리젝션 샘플링(best-of-n)**  
   – BC(또는 RL) 정책으로 n개의 답변 샘플링 후 RM 점수 상위 답변 선택  
   – 추가 학습 없이 추론 시 **컴퓨트 대비 성능 개선**  

***

## 3. 모델 구조 및 수식  

-  **보상 모델**:  
  – 입력: (질문, 답변+인용문) 토크나이즈  
  – 출력: 실수형 보상 $$R(a\mid q)$$  
  – 학습:  

$$
    \mathcal{L}\_{RM} = -\mathbb{E}_{(a_1, a_2, y)}\bigl[y\log\,\sigma(R(a_1)-R(a_2)) + (1-y)\log\,\sigma(R(a_2)-R(a_1))\bigr]
  $$  
  
  ($$y=1$$이면 $$a_1$$이 선호)  

-  **PPO 강화학습**:  
  – 보상 $$r = R_{RM}(a\mid q) - \beta\,KL[\pi(a\mid q)\parallel\pi_{BC}(a\mid q)]$$  
  – PPO 클리핑, 가치함수 및 GAE 이용  
  – $$\beta$$는 KL 벌점 계수  

-  **리젝션 샘플링**:  

– 답변 샘플 
```math
\{a_i\}_{i = 1}^n
```
 생성  

– 최적 답변 
```math
a^*=\arg\max_i R_{RM}(a_i)
```

***

## 4. 성능 향상  

### 4.1 ELI5 평가  
-  **인간 시연** 대비: 175B best-of-64 모델이 **56%** 우세[1]
-  **Reddit 최고투표 답변** 대비: 동 모델이 **69%** 우세  
→ 인간 수준을 뛰어넘는 긴 형식 답변 품질 달성  

### 4.2 TruthfulQA 평가  
-  GPT-3 대비 TruthfulQA ‘진실 & 유익’ 비율: GPT-3 최고 54% vs WebGPT 76%  
-  **모델 크기 증가**에 따라 진실성·유익성 동반 향상  

### 4.3 컴퓨트 효율 스케일링  
-  **Pareto 전선**: (760M, best-of-4), (13B, best-of-16), (175B, best-of-64)  
-  더 작은 모델도 적절한 n으로 보상 최적화 가능  

***

## 5. 한계 및 고려사항  

1) **출처 편향**: Bing API와 웹 페이지 분포에 의존 → 신뢰도 낮은 출처 인용 가능  
2) **자동화 편향(automation bias)**: 인용문이 권위 있어 보이나 오류 시 과도한 신뢰 유발  
3) **편향 강화**: 기본 GPT-3 편향 및 웹 텍스트 편향 → 편향적 답변  
4) **질문 프레이밍 영향**: 질문의 암묵적 가정(stance)에 따라 부정확 답변 유발 가능  

***

## 6. 일반화 성능 향상 가능성  

-  **데이터 다양화**: ELI5 이외에 TruthfulQA·TriviaQA 등 적대적 질문 포함 → 보상 모델의 견고성 강화  
-  **다중 시각화**: 증거 긍정·부정 양측 인용(Debate, Iterated Amplification)으로 편향 완화  
-  **도메인 적응**: 특정 분야용 브라우저 환경+전문가 시연 데이터로 사전적합 → 전문 영역 일반화  
-  **리젝션 샘플링 예측 개선**: 검증 보상 모델 기반 예측 기법[I]로 최적 n·모델 크기 조합 자동화  

***

## 7. 향후 연구 및 고려사항  

-  **안전성 검증**: 웹 액세스 모델의 잠재적 남용(악용 링크 생성 등) 방지 메커니즘 설계  
-  **신뢰성 메트릭**: 인용문 평가 기준 명문화·자동화 연구  
-  **윤리적 거버넌스**: 부적절·편향 답변에 대한 실시간 감시·수정 체계 마련  
-  **지속적 업데이트**: 웹 콘텐츠 변화에 따른 모델 재훈련 및 보상 모델 보강  

WebGPT는 **웹 검색 기반 질의응답**을 통해 언어 모델의 사실적 정확성과 투명성을 획기적으로 향상시켰다. 향후에는 다양한 도메인·질문 유형·언어로의 일반화를 위한 데이터·방법론 확장이 필요하다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/e4e9b6b3-4542-4bc5-a1a7-ff9c8a29b25e/2112.09332v3.pdf)
