# Prefix-Tuning: Optimizing Continuous Prompts for Generation

**핵심 주장 및 주요 기여**  
Prefix-Tuning은 대형 언어 모델(Pretrained LM)의 모든 파라미터를 동결한 채, 입력 앞에 **소량(0.1% 이하)의 연속적인 “프리픽스(prefix)” 벡터**만 학습하여 다양한 자연어 생성(NLG) 작업을 수행할 수 있음을 보인다. 대규모 파인튜닝 대비 저장 공간을 획기적으로 줄이면서도, 전체-데이터(full-data) 환경에서는 동등한 성능을, 저-데이터(low-data) 및 미지의 주제(extrapolation) 환경에서는 오히려 우수한 성능을 달성한다.

***

## 1. 해결하고자 하는 문제  
- **파인튜닝 비용**: GPT-2(774M 파라미터)나 GPT-3(175B 파라미터) 같은 대형 언어 모델은 태스크별로 전체 파라미터를 업데이트·저장해야 하므로, 다중 태스크·다중 사용자 환경에서 비효율적이다.  
- **경량화 요구**: adapter-tuning(2–4% 파라미터)조차도 여러 태스크를 지원할 때 저장·연산 비용이 높다.  
- **일반화 한계**: 전체 파인튜닝 기법은 다른 도메인·미지 주제에 대한 일반화 성능이 낮다.

***

## 2. 제안하는 방법  
### 2.1 프리픽스 튜닝 개념  
입력 시퀀스 $$x$$와 출력 시퀀스 $$y$$ 사이에, 길이 $$\lvert P\rvert$$의 연속 임베딩 벡터(“프리픽스”)를 삽입하여  

$$
z = [\underbrace{\text{PREFIX}}_{P};x;y]
$$  

로 모델에 공급하고, **언어 모델 파라미터 $$\phi$$**는 고정한 채, **프리픽스 파라미터 $$\theta$$**만 학습한다.  

### 2.2 모델 수식  
- **활성화 계산**:  

```math
h_i = 
\begin{cases}
P_\theta[i,:], & i \in P\\
\mathrm{LM}_\phi(z_i, h_{ < i}), & \text{otherwise}
\end{cases}
```

- **목표 함수**:  

$$
\max_\theta \sum_{i\in Y}\log p_\phi(z_i \mid h_{ < i })
$$  

($$\phi$$ 동결, $$\theta$$만 최적화)  

### 2.3 구조적 특징  
- **모듈성**: 태스크별 프리픽스만 저장·교체 → 한 대의 LM으로 다중 태스크·사용자 지원  
- **효율적 배칭**: 사용자별 프리픽스를 전처리한 뒤 동일한 배치로 연산 가능  
- **파라미터 효율**: GPT-2 Medium 기준 약 250K개(0.1%) 학습 → 저장 1000배 축소  

***

## 3. 성능 향상 및 한계  
### 3.1 전통 파인튜닝과 비교  
| 작업(Task)        | 전체 파인튜닝 | Prefix-Tuning (0.1–2%) | 차이 (0.1%)            |
|-------------------|--------------:|-----------------------:|-----------------------:|
| Table-to-Text (E2E)  | BLEU 68.2     | BLEU 69.7              | +1.5                   |
| Summarization (XSUM) | ROUGE-L 37.25 | ROUGE-L 36.05 (2%)     | –1.20                  |

- **Full-data**: Table-to-Text 유사 성능, Summarization 소폭 하락  
- **Low-data**: BLEU 기준 평균 +2.9 상승  
- **Extrapolation**: 미지 도메인·주제에서 Prefix-Tuning이 Full-fine-tune 대비 1–1.5포인트 높은 일반화 성능  

### 3.2 한계  
- **입력 길이 의존**: Summarization처럼 입력 길이가 길수록(512 토큰 이상) 프리픽스 효과 감소  
- **매개변수 최적화 민감도**: 프리픽스 길이, 초기화(실제 단어 활성화로) 등이 성능에 크게 영향  
- **지도 학습 한정**: 비지도(in-context) 학습·디코딩 제어 기법과 직접 비교 필요  

***

## 4. 일반화 성능 향상 메커니즘  
- **LM 파라미터 보존** → 프리트레이닝된 언어 지식 유지  
- **연속 벡터 기반 제어** → 어텐션을 통해 입력 및 출력 전반에 태스크 정보 전파  
- **모듈화된 개입** → 도메인·토픽 변화에 따른 과도한 파라미터 재학습 방지  

***

## 5. 향후 연구 영향 및 고려사항  
- **다중 태스크·개인화**: 사용자별·태스크별 소형 프리픽스 관리 연구  
- **프리픽스 최적화 이론**: 최적 길이·초기화·MLP 재매개변수화 구조 탐구  
- **비지도·제어 생성**: 디코딩 시간 제어(decoding-time control)·제한된 컨텍스트 환경 확장  
- **혼합 적응 기법**: Adapter-Tuning·LoRA 등과의 융합을 통한 파라미터 효율성 극대화  

Prefix-Tuning은 **작은 연속 프리픽스**만으로 대형 LM의 성능을 유지·향상시키며, **저자원·미지 도메인**에서 탁월한 일반화 능력을 보이므로, 경량화·개인화·다중 태스크 환경에서 혁신적 파라다임으로 자리잡을 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/4733eb81-559f-47a7-84e7-1a1ff67f8489/2101.00190v1.pdf)
