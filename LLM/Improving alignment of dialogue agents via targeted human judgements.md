# Improving Alignment of Dialogue Agents via Targeted Human Judgements

**핵심 주장 및 주요 기여**  
DeepMind의 Sparrow 연구팀은 대화형 AI 에이전트를 “도움됨(helpful)·정확함(correct)·무해함(harmless)”으로 정렬하기 위해, 23개의 세부 규칙(rule)을 정의하고 인간 평가자의 **목표 지향적 판단(targeted human judgements)** 을 활용한 강화학습(RLHF)을 도입했다. 주요 기여는 다음과 같다:[1]

1. **세부 규칙 기반 평가**: “인간 정체성 가장하지 않기”, “폭력적·위협적 언어 금지” 등 구체적 규칙을 나열하여 평가자가 각 규칙 위반 여부를 별도 판단하도록 유도.  
2. **다목적 RLHF**: 인간이 선호하는 응답을 최대화(preference)하고 규칙 위반을 최소화(rule violations)하도록 다중 목표 강화학습 수행.  
3. **인라인 증거 제시**: 웹 검색 결과 조각을 대화에 삽입하여 응답 근거를 직접 제시하고, 78%의 경우 증거가 응답을 뒷받침함을 확인.  
4. **세분화된 분석**: 선호도·규칙 위반·증거 적합성·정확도·신뢰도 및 분포적 편향(distributional bias)을 체계적 평가.

***

## 1. 해결하고자 하는 문제  
- 기존 대화 에이전트는 *도움됨*과 *무해함* 목표가 광범위하게 정의되어 있어, 실제 실패 모드를 세부적으로 식별·개선하기 어려움.  
- 특히 “사실적 오류(hallucination)” 문제를 완화하기 위해서는 응답마다 **근거 제공**이 필수적이나, 언제·어떻게 검색할지 판단하는 메커니즘 부재.

## 2. 제안 방법  
### 2.1. 규칙 정의 및 목표 분해  
- “도움됨·정확함·무해함” 세 가지 상위 목표를 23개 자연어 규칙으로 분해.  
- 평가지침에 따라 각 대화(turn)에 대해 특정 규칙 위반 여부를 명시적으로 평가.

### 2.2. 증거 기반 대화 생성  
- 대화 프롬프트에 Search Query, Search Result 역할 추가.  
- 필요 시 구글 검색 API 호출 → HTML 스크래핑(500자 이내) → 증거(fragment)로 삽입.  
- *항상 검색*, *절대 검색 안 함*, *선택적 검색* 방식으로 모델 생성 실험.

### 2.3. 보상 모델 학습  
- **Preference RM**: 인간 선호 데이터로부터 응답 간 선호도(Bradley–Terry 모델) 학습.  
- **Rule RM**: 규칙별 위반 여부를 Yes/No 분류 모델(instruction tuning)로 학습.  
- 두 모델 출력 조합(reranking)에 따라 최종 대화 응답 선택.

### 2.4. 강화학습 루프 (A2C)  
- 에피소드: 하나의 발화(turn). **행동**은 토큰 생성.  
- **보상**: 선호도 보상 + 규칙 미위반 보상(평균) – 길이·형식 페널티.  
- **분산적 대화(대화 버퍼)**: ELI5 질문, 인간 대화, LM 레드팀 질문, 자기 대화(self-play) 혼합.  
- 정책–가치 함수 공유 트렁크(상위 16개 레이어만 미세조정)로 TPU64 클러스터에서 학습.

```math
R_{\text{agent}}(s|c)=\tilde R_{\text{pref}}(s|c)+\frac{1}{n}\sum_{i=1}^n\tilde R_{\text{rule}_i}(s|c)-\beta T - \gamma \mathbf{1}_{\text{invalid}(s)}
```

(단, $$\tilde R$$는 표준화된 보상, $$T$$는 응답 길이, $$\beta,\gamma$$ 상수).[1]

***

## 3. 모델 구조 및 성능  
- **기본 모델**: Chinchilla-70B 기반. DPC(Dialogue-Prompted Chinchilla) 초기화.  
- **SFT**: 인간 선호·무해 대화로 LM 손실 미세조정.  
- **RLHF**: SFT 초기화 모델에 A2C 적용.  
- **테스트 타임**: @8 reranking(4 증거/4 비증거)으로 최종 응답 결정.

### 3.1. 성능 향상  
- **3-모델 선호율**: DPC 대비 선호도 +14%↑, 규칙 위반률 8%로 ↓.[1]
- **증거 지원률**: 84% 필요 시 증거 제시, 78%에서 증거가 응답 뒷받침(plausible&amp;supported).[1]
- **정확도**: 자유 대화(correctness) 평가에서 DPC 44%→Sparrow 63% “true”↑.[1]
- **신뢰도**: 사용자 설문 “trustworthy or very trustworthy” 23%→34%↑.[1]
- **분포적 편향**: BBQ·Winogender 등 테스트셋에서 편향 지표(bias score) 0.05→0.10↑ (RL 후 악화).[1]
- **Alignment Tax**: MMLU·TruthfulQA 벤치마크에서 RLHF 전후 성능 차이 `±0.5%`로 **무시 가능**.[1]

### 3.2. 한계  
- **분포적 편향 악화**: 규칙 기반 인스턴스 해소 후 장기적 편향 분포는 여전히 남음.  
- **증거 한계**: 한 번에 한 조각만 인라인 제시, 다중 문서·상호작용적 검색 미지원.  
- **규칙 세트 확장 필요**: 새로운 해악 탐지 위해 “일반 해악 룰” 수집 후 추가 규칙 계속 발굴.  
- **인간 평가 한계**: 저조한 IAA(inter-annotator agreement) 및 주관성 문제.

***

## 4. 일반화 성능 향상 가능성  
- **모듈화된 규칙 RM**: 규칙별 조건부 분류기로, 소량 데이터로도 **샘플 효율성** 높음.  
- **Reranking과 RLHF 병행**: 서로 보완적으로 전이 가능, 새로운 도메인 빠른 적응 기대.  
- **자기 대화(self-play)**: 에이전트–유저–레드팀 언어모델 반복 학습, **무인도 상황**에서도 지식·안전성 유지.  
- **대화형 증거 추가**: 멀티스텝 검색·추론(chain-of-thought)과 결합 시, 미지 도메인 일반화 강화 여지.  

***

## 5. 향후 연구 영향 및 고려사항  
- **대화 기반 감독(supervision)**: 다중 스텝 토론·데바이트 활용해 인간-에이전트 협력 정렬 연구 필수.  
- **규칙 참여적 정의**: 이해관계자·피해집단 참여(rule crafting) 및 갈등 해결 메커니즘 연구 필요.  
- **인지과학 연계**: 증거 제공이 실제 사용자 신념 변화로 이어지는지, 동기화편향(motivated reasoning) 극복 방안.  
- **분포적 공정성 개선**: 후처리·재학습 기법으로 RLHF 이후 악화된 편향 완화 전략 수립.  
- **증거 메커니즘 확장**: 다큐먼트 순회·지식 요약·웹 탐색 에이전트 통합으로 정확도·검증성 제고.  

**결론**  
Sparrow는 세부 규칙과 인간 선호도를 결합한 RLHF로 정보 탐색 대화 에이전트를 더욱 유용하고 신뢰할 만하게 만드는 중요한 발판을 마련했다. 정밀한 규칙, 증거 제시, 자가 대화 등 모듈화된 접근은 다양한 도메인으로의 일반화와 후속 안전·공정성 연구에 핵심 토대를 제공할 것이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7b560933-8fa9-446f-869c-0e7fa7126734/2209.14375v1.pdf)
