# DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference

**핵심 주장 및 주요 기여**  
DeeBERT는 사전학습된 트랜스포머 기반 언어 모델(BERT/RoBERTa)의 **중간 계층에서 동적으로 예측을 조기 종료(early exit)**함으로써 최대 40배의 추론 속도 향상을 달성하면서도 모델 성능 저하는 최소화할 수 있음을 보인다. 주요 기여는 다음과 같다.[1]
- **오프램프(off-ramps)**: 각 트랜스포머 층 뒤에 추가된 경량 분류기들로, 입력 샘플의 예측 확신도가 기준치를 넘으면 이후 계층 수행을 생략함.  
- **이중 단계 미세조정**: 마지막 층 성능을 유지하기 위해 먼저 전체 모델과 최종 분류기를 학습한 뒤, 중간 분류기만 추가로 미세조정함.  
- **효율·품질 절충 곡선**: 다양한 엔트로피 임계치 설정을 통해 속도와 정확도의 트레이드오프를 사용자 수준에서 선택 가능하게 함.  

***

## 1. 해결하고자 하는 문제  
사전학습 언어 모델은 뛰어난 성능에도 불구하고 **실시간·엣지 디바이스** 적용 시 높은 추론 지연(latency)과 연산 비용이 큰 제약이다. DeeBERT는 **모든 샘플에 동일한 깊이의 모델 연산**을 요구하는 기존 방식의 비효율성을 극복하고자 한다.[1]

***

## 2. 제안하는 방법  
### 2.1 모델 구조  
BERT/RoBERTa의 각 트랜스포머 층 $$i$$ 뒤에 **오프램프** 분류기 $$f_i$$를 추가한 구조를 사용한다(그림 1 참조).[1]

### 2.2 학습 손실  
1) 1단계: 최종 분류기만 사용하여 기존 BERT 미세조정과 동일하게 학습  

$$
L_n = \frac{1}{|D|}\sum_{(x,y)\in D} H\big(y, f_n(x)\big)
$$  

2) 2단계: 앞 단계에서 학습된 트랜스포머 파라미터를 고정(freeze)하고, **중간 오프램프** $$i=1\ldots n-1$$만 학습  

$$
\sum_{i=1}^{n-1} L_i,\quad L_i = \frac{1}{|D|}\sum_{(x,y)\in D} H\big(y, f_i(x)\big)
$$  

여기서 $$H$$는 교차엔트로피, $$D$$는 미세조정 데이터셋이다.[1]

### 2.3 추론 알고리즘  
입력 샘플 $$x$$가 오프램프 $$i$$에 도달하면, 분류기의 출력 확률 분포 $$z_i$$의 **엔트로피** $$\mathcal{H}(z_i)$$를 계산하고,  

$$
\text{if }\mathcal{H}(z_i)\le S,\quad \text{return }z_i;
\quad\text{else proceed to layer }i+1.
$$  

임계치 $$S$$를 높이면 속도가 빨라지지만 정확도는 다소 떨어지며, 반대의 효과도 가능하다.[1]

***

## 3. 성능 및 한계  
### 3.1 성능 향상  
GLUE 벤치마크 6개 과제에서 최대 **40배** 추론 속도 향상, 평균 **30–60%** 레이어 실행 절약을 달성하면서 정확도 저하는 **0.5% 이내**로 유지되었다.[1]
- BERT-base: SST-2에서 **34%** 런타임 절감, 정확도 0.2% 하락  
- RoBERTa-base: QNLI에서 **61%** 런타임 절감, 정확도 0.1% 하락  

### 3.2 한계  
- **임계치 선택 의존성**: 작업별·데이터셋별로 최적 $$S$$를 경험적으로 찾아야 함.  
- **Layer redundancy**: 일부 층이 높은 성능 기여 없이 남아 있어, 오프램프 학습이 비효율적일 수 있음.  
- **일반화 제약**: 드문 예측 패턴을 조기 종료가 잘 다루지 못하며, 미세조정 데이터 편향 시 오프램프 성능치가 크게 편향될 우려가 있음.

***

## 4. 일반화 성능 관점  
- 오프램프는 **고신뢰 샘플**에만 일찍 종료 결정을 내리므로, **드문/어려운 샘플**은 전체 모델을 통과해 더 깊은 특성 학습을 보장한다.  
- 그러나 임계치에 의해 **경계 샘플**(confidence가 임계치 근처인 샘플)은 일관되지 않은 종료로 과적합(overfitting) 우려가 있으며, **앙상블 기법** 또는 **적응적 임계치 학습** 연구가 필요하다.  
- 층별 예측 성능 분석 결과, BERT는 추후 층으로 갈수록 성능이 점진 향상되는 반면, RoBERTa는 일부 초기·후기 층이 정체 또는 성능 저하를 보였다. 이로 인해 **모델 일반화와 과적합 경향**을 층 단위로 세밀히 분석하는 것이 중요함을 시사한다.[1]

***

## 5. 향후 연구 과제  
- **임계치 자동화**: 샘플별·레이어별 적응적 임계치 학습으로 일반화 성능과 효율성을 동시에 최적화  
- **오프램프 구조 개선**: 단일 선형 분류기를 넘는 **다중 레이어·어텐션 기반** 분류기 도입 검토  
- **사전학습과 연계**: 프리트레이닝 단계에서 오프램프 가능성을 탐색해 층별 **표현 학습 균형**을 달성  
- **모델 압축 결합**: DistilBERT나 LayerDrop 기법과의 하이브리드로 사전학습·미세조정·추론 각 단계의 효율 극대화  

DeeBERT는 **실시간 응용과 엣지 디바이스**에 대규모 언어 모델을 실용적으로 적용하는 데 중요한 길잡이로 작용할 것이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4c16e4aa-9f07-4b99-9ade-b0d8ce9207e9/2004.12993v1.pdf)
