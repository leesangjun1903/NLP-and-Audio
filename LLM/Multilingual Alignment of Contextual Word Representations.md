# Multilingual Alignment of Contextual Word Representations

**핵심 주장 및 주요 기여**  
이 논문은 **문맥적 임베딩 간의 정렬(contextual alignment)** 개념을 제안하고, 이를 평가·강화하는 절차를 통해 멀티링구얼 BERT의 제로샷 성능을 획기적으로 향상시킨다.[1]
- **정렬 평가 지표**로서 병렬 코퍼스 기반의 *문맥적 단어 검색(task-specific retrieval)* 방식을 도입하여, 두 언어 간 번역어가 문맥에 따라 유사한 표현을 갖는지를 측정.[1]
- **정렬 강화 방법**으로, 유사 단어 쌍의 임베딩 차이를 최소화하는 손실 함수와 원본 임베딩 보존을 위한 정규화 항을 결합한 파인튜닝 절차를 제안.[1]
- **제로샷 XNLI** 과제에서, 영어로 학습된 모델을 블가리아어·그리스어 등으로 전이했을 때 평균 2.78%p 성능 향상을 달성하고, 일부 언어에서 준(準)완전지도(translate-train) 모델과 대등한 결과를 보임.[1]

***

## 1. 문제 설정  
멀티링구얼 BERT는 104개 언어 위키피디아로 사전학습되어 있으며, 단일 어휘(shared vocabulary)와 배치 내 다국어 샘플링만으로도 제로샷 전이가 가능하다. 그러나 그 내부가 부분적으로만 정렬된 까닭에 여전히 언어 간 불균형 및 문맥별 일관성 결여 문제가 존재한다.[1]

## 2. 제안 방법  

### 2.1 문맥적 단어 검색 (Contextual Word Retrieval)  
병렬 문장 $$C = \{(s_i, t_i)\}_{i=1}^N$$에서, 단어 위치 $$(i,j)$$가 번역어 쌍일 때  

$$
\text{neighbor}(i,s; f, C) = \underset{(j,t)\in C}{\arg\max}\;\text{sim}\bigl(f_{i,s}, f_{j,t}\bigr)
$$  

로 정의된 최근접 이웃 검색 정확도를 정렬 지표로 사용하며, 유사도는 CSLS 함수를 활용한다.[1]

### 2.2 정렬 손실과 정규화  
모델 $$f$$의 파라미터를 업데이트하기 위한 미니배치 $$B$$에 대해 다음 손실 함수를 최소화한다:[1]

$$
L(f; B) = \sum_{(s,t,i,j)\in B} \|f_{i,s} - f_{j,t}\|^2_2
\quad,\quad
R(f; B) = \lambda \sum_{(t,j)\in B} \|f_{j,t} - f^0_{j,t}\|^2_2
$$

여기서 $$f^0$$는 초기 사전학습 모델, $$\lambda$$는 정규화 계수이다.

### 2.3 모델 구조  
기존 멀티링구얼 BERT 아키텍처에 추가적인 파인튜닝 층을 도입하지 않고, 내부 Transformer 파라미터를 직접 미세조정(fine-tuning)하여 단어 임베딩 공간의 정렬을 강화한다.[1]

***

## 3. 성능 향상 및 한계  

### 3.1 제로샷 XNLI 전이 성능  
- 평균 +2.78%p 향상으로, 블가리아어·그리스어는 준완전지도 모델과 동등한 수준 달성  
- 10K·50K 소량 병렬 데이터만으로도 상당한 개선 효과 관찰.[1]

### 3.2 데이터 효율성  
- 10K 문장만으로도 평균 +0.8%p, 50K 문장으로 +1.9%p 성능 향상, 저자원 언어에도 적용 가능.[1]

### 3.3 문맥별·품사별 성능 분석  
- **폐쇄어(closed-class)** 품사는 초기 정렬이 비교적 양호했으나, **개방어(open-class)** 품사에서 현저히 낮은 정렬도 보임  
- 파인튜닝 정렬 후 양쪽 모두 유사한 수준으로 개선되어, 문맥 의존 정렬의 중요성 부각.[1]

### 3.4 한계  
- 여전히 병렬 데이터에 의존하며, 완전 무감독 상황에서는 적용이 어려움  
- 언어 간 사전학습 격차(예: 위키피디아 규모 차이)에 따른 정렬 품질 변동 존재  
- CSLS 기반 평가가 hubness 문제를 완벽히 해소하지 못함

***

## 4. 일반화 성능 향상 관점  
- **문맥적 다양성**이 많은 병렬 코퍼스에서 정렬 손실이 더욱 효과적으로 작용, 하나의 단어 유형에서 여러 번이 등장할수록 정렬 정밀도 증가  
- **저자원 언어**에서도 10K 문장만으로 가시적 이득을 확인, 소규모 데이터로도 일반화 가능성 타진[1]
- **사용 빈도 차이**가 큰 단어 쌍의 정렬 오류는 정규화 항과 파인튜닝으로 완화되어, 언어별 통계적 편향을 줄임[1]

***

## 5. 향후 연구 방향 및 고려사항  
- **무감독 정렬 기법**과의 결합: 병렬 데이터 없이도 문맥적 정렬을 달성하기 위한 자기 지도(self-supervised) 목표 개발  
- **사전학습 통합**: 학습 초기 단계부터 병렬 신호를 주입하여 정렬된 언어표현을 직접 학습하는 크로스링구얼 프리트레이닝  
- **도메인 일반화**: 의료·법률·회계 등 전문 분야 텍스트에 대한 정렬 효율성 및 전이 성능 검증  
- **다중 모달 확장**: 이미지·음성과 결합된 멀티모달 정렬 모델로 확장하여, 보다 풍부한 크로스링구얼 표현 학습 모색  

이상의 연구를 통해 문맥적 정렬은 멀티링구얼 사전학습 모델의 **이해·분석·강화**를 위한 핵심 개념으로 자리매김할 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4b7035a8-c68a-40cb-a9a7-1d37a55ef80d/2002.03518v2.pdf)
