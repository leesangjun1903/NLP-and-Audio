# BERT-of-Theseus: Compressing BERT by Progressive Module Replacing 

**핵심 주장:** BERT-of-Theseus는 대형 사전학습 언어모델 BERT를 **단계적 모듈 교체(Progressive Module Replacing)** 전략으로 점진적으로 경량화하면서, 전체적인 표현 능력과 일반화 성능을 크게 저하시키지 않고 모델 크기를 축소할 수 있음을 보인다.  

**주요 기여:**  
- *Theseus 모듈*이라 명명한 경량화 모듈을 도입하여, 각 Transformer 레이어를 원본 모듈과 대체 모듈이 섞인 앙상블 형태로 학습함으로써 지식 이전을 촉진함.  
- Replacement 비율을 점진적으로 높여가며(full → partial → none) 모델을 단계적 으로 교체하는 **진화적 학습 스케줄**을 제안함.  
- GLUE 벤치마크에서 원본 BERT 대비 최대 50% 파라미터 절감에도 불구하고 성능 저하 1% 이내를 달성함.  

---  

## 1. 해결하고자 하는 문제  
대형 Transformer 기반 모델(BERT)은 높은 성능에도 불구하고 수백만 혹은 수억 개의 파라미터로 인해 모바일·임베디드 환경에서 실시간 응용이 어려움.  
- 기존 지식 증류(Knowledge Distillation) 방식은 학습 안정성, 일반화 성능 저하 문제를 동반함.  

## 2. 제안하는 방법  
### 2.1 Progressive Module Replacing  
1. **Replacement 모듈 설계:** 각 레이어의 원본 서브레이어(S·A·M·P 등)를 크기·깊이가 작은 ‘Theseus 모듈’로 복제.  
2. **앙상블 학습:** 전체 학습 과정에서 레이어별로 원본 모듈과 Theseus 모듈을 확률 $$p$$에 따라 선택하여 순전파 수행.  
3. **진화적 스케줄:**  
   - 초기 $$p=0$$ (전부 원본 사용)  
   - 중간 점진적 증가: $$p \leftarrow p + \Delta p$$  
   - 최종 $$p=1$$ (전부 Theseus 모듈 사용)  
4. **손실 함수:**  

$$
     \mathcal{L} = \mathcal{L}_\text{task} + \lambda\,\mathcal{L}_\text{consistency}
   $$
   
여기서 $$\mathcal{L}_\text{consistency}$$는 원본과 대체 모듈 출력의 차이를 줄이는 정규화 항.  

### 2.2 모델 구조  
- 원본 BERT의 각 Transformer 블록 내  
  - Multi-Head Attention, Feed-Forward 등 주요 구성은 동일  
  - Theseus 모듈은 FFN 크기 축소 및 헤드 수 감소를 적용한 경량화 블록  
- 앙상블 결과를 이용해 지식 이전 없이 자체적으로 교체 학습 가능.  

## 3. 성능 향상 및 한계  
### 성능  
- **GLUE 평균 점수:**  
  - BERT-base: 80.5 → Theseus 50% 압축: 79.8 (-0.7)  
  - 파라미터 수: 110M → 55M (50%↓) 달성.  
- **추론 속도:** GPU 환경에서 1.3× 속도 개선 보고.  

### 한계  
- **압축 한계:** 60% 이상 압축 시 성능 급락 관찰  
- **학습 비용:** 앙상블 확률 조정 등 학습 스케줄 튜닝 필요  
- **도메인 특화 일반화:** 제안 방식이 GLUE 이외의 특정 도메인(의학·법률)에는 추가 검증 필요.  

## 4. 일반화 성능 관점  
- **앙상블 효과:** 원본·대체 모듈 혼합으로 과적합 감소 및 학습 안정성 강화  
- **Consistency Loss:** 출력 차이를 최소화함으로써 지식 왜곡 최소화  
- 실험 결과, 압축된 모델이 SST-2, RTE 등 소량 데이터 과제에서도 원본 대비 과적합 완화 및 **일반화 성능** 유지에 강점을 보임.  

## 5. 향후 연구 영향 및 고려 사항  
BERT-of-Theseus는 **점진적 모듈 교체**를 통한 경량화 연구에서 새로운 패러다임을 제시하며, 다음 연구 방향에 시사점을 준다.  
- **하위 모듈 다양화:** FFN 외 다른 서브레이어(예: 어텐션) 경량화 연구  
- **하이퍼파라미터 자동화:** 교체 비율 스케줄 및 λ 값 자동 최적화  
- **다양한 태스크 적용:** 의료·법률·실시간 대화 등 특화 도메인에서 일반화 및 압축 효과 재검증  
- **모델-하드웨어 공 co-design:** 저전력 디바이스에 최적화된 모듈 설계 연구  

이처럼, 본 논문은 대형 사전학습 모델의 **효율적 경량화**와 **일반화 성능**을 동시에 달성할 수 있는 실질적 방법론을 제시하며, 후속 연구에서 **모듈 단위 교체 프레임워크**를 확장·고도화하는 중요한 초석이 될 것이다.
