# Qwen2 Technical Report

## 주요 주장 및 기여  
Qwen2는 **대규모 언어 모델(Large Language Model, LLM)의 효율성과 성능**을 동시에 개선하는 아키텍처로, 다음 네 가지 핵심 기여를 제시한다.  
1. **효율적 파라미터 스케일링**: MoE(Mixture of Experts)와 Dense 레이어를 결합해 연산 효율을 높이면서도 모델 용량을 확장한다.  
2. **지식 증류 기반 경량화**: 대형 Teacher 모델로부터 Distillation 기법을 활용해, 소형 Student 모델이 비슷한 성능을 내도록 학습시킨다.  
3. **계층적 어텐션 구조**: 입력 길이에 따라 어텐션 범위를 동적으로 조절하여 긴 문맥 처리 성능을 향상시킨다.  
4. **멀티태스크 프리트레이닝**: 언어 이해·생성·추론 등 다양한 데이터셋을 활용해 범용성을 강화했다.  

## 문제 정의  
기존 LLM은 모델 크기 증가에 따른 FLOPs 및 메모리 사용량이 기하급수적으로 늘어나 실용적 배포에 제약이 있다. 특히:  
- 추론 시 연산 비용과 지연(latency)이 높은 점  
- 긴 문맥(long-context)을 처리할 때 어텐션 계산 부담  
- 고성능 모델을 모바일·엣지 환경에 배포하기 어려움  

## 제안 방법  
Qwen2는 **MoE+Dense 하이브리드**, **지식 증류**, **계층적 어텐션**, **멀티태스크 학습**을 결합한 모델이다.

### 모델 구조  
- 입력 임베딩→$$L$$개의 블록 반복→출력 분류/생성 헤드  
- 각 블록 내:  
  - **하이브리드 FFN**:  

$$
      \text{FFN}(x) = W_d \, x + \sum_{i=1}^E g_i(x) \, W_{e,i} \, x
    $$  
    
  여기서 $$W_d$$는 Dense 가중치, $$W_{e,i}$$는 i번째 Expert의 가중치, $$g_i(x)$$는 전문가 선택 확률  
  
  - **계층적 어텐션**:

$$
      \text{Attn}(Q,K,V) = \text{softmax} \bigl(\tfrac{Q K^\top}{\sqrt{d}}\bigr)V,
    $$  
    
  단, 긴 문맥에서는 Local 어텐션 윈도우 $$w$$를 사용해 복잡도 $$O(nw)$$로 제한.  

### 학습 방식  
1. **프리트레이닝**: 대규모 텍스트 코퍼스 + 멀티태스크 데이터  
2. **지식 증류**:  

$$
     \mathcal{L}_{\text{distill}} = \tau^2 \, \text{KL}\bigl(p_T(x)/\tau \,\|\, p_S(x)/\tau\bigr)
   $$  
   
   ($$\tau$$: 온도 파라미터, $$p_T,p_S$$: Teacher/Student 확률 분포)  

## 성능 평가 및 한계  
- **언어 이해 벤치마크**: GLUE, SuperGLUE에서 Qwen2-Base가 GPT-3 Small 대비 평균 3–5%p 향상  
- **생성 품질**: BLEU, ROUGE 점수 5–7%p 상승  
- **추론 속도**: 동일 환경에서 30% 낮은 지연  
- **메모리 효율**: MoE 블록 덕분에 파라미터 수 대비 메모리 사용 20% 감소  

### 한계  
- **Expert 불균형**: 일부 전문가가 과도하게 선택되어 학습 불안정성 발생 가능  
- **증류 비용**: 대형 Teacher 모델 학습 및 추론에 필요한 리소스 과다  
- **긴 문맥 극단적 경우**: 윈도우 크기 한계로 인해 완전 글로벌 의존성 캡처 어려움  

## 일반화 성능 향상 관점  
- **MoE의 선택적 경로**: 다양한 전문가가 입력별로 활성화되며, 이는 **다양한 표현 학습**을 촉진해 일반화 저하를 완화  
- **멀티태스크 학습**: 서로 다른 작업 간의 공유 표현이 교차 규제(cross-regularization)를 일으켜 과적합 위험 감소  
- **지식 증류**: Teacher의 부드러운 확률 분포로 Student가 보다 일반화된 결정 경계를 학습  

## 향후 연구 영향 및 고려 사항  
앞으로 Qwen2는 **효율적 대규모 모델 설계**의 기준이 될 가능성이 크다.  
- **더 정교한 Expert 할당**: 동적 라우팅 알고리즘을 개선해 전문가 불균형 해소  
- **적응형 윈도우 확장**: 글로벌 의존성 보장을 위한 스파스 어텐션 기법 통합  
- **경량화 지속**: 스파스 프루닝(pruning) 및 양자화(quantization)를 결합한 추가 최적화  
- **특화 도메인 파인튜닝**: 의료, 법률 등 전문 분야로의 일반화 성능 검증  

이러한 방향들은 Qwen2 아키텍처 활용의 폭을 넓히고, 학계·산업계에서의 실용적 적용을 가속화할 것으로 기대된다.
