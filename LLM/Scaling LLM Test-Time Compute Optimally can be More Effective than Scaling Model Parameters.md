
# Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters

## 1. 핵심 주장 및 주요 기여

본 논문(Snell et al., 2024)의 **핵심 주장**은 다음과 같습니다:[1]

LLM의 성능 향상을 위해 모델 파라미터를 확대하는 전통적인 접근 방식보다, 테스트 타임(추론 시간)에 계산 자원을 할당하는 것이 더 효과적일 수 있다는 것입니다. 특히 문제의 난이도에 따라 적응적으로 테스트 타임 계산을 할당하는 **"compute-optimal" 전략**을 제시합니다.[1]

**주요 기여**는 다음과 같습니다:[1]

- **통합적 관점 제시**: 프로포저(Proposer)와 베리파이어(Verifier) 프레임워크를 통해 다양한 테스트 타임 계산 방법을 단일 개념으로 통합
- **적응적 계산 할당**: 문제 난이도에 기반한 적응적 하이퍼파라미터 선택으로 4배 이상의 효율성 향상 달성
- **FLOPs 매칭 평가**: 작은 모델 + 테스트 타임 계산이 14배 큰 모델의 성능을 능가할 수 있음을 실증
- **일반화 성능 분석**: 쉬운 문제에서는 테스트 타임 계산이 유리하나, 매우 어려운 문제에서는 사전학습(pretraining)이 더 효과적임을 규명

***

## 2. 문제 정의, 제안 방법 및 모델 구조

### 2.1 해결하고자 하는 문제

논문이 다루는 핵심 문제는:[1]

**"고정된 추론 시간 계산 예산이 주어질 때, LLM이 이를 가장 효과적으로 활용하여 성능을 향상시킬 수 있는 방법은 무엇인가?"**

기존 연구에서는 테스트 타임 계산의 효과에 대해 상충된 결과가 나타났습니다. 일부는 성능 향상을 보여주지만, 수학 추론 같은 복잡한 작업에서는 효과가 제한적이었습니다.[1]

### 2.2 제안하는 방법: Compute-Optimal 전략

#### 2.2.1 개념적 프레임워크

논문은 테스트 타임 계산을 **프로포저-베리파이어 프레임워크**로 통합합니다:[1]

**프로포저(Proposal Distribution)**: 모델이 생성하는 응답의 분포
- 입력 수준 수정: 프롬프트에 추가 토큰 삽입
- 출력 수준 수정: 후처리를 통한 응답 개선

**베리파이어(Verifier)**: 최고 품질의 응답 선택을 위한 점수 함수
- 결과 기반 모델(ORM): 최종 답변만 평가
- 프로세스 기반 모델(PRM): 각 단계의 정확성 평가

#### 2.2.2 핵심 수식

**Compute-Optimal 스케일링 전략의 정의**:[1]

```math
\theta^*_{q,y^*(q)}(N) = \arg\max_{\theta} \mathbb{E}_{y \sim \text{Target}(\theta, N, q)} [\mathbf{1}_{y=y^*(q)}]
```

여기서:
- $\theta$: 테스트 타임 계산 하이퍼파라미터
- $N$: 계산 예산
- $q$: 주어진 프롬프트
- $y^*(q)$: 정답
- $\text{Target}(\theta, N, q)$: 생성되는 응답의 분포

**FLOPs 교환율**:[1]

사전학습과 추론 계산 사이의 트레이드오프를 분석하기 위해:

$$\text{Pretraining FLOPs} = 6ND_{\text{pretrain}}$$
$$\text{Inference FLOPs} = 2ND_{\text{inference}}$$

비율 정의:
$$R = \frac{D_{\text{inference}}}{D_{\text{pretrain}}}$$

테스트 타임 계산을 $M$배 증가시킬 때, 사전학습과 동일한 FLOPs를 맞추려면:

$$\text{테스트 타임 계산 배수} = M + 3\frac{D_{\text{pretrain}}}{D_{\text{inference}}}(M-1)$$

### 2.3 제안된 두 가지 주요 메커니즘

#### 메커니즘 1: 베리파이어를 통한 검색

**Best-of-N Weighted**: N개의 응답을 독립적으로 샘플링 후 베리파이어 점수로 선택[1]

**Beam Search**: 각 단계에서 상위 M개 후보만 유지:
1. N개의 초기 단계 샘플 생성
2. PRM의 step-wise reward-to-go로 점수 매김
3. 상위 $\frac{N}{M}$개 유지
4. 각각에서 M개 제안 샘플링 → 총 N개 후보로 복원
5. 최대 40 라운드 반복[1]

**Lookahead Search**: Beam search에 k-step 롤아웃 추가:
- 현재 단계의 가치 평가를 위해 k 단계 미리보기 실행
- 더 정확한 가치 추정 가능하나 계산 비용 증가
- 계산 비용: $N \times (k+1)$ 샘플[1]

#### 메커니즘 2: 프로포저 분포 수정 (Revisions)

**순차 개선(Sequential Revisions)**: 이전 오답을 컨텍스트로 제공하여 모델이 자가 수정:[1]

- 64개 응답을 병렬로 샘플링
- 정답에 대해 0-4개의 오답을 문맥으로 구성
- 문자 편집 거리 기반으로 관련 오답 선택
- SFT(Supervised Fine-Tuning)로 학습

**병렬 vs 순차 조합**: 

테스트 타임 예산을 $\sqrt{N}$개의 병렬 샘플과 $\sqrt{N}$개의 순차 개선으로 분배할 수 있습니다. 최적의 비율은 문제 난이도에 따라 달라집니다.[1]

***

## 3. 성능 향상 및 일반화 성능

### 3.1 테스트 타임 계산 스케일링 성능

**베리파이어 기반 검색 결과**:[1]

- Beam search는 낮은 계산 예산에서 best-of-N을 크게 능가
- 높은 예산에서는 성능 향상이 감소하여 best-of-N 이하로 하락
- **Compute-optimal 전략**: oracle 난이도 정보 사용 시 best-of-N 대비 **4배 적은 계산으로 동등 성능** 달성 (예: 64 vs 256 샘플)[1]

**개선 프로포저 결과**:[1]

- 순차 개선이 병렬 샘플링을 좁은 차이로 능가
- 쉬운 문제: 순수 순차 개선 최적
- 어려운 문제: 순차와 병렬의 최적 비율 존재
- **Compute-optimal 전략**: best-of-N 대비 **최대 4배 효율성 향상** (256 vs 64 샘플)[1]

### 3.2 일반화 성능 향상 가능성 (핵심 분석)

**문제 난이도별 성능 트렌드**:[1]

#### 쉬운 문제 (Difficulty Levels 1-2):
- 테스트 타임 계산이 **매우 효과적** (+21.6% ~ +27.8% 성능 향상)
- 왜: 기본 모델이 이미 합리적 응답 생성 가능 → 개선 필요
- 메커니즘: 순차 개선이 더 효율적 (이미 올바른 방향)

#### 중간 난이도 문제 (Difficulty Levels 3-4):
- 테스트 타임 계산이 **적당 효과** (+3.5% ~ +19.1%)
- 왜: 기본 모델이 비일관적 성능 → 다양한 접근 필요
- 메커니즘: 병렬 샘플링과 순차 개선의 혼합 최적

#### 매우 어려운 문제 (Difficulty Level 5):
- 테스트 타임 계산이 **거의 효과 없음** (-37.2% ~ -52.9% 성능 저하)
- 왜: 기본 모델의 능력이 부족 → 사전학습 필요
- 현상: 베리파이어 신호 착취(exploitation)로 인한 성능 저하[1]

### 3.3 FLOPs 매칭 평가: 사전학습 vs 테스트 타임 계산

**핵심 발견**:[1]

$R$ (추론 토큰 / 사전학습 토큰 비율)의 값에 따라 최적의 전략이 변함:

| 시나리오 | $R$ 값 | 성능 비교 | 최적 전략 |
|---------|--------|---------|---------|
| 낮은 추론 부하 | $R << 1$ | 테스트 타임 > 사전학습 | 테스트 타임 계산 (특히 쉬운/중간 문제) |
| 중간 추론 부하 | $R \approx 1$ | 혼합 | 문제 난이도 의존적 |
| 높은 추론 부하 | $R >> 1$ | 사전학습 > 테스트 타임 | 사전학습 (특히 어려운 문제) |

**구체적 수치**:[1]
- 쉬운 문제에서 작은 모델 + 테스트 타임 계산이 **14배 큰 모델을 능가** (FLOPs 동일)
- 매우 어려운 문제에서는 사전학습이 더 효과적

### 3.4 일반화 성능의 이론적 의미

**프롬프트 난이도 추정**: 모델의 pass@1 비율 기반 5단계 난이도 분류[1]

- **Oracle 난이도**: 정답 정확성 정보 사용 (상한)
- **예측 난이도**: 베리파이어 점수 평균 사용 (실제 배포 설정)
- 두 방식의 성능 차이 최소 → **예측 난이도도 실용적**

**일반화의 한계**:[1]
- 논문의 분석은 MATH 벤치마크 중심 → 다른 추론 작업에 대한 일반화 필요
- 어려운 문제에 대한 테스트 타임 계산 한계 → 새로운 방법 필요

***

## 4. 모델의 한계

### 4.1 기술적 한계

**베리파이어 신호 착취**:[1]
- 어려운 문제에서 beam search가 베리파이어 신호 과도 최적화
- 예: 정보량 없는 반복 단계 생성, 과도하게 짧은 1-2 단계 솔루션
- PRM 학습 데이터 분포 편향

**개선 모델 분포 편향**:[1]
- 테스트 타임에 올바른 답변이 문맥에 포함되면 다음 개선에서 오류로 변환될 가능성 (38%)
- 해결: 검증자 기반 선택 또는 다수결 투표

**난이도 추정 계산 비용**:[1]
- 난이도 예측을 위해 2048개 샘플 필요 → 추가 계산 비용
- 실제 배포에서 탐색-착취 트레이드오프 필요

### 4.2 방법론적 한계

**단일 데이터셋 평가**: MATH 벤치마크만 사용[1]
- 다른 추론 작업(코드 생성, 상식 추론 등)에 대한 일반화 미검증
- 도메인 특화 효과 가능성

**이론적 설명 부족**:[1]
- 왜 순차 개선이 병렬 샘플링을 능가하는가에 대한 이론적 근거 제한
- 베리파이어 신호 착취의 근본 원인 미규명

**실제 배포 비용 미고려**:[1]
- 오라클 난이도 정보 사용 시 성능이 더 우수하나, 실제로는 예측 난이도 필수
- 예측 난이도 추정의 계산 비용을 명시적으로 모델에 포함하지 않음

***

## 5. 앞으로의 연구에 미치는 영향 및 고려 사항

### 5.1 산업 및 학계에 미치는 영향

**패러다임 전환**:[2][3][4]

2024년 이후 주요 AI 연구소(OpenAI, Google DeepMind, Anthropic)는 **사전학습 스케일링에서 테스트 타임 계산 스케일링으로 전환** 중입니다. Snell et al.의 논문은 이러한 전환의 이론적 근거를 제공했습니다.[4][2]

- OpenAI o1/o3: 테스트 타임 계산을 대규모로 활용하여 ARC/FrontierMath 등 극도로 어려운 벤치마크에서 획기적 성능 달성[3][4]
- Google Gemini Flash 2.0: 적응적 테스트 타임 계산 전략 도입[4]

**경제적 의미**:[5]
- 소규모 온디바이스 모델 + 테스트 타임 계산 = 대규모 데이터센터 모델의 대체 가능성
- 인퍼런스 최적화 업체의 중요성 증대
- Nvidia 의존도 감소 가능성 (추론 특화 칩셋 수요 증가)

**지배력 구조 변화**:[5]
- 오픈 웨이트 모델(예: LLaMA)의 중요성 감소 (테스트 타임 계산은 폐쇄형이 유리)
- 첫 인간 수준 모델의 영향력 감소 (능력 달성 타이밍의 중요성 낮아짐)

### 5.2 최근 관련 연구 (2025년 기준)

#### 1. **일반화된 테스트 타임 계산 방법**

**SETS (Self-Enhanced Test-Time Scaling, 2025)**[2]
- 문제: 보상 모델 학습 비용 제거
- 방법: 최근 LLM의 자가 검증, 자가 수정 능력 활용
- 개선: 기존 방법 대비 보상 모델 불필요[2]

**Think Twice (2025)**[3]
- O1/DeepSeek-R1의 다중 라운드 사고 과정 모델링
- 길이 처리 및 RL 효율성 개선
- 컨텍스트 길이 한계 극복[3]

#### 2. **효율적 테스트 타임 계산**

**Z1: Code-Based Test-Time Scaling (2025)**[6]
- 코드 생성에 특화된 테스트 타임 계산
- 과잉 추론 토큰 제거로 효율성 향상[6]

**S*: Hybrid Framework for Code (2025)**[7]
- 코드 생성 영역으로 확장
- 병렬 + 순차 스케일링 결합[7]

#### 3. **이론적 진전**

**Simple and Provable Scaling Laws (2025)**[8]
- 보상 모델 불필요한 블랙박스 알고리즘
- 증명 가능한 스케일링 법칙 제시
- Knockout 기반 간단한 방법[8]

**Inference Scaling Laws (2025)**[9]
- compute-optimal 추론 전략 분석
- 작은 모델 + 고급 알고리즘이 Pareto 최적임 실증
- 예: Llemma-7B + 트리 검색 > Llemma-34B[9]

#### 4. **아키텍처 혁신**

**Latent Reasoning with Recurrent Depth (2025)**[10]
- 토큰이 아닌 **잠재 공간에서의 추론** (Chain-of-Thought의 대안)
- 특화 학습 데이터 불필요
- 작은 컨텍스트 윈도우 지원[10]

### 5.3 미래 연구 시 고려 사항

#### 1. **다양한 작업 영역으로 확장**

- **현재**: 수학 추론 중심 평가
- **필요**: 코드 생성, 상식 추론, 창의 작업, 기계 번역 등 다양한 도메인 평가[1]
- **시사점**: 도메인별 최적 전략 다를 가능성 높음

#### 2. **난이도 추정 효율화**

**현재의 문제**:[1]
- Oracle/예측 난이도 모두 추가 계산 비용 발생
- 실제 배포에서 cost-benefit 분석 필요

**미래 방향**:
- 질문만 보고 난이도 직접 예측하는 소형 모델 개발[1]
- 동적 난이도 조정 메커니즘
- 강화학습 기반 적응적 할당[1]

#### 3. **테스트-트레이닝 계산 상호작용**

**Distillation & Amplification**:[5][1]
- 테스트 타임 계산 출력을 기본 모델에 재증류(distill)
- 반복적 자가 개선 루프 구성
- 이론적 근거: 모델이 자신보다 더 잘하는 방법을 학습 가능[1]

#### 4. **어려운 문제 해결 방법**

**Snell et al.의 한계**:[1]
- Level 5 (매우 어려운) 문제에서 테스트 타임 계산 실패
- 왜: 베리파이어 신호 착취 및 기본 모델 능력 부족

**개선 방향**:
- 새로운 테스트 타임 계산 방식 개발 (예: 잠재 공간 추론)[10]
- 하이브리드 접근: 사전학습 + 테스트 타임 계산 결합[9]
- 문제별 메타-전략 학습[1]

#### 5. **지배력 구조 변화에 대응**

**AI 거버넌스 함의**:[5]
- 계산 임계값 기반 규제 모델의 재검토 필요
- 추론 계산의 투명성 및 검증 메커니즘 필요
- 동적 할당(조건부 계산) 규제 프레임워크 개발

#### 6. **멀티모달 및 에이전트 작업**

최근 연구: 테스트 타임 계산을 언어 에이전트(도구 사용, 다단계 계획)로 확장[11]
- 단순 추론을 넘어 의사결정 과정에까지 적용
- 에이전트 성능 향상의 새로운 수단

***

## 결론

Snell et al.의 논문은 **LLM 스케일링 패러다임의 근본적 전환**을 제시합니다. 사전학습 계산 중심에서 테스트 타임 계산 중심으로의 이동이 실현 가능하고 효율적임을 실증했으며, 이는 2024년 이후 OpenAI o1/o3, Google Gemini 등 최신 모델들에서 구체화되었습니다.[4][2][3]

**핵심 가치**:
- 작은 모델의 실용성 향상 (온디바이스 배포 가능)
- 계산 자원의 더 효율적 활용 (14배 파라미터 절감)
- 적응적 할당을 통한 4배 이상 효율성 증가

**남은 과제**:
- 어려운 문제에 대한 강화 방법 개발[1]
- 다양한 도메인으로 확장[1]
- 효율적 난이도 추정 및 동적 할당[1]
- 테스트-트레이닝 계산의 최적 결합[1]

이러한 진전은 2025년의 SETS, Think Twice, Z1 등 후속 연구들로 이어지고 있으며, AI 스케일링의 새로운 황금기를 열고 있습니다.[6][2][3]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/fa008e4d-62ae-469c-aab5-4b8798d27bd4/2408.03314v1.pdf)
[2](http://arxiv.org/pdf/2501.19306.pdf)
[3](https://arxiv.org/abs/2503.19855)
[4](https://www.jonvet.com/blog/llm-test-time-compute)
[5](https://www.forethought.org/research/inference-scaling-reshapes-ai-governance)
[6](https://arxiv.org/pdf/2504.00810v1.pdf)
[7](http://arxiv.org/pdf/2502.14382.pdf)
[8](http://arxiv.org/pdf/2411.19477.pdf)
[9](https://openreview.net/forum?id=VNckp7JEHn)
[10](https://arxiv.org/abs/2502.05171)
[11](https://openreview.net/pdf?id=BSfnw8JUTF)
[12](http://arxiv.org/pdf/2412.06540v3.pdf)
[13](https://arxiv.org/pdf/2502.05449v1.pdf)
[14](https://arxiv.org/abs/2408.03314)
[15](https://icml.cc/virtual/2025/poster/44851)
[16](https://en.wikipedia.org/wiki/Neural_scaling_law)
[17](https://www.emergentmind.com/topics/inference-time-optimization)
[18](https://nebius.com/blog/posts/inference-optimization-techniques-solutions)
[19](https://www.ikangai.com/test-time-compute-the-next-frontier-in-ai-scaling/)
