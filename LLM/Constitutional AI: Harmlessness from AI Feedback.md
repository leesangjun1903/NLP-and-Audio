# Constitutional AI: Harmlessness from AI Feedback

# 핵심 요약

**Constitutional AI: Harmlessness from AI Feedback**는 인간의 무해성(harmlessness) 레이블 없이 AI 스스로의 피드백만으로 무해성을 훈련하는 방법을 제안한다.  
주요 기여는 다음과 같다.  
- 최소 열여섯 개의 **“헌법(principles)”**을 활용해 모델이 자신의 유해한 응답을 비판(critique)하고 수정(revision)하도록 하는 **자기 개선(self-improvement)** 파이프라인 설계  
- 수정된 응답으로 지도학습(SL) 단계에서 모델을 미세조정하여 **SL-CAI** 모델 획득  
- SL-CAI를 기반으로, AI가 평가한 무해성 비교(label)를 통합해 RLHF 방식으로 추가 학습함으로써 **RL-CAI** 모델 획득  
- **유해성-도움됨(harmlessness–helpfulness) 트레이드오프**를 완화하고, **비회피(non-evasive)** 응답을 유지하면서 무해성을 크게 향상  

# 문제 정의 및 해결 방법

## 문제 정의  
기존 RLHF는 무해성 레이블 수집에 수만 건의 인간 피드백을 필요로 하며, 무해성 강화 시 모델이 **“I don’t know”** 식으로 대화를 회피(evasive)하는 부작용이 나타난다.

## 제안 방법  
1. **헌법 기반 자기 비판 & 수정**  
   - 모델이 인간을 도발하는 ‘레드팀(red-team)’ 프롬프트에 답변 → 해당 응답에 대해 헌법 원칙을 무작위로 선택해 비판 → 비판을 반영해 수정 응답 생성  
   - 이 과정을 $$n$$회 반복하여 다수의 수정 응답 생성  
2. **지도학습 단계 (SL-CAI)**  
   - 최종 수정 응답을 원본 프리트레인 모델에 대해 한 에폭 동안 fine-tune  
3. **RL 단계 (RL-CAI)**  
   - SL-CAI로부터 프롬프트+응답 쌍 생성  
   - AI 피드백 모델(프리트레인 LM 또는 RLHF 모델)에 다중선택 질문 형식으로 무해성 비교 레이블 생성  
   - 인간 피드백 도움말(helpfulness) 비교 레이블과 혼합하여 **선호도 모델(PM)** 학습  
   - PM을 보상 신호로 이용한 PPO 기반 강화학습 수행  

수식  
- 무해성 비교 레이블은 피드백 모델의 로그확률 $$\log p(A)$$와 $$\log p(B)$$를 정규화하여 소프트 타깃 $$\hat y = \frac{\exp(\log p(A))}{\exp(\log p(A))+\exp(\log p(B))}$$로 활용  
- PPO 손실:  

$$
    L_{\mathrm{PPO}} = -\mathbb{E}_t \min\bigl(r_t(\theta)\hat y_t,\ \mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat y_t\bigr)
  $$

# 모델 구조 및 훈련 데이터

- **프리트레인 모델**: 52B 매개변수까지 다양한 크기  
- **헌법 원칙**: 16개 자연어 지침(rule)  
- **레드팀 프롬프트**: 182K개, 유해 콘텐츠 유도를 위한 crowd-sourced 및 모델생성 질문  
- **도움됨 프롬프트**: 135K개, 일반 지시질문  
- SL-CAI 학습: 무해성 수정 응답 4개/프롬프트, 도움됨 응답 2개/프롬프트  
- RL-CAI 학습: 무해성 비교 182K개(모델생성), 도움됨 비교 135K개(인간)

# 성능 향상 및 한계

| 모델        | 도움됨 Elo ↑  | 무해성 Elo ↑ | 회피성 감소 |
|-------------|-------------|-------------|-----------|
| SL-CAI      | 중간        | 개선        | 낮음      |
| HH RLHF     | 높음        | 보통        | 높음      |
| RL-CAI      | 높음        | 최고        | 매우 낮음 |
| RL-CAI+CoT  | 다소 감소   | 최고        | 매우 낮음 |

- **무해성 향상**: RL-CAI가 HH RLHF 대비 무해성 Elo 약 20–30점 향상  
- **회피성 해결**: 비회피 응답 유지, 민감 대화에도 명료한 이유 설명  
- **도움됨 손실 최소화**: CoT 활용 시 무해성↑ 도움됨↓ 소폭 트레이드오프  
- **한계**:  
  - **Goodhart 현상**: 과도하게 경직된 응답(boilerplate) 발생  
  - **비판의 부정확성**: 비판 단계에서 오판이 종종 관찰됨  
  - **절대 무해성 척도 불안정**: 인간 평가는 주관적 스케일에 의존

# 일반화 성능 및 향후 영향

- **일반화 촉진**:  
  - *여러 헌법 원칙* → 다양한 수정 경로 제공, RL 탐색 강화  
  - *CoT 평가* → 레이블 캘리브레이션 향상, 미묘한 유해 식별력↑  
- **연구 영향**:  
  - 인간 레이블 의존도 축소 → *고품질 소규모 원칙* 중심 훈련  
  - 모델 행동 투명성↑ → *추론 경로(chain-of-thought)*로 안전성·검증 가능성 강화  
- **고려할 점**:  
  - *헌법 원칙의 설계*: 원칙 수·내용 균형 및 다학제적 합의 필요  
  - *자동화된 red-teaming*: AI 주도 재훈련 주기 설계  
  - *다축 행동 조절*: 무해성 외에 정직성·공정성 등 추가 제어 가능성  
  - *윤리적·사회적 리스크*: 무분별한 자동화의 오·남용 및 책임 소재 명확화  

이 방법은 AI 안전성 연구에서 “자기 감독(self-supervision)”과 “원칙 기반 거버넌스”의 새로운 패러다임을 제시하며, 향후 소규모 원칙·체계적 추론·자동화 실험을 통한 AI 정렬(alignment) 연구에 중대한 기반을 제공할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/1efa23b6-465a-42bb-97d3-303505dc9f30/2212.08073v1.pdf)
