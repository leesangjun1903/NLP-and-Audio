# Mixtral of Experts

**주요 주장 및 기여**  
Mixtral 8×7B은 Mistral 7B 아키텍처 기반에 **Sparse Mixture of Experts (SMoE)** 레이어를 도입하여, 각 토큰당 13B 활성 파라미터만 활용하면서 총 47B 파라미터 규모의 모델 성능을 구현한다. 이로써 Llama 2 70B 및 GPT-3.5 수준의 성능을 능가하거나 동등한 결과를 달성하며, 특히 수학 문제, 코드 생성, 다국어 처리에서 탁월한 성능 향상을 보였다.[1]

***

## 1. 해결하고자 하는 문제  
- **파라미터 효율성 한계**: 대형 언어 모델은 성능 향상을 위해 파라미터 수를 늘리나, 실제 추론 시 모든 파라미터를 활용하면 비용과 지연이 크게 증가한다.  
- **장기 문맥 처리**: 32K 토큰 이상의 긴 문맥을 안정적으로 활용하는 데 제약이 있었다.  

Mixtral은 SMoE를 통해 토큰당 연산량을 고정하면서 파라미터 수를 확장하여, 비용 대비 성능을 최적화하고, 긴 문맥 활용 능력을 개선하고자 한다.[1]

***

## 2. 제안 방법  

### 2.1 Sparse Mixture of Experts 레이어  
각 Transformer 블록의 FFN을 다음 식과 같은 SMoE 레이어로 대체한다:  

$$
y = \sum_{i=0}^{n-1} \text{SoftmaxTop}_K(x W_{g})_i \cdot \text{SwiGLU}_i(x)
$$

여기서  
- $$n=8$$ 은 전문가(expert) 수,  
- $$K=2$$ 는 토큰당 선택 전문가 수,  
- $$W_{g}$$ 는 라우팅용 선형층 파라미터,  
- \text{SoftmaxTop}_K$$ 은 상위 $$K$$개 로짓에만 softmax를 적용하는 스파스 게이팅 기법이다.[1]

이로써 토큰당 활성 파라미터는 

```math
K \times \text{expert\_size}
```

로 고정되며, 모델 전체 파라미터는 $$n$$-배 확장된다.

### 2.2 모델 구조  
- 레이어 수: 32  
- 은닉 차원: 4096  
- 헤드 수: 32  
- 컨텍스트 길이: 32,768 토큰  
- 전문가 수: 8, 전문가당 SwiGLU FFN 블록  
- 활성 파라미터: 13B, 전체 파라미터: 47B[1]

***

## 3. 성능 향상  
- **일반 벤치마크**: MMLU, HellaSwag, Winogrande 등에서 Llama 2 70B와 동등하거나 상회  
- **수학 & 코드**: GSM8K (8-shot)에서 58.4→58.4 점, MBPP (3-shot)에서 49.8→60.7 점으로 대폭 향상  
- **장기 문맥**: Passkey 과제에서 문맥 길이·위치와 무관하게 100% 정확도 달성, 증명문서(perplexity) 평가에서 문맥 증가 시 연속적 하락[1]
- **다국어**: 프랑스어, 독일어, 스페인어, 이탈리아어 MMLU 등에서 Llama 2 70B 대비 평균 6–10%p 향상[1]

***

## 4. 일반화 성능 향상 요인  
1. **컨텍스트 크기 확대**: 32K 토큰 전 범위를 디코더에 완전 지원하여 장기 의존성 학습 강화  
2. **전문가 라우팅의 유연성**: 토큰별로 매 시점마다 다른 전문가 조합을 활용해 다양한 의미·문법 구조에 대응  
3. **파라미터 확장**: 활성 파라미터를 고정하면서 전체 모델 파라미터를 늘려 표현력 증대  
4. **라우터 학습**: 훈련 중 데이터 도메인별(수학, 코드, 일반문장) 라우팅 분포 학습으로 특정 영역 일반화 능력 강화[1]

***

## 5. 한계 및 고려사항  
- **메모리 오버헤드**: Sparse 파라미터 저장 시 47B 규모 GPU 메모리 요구  
- **라우팅 비용**: 게이팅 연산 및 토큰 이동으로 인한 추가 지연  
- **로드 밸런싱**: Expert Parallelism 시 토큰 편중 배치로 인한 GPU 활용 불균형  
- **편향**: Bias 벤치마크(BBQ, BOLD)에서 Llama 2 대비 편향 감소는 확인되나, 여전히 일부 그룹 간 변동 존재[1]

***

## 6. 향후 연구에 미치는 영향 및 고려점  
Mixtral은 **공개 가중치**의 SMoE 모델 중 최첨단 성능을 입증하여, 대규모 언어 모델에서 **파라미터 효율성**과 **장기 문맥 활용**의 새로운 가능성을 제시한다.  
향후 연구 시 고려해야 할 점은 다음과 같다.  
- **하드웨어 최적화**: 라우팅 로드 밸런싱 및 캐싱 전략 개선  
- **정교한 게이팅**: 전문가 선택 알고리즘 다양화로 특화 영역 일반화 강화  
- **편향 및 안전성**: 다양한 도메인·언어에서 편향 완화 기법 병행 적용  
- **적은 자원에서의 학습**: SMoE의 데이터 효율성·로우 리소스 환경 적용 검토  

Mixtral의 공개 라이선스는 연구·산업 양측에서 SMoE 활용과 개선 연구를 촉진할 것이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c20ab03d-c7ce-497a-bf69-8fdcb715e7c2/2401.04088v1.pdf)
