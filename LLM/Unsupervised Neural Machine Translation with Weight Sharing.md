# Unsupervised Neural Machine Translation with Weight Sharing

## 주요 주장 및 기여  
**핵심 주장:** 기존의 비지도 기계번역(Unsupervised NMT) 모델이 단일 공유 인코더(shared encoder)를 사용함으로써 각 언어의 고유 특성을 충분히 보존하지 못하는 문제를 지적하고, 부분적 가중치 공유(weight sharing)를 통해 언어별 고유 특성과 공통 잠재 공간(shared-latent space)을 모두 보존할 것을 제안한다.  
**주요 기여:**  
- 언어별 독립 인코더·디코더 구조에 **가중치 공유 제약**을 도입하여 고수준 표현의 일부만 공유  
- **임베딩 강화 인코더**(embedding-reinforced encoder)로 고정된 사전학습 크로스링구얼 임베딩을 보조 입력으로 활용  
- 잠재 표현 분포 정렬을 위한 **로컬 GAN**과 문장 생성 품질 향상을 위한 **글로벌 GAN** 통합  
- **방향성 자기어텐션**(directional self-attention)으로 순서 정보 보강  
- 영어·독일어, 영어·프랑스어, 중국어·영어 비지도 번역에서 기존 기법 대비 BLEU +1.6~+1.9 향상  

***

## 1. 문제 정의  
1) **단일 공유 인코더의 한계**: 소스·타겟 언어를 같은 인코더로 처리하면 용어, 문체, 구문 등 각 언어 내부 특성을 학습하지 못해 번역 품질 저하  
2) **공통 표현 학습 부재**: 언어별 잠재 공간 매핑이 불충분하여 언어 간 변환 성능이 제한  

***

## 2. 제안 방법  
### 2.1 모델 구조  
- 인코더 Encs·Enct, 디코더 Decs·Dect를 **언어별 독립**으로 두되,  
  - 인코더의 마지막 ℓ<sub>e</sub>개 층, 디코더의 처음 ℓ<sub>d</sub>개 층에 **가중치 공유**  
- **임베딩 강화:**  

$$ H_r = g \odot H + (1-g)\odot E,\quad g=\sigma(W_1E + W_2H + b) $$  
  
  E: 고정된 크로스링구얼 임베딩, H: 인코더 출력을 게이트로 결합  
- **방향성 자기어텐션:** 순방향·역방향 positional mask $$M_f, M_b$$ 적용하여 순서 정보 보강  

### 2.2 학습 전략  
1) **Denoising Auto‐Encoding**: 입력 문장 순서 노이즈(shuffle) 후 복원  
2) **Back‐Translation**: 가짜 병렬문장 생성→역번역 재학습  
3) **Local GAN**: 잠재 표현 분포를 구분기(D<sub>l</sub>)로 구분 불가하도록 인코더 적대적 학습  
4) **Global GAN**: 생성문이 실제 문장과 유사하도록 디코더·인코더 통합(generator)과 CNN 기반 구분기(D<sub>g</sub>)를 RL(policy gradient)로 함께 학습  

***

## 3. 성능 향상 및 한계  
| 비교군 | En→De BLEU | En→Fr BLEU | Zh→En BLEU |  
|:------:|:---------:|:---------:|:---------:|  
| Word‐by‐word | 5.85 | 3.60 | 5.09 |  
| Lample et al. (2017) | – | 15.05 | – |  
| **제안 기법** | **10.86** | **16.97** | **14.52** |  
| 지도학습(upper‐bound) | 24.07 | 30.50 | 40.02 |  

- 가중치 공유 계층수 실험에서 ℓ<sub>e</sub>=ℓ<sub>d</sub>=1일 때 최적[0.85][0.53][1.66]  
- 주요 컴포넌트 제거 시 성능 저하(가중치 공유 제약 제거 시 –0.63~–0.78 BLEU)  

**한계 및 고려사항:**  
- 지도학습 대비 여전히 큰 BLEU 격차(12~25점)  
- 순서 정보 보강 방안과 GAN 안정성, RL 학습의 복잡도  

***

## 4. 일반화 성능 향상 관점  
- **부분 공유 가중치**로 두 언어 간 공통·개별 특성 동시 학습 가능 → 서로 다른 언어 쌍, 도메인으로 확장성  
- **임베딩 강화**가 오픈 도메인 고정 임베딩 활용에도 견고성 제공  
- **GAN 기반 분포 정렬**은 새로운 언어·소스 도메인간 잠재 분포 차이 완화  
- **방향성 어텐션**은 RNN 의존 없이 다양한 구조에 적용 가능  

***

## 5. 미래 영향 및 연구 시 고려점  
- **영향:** 비지도 MT 모델 설계 패러다임 전환—“단일 인코더 공유”에서 “부분 가중치 공유+GAN 통합”으로  
- **고려점:**  
  - GAN 학습 안정성과 RL 샘플 효율성 개선  
  - 언어 모델·구문 정보 결합으로 추가 일반화  
  - 더 먼 언어 계열(아프리카어·소수어) 및 다중 도메인 적용 실험  
  - 순서 정보 학습을 위한 어텐션 변형 탐색  

이 논문은 **비지도 NMT의 잠재 공간 학습**과 **언어별 표현 보존** 간 균형을 맞춘 모델 구조를 제시함으로써, 이후 **크로스‐도메인·크로스‐언어 일반화** 연구에 중요한 기틀을 마련했다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/501858eb-814a-4a6e-a3e2-90878249152e/1804.09057v1.pdf)
