# Titans: Learning to Memorize at Test Time

## 1. 핵심 주장 및 주요 기여
**Titans**는 인간의 기억 시스템에서 영감을 받아, *짧은 문맥(attention)*과 *장기간 기억(neural memory)*을 결합한 새로운 딥러닝 아키텍처입니다.  
- **핵심 주장**: 기존 Transformer 계열 모델들이 고정 길이의 컨텍스트만 처리하는 반면, Titans는 테스트 시점에 *온라인 메타-러닝* 방식으로 장기 정보를 신속·효율적으로 학습·기억할 수 있다.[1]
- **주요 기여**:  
  1. **Neural Long-Term Memory Module**: 충격도(surprise)에 기반해 과거 정보를 모멘텀과 가중치 소실(weight decay)로 관리하며, 테스트 단계에서도 파라미터를 업데이트하는 메타-학습 구조를 도입.  
  2. **Titans 아키텍처**: 메모리를 입력 문맥에 ‘컨텍스트(Context)’, ‘게이트(Gating)’, ‘레이어(Layer)’ 세 가지 방식으로 통합한 세 가지 변형 모델(MAC, MAG, MAL)을 설계.  
  3. **대규모 장문 실험**: 2M 토큰 이상의 긴 시퀀스에서도 Transformer보다 우수한 성능을 보이며, Needle-in-a-Haystack 과제와 장기 추론 벤치마크에서 탁월함을 입증.  

## 2. 문제 정의 및 제안 방법

### 2.1 해결하고자 하는 문제
- **고정 길이 컨텍스트 제약**: Transformer의 attention은 $$O(N^2)$$ 복잡도와 고정 윈도우 크기로 장기 의존성 포착에 한계.  
- **장기 기억 부족**: RNN·Linear Transformer 등은 장기 정보를 고정 크기 상태 벡터에 압축해 일반화가 저하되고, 메모리 관리(망각, 강조)에 한계.[1]

### 2.2 제안 방법

#### 2.2.1 Neural Memory 업데이트
장기 메모리 $$M_t$$를 **모멘텀 기반 기억**과 **가중치 소실**로 관리:

$$
\begin{aligned}
S_t &= \eta_t S_{t-1} - \theta_t \nabla \ell(M_{t-1}; x_t), \\
M_t &= (1 - \alpha_t) M_{t-1} + S_t,
\end{aligned}
$$

여기서  
- $$\ell(M_{t-1}; x_t) = \|M_{t-1}(k_t) - v_t\|^2$$ 은 키-값 연관 학습 손실,  
- $$\eta_t$$, $$\theta_t$$ 는 데이터 의존적 모멘텀·학습률,  
- $$\alpha_t$$ 는 게이트(망각) 계수이다.[1]

#### 2.2.2 Titans 아키텍처
1. **MAC (Memory as Context)**  
   - 장기 메모리로부터 꺼낸 히스토리 $$\,h_t=M^*_{t-1}(q_t)\,$$와 지속 메모리 $$\,P$$를 현재 입력에 연결(concatenate) 후 전체 attention 수행.  
2. **MAG (Memory as Gating)**  
   - 슬라이딩 윈도우 attention 출력 $$y$$와 장기 메모리 출력 $$M(x)$$를 비선형 게이트 $$\otimes$$로 결합.  
3. **MAL (Memory as Layer)**  
   - 입력에 먼저 메모리 모듈 적용 후(sliding-window) attention 수행.  

Persistent memory $$P$$는 입력-독립적 파라미터로 태스크 지식 보유에 사용된다.  

## 3. 성능 향상 및 한계

### 3.1 성능 향상
- **언어 모델링 및 상식 추론**: Transformer 대비 perplexity 최대 10%↓, Commonsense 벤치마크 평균 정확도 3–5%p↑.  
- **Needle-in-a-Haystack**: 2K–16K 시퀀스에서 기존 Linear 모델 대비 정확도 10–30%p 크게 향상.  
- **BABILong 장문 추론**: Few-shot과 파인튜닝 모두 GPT-4 및 대형 LLM보다 높은 정답률 달성.  
- **다양한 도메인**: 시계열 예측·유전체 분류 등에서도 일관되게 최상위권 성능.  

### 3.2 한계
- **연산 비용**: 모멘텀·소실·게이트 연산으로 Linear 모델 대비 다소 느린 학습·추론.  
- **메모리 크기 제약**: 가중치 소실이 필요 없는 장기 이벤트 남용 시 메모리 누수 가능.  
- **하이퍼파라미터**: $$\eta_t,\theta_t,\alpha_t$$ 설계가 모델별 튜닝 필요.  

## 4. 일반화 성능 향상 가능성
- **온라인 메타-학습**: 테스트 시점에도 파라미터 업데이트로 분포 변화에 대응, OOD 상황에서도 적응력 증가.  
- **모멘텀 기반 Surprise**: 과거 중요 정보 지속 보존과 최신 이벤트 반영의 균형으로 과잉 적합 방지.  
- **가중치 소실(망각)**: 불필요 정보 제거로 모델 복잡도 제어, 과거 데이터 과잉 의존 억제.  

이로 인해 기존 장기 기억 구조보다 **일반화** 및 **장문 extrapolation** 능력이 현저히 개선된다.  

## 5. 향후 연구에 미치는 영향 및 고려 사항
- **확장성**: 메모리 모듈 심화 및 경량화(청크 단위 파라미터), 분산학습 최적화 연구가 필요.  
- **자동 하이퍼파라미터 학습**: $$\eta_t,\theta_t,\alpha_t$$의 학습 기반 자동화로 적용 간소화.  
- **모듈화 적용**: ViT·음성·그래프 모델 등 다양한 아키텍처에 Neural Memory를 통합해 범용성 검증.  
- **안정성·해석성**: 테스트 시점 파라미터 변화 추적, 메모리 활용 추론 경로 해석 연구로 신뢰성 강화.  

Titans는 **장문 처리와 일반화**를 함께 추구하는 새로운 패러다임을 제시하며, 향후 대규모 시퀀스·멀티모달 학습 연구에 중대한 이정표가 될 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/c4d3216a-96f7-4327-9956-3d88ae71e287/2501.00663v1.pdf
