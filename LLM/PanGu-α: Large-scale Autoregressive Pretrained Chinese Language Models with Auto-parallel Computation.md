# PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation

## 1. 핵심 주장 및 주요 기여  
PanGu-α는 최대 2천억 개 파라미터를 갖는 대규모 자회귀(autoregressive) 중국어 언어 모델로,  
MindSpore의 Auto-parallel 기능을 활용한 오토 병렬 처리를 통해 2,048개의 Ascend 910 AI 프로세서에서 효과적으로 학습되었음을 주장한다.  
주요 기여는 다음과 같다.  
- **모델 규모 확대**: 2.6B, 13B, 200B 규모 변형을 제안하여, 모델 크기 증가에 따른 성능 향상 추이를 체계적으로 분석.  
- **대규모 병렬화 전략**: 데이터·모델·파이프라인·옵티마이저 병렬화와 재계산(rematerialization)을 결합한 5차원 병렬화 기법을 설계·구현.  
- **1.1TB 고품질 중국어 코퍼스 구축**: 웹 크롤링·룰 기반 정제·모델 기반 필터링·퍼지 중복 제거 과정을 거쳐 1.1TB 규모 말뭉치 확보.  
- **Few-Shot 제로·한샷·소수샷 평가**: 16개 NLP 과제에서 소수샷(in-context) 학습 능력을 실험적으로 검증.  

## 2. 문제 정의 및 제안 기법  
### 2.1 해결하려는 문제  
- 기존 PLM의 영어 중심성과 API 접근 제한  
- 수백억 파라미터 모델 학습을 위한 메모리·통신 병목  
- 대규모 중국어 말뭉치의 품질·다양성 확보 어려움  

### 2.2 모델 학습 목표  
시퀀스 $$X=\{x_1,\dots,x_N\}$$ 의 확률 최대화:  

$$
L=\sum_{n=1}^N \log p(x_n \mid x_{1:n-1};\theta)
$$  

### 2.3 PanGu-α 구조  
- **Transformer 기반 디코더**  
  – $$L$$개 층의 표준 Multi-Head Self-Attention과 FFN  
- **쿼리 레이어**: 위치 임베딩 $$p_n$$을 쿼리로 활용하여 다음 토큰 예측 강화  
- **모델 변형**:  
  – 2.6B: 32층·d=2560·Nh=40  
  – 13B: 40층·d=5120·Nh=40  
  – 200B: 64층·d=16384·Nh=128  

### 2.4 5차원 병렬화  
1. 데이터 병렬  
2. 연산(op) 수준 모델 병렬  
3. 파이프라인 병렬  
4. 옵티마이저 병렬  
5. 재계산(rematerialization)  

하드웨어 토폴로지 인지 스케줄링으로 각 병렬 방식을 최적 조합, 2,048개 프로세서 활용.  

## 3. 성능 향상 및 한계  
### 3.1 언어 모델링 퍼플렉서티  
- 2.6B: PPL 19.33 → 13B: 17.69 → 200B: 15.59  
  → 모델 규모 확대에 따라 언어 모델링 능력 지속 개선  

### 3.2 Few-Shot NLP 과제 성능  
- **리딩 컴프리헨션**: 2.6B→13B, F1 약 23→29% (소수샷)  
- **클로즈 테스트**: 2.6B→13B, 정확도 약 37→44%  
- **제로·한샷·소수샷 전반**: 대체로 모델 크기 증가 시 평균 3–7% 포인트 개선  
- **한계**: NLI 일부 과제에서는 소규모 모델이 더 안정적 결과, 대규모 모델은 과적합·추론 불안정성 잔존  

## 4. 일반화 성능 향상 가능성  
- **다양한 데이터 소스**: 백과·뉴스·전자책·웹크롤링 병합으로 도메인 커버리지 확대  
- **모델 크기 스케일업**: 파라미터 수 증가가 zero/few-shot 일반화에 긍정적 기여  
- **쿼리 레이어**: 위치 임베딩 쿼리 활용으로 다음 토큰 예측 집중, 상황별 적응력 강화  

## 5. 향후 연구 영향 및 고려 사항  
- **영향**: 중국어 대규모 PLM 연구 활성화, 오토 병렬화 기법 확산, few-shot 학습 연구 확대  
- **고려 사항**:  
  1. **추론 비용 절감**: 압축·지식 증류·동적 토큰 절삭 등으로 실시간 응용 최적화  
  2. **안정적 일반화**: NLI·논리 추론 과제에서 대규모 모델의 불안정성 완화 연구  
  3. **다중 모달 확장**: 언어·비전·음성 통합 학습으로 멀티모달 PLM 성능 스케일링  
  4. **해석 가능성**: 거대 모델의 내재적 작동 원리 및 바이어스 분석 통한 신뢰성 확보

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/8dee48a0-4686-44f1-a740-6a83fd0baa8a/2104.12369v1.pdf
