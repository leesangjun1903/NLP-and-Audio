# Larger Language Models Do In-Context Learning Differently

## 1. 핵심 주장 및 주요 기여  
**핵심 주장**  
- 대형 언어 모델(Large LMs)은 소형 모델과 달리 **프리트레이닝된 의미적 사전지식(semantic priors)**을 넘어, 프롬프트 내 제시된 **입력–레이블 매핑(input–label mappings)**을 학습할 수 있는 **새로운 능력**을 보유한다.[1]

**주요 기여**  
- **Flipped-Label ICL** 실험: 프롬프트 내 레이블을 완전히 뒤집었을 때, 대형 모델만이 사전지식을 무시하고 새로운 매핑을 따라 예측함을 입증.  
- **SUL-ICL (Semantically-Unrelated Label ICL)** 설정: ‘Foo/Bar’와 같은 무의미 레이블로 교체해도 대형 모델은 매핑을 학습해 높은 정확도를 유지함을 보임.  
- **Instruction Tuning** 비교: 지시문 튜닝(instruction tuning)이 의미적 사전지식 의존성과 매핑 학습 능력을 모두 강화하지만, 사전지식 의존성 강화 효과가 더 큼을 발견.  
- **고차원 선형 분류** 실험: 16차원 이상의 선형 분류에서도 대형 모델만이 랜덤을 크게 상회하는 성능을 보이며, 이 능력도 규모에 따른 **emergent ability**임을 시사.  

## 2. 문제 정의, 제안 방법, 구조, 성능 및 한계  

### 2.1 해결하고자 하는 문제  
언어 모델의 **In-Context Learning(ICL)** 성능이  
1) 사전학습된 의미적 지식 활용(semantic priors)  
2) 프롬프트 내 제시된 입력–레이블 관계 학습(input–label mappings)  
두 요소 중 어디에 얼마나 의존하는지, 그리고 **모델 스케일이 이를 어떻게 변화시키는지**를 규명하고자 함.[1]

### 2.2 제안 방법  
1. **Flipped-Label ICL**  
   - 이진 분류 데이터에 대해 프롬프트 내 레이블을 $$p$$% 비율로 뒤집어 제시.  
   - 평가 레이블은 원래대로 유지($$p=100$$%일 때, 완벽히 뒤집혀야 0% 정확도).  
   - 대형 모델만이 아래 수식과 같이 오류율이 높아지는 양상을 보임:  

$$ \text{Accuracy}(p) \to 0\% \quad (p\to100\%) $$

2. **SUL-ICL**  
   - 자연어 레이블(예: Positive/Negative) 대신 무의미 기호(예: Foo/Bar) 사용.  
   - 의미적 사전지식이 완전히 제거된 환경에서 입력–레이블 매핑만 학습하도록 강제.  
   - 성능 변화는 다음과 같음:  

$$ \Delta_{\text{small}} \gg \Delta_{\text{large}}\quad(\Delta=\text{regular ICL}-\text{SUL-ICL}) $$

3. **Instruction Tuning 비교**  
   - PaLM vs. Flan-PaLM 모델을 동일 실험에 적용.  
   - 지시문 튜닝된 모델은 SUL-ICL에서 더 높은 성능, flipped-label ICL에서 더 낮은 성능을 보임.  

4. **고차원 선형 분류**  
   - $$N$$차원 점들을 임의 생성 후, 초평면 기준 이진 분류 과제로 ICL 수행.  
   - 대형 모델만이 $$N\ge16$$에서도 랜덤 대비 유의미한 성능 차이(≈+-20%)를 달성.  

### 2.3 모델 구조  
- GPT-3, InstructGPT, Codex, PaLM(8B/62B/540B), Flan-PaLM(8B/62B/540B) 등  
- 동일한 트레이닝 데이터 및 프로토콜, 파라미터 수만 다른 PaLM 시리즈를 통해 **규모 효과** 집중 분석.[1]

### 2.4 성능 향상  
- **Flipped-Label ICL**: 대형 모델(text-davinci-002, code-davinci-002, PaLM-540B)만 0% 근접 성능 달성.  
- **SUL-ICL**: 소형 모델은 50–60% 정확도 하락, 대형 모델은 10% 미만 하락.  
- **Exemplar 수 민감도**: 대형 모델은 $$k$$ 기댓 수(2→16)마다 20–30% 성능 향상, 소형 모델은 5–10%에 그침.  
- **Instruction Tuning**: Flan-PaLM-8B은 PaLM-62B 대비 SUL-ICL에서 동등 또는 상회.  
- **선형 분류**: code-davinci-002은 64차원까지 랜덤(50%) 대비 +19% 성능.

### 2.5 한계  
- **맥락 길이 제약**: RTE 등 긴 프롬프트의 경우 PaLM-540B에서도 예제 수 제한.  
- **비교 대상 부재**: 실제 학습 기반(fine-tuning) vs. 프롬프트 기반 ICL 비교 미비.  
- **내부 메커니즘 불투명**: 왜 규모 확장 시 매핑 학습 능력이 emergent하는지 이론적 설명 부족.

## 3. 일반화 성능 향상 가능성  
- **무의미 레이블**로도 매핑 학습이 가능하다는 점은, 새로운 태스크 및 레이블 체계에 대한 **빠른 적응력(fast generalization)**을 시사.  
- **고차원 선형 분류** 결과는, 자연어 외 **수치·구조적 태스크**에도 ICL 활용 가능성을 확장.  
- **지시문 튜닝**이 소·중·대형 모델 전반의 매핑 학습 능력을 강화하므로, 다양한 도메인에서 일관된 **제로샷–페어샷 일반화** 달성이 가능.

## 4. 향후 연구 영향 및 고려사항  
- **이론적 뒷받침**: emergent 현상의 원인을 설명하는 수학적·이론적 모델 개발 필요.  
- **효율성 최적화**: 매핑 학습 능력을 소형 모델에서도 발현시키는 **프롬프트 설계** 혹은 **모델 압축** 연구.  
- **다양한 태스크 확장**: 시계열, 그래프, 이미지 분류 등 비언어적 태스크로 범위 확대.  
- **안정성 및 윤리**: 무의미 레이블 사용 시 **오분류 위험** 및 **안전성 보증 메커니즘** 마련 필요.  
- **데이터 편향**: 입력–레이블 매핑 학습이 데이터 편향을 증폭하지 않는지 지속 검증.

***

 첨부 파일 “2303.03846v2.pdf”[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d1579e2d-78ac-4639-ac25-4b77c1c65f0d/2303.03846v2.pdf)
