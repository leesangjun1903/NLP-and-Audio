# KTO: Model Alignment as Prospect Theoretic Optimization

## 1. 핵심 주장과 주요 기여  
“Model Alignment as Prospect Theoretic Optimization(KTO)” 논문은 기존 LLM 정렬(alignment) 기법이 인간의 선호를 확률적 보상 모형으로만 이해해 왔음을 지적하고, 카너먼-트버스키의 전망 이론(Prospect Theory)을 적용한 새로운 손실 함수 군(인간-인지 손실, Human-Aware Losses; HALOs)을 제안한다. 그중 특히 이진(바이너리) 인간 피드백만으로도 선호 기반 방법(RLHF, DPO) 이상의 성능을 내는 KTO를 도입하여 다음을 보여준다.

- **HALO 프레임워크 제시**: 기존 DPO·PPO-Clip 등이 손실 함수 형태로 전망 이론의 *가치 함수*(value function) 속성을 일부 구현해 왔음을 규명.  
- **KTO 제안**: Kahneman–Tversky 가치 함수를 변형하여 이진 긍정/부정 피드백만으로 직접 인간 효용(utility)을 최적화하는 손실 함수를 설계.  
- **실험적 검증**: 1B~30B 규모 모델(Pythia, Llama, Mistral)에서 DPO 대비 동등하거나 더 우수한 성능(자동 판정·GPT-4 평가 및 휴먼 평가) 확인.  
- **데이터 효율성**: 선호 쌍당 2개의 데이터 대신 1개의 이진 예시만으로도 DPO와 동등 성능, 극심한 불균형(양성 샘플 10%)에서도 견고함.  
- **이론적 분석**: 전망 이론 손실이 noisy·intransitive 피드백에 대한 최악 보장(worst-case guarantee)을 제공함을 증명.  

## 2. 문제 정의·제안 방법·모델 구조·성능 및 한계  

### 2.1 해결하고자 하는 문제  
- 기존 RLHF·DPO 등은 고비용의 쌍별(preference) 피드백을 기반으로 함.  
- 선호 데이터를 얻기 어려울 뿐 아니라, 최적화된 보상 함수가 인간 효용과 일치하지 않을 수 있음.  
- 인간은 확률적 보상보다 전망 이론적 가치 함수에 따라 의사결정을 하므로, 이를 모델 학습에 반영할 필요가 있음.  

### 2.2 제안하는 방법  
1. **HALO(인간-인지 손실) 정의**  
   - 모델 보상 $$r_θ(x,y)=l(y)\log\frac{π_θ(y|x)}{π_{\mathrm{ref}}(y|x)}$$ 에 대한 전망 이론 가치 함수 $$v(z)$$ 적용  
   - 일반 형태:  

$$
       \mathcal{L}\_{\mathrm{HALO}}
       = \mathbb{E}\_{x,y\sim D}\bigl[a_{x,y}\,v\bigl(r_θ(x,y)-\mathbb{E}_{y'}[r_θ(x,y')]\bigr)\bigr]+C_D
     $$

2. **KTO 손실 함수**  
   - 이진 긍정(바람직)·부정(비바람직) 예시에 대해 각각 다른 손실 함수를 적용  
   - 핵심 식:  

$$
       \mathcal{L}\_{\mathrm{KTO}}
       =\mathbb{E}\_{x,y\sim D}\bigl[\lambda_y\bigl(1 - \sigma\bigl(\beta(r_θ(x,y)-z_0)\bigr)\bigr)\bigr]
     $$
     
여기서  
     $$\sigma$$ = 시그모이드,  
     $$\beta$$ = 위험(aversion) 조절 파라미터,  
     $$\lambda_y$$ = 이득·손실 민감도,  
     $$z_0$$ = 레퍼런스 포인트(미니배치 내 KL 편향 추정)  
   - SFT 생략 가능하며, π_ref는 사전학습 또는 SFT 모델 모두 활용  

### 2.3 모델 구조  
- 기존 LLM(예: Llama, Pythia, Mistral)에 KTO 손실을 추가로 적용하는 미세조정(fin­tuning) 구조  
- π_ref로부터 클리핑 메뉴얼이 불필요하며, 부호화된 control token도 사용하지 않음  
- 익숙한 토크나이저·아키텍처 변경 없이 손실 함수 설계만으로 정렬 달성  

### 2.4 성능 향상  
- **자동 평가**: GPT-4 judged winrate에서 DPO 대비 동등·초과 성능 확인 (7B 이상에서 유의미 우위)  
- **휴먼 평가**: OpenAssistant 대화에서 human 선호도 평가 시 KTO > DPO (72.9% vs. 62.1%)  
- **응용 벤치마크**: GSM8K, BBH, MMLU, HumanEval 등에서 일관된 개선  
- **데이터 효율성**: 불균형 상황(긍정:부정=1:10)에서도 성능 유지  
- **SFT 생략**: 대형 모델(Llama-13B, 30B)에서 사전 SFT 없이 KTO만으로도 우수한 안정성  

### 2.5 한계  
- **가치 함수 단일화**: 현시점 KTO는 Kahneman–Tversky 금전 가치 함수 모형에 기반해, 언어적 효용에 완전 일치하지 않을 수 있음  
- **하이퍼파라미터 민감도**: β·λ 설정에 따라 성능 변화 폭이 크며, 타 도메인 재조정 필요  
- **이론적 보장 범위**: noisy/intransitive 피드백에는 강하지만, 복잡한 다목적 멀티 어트리뷰트 튜닝에는 추가 연구 필요  

## 3. 모델 일반화 성능 향상 관련 고찰  
- **전망 이론적 가치 함수**는 감각적 이득·손실 대비 민감도를 정확히 반영하므로, 모델이 지나친 과최적화(overfitting)와 과도한 리워드 취급을 방지  
- **KL 기반 레퍼런스**는 SFT 분포와의 급격한 이탈을 억제해, 새로운 도메인(제로샷/소샷)에서도 안정적 일반화  
- **이진 피드백 활용**: 보다 다양한 실제 사용자 스코어·등급 데이터를 활용할 수 있어, 여러 태스크에 걸친 범용적 튜닝 가능성  
- **이론 분석**에서 DPO 대비 worst-case 선호 일관성(majority-preferred output 보장)이 증명되어, 불확실·변동적 사용자 환경에서도 신뢰성 확보  

## 4. 향후 연구 영향 및 고려사항  
- **도메인별 가치 함수 탐색**: 언어 생성·요약·번역 등 태스크 특성에 맞는 전망 이론적 가치 함수 설계  
- **멀티어트리뷰트 HALO**: 단일 binary → 다중 스코어·등급을 반영하는 확장 가치 함수  
- **모달리티 확장**: 비전·음성 모델에도 HALO 개념 적용 가능성 탐색  
- **온라인 정렬**: 실시간 사용자 피드백을 활용한 KTO 온라인 버전 개발  
- **공정성·다양성 고려**: 다인종·다문화 사용자 그룹 간 피드백 불일치를 조화롭게 해결하는 HALO 전략  

KTO는 인간 결정 이론을 LLM 정렬에 직접 도입함으로써, 이론적 기반을 강화하고 실무적 데이터 효율성을 크게 높인 기여를 제시한다. 후속 연구에서는 다양한 인간 효용 모델과 멀티태스크·온라인 환경으로의 확장에 주목해야 한다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/bdf4f227-dd39-4fcc-997c-fba387c09e98/2402.01306v4.pdf
