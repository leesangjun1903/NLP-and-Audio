# News Summarization and Evaluation in the Era of GPT-3

## 핵심 주장과 주요 기여

이 논문은 **GPT-3의 등장이 텍스트 요약 연구에 패러다임 전환을 가져왔다**는 핵심 주장을 제시합니다. 주요 기여는 다음과 같습니다:[1]

**1. 인간 평가에서의 GPT-3 우월성 입증**
- 인간 평가자들은 fine-tuned 모델(BRIO, T0)보다 GPT-3 요약을 압도적으로 선호 (CNN: 58%, BBC: 57%)
- 단순한 작업 설명만으로 프롬프팅된 GPT-3가 대규모 데이터셋으로 훈련된 모델들을 능가

**2. 기존 평가 메트릭의 한계 발견**
- ROUGE, BERTScore 등 기존 자동 평가 메트릭이 GPT-3 요약을 신뢰성 있게 평가하지 못함
- GPT-3가 자동 메트릭에서는 7 ROUGE-L 점수 낮음에도 불구하고 인간 평가에서는 우수

**3. 키워드 기반 요약에서의 유연성**
- GPT-3가 키워드 기반 요약에서 70% 승률로 CTRLSum을 능가
- 프롬프트 기반 접근법의 범용성과 적응성 입증

## 해결하고자 하는 문제와 제안 방법

### **문제 정의**
기존 요약 연구는 대규모 데이터셋에서 fine-tuning된 모델(BART, PEGASUS, BRIO)에 의존했으나, 이들이 실제 사용자 선호도와 일치하는지, 그리고 새로운 프롬프트 기반 모델과 어떻게 비교되는지 체계적으로 연구되지 않았습니다.[1]

### **연구 방법론**

**1. 모델 비교 설계**
```
모델 비교 대상:
- GPT-3 (text-davinci-002): 프롬프트 기반 모델
- BRIO: CNN/DM, XSum에서 fine-tuned된 SOTA 모델  
- T0: 멀티태스크 instruction-tuned 모델
```

**2. 프롬프트 설계**
```
표준 프롬프트 형식:
Article: {{article}}
Summarize the above article in N sentences.

여기서 N = 1 (XSum/BBC), N = 2 (Newsroom), N = 3 (CNN/DM)
```

**3. 평가 방법론**
- **A/B 테스트**: 100개 최신 뉴스 기사 (CNN-2022, BBC-2022)
- **인간 평가**: 총 60명 참여자, 각 기사당 3명씩 주석
- **자동 메트릭**: ROUGE, BERTScore, QAEval 등 9개 메트릭

## 모델 구조 및 성능 향상

### **GPT-3 모델 특성**
이 연구는 새로운 모델 구조를 제안하지 않고, 기존 GPT-3 (175B 파라미터)의 instruction-tuned 버전을 활용합니다. 핵심은 **프롬프트 엔지니어링**을 통한 태스크 적응입니다.[1]

### **성능 비교 결과**

**인간 평가 (가장 선호되는 요약)**
- CNN: GPT-3 (58%) > BRIO (36%) > T0 (8%)
- BBC: GPT-3 (57%) > T0 (30%) > BRIO (20%)

**자동 메트릭 (ROUGE-L 점수)**
- CNN: BRIO (31.44) > GPT-3 (24.71)
- XSum: BRIO (41.04) > GPT-3 (20.60)

이는 **자동 메트릭과 인간 평가 간의 심각한 불일치**를 보여줍니다.[1]

## 일반화 성능 향상 가능성

### **프롬프트 기반 접근법의 장점**

**1. 도메인 적응성**
- 키워드 기반 요약에서 70% 승률 달성
- aspect-based 요약에서는 혼재된 결과 (일부 실패 사례 존재)

**2. 스타일 제어 가능성**
```python
# 길이 제어 예시
"Summarize the above article in 1 sentence."  # XSum 스타일
"Summarize the above article in 3 sentences." # CNN 스타일
```
98%의 경우에서 지정된 문장 수 제약을 정확히 따름.[1]

**3. 훈련 데이터 독립성**
- 특정 데이터셋의 편향에서 벗어남
- CNN/DM의 추출적(extractive) 편향, XSum의 첫 문장 제거 문제 등을 회피

### **일반화 성능의 한계**

**1. 장문서 요약 문제**
segment-then-summarize 접근법에서 반복적인 서론과 일관성 부족 문제 발생.[1]

**2. Aspect-based 요약에서의 실패**
"누가 피고인인가?" 질문에 대해 여러 무관한 인물을 나열하는 등의 오류.[1]

## 연구의 한계점

### **방법론적 한계**

**1. 평가 설계의 제약**
- 전반적 품질만 평가하고 세부 차원별 분석 부족
- Likert scale 대신 상대적 선호도만 측정

**2. 도메인 제한성**
- 영어 뉴스 도메인에만 국한
- 과학 논문 등 다른 도메인에서는 결과가 다를 수 있음

**3. 모델 훈련 정보 부족**
- GPT-3의 정확한 훈련 데이터와 과정을 알 수 없음
- RLHF 과정에서 요약 예제가 포함되었을 가능성

### **기술적 한계**

**1. 컨텍스트 길이 제한**
장문서 처리 시 segmentation 필요로 인한 품질 저하.[1]

**2. 일관성 문제**
복잡한 aspect-based 질문에서 사실적 부정확성 발생.[1]

## 향후 연구에 미치는 영향

### **평가 패러다임의 전환**

**1. 메트릭 개발의 필요성**
기존 ROUGE 기반 평가에서 벗어나 새로운 평가 프레임워크 개발 필요. 특히:[1]
- 프롬프트 기반 모델에 적합한 reference-free 메트릭
- 사용자 선호도를 반영하는 평가 방식

**2. 인간 평가의 중요성**
자동 메트릭의 신뢰성 저하로 인해 인간 평가의 표준화와 비용 효율성 향상 필요.[1]

### **연구 방향의 변화**

**1. 데이터셋 중심에서 태스크 중심으로**
- CNN/DM, XSum 등 기존 벤치마크의 한계 인식
- 실제 사용 사례를 반영한 요약 태스크 개발

**2. 제어 가능한 요약 연구**
키워드, aspect, 스타일 제어가 가능한 요약 시스템 개발.[1]

### **향후 연구 고려사항**

**1. 기술적 개선 방향**
- 장문서 요약을 위한 새로운 프롬프팅 기법
- aspect-based 요약에서의 정확성 향상
- 다국어 및 도메인 특화 요약 연구

**2. 평가 방법론 개선**
- 비용 효율적인 인간 평가 프로토콜 개발
- 프롬프트 기반 모델 전용 자동 메트릭 설계
- 실제 사용자 시나리오를 반영한 평가 설계

**3. 윤리적 고려사항**
- 편향성과 공정성 평가
- 사실성과 신뢰성 보장 메커니즘
- 저자권과 지적재산권 문제

이 연구는 **요약 연구의 패러다임 전환점**을 제시하며, 향후 연구는 프롬프트 기반 접근법의 장점을 활용하면서도 그 한계를 극복하는 방향으로 발전해야 할 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7bb0d424-b0b3-4360-810f-e10587a11d93/2209.12356v2.pdf)
