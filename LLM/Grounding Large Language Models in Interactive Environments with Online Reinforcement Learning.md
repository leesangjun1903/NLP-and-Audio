# Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning

### 1. 핵심 주장 및 주요 기여

**GLAM(Grounding LAnguage Models)** 논문의 핵심 주장은 대규모 언어모델(LLM)이 환경과의 상호작용을 통한 **온라인 강화학습(Online RL)**으로 **기능적 그라운딩(Functional Grounding)**을 달성할 수 있다는 것입니다.[1]

주요 기여는 다음과 같습니다:[1]

- **GLAM 방법론**: LLM을 에이전트 정책으로 직접 사용하면서 PPO(Proximal Policy Optimization)를 통해 환경 보상으로 점진적으로 파인튜닝하는 방법 제시
- **BabyAI-Text 환경**: 텍스트 기반의 대화형 환경 구축으로 공간 및 네비게이션 작업 체계적 연구 가능
- **Lamorel 라이브러리**: RL 실무자들을 위한 LLM 분산 추론 및 학습 프레임워크 개발

### 2. 문제 정의 및 핵심 이슈

논문이 해결하고자 하는 근본적인 문제는 **LLM의 그라운딩 부족(Lack of Grounding)**입니다. LLM은 대규모 텍스트 데이터로 훈련되지만 다음의 세 가지 한계를 갖습니다:[1]

1. 다음 단어 예측에만 최적화된 훈련 프로세스
2. 인과관계 구조 파악을 위한 환경 개입 능력 부족
3. 환경과의 상호작용 데이터로부터의 학습 능력 부재

이를 극복하기 위해 논문은 **기능적 그라운딩**이라는 개념을 제안합니다. 이는 LLM의 내부 기호 시스템이 외부 환경의 동역학(dynamics)과 정렬되어, 에이전트가 이를 활용하여 환경에서 작업을 해결할 수 있도록 하는 과정입니다.[1]

### 3. 제안 방법론 및 수식

#### 3.1 문제 설정

환경은 목표 조건화된 부분 관측 마르코프 결정 과정(POMDP)으로 모델링됩니다:[1]

$$M = \langle S, V, A, T, R, G, O, \gamma \rangle$$

여기서:
- $S$: 상태 공간
- $V$: 언어 어휘
- $A \subseteq V^N$: 동작 공간 (토큰 수열)
- $T: S \times A \to S$: 전이 함수
- $R: S \times A \times G \to \mathbb{R}$: 목표 조건화된 보상 함수
- $G \subseteq V^N$: 목표 공간
- $O: S \to V^N$: 텍스트 기술로 상태를 매핑하는 관측 함수
- $\gamma$: 할인 인자

#### 3.2 LLM 정책 계산

LLM의 동작 확률은 언어모델링 헤드를 통해 계산됩니다:[1]

$$L_{LLM}(a_i | p) = \sum_{j=0}^{|a_i|} \log P_{LLM}(w_j | p, w_{j-1})$$

여기서 $P_{LLM}(w_j | p, w_{j-1})$는 프롬프트 $p$와 이전 토큰 $w_{j-1}$이 주어졌을 때 토큰 $w_j$의 조건부 확률입니다.

로그 확률을 정규화하여 동작 분포를 얻습니다:

$$P(a_i | p) = \frac{e^{L_{LLM}(a_i|p)}}{\sum_{a_j \in A} e^{L_{LLM}(a_j|p)}}$$

#### 3.3 PPO 기반 파인튜닝

정책 $\pi_\theta(a | o, g)$는 다음의 목표값을 최대화하도록 학습됩니다:[1]

```math
V^*(s, g) = \mathbb{E}_{a \sim \pi(s,g)} \left[R(s, a, g) + \gamma V^* (T(s, a), g) \right]
```

PPO 알고리즘을 사용하여 정책과 가치함수 $V(o | g) \approx V^*(o, g)$를 동시에 학습합니다. 가치함수는 LLM의 디코더 블록 첫 번째 레이어 위에 MLP 헤드로 추가됩니다.

표본 효율성 측도는 다음과 같이 정의됩니다:[1]

$$SE = \frac{1}{T} \sum_{t=0}^{T} SR_t$$

여기서 $T$는 총 스텝 수, $SR_t$는 스텝 $t$에서의 성공률입니다.

### 4. 모델 구조 및 아키텍처

#### 4.1 GLAM 구조

GLAM의 파이프라인은 다음과 같습니다:[1]

1. **입력 프롬프트 구성**: 목표 설명 + 관측 설명 + 가능한 동작 목록
2. **동작 확률 계산**: 각 동작에 대해 LLM을 순차 통과시켜 확률 계산
3. **동작 샘플링**: Softmax 분포에서 동작 샘플링
4. **환경 상호작용**: 환경으로부터 보상 수집
5. **PPO 파인튜닝**: 수집된 경험으로 LLM과 가치함수 업데이트

#### 4.2 주요 설계 선택

**언어모델링 헤드 사용**: 새로운 액션 헤드를 추가하는 대신 기존 언어모델링 헤드를 재활용합니다. 이는 다음의 이점을 제공합니다:[1]

- 사전 훈련된 작업 활용
- 임의의 동작 공간에 대한 견고성
- 환경 특화 매핑의 필요성 제거

**분산 추론**: 계산 병목을 해결하기 위해 여러 LLM 워커에서 동작을 병렬 처리합니다.[1]

### 5. 성능 향상 및 실험 결과

#### 5.1 표본 효율성 (Q1)

**GFlan-T5는 다음의 우수한 성능을 보였습니다:**[1]

- 25만 스텝 후 0.8 성공률 달성
- 60만 스텝 후 0.9 성공률 달성
- 비교: DRRN과 NPAE-Flan-T5는 150만 스텝 후에도 0.2 이하

**Symbolic-PPO(기호적 관측 사용)와의 비교:**[1]

- GFlan-T5: 150만 스텝 후 약 0.75 성공률
- Symbolic-PPO: 약 0.4 성공률

GFlan-T5가 훨씬 더 복잡한 텍스트 관측을 처리하면서도 더 높은 효율성 달성

#### 5.2 동작 공간 크기의 영향

동작 공간을 3개(제한), 6개(정규), 9개(확장)로 변화시킨 결과:[1]

- **GFlan-T5**: 모든 경우에 성능 불변 (유용하지 않은 동작 빠르게 제거)
- **다른 에이전트**: 동작 공간 증가에 따라 성능 저하

#### 5.3 방해요소(Distractors)의 영향

방해요소 개수 변화(4개 → 8개 → 16개):[1]

- **GFlan-T5**: 성공률 14% 감소
- **Symbolic-PPO**: 성공률 38% 감소

#### 5.4 일반화 성능 (Q2: 새로운 객체에 대한 일반화)

**어휘 외 명사(Out-of-Vocabulary Nouns):**[1]
- GFlan-T5 성공률: 0.87 ± 0.05 (변화 없음)

**발명된 객체(Invented Objects):**[1]
- GFlan-T5 성공률: 0.88 ± 0.06 (13% 감소, 여전히 강력)

이는 GFlan-T5가 객체 정체성이 아닌 **환경의 기하학적 구조**를 학습했음을 시사합니다.

#### 5.5 새로운 작업에 대한 일반화 (Q3)

**학습된 작업의 새로운 구성:**[1]
- 훈련 중 학습한 작업들의 새로운 조합 테스트
- GFlan-T5: 0.12 성공률 (Flan-T5: 0.07, 무작위: 0.05)
- 낮은 성공률은 구성된 작업을 훈련 중에 완전히 숙달하지 못했기 때문

**동의어 동작 치환:**[1]
- 훈련된 동작을 동의어로 변경 (예: "go forward" → "move ahead")
- GFlan-T5: 0.12 (Flan-T5: 0.01)
- 하지만 원본 설정 대비 87% 성능 저하 (동작 어휘 과적합)

**새로운 언어(프랑스어):**[1]
- GFlan-T5: 0.02 성공률 (무작위: 0.30)
- **중요한 발견**: 너무 많은 그라운딩된 기호가 동시에 변경되면 기능적 그라운딩이 전이되지 않음

#### 5.6 온라인 RL vs 행동 복제 (Q3)

**비교 설정:**[1]
- GFlan-T5 (RL 훈련): 40만 전이에 대해 PPO로 훈련
- BC-GFlan-T5: GFlan-T5가 수집한 40만 전이로 행동 복제 훈련
- BC-Bot: 최적 정책이 수집한 40만 전이로 행동 복제 훈련

**결과:**[1]
- Go To 작업: GFlan-T5 (0.82) > BC-GFlan-T5 (0.69) > BC-Bot (0.73)
- 발명된 명사/형용사: GFlan-T5 (0.82 → 0.74) > BC-Bot (0.73 → 0.63)

온라인 RL의 시행착오 과정이 더 견고한 그라운딩을 생성합니다.

#### 5.7 LLM 크기의 영향

세 가지 모델 규모 비교:[1]

| 모델 | 매개변수 | 성능 |
|------|--------|------|
| GFlan-T5 소형 | 8천만 | 약 0.3 (낮음) |
| GFlan-T5 | 7억 8천만 | 약 0.8-0.9 |
| GFlan-T5 XL | 30억 | 더 높은 성능 |

**핵심 통찰**: 사전 훈련 이점은 **모델이 충분히 클 때만** 나타나는 창발적 능력(emergent ability)입니다.

### 6. 한계(Limitations)

#### 6.1 환경 제약

- **제한된 환경**: BabyAI-Text만 사용하여 단일 환경에서의 결과
- **작은 모델 규모**: 계산 효율성으로 인해 7억 8천만 파라미터 모델로 제한
- **동작 공간 크기**: 복잡한 동작 공간으로는 확장 어려움

#### 6.2 일반화의 한계

- **새로운 언어로 완전한 실패**: 다국어 환경으로 일반화 실패
- **구성(Composition) 능력 제한**: 새로운 작업 조합에 대한 성능 (0.12)이 매우 낮음
- **임시 개념(Temporal Concepts) 어려움**: "then" vs "after"의 그라운딩 불완전

#### 6.3 계산 효율성

- **추론 오버헤드**: 각 동작마다 LLM의 전방 통과 필요
- **GPU 사용량**: Flan-T5 780M 실험에 18,880 A100-GPU 시간 소요
- **확장성**: 더 큰 모델이나 더 큰 동작 공간으로의 확장 어려움

#### 6.4 기타 한계

- **프롬프트 엔지니어링 부족**: 단순한 고정 프롬프트 사용으로 성능 향상 여지 있음
- **희소 보상 환경**: 희소 보상에 대한 처리 능력 제한적
- **다중 모달리티 부재**: 텍스트만 사용하며 시각 정보 미포함

### 7. 일반화 성능 향상 가능성

#### 7.1 강점

**객체 일반화:**[1]
- 어휘 외 명사에 대한 완벽한 일반화 (0.87 vs 0.02 기준선)
- 환경 기하학의 견고한 그라운딩

**작업 일반화의 부분적 성공:**[1]
- 동의어 동작 인식 가능 (0.12 vs 0.01)
- 관련 언어로의 일부 전이 가능

**견고성:**[1]
- 동작 공간 크기에 불민감
- 방해요소 증가에 대한 저항성 (14% vs 38% 감소)

#### 7.2 약점 및 개선 방향

**언어 경계 넘기의 실패:**[1]
- 동시에 많은 기호 변경 시 완전한 실패
- 제안: 점진적 미세 조정 또는 다중 언어 사전 훈련 필요

**구성 능력 제한:**[1]
- 새로운 작업 조합에 대한 0.12 성공률은 진정한 구성 능력 부족 시사
- 개선안: 계층적 표현 학습 또는 추상화 수준 높이기

**시간 개념 그라운딩:**[1]
- "then" (0.22)과 "after" (0.17)의 불완전한 그라운딩
- 문제: 동작 순서가 명령어 순서와 불일치할 때 성능 저하

### 8. 연구에 미치는 영향 및 최신 동향 (2024-2025)

#### 8.1 논문의 기여도

**GLAM은 다음 분야에 중요한 기여를 했습니다:**[2]

1. **훈련 기반 그라운딩의 선두주자**: 오프라인 행동 복제 대신 온라인 RL 기반 그라운딩의 우월성 입증
2. **실용적 도구 제공**: Lamorel 라이브러리로 RL 커뮤니티에 기여
3. **체계적 연구 프레임워크**: 샘플 효율성, 일반화, 온라인 학습의 영향을 구분하여 분석

#### 8.2 후속 연구 동향

**계층적 강화학습 통합:**[3]
- **GLIDER** (2025): 오프라인 계층적 RL을 사용하여 LLM 정책을 장기 작업에 더 효과적으로 그라운드
- 저수준 기술 학습과 고수준 계획 분리

**환경 적응 및 생성:**[4][5]
- **EnvGen** (2024): LLM이 훈련 환경을 동적으로 생성하고 적응시켜 작은 RL 에이전트 훈련
- GPT-4보다 우수한 성능으로 계산 비용 감소

**효율성 개선:**[6][7]
- **ReAd (Reinforced Advantage Feedback)** (2025): 다중 에이전트 협력에서 LLM 쿼리를 대폭 감소
- 장점함수(Advantage Function) 기반 피드백으로 효율성 증대

**검증 가능한 보상 기반 RL:**[8]
- 2024년 이후 새로운 패러다임: 객관적으로 검증 가능한 보상 신호 사용
- 코드 생성, 수학 추론 등에서 LLM 향상

**안전성과 정렬:**[6][2]
- LLM 그라운딩 시 안전성 고려 증가
- 다중 피드백 신호와 제약 조건 통합

#### 8.3 현재 연구의 공개 과제

**1. 계산 효율성:**[3][1]
- GLAM의 계산 비용 (18,880 GPU 시간) 문제
- 향후 연구: 증류(Distillation), 양자화(Quantization), 어댑터 기반 미세 조정

**2. 보다 복잡한 환경으로의 확장:**[3][1]
- 현재: 텍스트 기반 네비게이션 작업
- 미래: 로봇공학, 복잡한 게임 환경, 멀티 에이전트 시나리오

**3. 진정한 구성성(Compositionality):**[1]
- 새로운 작업 조합에 대한 0.12 성공률 개선 필요
- 해결책: 추상화 수준 향상, 메타 학습, 점진적 커리큘럼

**4. 다중 모달리티:**[1]
- 텍스트만 사용 → 시각, 청각, 기타 모달리티 통합 필요
- FLAMINGO 등 다중 모달 기초 모델 활용 가능

**5. 도메인 일반화:**[1]
- BabyAI-Text 단일 환경에서의 벗어남
- 다양한 환경 간 전이 학습 연구

### 9. 향후 연구 고려 사항

#### 9.1 단기 개선 (1-2년)

1. **현재 환경의 깊이 있는 탐색:**
   - 더 복잡한 작업과 추론 요구사항 추가
   - 더 큰 LLM 모델 및 더 큰 동작 공간 탐색

2. **효율성 최적화:**
   - 동작 헤드 추가로 비교 (본 논문에서 실패한 이유 분석)
   - 희소 어댑터 또는 로라(LoRA) 기반 접근법

3. **일반화 메커니즘 이해:**
   - 왜 일부 기호는 전이되고 다른 기호는 실패하는가?
   - 그라운딩 과정의 해석 가능성 연구

#### 9.2 중기 개전 (2-3년)

1. **계층적 및 모듈형 그라운딩:**
   - 다양한 환경에서 재사용 가능한 모듈식 기능 학습
   - 전이 학습 개선

2. **다중 모달 그라운딩:**
   - 텍스트, 이미지, 비디오 통합
   - 크로스 모달 개념 정렬

3. **온라인 적응 메커니즘:**
   - 환경 변화에 대한 빠른 적응
   - 메타 학습 및 연속 학습 통합

#### 9.3 장기 방향 (3년 이상)

1. **현실 세계 배포:**
   - 로봇공학 및 구체화된 AI 응용
   - 안전성, 견고성, 설명 가능성 강화

2. **이론적 이해:**
   - LLM 그라운딩의 표본 복잡도 한계 규명
   - 그라운딩 가능성과 한계에 대한 형식적 분석

3. **윤리적 고려사항:**
   - 본 논문의 결론에서도 언급된 대로 실제 배포 준비 전 윤리적 고려 필요
   - 안전 정렬(Safe Alignment) 연구와의 통합

### 10. 결론

GLAM 논문은 **LLM의 기능적 그라운딩**에 대한 체계적인 첫 연구로, 온라인 강화학습이 행동 복제보다 훨씬 효과적임을 입증했습니다. 특히 다음이 중요합니다:[1]

**핵심 성과:**
- 사전 훈련 LLM의 표본 효율성 우월성 입증 (250K 스텝 vs 1.5M 스텝)
- 구조적 개념에 대한 강력한 일반화 (객체 일반화 87% 성공)
- 온라인 상호작용의 필수성 증명

**주요 한계:**
- 계산 비용 및 확장성
- 다언어 및 구성 능력 부족
- 단일 환경 테스트

**향후 영향:**
최근 후속 연구(EnvGen, GLIDER, ReAd 등)에서 GLAM의 핵심 아이디어를 발전시키고 있으며, 2024-2025년 LLM-RL 통합 연구에서 중요한 기준점이 되고 있습니다. 계층적 학습, 동적 환경 생성, 다중 에이전트 협력 등으로 확장되는 중입니다.[4][6][3]

***

## 참고 문헌 출처

논문과 최신 연구 기반 분석으로, 이 보고서는 GLAM이 LLM 기반 의사결정 분야에서 어떻게 발전하고 있는지, 그리고 향후 어떤 방향으로 진행될 것인지를 종합적으로 보여줍니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/da0e3f01-f1cc-4fea-8459-38aa9a8e72d6/2302.02662v4.pdf)
[2](https://www.ijcai.org/proceedings/2025/1198.pdf)
[3](https://openreview.net/forum?id=pdNtji3ktF&noteId=TnkdScrvgI)
[4](https://openreview.net/forum?id=F9tqgOPXH5)
[5](https://arxiv.org/abs/2403.12014)
[6](https://aclanthology.org/2025.findings-acl.84/)
[7](https://arxiv.org/html/2405.14314v4)
[8](https://arxiv.org/html/2509.16679v1)
[9](http://arxiv.org/pdf/2411.14457.pdf)
[10](http://arxiv.org/pdf/2405.14751.pdf)
[11](https://arxiv.org/html/2410.08632)
[12](https://arxiv.org/pdf/2310.12773.pdf)
[13](https://arxiv.org/pdf/2402.19299.pdf)
[14](https://arxiv.org/pdf/2303.11366.pdf)
[15](https://arxiv.org/pdf/2305.18341.pdf)
[16](https://arxiv.org/pdf/2401.02991.pdf)
[17](https://neptune.ai/blog/llm-grounding)
[18](https://dl.acm.org/doi/10.5555/3618408.3618558)
[19](https://arxiv.org/html/2509.04731v3)
