# The Power of Scale for Parameter-Efficient Prompt Tuning

## 1. 핵심 주장 및 주요 기여  
이 논문은 **대규모 언어 모델(frozen)을 소규모 파라미터 ‘소프트 프롬프트’로만 조정(prompt tuning)**함으로써, 전체 모델 파인튜닝과 유사한 성능을 달성할 수 있음을 보였다.[1]
- 모델 크기가 커질수록 소프트 프롬프트 방식의 성능 격차가 줄어들며, 최종적으로 T5-XXL(11B)에서는 전체 파인튜닝 수준의 SuperGLUE 성능(≈89.3)을 매치한다.  
- 프롬프트 튜닝은 각 태스크마다 0.01% 미만의 파라미터만 학습하므로, 모델 저장·서빙 비용이 대폭 절감된다.  

## 2. 문제 정의  
- 전통적 **모델 튜닝**은 모든 파라미터를 업데이트하며 태스크마다 모델 복사본을 생성해야 하므로, 모델 서버 메모리 부담이 크다.  
- 반면 **프롬프트 디자인(few-shot)**은 튜닝 없이 고정된 텍스트 프롬프트만으로 동결된 모델을 제어하지만, 입력 길이 제약과 휴먼 엔지니어링 오류로 성능이 크게 낮다.  
- 따라서 대규모 언어 모델을 **동결(frozen) 유지하되**, 극히 작은 태스크 전용 학습가능 파라미터로 **모델 튜닝 급의 성능**을 내는 방법이 필요하다.

## 3. 제안 방법  
### 3.1 소프트 프롬프트 구성  
- 입력 임베딩 앞에 길이 *P*의 **학습가능 임베딩 행렬** $$P_e ∈ ℝ^{P×E}$$을 붙여 언어 모델에 입력  
- 언어 모델 파라미터 θ는 고정하고 오직 θ_P(프롬프트 임베딩)만 업데이트하여  

$$
\underset{θ_P}{\mathrm{maximize}}\;\sum_{(X,Y)} \log P_{θ,θ_P}(Y \mid [P;X])
$$  

- 프롬프트 초기화 방법: 무작위, 어휘 샘플링, 또는 **클래스 레이블 임베딩** 활용(성능 우수)

### 3.2 아키텍처 및 학습  
- 베이스 모델: T5 1.1 Small∼XXL  
- 사전학습 수정(“LM adaptation”): span corruption→일반 언어 모델링으로 100K 스텝 추가 학습  
- 프롬프트 길이: 1∼150 토큰, 주로 20~100 토큰 사용  
- 옵티마이저: Adafactor, 학습률 0.3, 배치 32, 얼리 스톱

## 4. 성능 평가  
| 모델 방식          | T5-XXL SuperGLUE | 태스크별 파라미터 비율 |
|-------------------|------------------|------------------------|
| 모델 튜닝(전체)   | 89.3             | 100%                   |
| 프롬프트 튜닝     | 89.3             | ≤0.01%                 |
| GPT-3 few-shot    | 71.8             | –                      |

- SuperGLUE 벤치마크: 모델 크기 증가에 따라 성능 격차가 소멸, XXL에서는 동등.[1]
- 도메인 전이(Zero-shot) 실험: SQuAD→TextbookQA 등, 프롬프트 튜닝이 평균 +1.2∼+12.5F1 향상.[1]
- 앙상블: T5-XXL 기반 프롬프트 5개 앙상블로 SuperGLUE 91.3 획득.[1]

## 5. 한계 및 고려사항  
- **사전학습 방식 의존성**: span corruption만 사용 시 중간 크기 모델에서 불안정 학습 발생 → LM adaptation 필요  
- **프롬프트 해석성**: 연속 임베딩이어서 자연어 해석 어려우며, 프롬프트 토큰별 의미 분석도 제한적  
- **조건부 입력 길이**: 입력 길이 증가에 따라 프롬프트 길이 설계가 민감하나, 대형 모델에서는 단일 토큰도 가능  

## 6. 앞으로의 연구 영향 및 고려점  
- **파라미터 분리 설계**: 일반 언어 이해 파라미터와 태스크 정의 파라미터를 명확히 분리함으로써 확장 가능성 제시  
- **멀티태스크·앙상블 효율화**: 단일 모델 서버로 다수 태스크·앙상블 지원  
- **프롬프트 해석성 연구**: 연속 임베딩의 의미적 구조를 이해하기 위한 시각화·설명 기법 필요  
- **사전학습 목표 탐색**: span corruption 외 다른 프리트레이닝 목표도 프롬프트 튜닝 친화적으로 개선  

이상으로, **프롬프트 튜닝은 대규모 언어 모델 활용의 비용 효율성과 확장성을 제공하며**, 향후 범용 프리트레인 모델 연구에서 핵심 어댑터(adaptor)로 자리매김할 전망이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/2570d683-d89a-4a33-8aae-8481a3091573/2104.08691v2.pdf)
