# Deduplicating Training Data Makes Language Models Better

## 1. 핵심 주장과 주요 기여  
**핵심 주장**  
대규모 언어 모델의 학습 데이터에서 중복(duplicate)된 텍스트를 제거하면  
- 모델이 훈련 데이터를 과도하게 암기하여 생성하는 비율을 10배 이상 줄이고  
- 동일한 학습 성능(또는 더 나은 성능)을 유지하면서 학습 효율을 크게 높일 수 있으며  
- 반복되는 구문을 제거해 평가 정확도를 올릴 수 있다.[1]

**주요 기여**  
- 두 가지 대규모 텍스트 중복 제거 기법(EXACTSUBSTR, NEARDUP) 제안 및 구현  
- C4, RealNews, Wiki-40B, LM1B 등 표준 데이터셋의 중복 분석 및 제거  
- 중복 제거 전·후 모델(1.5B 파라미터 T5 XL) 훈련·평가를 통해 과적합 및 암기 감소, 학습 효율 개선 입증[1]

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계  

### 2.1 해결하고자 하는 문제  
- 대규모 인터넷 크롤링 데이터셋에 중복 문서·문자열이 다수 존재  
- 중복 데이터로 인해  
  1) 모델이 훈련 데이터 구문을 과도 암기(=저 다양도 생성)  
  2) 학습 비용 및 시간 비효율  
  3) 검증(Validation) 집합의 일부분이 훈련 데이터와 중복돼 평가 편향 발생  
발생함.[1]

### 2.2 제안하는 방법  
1. **EXACTSUBSTR**: 접미사 배열(suffix array)을 이용한 *정확한* k-토큰(여기서 $$k=50$$) 이상의 중복 부분 문자열 제거  
   - 접미사 배열 $$A(S)$$ 구축 후 인접 접미사 간 공통 접두사 길이 ≥50인 경우 중복으로 간주.[1]

2. **NEARDUP**: MinHash 기반의 n-gram 근사 비교  
   - 각 문서를 5-그램 집합 $$d_i$$로 표현 후, MinHash 시그니처(길이 9,000) 생성  
   - Jaccard 유사도 $$J(d_i,d_j)=\frac{|d_i\cap d_j|}{|d_i\cup d_j|}$$가 0.8 이상인 후보에 대해  
     편집 거리 기반 유사도 $$\text{EditSim}(x_i,x_j)=1-\frac{\text{EditDistance}}{\max(|x_i|,|x_j|)}$$  
     ≥0.8 조건 만족 시 near-duplicate로 제거.[1]

### 2.3 모델 구조 및 학습 설정  
- **아키텍처**: T5 기반 디코더 전용 Transformer  
  - XL(1.5B 파라미터): 24층, 임베딩 2048, FFN 5120, 32 헤드  
  - Base(110M): 12층, 임베딩 768, FFN 2048, 12 헤드  
- **학습**  
  - C4 원본 및 중복 제거 데이터셋별 2 에폭 훈련(총 약 5.5일)  
  - Adafactor 옵티마이저, 학습률 XL:0.001, Base:0.01  
  - 최대 시퀀스 길이 512 토큰, BPE 어휘 50K.[1]

### 2.4 성능 향상  
| 평가 지표                | 중복 데이터 원본 | EXACTSUBSTR | NEARDUP  |
|-------------------------|----------------|-------------|----------|
| 불러오기 없는(unprompted) 암기 비율 | 1.57%        | 0.17%       | 0.26%    |
| C4 검증 세트 중복 예제 PPL 감소 | –            | –           | –        |
| LM1B PPL                | 42.16 (GPT-2)  | 34.1        | **33.8** |
| Wiki-40B PPL 차이       | –             | –           | –        |

- 암기 비율은 10배 이상 감소.[1]
- 중복 예제 제외 검증 시 PPL 상승(=과적합 완화)  
- 중복 제거 데이터에 대해 LM1B, Wiki-40B 일반화 성능 소폭 개선  

### 2.5 한계  
- **문장 응집성 손상 가능성**: EXACTSUBSTR로 문서 일부 제거 시 문맥 연속성 훼손 우려  
- **민감 데이터 잔존**: 암호·개인식별 정보 제거 불충분  
- **과도한 제거 시 잔류 지표 악화**: 너무 엄격한 기준은 학습 데이터 부족으로 이어질 수 있음.[1]

## 3. 일반화 성능 향상 관점 분석  
- **과적합 감소**: 중복 예제 제외 검증 시 PPL 증가→오리지널 데이터에서 암기 의존 높음  
- **다양한 도메인 전이**: C4로 훈련 후 LM1B, Wiki-40B 평가에서 NEARDUP 모델이 최저 PPL 기록→데이터 품질↑ 일반화↑  
- **학습 속도**: 중복 제거로 데이터 크기 3–19% 축소, 동등 성능에 5–10% 학습 시간 절감.[1]

## 4. 향후 연구에 대한 영향 및 고려 사항  
- **영향**:  
  - 대규모 언어 모델 데이터셋 전처리 표준으로 중복 제거 확산  
  - 개인 정보 노출 위험 저감 기회 제공  
  - 효율적 학습·평가 파이프라인 구축 가능  

- **고려 사항**:  
  1. **적절한 중복 기준 설정**: 도메인·어플리케이션별 k, Jaccard 임계치 최적화  
  2. **부분 중복 보존**: 인용·출처 등 중요한 반복 정보 제거 방지  
  3. **민감 데이터 필터링**: 중복 제거 외 별도 개인 식별 정보 제거  
  4. **응용별 기억 메커니즘**: 검색 응답용 등 메모리 유지가 필요한 사용 사례 구분  

중복 제거는 **모델의 암기 의존도를 낮추고**, **일반화 성능을 향상**시키며, **학습·평가 효율을 높이는** 보편적 전처리 기법으로 자리매김할 전망이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/00ae3f40-8bbc-40bb-95b9-91a591a37f5f/2107.06499v2.pdf)
