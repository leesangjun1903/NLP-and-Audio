# OpenAssistant Conversations - Democratizing Large Language Model Alignment

## 핵심 주장 및 주요 기여  
**OpenAssistant Conversations** 데이터셋은 대규모 언어 모델(LLM)을 인간 선호에 정렬(alignment)시키기 위해 필요한 대규모 인간 피드백 데이터를 공개함으로써, 연구 커뮤니티의 접근성을 획기적으로 높인다.[1]
주요 기여:  
- 161,443개의 메시지(35개 언어), 10,968개의 완전 주석 대화 트리, 461,292건의 품질 평가 레이블을 포함하는 방대한 공개 데이터셋 구축  
- 참여자 13,500명 이상의 크라우드소싱으로 생성·주석 작업 수행  
- SFT, RM, RLHF 모델을 학습·공개하여 벤치마크 성능 검증  

## 해결하고자 하는 문제  
기존 RLHF 기반 LLM 정렬 기법은 고품질의 인간 피드백 데이터를 필요로 하나, 대부분이 비용이 높고 독점적으로 관리되어 왔다. 이로 인해 오픈 리서치가 제한되며, 다양하고 투명한 연구가 어렵다는 한계가 존재한다.  

## 제안하는 방법 및 모델 구조  
### 데이터 수집 및 형식화  
- 대화 트리 구조(Tree)로 저장:  
  - 프롬프터(prompt)와 어시스턴트(reply) 역할을 번갈아 표현  
  - 각 어시스턴트 응답에 다중 순위(rank) 정보 부여  
- 수집 단계:  
  1. 초기 프롬프트 생성  
  2. 어시스턴트/프롬프터 응답 작성  
  3. 품질 및 스팸 레이블링  
  4. 다중 응답 순위 매기기  

### 수학적 모델링  
1. **SFT (Supervised Fine-Tuning)**  
   입력: 대화 스레드  
   학습 대상: 어시스턴트 응답 토큰 예측  
2. **RM (Reward Model)**  
   - 출력 스칼라 $$r_\theta(x,y)$$  
   - Loss:  

```math
       \mathcal{L}(\theta) = -\mathbb{E}_{(x,y_w,y_l)}\bigl[\log \sigma\bigl(r_\theta(x,y_w)-r_\theta(x,y_l)\bigr)\bigr]
```
   
- $$y_w$$: 선호 응답, $$y_l$$: 비선호 응답  
3. **RLHF (PPO 기반 강화학습)**  
   - SFT 모델을 초기 정책으로 사용  
   - RM으로 평가된 보상에 토큰별 KL 페널티를 추가하여 안정적 정책 업데이트  

## 성능 향상  
| 모델                         | LMEH   | VEL    | OAIE   | HE     |
|-----------------------------|--------|--------|--------|--------|
| gpt-3.5-turbo (ChatGPT)     | 1110   | 0.87   | 0.72   | 997    |
| Pythia-12B                  | 60.33  | –      | –      | –      |
| Pythia-12B SFT-v8           | 60.28  | –      | –      | –      |
| Falcon-40B                  | 72.29  | –      | –      | –      |
| Falcon-40B SFT-top1         | 74.04  | –      | –      | –      |
| Falcon-40B SFT-mix          | 74.40  | –      | –      | –      |
| LLaMA-30B SFT               | 68.03  | –      | –      | –      |
| LLaMA-30B RLHF              | 68.51  | –      | –      | –      |  

- OpenAssistant 대화 데이터로 학습한 SFT·RLHF 모델이 원본 베이스라인 대비 **일관된 성능 향상**을 보인다.[1]
- 일부 벤치마크에서 ChatGPT에 근접 또는 초과하는 성과 확인  

## 한계  
- **보상 모델 데이터 편향**: 인간 생성 메시지 기반 RM 학습으로 RLHF 단계에서 SFT 대비 일관된 개선 폭이 제한적  
- **기여자 편향**: 89.1%가 남성, 특정 파워유저 의존으로 문화적·주관적 편향 가능성  
- **잔존 위험 콘텐츠**: 자동·수동 필터링에도 불구하고 미검출 위험 내용 존재  

## 일반화 성능 향상 가능성  
- 대화 다양성 확보: 35개 언어, 수천 개 트리로 훈련된 모델은 **다양한 도메인**에서의 일반화 능력 우수  
- 크라우드소싱 방식: 실제 인간 평가 기반 레이블이 **도메인 간 전이 학습**에 유리  
- 혼합 데이터 학습(SFT-mix): OASST1 외 다중 instruction 데이터 결합으로 **다양한 과제 적응력** 강화  

## 향후 연구 영향 및 고려 사항  
- **데이터 확대**: 저대표 언어·문화권 참여 확대, 채널별 균형 조정 필요  
- **RM 학습 개선**: SFT 모델 생성 순위 데이터 활용, 보상 모델 품질 개선  
- **안전성 검증**: Hallucination·편향·악용 방지 위한 추가 안전 장치·평가 메트릭 개발  
- **오픈 생태계 강화**: 투명·재현성 있는 연구 진행을 통해 AI 정렬 분야의 민주화 지속

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b8139132-6d39-4f20-9176-fdb9507cc337/2304.07327v2.pdf)
