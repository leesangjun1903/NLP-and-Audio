# XLNet: Generalized Autoregressive Pretraining for Language Understanding

## 핵심 주장 및 주요 기여

**XLNet**은 기존 사전학습 방식의 한계를 극복하는 혁신적인 접근법을 제시합니다. 논문의 핵심 주장은 다음과 같습니다:[1]

1. **순열 언어 모델링(Permutation Language Modeling)**: 고정된 인수분해 순서 대신 모든 가능한 순열에 대해 기댓값 우도를 최대화함으로써, 양방향 문맥을 자동회귀 프레임워크 내에서 학습 가능하게 만들었습니다.[1]

2. **BERT의 한계 극복**: BERT의 주요 문제점인 **독립성 가정**(마스크된 토큰들이 서로 독립이라는 가정), **사전학습-미세조정 불일치**(MASK 토큰은 실제 데이터에 없음), **의존성 무시**를 모두 해결합니다.[1]

3. **양방향 문맥 학습**: 순열을 통해 각 위치가 양쪽 모든 토큰으로부터 문맥 정보를 수집하여, 기댓값에서 진정한 양방향 이해를 달성합니다.[1]

**주요 기여**: XLNet은 20개 과제에서 BERT를 능가하며, 특히 질의응답, 독해이해, 감정분석, 문서 랭킹 등에서 큰 성능 향상을 보여줍니다.[1]

---

## 문제 정의 및 비교 분석

### XLNet이 해결하는 문제

기존의 두 가지 사전학습 방식은 각각의 한계를 가집니다:[1]

| 측면 | 자동회귀(AR) 모델 | BERT(수정 자동인코더) | XLNet |
|------|---|---|---|
| **독립성 가정** | ✓ 없음 (곱셈 규칙) | ✗ 마스크된 토큰들 독립 가정 | ✓ 없음 |
| **입력 노이즈** | ✓ 없음 | ✗ MASK 토큰 사용 | ✓ 없음 |
| **양방향 문맥** | ✗ 한 방향만 | ✓ 양방향 | ✓ 양방향 |

예를 들어, "New, York"을 예측할 때:[1]
- BERT: $$\log p(\text{New} \mid \text{is a city}) + \log p(\text{York} \mid \text{is a city})$$ - New와 York 간 의존성 무시
- XLNet: $$\log p(\text{New} \mid \text{is a city}) + \log p(\text{York} \mid \text{New, is a city})$$ - 의존성 학습

***

## 방법론 및 수식

### 1. 순열 언어 모델링 목적함수

XLNet의 핵심 목적함수는 다음과 같습니다:[1]

```math
\max \mathbb{E}_{z \sim Z_T} \sum_{t=1}^{T} \log p(x_{z_t} \mid x_{z_ < t})
```
 (식 3)

여기서:
- $$Z_T$$: 길이 T인 인덱스 수열의 모든 순열 집합
- $$z_t$$: 순열 z의 t번째 원소
- $$z_{<t}$$: 순열 z의 처음 t-1개 원소
- 같은 모델 매개변수가 모든 인수분해 순서에 공유됨[1]

**중요한 주석**: 순열은 **인수분해 순서**만 순열화하고, 입력 수열 순서는 유지됩니다. 이는 미세조정 시 자연스러운 토큰 순서를 만나기 때문입니다.[1]

### 2. 목표-인식 표현(Target-Aware Representation)

표준 Softmax 매개변수화는 목표 위치 $$z_t$$에 따라 다른 분포를 생성하지 못하는 문제가 발생합니다. 이를 해결하기 위해:[1]

$$p_{X_{z_t}}(x \mid x_{z_<t}) = \frac{\exp(e_x^T g(x_{z_<t}, z_t))}{\sum_{x'} \exp(e_{x'}^T g(x_{z_<t}, z_t))}$$ (식 4)

여기서 $$g(x_{z_<t}, z_t)$$는 목표 위치 정보를 포함하는 새로운 표현입니다.[1]

### 3. 이중 스트림 자기 주의(Two-Stream Self-Attention)

이 메커니즘은 두 가지 모순된 요구사항을 해결합니다:[1]

**콘텐츠 스트림** ($$h$$):
- 위치 정보와 토큰 콘텐츠 모두 활용
- 표준 자기 주의와 동일

**쿼리 스트림** ($$g$$):
- 위치 정보만 활용, 콘텐츠 $$x_{z_t}$$ 접근 불가
- 초기화: 학습 가능한 벡터 $$g_i^{(0)} = w$$

각 층 $$m = 1, ..., M$$에서 업데이트:[1]

$$g_t^{(m)} = \text{Attention}(Q=g_t^{(m-1)}, K,V=h_t^{(m-1)}, \text{mask})$$

$$h_t^{(m)} = \text{Attention}(Q=h_t^{(m-1)}, K,V=h_t^{(m-1)})$$

미세조정 시 쿼리 스트림을 버리고 콘텐츠 스트림만 사용하면 표준 Transformer-XL이 됩니다.[1]

### 4. 부분 예측(Partial Prediction)

최적화 어려움을 줄이기 위해, 순열 z를 비목표 부분 $$z_{<c}$$와 목표 부분 $$z_{\geq c}$$로 나누고:[1]

```math
\max \mathbb{E}_{z \sim Z_T} \log p(x_{z_{\geq c}} \mid x_{z_{ < c}}) = \mathbb{E}_{z \sim Z_T} \sum_{t=c}^{T} \log p(x_{z_t} \mid x_{z_ < t})
```
 (식 5)

하이퍼파라미터 K로 약 1/K의 토큰만 예측합니다(K=6 사용).[1]

### 5. 상대 위치 인코딩

상대 인코딩을 세그먼트 관계까지 확장:[1]

$$a_{ij} = q_i \cdot b + q_i \cdot s_{ij}$$

여기서 $$s_{ij} = s^+$$ (같은 세그먼트) 또는 $$s^-$$ (다른 세그먼트)로, 절대 세그먼트 임베딩보다 일반화성이 뛰어납니다.[1]

***

## 모델 구조

### 아키텍처 개요

XLNet은 **Transformer-XL**을 기반으로 하며, 다음 핵심 특성을 포함합니다:[1]

1. **24층 구조** (XLNet-Large)
   - 히든 크기: 1024
   - 주의 헤드: 16개
   - FFN 내부 크기: 4096
   - 최대 수열 길이: 512[1]

2. **상기 메커니즘(Recurrence Mechanism)**
   - 이전 세그먼트의 콘텐츠 표현 $$h_m$$ 캐싱
   - 메모리 업데이트: $$h_t^{(m)} = \text{Attention}(Q=h_t^{(m-1)}, K,V=[h_m^{\text{cached}}, h_t^{(m-1)}])$$
   - 위치 인코딩이 원본 수열 위치에만 의존하므로, 순열 순서를 모르고도 메모리 재사용 가능[1]

3. **다중 세그먼트 모델링**
   - BERT 형식 따름: [CLS], A, [SEP], B, [SEP]
   - NSP(다음 문장 예측)은 미사용 (성능 향상 없음)[1]

### 모델 파이프라인

1. **사전학습 단계**: 32.89B 부분어 토큰에서 학습 (위키백과, 북스코퍼스, Giga5, ClueWeb, Common Crawl)
2. **미세조정**: 쿼리 스트림 제거 후 표준 Transformer-XL 사용
3. **성능 평가**: GLUE, SQuAD, RACE, 텍스트 분류 등 다양한 벤치마크[1]

***

## 성능 향상

### 공정한 비교 (동일 데이터, 동일 설정)

BERT와의 직접 비교 결과:[1]

| 과제 | BERT-Large | XLNet-Large-wikibooks | 향상도 |
|---|---|---|---|
| SQuAD 1.1 | 86.7/92.8 | 88.2/94.0 | +1.5/+1.2 |
| SQuAD 2.0 | 82.8/85.5 | 85.1/87.8 | +2.3/+2.3 |
| RACE | 75.1 | 77.4 | +2.3 |
| MNLI | 87.3 | 88.4 | +1.1 |
| QNLI | 93.0 | 93.9 | +0.9 |

### RoBERTa와의 비교 (전체 데이터)

큰 규모의 사전학습 후:[1]

| 과제 | BERT | RoBERTa | XLNet |
|---|---|---|---|
| RACE (높음) | 76.6 | 86.5 | **88.6** |
| RACE (중간) | 72.0 | 83.2 | **85.4** |
| SQuAD 2.0 EM | 80.0 | 86.8 | **87.9** |
| IMDB 오류율 | 4.51 | - | **3.20** |
| Yelp-5 오류율 | 29.32 | - | **27.05** |

### 제거 연구(Ablation Study)

각 설계 선택의 중요성:[1]

| 모델 | RACE | SQuAD2.0 | MNLI | SST-2 |
|---|---|---|---|---|
| BERT-Base | 64.3 | 76.30 | 73.66 | 92.78 |
| DAE Transformer-XL | 65.03 | 79.56 | 76.80 | 92.60 |
| XLNet-Base (K=6) | **66.66** | **80.98** | **78.18** | **93.35** |
| - 메모리 | 65.55 | 80.15 | 77.27 | 92.78 |
| - 범위 예측 | 65.95 | 80.61 | 77.91 | 93.12 |
| - 양방향 파이프라인 | 66.34 | 80.65 | 77.87 | 92.66 |

**결론**: 순열 LM과 Transformer-XL 모두 성능 향상에 핵심적, 메모리 캐싱은 긴 문맥 과제(RACE)에서 특히 중요.[1]

***

## 일반화 성능 향상 가능성

### 1. 상대 인코딩의 일반화 이점

상대 위치 인코딩은 다음과 같은 이유로 일반화 성능을 향상시킵니다:[1]

- **외삽 가능성**: 훈련 길이보다 긴 수열에 적응 가능
- **위치 관계 중심**: 절대 위치가 아닌 상대적 관계만 학습
- **세그먼트 간 확장성**: 2개보다 많은 세그먼트를 가진 과제로 미세조정 가능[1]

### 2. 순열 LM의 데이터 효율성

순열 언어 모델링은 더 많은 훈련 신호를 제공합니다:[1]

BERT는 특정 대상-문맥 쌍만 학습하지만, XLNet은 **더 밀도 높은 의존성 쌍**을 학습합니다. 예를 들어, 목표 집합 T와 비목표 집합 N이 주어질 때:[1]

- U ⊆ N인 경우: BERT와 XLNet 모두 의존성 학습 가능
- U ⊆ N ∪ T_x이고 U ⊆ T_x인 경우: **XLNet만 학습 가능**

결과: XLNet은 같은 대상 수로 더 많은 의존성 쌍을 포함하므로 **더 효율적인 훈련 신호** 보유.[1]

### 3. 더 나은 표현 학습

초기 사전학습에서만 순열 LM의 이점이 명백하지 않지만, 미세조정 과정에서 학습된 표현의 질이 높아집니다. XLNet에서만 관찰되는 주의 패턴:[1]

- **자기 배제(Self-exclusion)**: 자신을 제외한 모든 토큰에 주의 → 빠른 전역 정보 수집
- **상대 스트라이드(Relative-stride)**: 특정 간격마다 위치에 주의 → 장거리 의존성 캡처
- **편측 마스킹(One-side masked)**: 우측 반을 마스킹 → 방향성 정보 학습

이 패턴들은 모두 상대 위치 메커니즘으로 인해 가능하며, XLNet의 우수한 일반화 능력을 뒷받침합니다.[1]

### 4. 긴 문맥 이해 능력

Transformer-XL 기반 구조로 인해, XLNet은 **더 긴 텍스트 수열**에 대해 더 큰 성능 향상을 보입니다:[1]

- **RACE** (평균 길이 >300): 상대적으로 큰 향상
- **SQuAD** (일반적 길이): 중간 정도 향상
- **텍스트 분류** (짧은 텍스트): 여전히 향상하지만 더 작음

상기 메커니즘과 상대 인코딩의 결합은 모델이 훈련 길이보다 긴 문맥을 효과적으로 다룰 수 있게 합니다.[1]

***

## 한계

### 1. 계산 복잡성

- **훈련 시간**: 512 TPU v3에서 500K 스텝에 약 5.5일 소요[1]
- **메모리 요구**: 쿼리 스트림의 추가 메모리 사용
- 최적화 난이도: 순열 목적함수가 훨씬 더 도전적[1]

### 2. 하이퍼파라미터 민감성

- **부분 예측 상수 K**: 최적화 어려움과 성능 간 균형 필요
- 제거 연구에서 K=6과 K=7의 성능 차이 관찰[1]

### 3. 이론적 명확성

순열 LM이 미세조정에서의 이점 메커니즘이 완전히 설명되지 않음. 논문은 "순열 LM이 데이터 효율성에 주로 기여하며, 질적 시각화에서는 명백하지 않을 수 있다"고 언급합니다.[1]

### 4. 특정 과제에서의 제한

- **WNLI**: 테스트 세트에서 92.5% 성능 (여전히 도전적)
- **RTE**: 90.4% (고정된 해석 필요)
- 일부 분류 과제에서 BERT와 거의 동등한 개선[1]

***

## 앞으로의 연구 영향 및 고려사항

### 1. 논문이 미치는 영향

**패러다임 전환**: XLNet은 **자동회귀 프레임워크 내에서 양방향 학습이 가능**함을 보여주며, 이는 언어 모델 연구의 근본적인 한계를 극복합니다. 이는 향후 사전학습 방법론에 큰 영향을 미쳤습니다.[1]

**언어 모델링과 사전학습의 연결**: 순열 LM을 통해 밀도 추정 연구와 실무 응용 사이의 격차를 줄입니다. 언어 모델링의 빠른 진전이 직접 사전학습 성능으로 이어질 수 있게 합니다.[1]

### 2. 후속 연구의 필수 고려사항

**모델 크기 확장성**: XLNet은 여전히 데이터에 언더피팅됨을 보였습니다. 더 큰 모델과 데이터셋에서 순열 LM의 효과를 연구할 필요가 있습니다.[1]

**다국어 및 도메인 적응**: 현재 영어 데이터 중심의 평가입니다. 다양한 언어와 전문 도메인에서의 일반화 능력을 검증해야 합니다.

**순열 목적함수의 이론적 분석**: 부분 예측과 전체 순열 LM의 수렴 특성, 통계적 효율성의 공식적 분석이 필요합니다.

**아키텍처 혁신**: 두 스트림 주의의 생물학적 타당성, 계산 효율성을 개선할 수 있는 더 나은 메커니즘 탐색이 가치 있습니다.

**사전학습-미세조정 일치성**: 상대 인코딩이 다양한 다운스트림 과제에서 일관되게 미세조정을 개선하는 메커니즘의 심화 연구가 중요합니다.[1]

### 3. 실무 적용 시 권고사항

- **컴퓨팅 리소스**: 순열 LM의 훈련 비용이 크므로, 자원 제한 환경에서는 신중한 하이퍼파라미터 선택 필요
- **성능-비용 트레이드오프**: 많은 과제에서 수 % 개선만 이루므로, 비용 대비 효과를 평가해야 함
- **도메인 특화 미세조정**: XLNet의 강점(긴 문맥, 복잡한 의존성)이 있는 과제에 우선 적용 권장

***

## 결론

**XLNet**은 순열 언어 모델링을 통해 자동회귀 프레임워크의 효율성과 양방향 문맥 학습을 결합한 획기적 방법입니다. 상대 인코딩과 Transformer-XL 아키텍처의 통합으로 일반화 성능이 크게 향상되었습니다. 특히 긴 문맥 이해와 복잡한 의존성 모델링에서 BERT보다 우수하며, 데이터 효율성 측면에서도 이점이 있습니다.[1]

다만 계산 복잡성, 하이퍼파라미터 민감성, 이론적 명확성 부족이 제한 요소입니다. 향후 연구는 모델 확장성, 다국어 일반화, 순열 목적함수의 이론적 분석, 그리고 보다 효율적한 아키텍처 개발에 초점을 맞춰야 합니다. XLNet의 통찰력은 현대 대규모 언어 모델 개발의 기초를 제공하며, 특히 사전학습-미세조정 패러다임의 진화에 중요한 역할을 하고 있습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8f735550-ed81-4abe-944a-c921aaa04f09/1906.08237v2.pdf)
