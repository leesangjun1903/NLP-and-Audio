# Yi: Open Foundation Models by 01.AI

## 1. 핵심 주장과 주요 기여

### **핵심 주장**
Yi 모델 패밀리는 **데이터 품질이 모델 성능의 핵심**이라는 가설을 바탕으로, 정교한 데이터 엔지니어링을 통해 GPT-3.5 수준의 성능을 달성할 수 있음을 입증했습니다.[1]

### **주요 기여**
1. **데이터 품질 중심 접근법**: 3.1조 토큰의 고품질 이중언어(영중) 코퍼스를 구축하고, 1만 개 미만의 세밀하게 큐레이션된 파인튜닝 데이터셋을 활용[1]
2. **효율적 추론**: 4비트 양자화를 통해 RTX 4090과 같은 소비자용 GPU에서도 배포 가능한 모델 제공[1]
3. **다차원 확장**: 200K 토큰 장문맥 처리, 시각-언어 모델, 깊이 확장(depth upscaling) 등 다양한 능력 확장[1]

## 2. 문제, 방법, 구조, 성능 및 한계

### **해결하고자 하는 문제**
- 기존 대형 언어 모델의 높은 추론 비용과 배포 어려움
- 데이터 규모 증가에만 의존하는 접근법의 한계
- 소비자용 하드웨어에서 실행 가능한 고성능 모델의 부재

### **제안하는 방법**

#### **데이터 엔지니어링 파이프라인**
논문에서 제시한 핵심 수식은 명시적으로 드러나지 않으나, 데이터 품질을 위한 다단계 필터링 시스템을 구현했습니다:[1]

1. **휴리스틱 규칙 필터**: URL, 도메인, 단어 차단 목록, 문서 길이, 특수 기호 비율 등
2. **학습 기반 필터**: 
   - 복잡도 점수(Perplexity Scorer) 
   - 품질 점수(Quality Scorer) - Wikipedia 품질 유사성 기준
   - 안전성 점수(Safety Scorer) - 유해 콘텐츠 식별
   - 문서 일관성 점수(Document Coherence Scorer)
3. **클러스터 기반 필터**: 비지도 의미 클러스터링으로 유사 문서 그룹화
4. **중복 제거**: MinHash 기반 문서 레벨 중복 제거 및 하위 문서 정확 일치 중복 제거

#### **모델 구조**
- **기반 아키텍처**: 표준 디코더 전용 Transformer
- **Grouped-Query Attention (GQA)**: 6B와 34B 모델 모두에 적용하여 훈련 및 추론 비용 절감
- **SwiGLU 활성화 함수**: 파라미터 효율성과 성능 균형
- **RoPE (Rotary Position Embedding)**: 기본 주파수 조정으로 200K 토큰까지 지원

| 모델 | 은닉 크기 | Q-헤드 | KV-헤드 | 레이어 | 시퀀스 길이 | 최대 학습률 |
|------|-----------|---------|---------|--------|-------------|-------------|
| 6B   | 4096      | 32      | 4       | 32     | 4096        | 3×10⁻⁴     |
| 34B  | 7168      | 56      | 8       | 60     | 4096        | 1.5×10⁻⁴   |

### **성능 향상**
1. **기본 모델**: MMLU에서 Yi-34B가 76.3점으로 GPT-3.5(69.1점)를 상회[1]
2. **채팅 모델**: AlpacaEval에서 94.08점, LMSys Chatbot Arena에서 1110점 달성[1]
3. **양자화 성능**: 4비트 양자화 후에도 성능 저하 1% 미만 유지[1]

### **한계점**
1. **수학/코딩 능력**: 여전히 GPT-4 대비 상당한 격차 존재 (Yi-34B MATH: 14.4 vs GPT-4: 40.2)[1]
2. **추론 능력**: BigBench Hard와 같은 복잡한 추론 태스크에서 성능 격차
3. **모델 규모**: 34B 파라미터로는 일부 복잡한 태스크에서 한계

## 3. 일반화 성능 향상

### **장문맥 일반화**
Yi는 기본 4K 토큰에서 200K 토큰으로 확장 시, **내재적 능력 가설**을 제시합니다. 모델이 이미 긴 의존성을 모델링할 능력을 내재적으로 보유하고 있으며, 추가 훈련은 단순히 이 능력을 "해제"하는 역할을 한다는 것입니다.[1]

실제로 1-2B 토큰의 경량화된 지속 사전훈련만으로도 4K-200K 길이에서 낮은 손실로 수렴하며, Needle-in-a-Haystack 테스트에서 거의 완벽한 성능을 보였습니다.[1]

### **In-Context Learning 능력**
Yi-34B는 선형 계수 추론 태스크에서 우수한 성능을 보이며, 모델 규모 증가 시 더 복잡한 함수를 in-context learning으로 추론할 수 있는 창발적 능력을 확인했습니다.[1]

### **교차 태스크 견고성**
파인튜닝에서 **다양성 중심 샘플링 알고리즘**과 **명령어 태깅 시스템**을 통해 다양한 능력 영역의 균형잡힌 분포를 확보하여 교차 태스크 견고성을 달성했습니다.[1]

## 4. 연구에 미치는 영향과 고려사항

### **미래 연구에 미치는 영향**

1. **데이터 품질 우선 패러다임**: 
   - 데이터 규모보다 품질이 중요하다는 점을 입증 (3T 토큰 고품질 vs 10T 토큰 저품질)
   - 향후 모델 개발에서 데이터 엔지니어링의 중요성 부각

2. **효율적 모델 배포**: 
   - 소비자용 하드웨어에서 고성능 모델 실행 가능성 제시
   - 프라이버시 보호와 비용 효율성을 위한 로컬 배포 촉진

3. **다모달 확장 방법론**: 
   - 3단계 비전-언어 훈련 접근법 제시
   - 깊이 확장을 통한 성능 향상 검증

### **향후 연구 시 고려사항**

1. **데이터 엔지니어링 복잡성**: 
   - 정교한 필터링 파이프라인 구축을 위한 상당한 엔지니어링 투자 필요
   - 언어별, 도메인별 특화된 필터링 전략 개발

2. **수학/추론 능력 강화**: 
   - 전문 도메인 데이터 증강 및 특화된 훈련 기법 필요
   - CoT(Chain-of-Thought) 및 고급 추론 기법 통합

3. **확장성 고려**: 
   - 더 큰 모델 규모에서도 데이터 품질 중심 접근법의 효과성 검증 필요
   - 다언어 확장 시 언어별 데이터 품질 균형 문제

Yi 논문은 **"Quality over Quantity"** 철학을 통해 효율적이고 실용적인 대형 언어 모델 개발의 새로운 방향을 제시했으며, 특히 한정된 자원으로도 고성능을 달성할 수 있는 방법론을 입증했다는 점에서 큰 의의가 있습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/1f1ec474-177b-40fb-87c4-bed1ae1c47a3/2403.04652v3.pdf)
