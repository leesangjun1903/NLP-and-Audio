# Efficient Memory Management for Large Language Model Serving with PagedAttention

# 요약

**핵심 주장:**  
본 논문은 대규모 언어 모델(LLM) 서비스 시 반복적으로 생성되는 키-값 캐시(KV cache)의 메모리 단편화 및 중복 문제를 해결하기 위해, 운영체제의 가상 메모리·페이징 기법을 차용한 **PagedAttention** 알고리즘과 이를 기반으로 한 고효율 LLM 서빙 시스템 **vLLM**을 제안한다.[1]

**주요 기여:**  
1. **PagedAttention:** KV 캐시를 고정 크기 블록(페이지) 단위로 관리하여 내부·외부 단편화를 제거하고, 필요 시 블록 단위 동적 할당·해제를 가능케 함.[1]
2. **vLLM 시스템 설계:** 페이징 기반 KV 캐시 관리자, 중앙 스케줄러, GPU 블록 엔진 등으로 구성된 분산 서빙 아키텍처를 구현.[1]
3. **메모리 공유 최적화:** 병렬 샘플링, 빔 서치, 프리픽스 공유 시 블록 단위 참조 계수 및 복사-온-쓰기(copy-on-write)로 캐시 재사용을 극대화.[1]
4. **성능 평가:** 동일 지연(latency) 조건 하에서 기존 FasterTransformer, Orca 대비 **2–4×** 향상된 처리량(throughput)을 달성.[1]

***

# 상세 설명

## 1. 해결하고자 하는 문제  
- **대규모 KV 캐시 메모리:** 트랜스포머 기반 LLM은 출력 토큰 수에 비례해 키·값 벡터(𝑘,𝑣)를 순차 생성하고 캐싱함. 최대 2048토큰×모델층 수 등으로 1요청 당 최대 수 GB가 소요되어 GPU 메모리 병목 초래.[1]
- **메모리 단편화:** 기존 프레임워크들은 연속 메모리에 KV 텐서를 저장하므로  
  1. **내부 단편화:** 최대 길이(예:2048토큰)로 사전 예약된 공간 중 실제 사용량이 적어 낭비  
  2. **외부 단편화:** 예약 크기가 요청마다 달라 필요 공간을 찾지 못함  
- **메모리 공유 불가:** 병렬 샘플링·빔 서치 등에서 공통 컨텍스트 캐시 공유 기회 상실  

## 2. 제안 방법: PagedAttention  
1. **KV 블록화:** 한 블록당 𝐵토큰 분량의 키·값 벡터를 묶어 물리적 페이지처럼 관리  
2. **논리-물리 매핑:** 각 요청별 논리 블록을 블록 테이블로 물리 블록에 매핑하고, 필요 시 블록 할당·해제  
3. **블록 단위 주기적 확장:** 새 토큰 생성 때마다 마지막 논리 블록에 빈 슬롯이 남으면 그곳에 저장, 꽉 차면 새 물리 블록 할당  
4. **수식:**  

$$A_{ij} = \frac{\exp(q_i^\top K_j / \sqrt{d})}{\sum_{t=1}^{\lceil i/B\rceil}\exp(q_i^\top K_t/\sqrt{d})},\quad o_i = \sum_{j=1}^{\lceil i/B\rceil} V_jA_{ij}$$  
   
   여기서 $$K_j,V_j$$는 $$j$$-번째 KV 블록.[1]

### 모델 구조  
- 중앙 **스케줄러** → 요청 수집·FCFS 스케줄링  
- **KV 캐시 관리자:** 논리-물리 블록 매핑, 복사-온-쓰기 관리  
- **GPU 블록 엔진:** CUDA 커널로 페이징된 KV 블록 읽기·쓰기, PagedAttention 연산  
- **분산 실행:** Megatron-LM 유사 SPMD 방식으로 각 GPU가 일부 헤드를 처리하며 공통 블록 매핑 참조  

## 3. 성능 향상  
| 시스템              | 처리량 향상      | 주요 원인                        |
|-------------------|---------------|-------------------------------|
| vLLM vs Orca (Max) | 2.7×–8×↑     | 내부·외부 단편화 제거, 공유 최적화     |
| vLLM vs Orca (Oracle) | 1.7×–2.7×↑   | 동적 블록 할당, 최소 예약           |
| vLLM vs FasterTransformer | 최대 22×↑ | 세분 배칭+페이징 메모리 관리     |

- **병렬 샘플링:** 프롬프트 블록 공유로 6–9% 메모리 절감  
- **빔 서치:** 최대 55% 캐시 공유[1]
- **프리픽스 공유:** 1샷 공유 시 1.67×↑, 5샷 공유 시 3.58×↑  

## 4. 한계  
- **커널 오버헤드:** 테이블 참조·비연속 접근으로 기존 Attention 대비 20–26% 지연 증가[1]
- **블록 크기 민감성:** 너무 작으면 병렬성 저하, 너무 크면 내부 단편화→기본 $$B=16$$ 권장  
- **교체 정책 단순화:** 모든 블록 일괄 스왑 아웃, 복원 시 완전 재계산 또는 CPU RAM 스왑 의존  
- **GPU 특화:** 비(非)LLM 워크로드에는 오버헤드가 성능 저하 초래 가능  

***

# 일반화 성능 향상 가능성

- **모델 일반화(Generalization):** PagedAttention 자체는 순전파 추론 시 메모리 관리 기법이므로 모델 파라미터나 학습에는 직접 관여하지 않는다. 다만  
  - *프리픽스 공유*를 통해 고품질 프롬프트(pre-prompt)에 대한 반복 캐싱이 가능해지며, **프롬프트 튜닝**·**프리픽스-튜닝(prefix-tuning)** 연구와 결합 시 일반화 성능 향상 여지  
  - 배치 크기 증가로 장기 컨텍스트 활용 효율이 좋아져, 장문 이해·생성 작업에서도 모델의 **컨텍스트 일반화** 효과 기대  

***

# 향후 연구 영향 및 고려 사항

- **메모리 관리 적용 확대:** 다른 동적 메모리 워크로드(예: 대화형 비LLM 추론, 대규모 그래프 처리)에 페이징 개념 적용  
- **교체 정책 고도화:** 접근 예측 기반 블록 단위 LRU/LFU 도입, 스트리밍·QoS 고려한 우선순위 스케줄링  
- **커널 최적화 병행:** 페이징 기법의 인덱싱 오버헤드를 더욱 줄이는 하드웨어 지원 가능성 연구  
- **하이브리드 복원 전략:** 스왑·재계산의 동적 조합으로 지연 최소화  
- **프롬프트 학습 연계:** 프롬프트 엔지니어링·튜닝과 통합해 *프리픽스 공유*가 일으키는 일반화 이득을 계량 분석  

위 고려 사항들은 **메모리 효율성**뿐만 아니라 **추론 품질과 일반화 능력** 향상에도 기여할 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/6cefcaad-6d5c-4aca-a804-4f9beaddc0c2/2309.06180v1.pdf)
