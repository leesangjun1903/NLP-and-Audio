# Learning to Summarize from Human Feedback

## 1. 핵심 주장 및 주요 기여  
“Learning to Summarize from Human Feedback”(Stiennon et al., NeurIPS 2020)는 **기존 최대우도 학습만으로는 요약 품질을 충분히 반영하지 못하므로**, 사람의 선호를 보상 함수로 학습하여 요약 성능을 **비약적으로 향상**시킬 수 있음을 입증한다.  
주요 기여  
- 대규모 인간 비교 데이터(64,832개 요약 쌍)를 수집·공개  
- 사람의 선호를 예측하는 보상 모델(RM)을 학습  
- RM 보상을 최적화하는 RL 정책(PPO)을 통해 요약 품질 개선  
- Reddit TL;DR → CNN/DailyMail 전이 실험에서 지도학습 대비 **우수한 일반화 성능** 달성  

## 2. 문제 설정·제안 방법·모델 구조·성능·한계  
### 2.1 해결하고자 하는 문제  
- 기존 요약 모델은 최대우도(supervised) 학습에 의존하며, ROUGE 등 자동 지표는 **사람이 느끼는 품질과 불일치**  
- 요약의 *정확성*, *응집력*, *정보 커버리지* 등 인간 선호를 직접 반영하길 원함  

### 2.2 제안 방법  
1) **비교 데이터 수집**  
   - Reddit TL;DR 게시글에 대해 다양한 요약(원본·SFT·무작위 샘플)을 사람에게 A/B 비교[Section 3.1].  
2) **보상 모델 학습**  
   - 입력: 게시글 x, 요약 y₀·y₁  
   - 출력: 사람 선호 예측 점수 rθ(x,y)  
   - 손실:  

```math
       \mathcal{L}(\theta) = -\mathbb{E}_{(x,y_i,y_{1-i})}\bigl[\log\sigma(r_\theta(x,y_i)-r_\theta(x,y_{1-i}))\bigr]
``` 

3) **강화학습 정책 최적화**  
   - 초기화: supervised fine-tuned 모델 π_SFT  
   - PPO로 RM 보상 rθ(x,y) 최적화  
   - KL 페널티로 π_SFT와 과도한 괴리 방지:  

$$
       R(x,y) = r_\theta(x,y) - \beta\mathrm{KL}\bigl[\pi_\phi^{RL}(y|x)\|\pi^{SFT}(y|x)\bigr]
     $$  

### 2.3 모델 구조  
- Transformer-decoder(GPT-3 스타일)  
- 모델 크기: 1.3B, 6.7B 파라미터  
- 보상 모델: SFT 모델 위에 선형 헤드 추가  
- 정책·가치 함수 네트워크 분리  

### 2.4 성능 향상  
- TL;DR 원본 요약 대비 **6.7B RL 모델이 65% 이상의 우선 선호** 획득(원본 대비)  
- 1.3B RL 모델도 6.7B SFT 모델을 능가(61% vs. 43%)[Fig. 1]  
- CNN/DailyMail 전이: 뉴스 도메인 미학습에도 **SFT CNN/DM 모델에 근접하는 성능** 달성[Fig. 4]  
- 자동 지표(ROUGE, log-prob)보다 RM 예측 정확도 우수 → 인간 선호와 높은 상관[Fig. 6]  

### 2.5 한계  
- **레이블링 비용·시간 과다**: 수천 시간의 인간 비교 필요  
- 과도 최적화 시 RM 오버피팅 → 실제 품질 악화[Fig. 5]  
- 편향된 Reddit 데이터 → 민감 콘텐츠·사회적 편향 학습 위험  
- 대규모 파라미터·컴퓨팅 자원 의존  

## 3. 일반화 성능 향상 가능성  
- **도메인 전이 우수**: Reddit→뉴스(Q&A, 기술기사) 요약 시 지도학습 대비 우수한 전이 성능 관찰  
- **인간 선호 데이터의 다양성**이 모델의 일반화 핵심: 정형화된 ROUGE 최적화와 달리, 다양한 비교 샘플로 학습  
- 모델·데이터 증가 시 RM 예측력 향상: 파라미터 두 배 ↑ → 정확도 약 +1.8%, 데이터 두 배 ↑ → +1.1%[Fig. 6]  

## 4. 향후 연구 영향 및 고려사항  
- **다양한 피드백 형태**(편집, 등급, 설명) 통합 연구  
- **피드백 효율화**: 레이블 사용 최소화 및 약한 전문가 활용  
- **고차원·장문 생성**에서 RL 기법의 적용성 확장  
- **윤리·편향 문제**: 레이블러 다양성 확보, 데이터 편향 완화  
- **과도 최적화 방지**: RM 일반화·안정성 기법 연구  

> 인간의 선호를 학습 목표로 활용함으로써 요약 품질 향상뿐 아니라, AI 시스템의 **목표 정렬(alignment)** 연구 방향에도 중요한 이정표를 제시하였다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/ee2627fe-69db-48d9-af9c-a20e4dc6d511/2009.01325v3.pdf
