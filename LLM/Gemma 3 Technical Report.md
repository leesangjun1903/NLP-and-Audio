# Gemma 3 Technical Report

### 1. 핵심 주장과 주요 기여 요약

**Gemma 3**는 Google DeepMind에서 개발한 차세대 경량 오픈 모델로서, 1B부터 27B 매개변수 규모의 다양한 크기로 제공됩니다. 이 모델의 핵심 주장은 **경량 모델임에도 불구하고 매개변수가 훨씬 큰 모델들과 경쟁 가능한 성능**을 달성한다는 것입니다.[1]

주요 기여는 다음 네 가지입니다:[2][1]

- **멀티모달 능력**: 텍스트, 이미지, 짧은 영상을 처리할 수 있는 통합된 인터페이스 제공
- **장문맥 처리**: 128K 토큰의 컨텍스트를 지원하면서 메모리 효율성 유지
- **다국어 지원**: 140개 이상의 언어를 처리 가능하도록 확장
- **효율적인 아키텍처 설계**: 소비자급 하드웨어(휴대폰, 노트북 등)에서 실행 가능

### 2. 해결 문제, 제안 방법, 모델 구조

#### 2.1 문제 정의

기존 대규모 언어 모델들이 직면한 주요 문제들은:[3]

- **메모리 비효율성**: 긴 컨텍스트 처리 시 KV 캐시의 메모리 사용량이 기하급수적으로 증가
- **제한된 멀티모달 능력**: 텍스트 중심의 모델로 시각적 정보 처리 능력 부족
- **다국어 성능 불균형**: 주요 언어 위주의 최적화로 저자원 언어의 성능 저하
- **하드웨어 비효율성**: 고사양 GPU를 필요로 하여 일반 개발자 접근성 제한

#### 2.2 핵심 제안 방법

**로컬-글로벌 어텐션 메커니즘**:[1][3]

기존의 전역 어텐션 만을 사용하는 구조에서 벗어나, 로컬 어텐션과 글로벌 어텐션을 교대로 배치하는 혁신적인 방식을 도입했습니다. 아키텍처는 **5개의 로컬 어텐션 레이어마다 1개의 글로벌 어텐션 레이어를 배치**합니다.[1]

각 메커니즘의 특성:

$$\text{Local Attention Span} = 1024 \text{ tokens}$$

$$\text{Global Attention Span} = 128K \text{ tokens (full context)}$$

$$\text{Layer Ratio} = 5:1 \text{ (local:global)}$$

이 구조를 통해 메모리 사용량을 획기적으로 감소시킵니다. 실험 결과, 전역 어텐션 전용 구성에 비해 메모리 사용량을 15% 이하로 유지하면서도 문맥 이해 성능을 보존합니다.[1]

**RoPE (Rotary Position Embedding) 개선**:[1]

글로벌 어텐션 레이어의 경우 RoPE 기본 주파수를 10k에서 1M으로 증가시켜 더 긴 시퀀스를 처리할 수 있도록 개선했습니다. 로컬 어텐션 레이어는 기존의 10k를 유지하여 계산 효율성을 보존합니다.

**QK-Norm 도입**:[1]

Gemma 2의 soft-capping을 대체하여 Query-Key 정규화를 적용했습니다. 이는 어텐션 가중치의 안정성을 높이고 수렴 성능을 개선합니다.

#### 2.3 모델 구조

Gemma 3는 기본적인 디코더 전용 Transformer 구조를 기반으로 하되, 다음과 같은 개선사항을 포함합니다:[1]

| 모델 크기 | 비전 인코더 | 임베딩 파라미터 | 비임베딩 파라미터 | 총 파라미터 |
|---------|----------|------------|-------------|----------|
| 1B | 0 | 302M | 698M | 1B |
| 4B | 417M | 675M | 3,209M | 4B+ |
| 12B | 417M | 1,012M | 10,759M | 12B+ |
| 27B | 417M | 1,416M | 25,600M | 27B+ |

**비전 인코더**: SigLIP 기반의 400M 파라미터 Vision Transformer를 사용하며, 4B, 12B, 27B 모델에서 공유됩니다.[1]

**Pan & Scan (PS) 알고리즘**:[1]

고정 해상도(896×896)의 한계를 극복하기 위해 채택된 적응형 윈도우 처리 알고리즘입니다. 이미지를 동일한 크기의 비중첩 크롭으로 분할하고, 각각을 896×896으로 리사이징하여 처리합니다.

결과적 성능 향상:

$$\text{DocVQA 성능 향상} = +8.2\%$$
$$\text{InfoVQA 성능 향상} = +17.0\%$$
$$\text{TextVQA 성능 향상} = +1.9\%$$

### 3. 성능 향상 및 한계

#### 3.1 성능 향상

**벤치마크 성능**:[1]

Gemma 3 instruction-tuned 모델의 주요 성능 지표:

| 벤치마크 | Gemma 3 4B | Gemma 3 12B | Gemma 3 27B | Gemini 1.5 Pro |
|---------|-----------|------------|-----------|--------------|
| MMLU-Pro | 43.6 | 60.6 | 67.5 | 67.3 |
| MATH | 75.6 | 83.8 | 89.0 | 77.9 |
| HiddenMath | 43.0 | 54.5 | 60.3 | 47.2 |
| SimpleQA | 4.0 | 6.3 | 10.0 | 8.6 |
| LiveCodeBench | 12.6 | 24.6 | 29.7 | 30.7 |

**Chatbot Arena 성능**:[1]

- **Gemma 3 27B-IT**: Elo 점수 1,338 (전체 순위 9위)
- **이전 모델 대비**: Gemma 2 27B (Elo 1,220) 대비 **118점 향상**
- **경쟁 모델 대비**: LLaMA 3.1 405B (1,257) 능가

특히 주목할 점은 작은 모델의 성능입니다. **Gemma 3 4B-IT는 Gemma 2 27B-IT와 경쟁 가능한 수준**의 성능을 달성했습니다.[1]

**일반화 능력 향상**:[2][1]

Gemma 3는 이전 버전 Gemma 2 대비 다음 영역에서 눈에 띄는 개선을 보였습니다:

- 수학 능력: 큰 폭의 향상
- 코드 능력: STEM 분야 전반에 걸친 개선
- 다국어 능력: 저자원 언어 포함 전반적 향상
- 추론 능력: 복잡한 작업 수행 성능 개선

#### 3.2 한계

**장문맥 성능 한계**:[1]

- 128K 토큰까지는 성능을 유지하지만, 그 이상의 컨텍스트 길이에서는 급격히 성능이 저하
- 특히 RULER 벤치마크에서 128K에서의 성능: Gemma 3 27B-IT는 66.0% (32K에서는 91.1%)

**작은 모델의 멀티모달 한계**:[1]

- 1B 모델은 비전 인코더를 포함하지 않음
- 작은 모델들의 복잡한 이미지 이해 작업에서의 성능 제약

**계산 비용**:[1]

긴 컨텍스트 처리 시에도 KV 캐시 메모리 요구사항:

$$\text{KV Cache (32K, 27B)} \approx 72.7 \text{ GB (bf16)}$$

양자화 후에도 상당한 메모리 필요:

$$\text{KV Cache (32K, 27B, Int4)} \approx 32.8 \text{ GB}$$

### 4. 일반화 성능 향상 가능성

#### 4.1 기술적 개선 요소

**지식 증류의 역할**:[1]

사전 훈련 및 후훈련 단계에서 지식 증류를 전면적으로 적용했습니다. 구체적으로:

$$\text{Sampled Logits per Token} = 256$$

교사 모델의 확률 분포를 학습하되, 샘플링되지 않은 로짓의 확률은 0으로 설정하고 정규화합니다. 흥미로운 발견은 **소형 모델을 학생으로 훈련할 때 정규화 효과 때문에 장기 훈련에서는 대형 교사 모델이 더 효과적**이라는 것입니다.[1]

**향상된 후훈련 레시피**:[1]

기존 RLHF의 한계를 극복하기 위해 다음 기법들의 개선된 버전을 조합:

- **BOND (Best-of-N Distillation)**: 교사 모델로부터의 최적 출력 증류
- **WARM (Weight Averaged Reward Models)**: 보상 모델의 앙상블
- **WARP (Weight Averaged Rewarded Policies)**: 정책 가중치 평균화

이들 기법을 통해 다음 영역에서 동시 향상:

- 수학 문제 해결 능력
- 코딩 작업 성능
- 추론 능력
- 지시따르기 정확도
- 다국어 처리 능력

#### 4.2 데이터 기반 개선

**다국어 데이터 확충**:[1]

- 기존 Gemma 2 대비 다국어 데이터 비중 대폭 증가
- 언어 샘플링 불균형 처리를 위해 Unimax 방식 활용
- 결과: 140개 이상 언어에서 경쟁력 있는 성능

**데이터 필터링**:[1]

- 개인정보 제거 및 감수성 데이터 필터링
- 저품질 데이터 감지 및 제거 (quality reweighing)
- 평가 셋 오염 제거 (decontamination)
- 기억화 위험 감소를 위한 민감 출력 최소화

#### 4.3 아키텍처 선택의 일반화 영향

**로컬-글로벌 비율 최적화**:[1]

절제(ablation) 연구에서 로컬-글로벌 비율이 1:3부터 5:1까지 변해도 **혼란도(perplexity)에 최소한의 영향**을 미치는 것으로 확인되었습니다. 이는 로컬 어텐션이 단기 의존성을 효과적으로 포착하면서도, 글로벌 어텐션이 장거리 정보를 충분히 처리할 수 있음을 의미합니다.

**슬라이딩 윈도우 크기 선택**:[1]

로컬 어텐션의 슬라이딩 윈도우 크기를 512부터 4096까지 변화시켜도 혼란도 변화는 최소였으며, 최종적으로 **1024 토큰**을 선택함으로써 메모리 효율성과 성능의 균형을 달성했습니다.

### 5. 기억화(Memorization) 및 프라이버시

**획기적인 기억화 감소**:[1]

Gemma 3 모델들은 기존 모델들과 비교하여 현저히 낮은 기억화 비율을 보였습니다:

- 정확 기억화(Exact Memorization): Gemma 3는 이전 모델들 (Gemma 2, Gemini 1.5)보다 로그 스케일로 훨씬 낮음
- 근사 기억화(Approximate Memorization): 대부분의 기억화가 편집 거리 10 이내의 근사 형태
- 개인정보 포함 비율: 모든 Gemma 3 모델에서 기억화된 출력에 개인정보 없음

### 6. 안전성 및 책임감 있는 개발

**안전 정책 통합**:[1]

모델은 다음과 같은 안전 정책을 준수하도록 훈련됨:

- 아동 학대 및 착취 방지
- 개인 식별 정보 유출 방지
- 혐오 발언 및 괴롭힘 방지
- 자해 또는 해로운 행동 지도 방지
- 성적 명시적 콘텐츠 생성 방지
- 과학적 합의에 위배되는 의료 조언 제공 방지

**ShieldGemma 2의 역할**:[1]

Gemma 3를 기반으로 한 4B 이미지 안전 분류기로, 위험한 콘텐츠, 성적 명시적 콘텐츠, 폭력 콘텐츠 등을 분류합니다.

### 7. 향후 연구에 미치는 영향

#### 7.1 현황 분석 (2025년 최신 연구 기반)

**LLM 후훈련 기술의 진화**:[4]

2025년 현재, LLM 개발 커뮤니티에서는 다음과 같은 트렌드가 관찰됩니다:

- **PPO의 한계 극복**: DPO (Direct Preference Optimization)와 GRPO (Generalized Reward Policy Optimization) 등 새로운 강화학습 기법 등장
- **합성 데이터의 중요성**: OpenAI와 다른 연구팀들이 합성 데이터 생성과 반복적 개선에 집중
- **효율성 극대화**: 기존 구조에서 훈련 후(post-training)를 통해 **2-5배 더 많은 가치 추출** 가능

Gemma 3의 향상된 RLHF 기법들 (BOND, WARM, WARP)은 이러한 최신 트렌드와 정렬되어 있으며, 업계 표준을 제시하는 역할을 합니다.

#### 7.2 일반화 성능 향상에 대한 시사점

**아키텍처 설계의 중요성**:

로컬-글로벌 어텐션 비율이 모델의 일반화 능력에 미치는 영향에 대한 체계적인 절제 연구는 향후 경량 모델 개발에서 메모리 효율성과 성능 간의 균형을 맞추는 데 중요한 기준을 제시합니다.[1]

**지식 증류의 재평가**:

장기 훈련에서 대형 교사 모델이 소형 모델 교사보다 효과적이라는 발견은 모델 스케일링과 증류 전략에 대한 기존 가정을 도전합니다. 이는 향후 연구에서 교사-학생 모델 크기의 적정 비율을 재검토하도록 유도합니다.[1]

**멀티모달 학습의 확장성**:

SigLIP 인코더와 Pan & Scan 알고리즘의 조합을 통해 이미지 해상도 처리의 메모리 효율성 문제를 해결한 것은, 멀티모달 경량 모델 개발의 새로운 가능성을 제시합니다.[1]

#### 7.3 향후 연구 시 고려할 점

**1. 장문맥 처리의 한계 극복**

현재 128K 토큰 이상에서 성능 저하가 발생합니다. 향후 연구는:[1]

- RoPE 기본 주파수의 더욱 정교한 조정
- 적응형 위치 인코딩 기법 개발
- 계층적 컨텍스트 모델링 접근법 탐구

**2. KV 캐시 메모리 문제 심화**

긴 컨텍스트 처리 시 KV 캐시가 여전히 상당한 메모리를 필요로 합니다. 해결책:

- 다중 쿼리 어텐션(Multi-Query Attention)의 더욱 공격적인 적용
- 스파스(Sparse) 어텐션 메커니즘의 고도화
- KV 캐시 동적 압축 기법 개발

**3. 1B 모델의 멀티모달 확장**

현재 1B 모델은 비전 인코더를 포함하지 않습니다. 향후:

- 더욱 경량화된 비전 인코더 개발
- 모듈식 아키텍처 설계로 유연성 증대
- 리소스 제약 환경에서의 멀티모달 처리 최적화

**4. 다국어 성능의 정밀화**

140개 언어 지원은 적용되었으나, 저자원 언어에서의 성능 격차는 여전합니다:

- 언어 샘플링 전략의 고도화
- 교차언어 전이 학습(cross-lingual transfer) 강화
- 문화 특수적 정보 처리 능력 개선

**5. 계산 효율성 최적화**

양자화 기법의 발전:[5]

- Int4 양자화로 메모리 사용량 75% 감소 가능
- SFP8 등 새로운 양자화 포맷 활용
- 양자화 인식 훈련(QAT) 기법의 정밀화

**6. 안전성과 성능의 동시 달성**

기억화 감소와 안전 필터링이 성능 저하를 초래할 가능성:

- 안전-성능 트레이드오프의 수학적 분석
- 선택적 필터링 전략 개발
- 안전과 기능성의 최적 균형점 탐색

### 결론

Gemma 3 기술 보고서는 경량 모델에서도 **아키텍처 혁신, 효율적인 훈련 기법, 세심한 데이터 관리**를 통해 대형 모델에 필적하는 성능을 달성할 수 있음을 명확히 입증했습니다. 특히 로컬-글로벌 어텐션 메커니즘은 메모리 효율성과 장문맥 이해의 이진 선택이 아닌 **최적화 가능한 균형**임을 보여줍니다.[2][1]

2025년 현재의 AI 연구 트렌드에 비추어 볼 때, Gemma 3의 기여는 단순한 모델 개선을 넘어 **AI 기술의 민주화와 효율적 스케일링의 새로운 패러다임**을 제시합니다. 향후 연구는 이 기초 위에서 더욱 정교한 아키텍처, 고급 후훈련 기법, 그리고 멀티모달 능력의 확장을 추구할 것으로 예상됩니다.[4][2]

***

**주요 참고 자료**

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0c33f31a-6446-4b4c-a1ea-506901878ed9/2503.19786v1.pdf)
[2](https://datasciencebeehive.tistory.com/252)
[3](https://bits-bytes-nn.github.io/paper%20reviews/multimodal-learning/2025/03/25/gemma-3-technical-report.html)
[4](https://macaron.im/ko/blog/post-training-llm-techniques-2025)
[5](https://news.hada.io/topic?id=20444)
[6](https://arxiv.org/pdf/2407.21772.pdf)
[7](http://arxiv.org/pdf/2403.11807v1.pdf)
[8](https://arxiv.org/html/2404.07839)
[9](https://arxiv.org/html/2503.19786)
[10](http://arxiv.org/pdf/2407.07726v1.pdf)
[11](https://pmc.ncbi.nlm.nih.gov/articles/PMC11528166/)
[12](http://arxiv.org/pdf/2403.05530.pdf)
[13](https://arxiv.org/pdf/2404.01331.pdf)
[14](https://digitalbourgeois.tistory.com/877)
[15](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/gemini-v1-5/)
[16](https://velog.io/@aldente0630/2024-AI-ML-Blog-Highlights-CV)
[17](https://www.chatpaper.ai/ko/papers/2025-05-21)
[18](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/gvl/)
[19](https://news.hada.io/topic?id=23129)
[20](https://blog.naver.com/PostView.naver?blogId=makersungt&logNo=223743430763)
