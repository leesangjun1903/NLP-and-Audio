# Large Language Models Can Self-Improve

**핵심 주장 및 기여**  
이 논문은 대규모 언어 모델(LLM)이 **외부 레이블 없이** 스스로 생성한 Chain-of-Thought(CoT) 추론 경로를 활용해 자기 자신을 미세조정(self-improve)함으로써, 지도 레이블 없이도 추론 능력과 일반화 성능을 크게 향상시킬 수 있음을 보인다.[1]

***

## 1. 해결하려는 문제  
기존 LLM의 성능 향상은 대규모 고품질 지도 데이터에 의존하며, 데이터 수집 및 주석 작업에 큰 비용이 든다. 인간은 메타인지(metacognition)를 통해 외부 피드백 없이 스스로 학습하는데, 이를 LLM에도 적용하여 **무감독 환경**에서 추론 능력을 향상시키고자 한다.[1]

***

## 2. 제안 방법  
### 2.1 여러 추론 경로 생성 및 필터링  
- 입력 질문에 대해 **few-shot CoT** 예시를 제시하고, 샘플링 온도 $$T>0$$로 $$m$$개의 서로 다른 추론 경로와 답안을 생성.  
- 다수결(self-consistency)로 “가장 일관된” 답 $$\tilde{y}$$를 선택하고, 이 답을 산출한 추론 경로만 **고신뢰(self-consistent)** 경로로 여겨 학습 데이터로 사용.[1]

### 2.2 Mixed-Format 학습 샘플  
고신뢰 추론 경로를 네 가지 형식으로 증강하여 오버피팅을 방지:  
  1. CoT 예제 포함 → 전체 추론·답안 출력  
  2. 표준 QA 예제 포함 → 정답만 출력  
  3. CoT 예제 없이 “Let’s think step by step.” → 전체 추론·답안 출력  
  4. CoT 예제 없이 질문만 → 정답만 출력.[1]

### 2.3 스스로 질문·프롬프트 생성  
- **질문 생성:** 기존 질문을 연결해 모델로부터 새로운 질문 생성 후, self-consistency로 신뢰 높은 문항만 필터링.  
- **프롬프트 생성:** “A: Let’s think step by step.”로 zero-shot CoT 예시를 모델이 자체 생성.[1]

***

## 3. 모델 구조 및 학습 설정  
- **모델:** PaLM-540B (5400억 매개변수)  
- **학습:** 전체 데이터셋에 대해 $$m=32$$, fine-tuning 10K 스텝, 학습률 $$5\times10^{-5}$$, 배치 크기 32  
- **디코딩 온도:** self-consistency 전후 각각 $$T=0.7$$, $$T=1.2$$ 사용.[1]

***

## 4. 성능 향상  
| 데이터셋       | Baseline Self-Consistency | LMSI 후 Self-Consistency 개선폭 |
|---------------|--------------------------|-----------------------------|
| GSM8K         | 74.4%                    | 82.1% (+7.7%)              |
| DROP          | 78.2%                    | 83.0% (+4.8%)              |
| OpenBookQA    | 90.0%                    | 94.4% (+4.4%)              |
| ANLI-A3       | 63.4%                    | 67.9% (+4.5%)              |

- Out-of-Domain 과제에서도 CoT-prompting accuracy가 대체로 4% 이상 향상되어, 모델의 **일반화 능력**이 크게 개선됨을 확인.[1]

***

## 5. 일반화 성능 개선 강조  
- **Multi-task self-training:** 6개 인도메인 과제 합쳐 fine-tuning 후, 6개 OOD 과제에서 일관된 성능 향상 관찰.[1]
- **하이퍼파라미터:** LMSI 이후 더 높은 온도 $$T=1.2$$가 적합하며, 샘플 수 $$m\ge15$$만으로도 우수한 성능 달성.[1]
- **소형 모델로 지식 증류:** LMSI로 학습된 540B 모델의 CoT 데이터로 62B, 8B 모델을 fine-tuning 시, 각각 한 단계 상위 모델보다 우수한 성능을 보임.[1]

***

## 6. 한계 및 고려 사항  
- 모델 크기에 따라 self-improvement 효과가 다르며, 소형 모델은 이득이 제한적일 수 있음.  
- self-generated 질문·프롬프트의 품질 의존성 존재: 실제 학습 질문보다 다소 낮은 성능 개선폭 관찰.[1]
- 완전히 무감독으로 학습된 데이터에 내포된 편향·오류가 fine-tuning 과정에서 증폭될 위험.

***

## 7. 향후 연구 영향 및 고려점  
- **비지도 학습 확장:** 자가생성 데이터+소량 지도 데이터 혼합 방식 연구로 비용 효율적 모델 개선 가능.  
- **추론 정제 기법 통합:** 더 고품질 CoT 추론 생성(예: 투표 검증, 트리 추론)과 결합 시 추가 성능 상승 기대.  
- **다국어·다모달 일반화:** 자가 학습 방식이 언어 범위·문서 형식에 걸쳐 확장될 수 있는지 탐색 필요.  

이 논문은 LLM의 **무감독 자가 개선(self-improving)** 가능성을 입증하며, 데이터 효율적 학습과 광범위한 일반화를 위한 새로운 연구 방향을 제시한다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b9568d32-d8bf-4ffa-9336-2d3ebb38d849/2210.11610v2.pdf)
