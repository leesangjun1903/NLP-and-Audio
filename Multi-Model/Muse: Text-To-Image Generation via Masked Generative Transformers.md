# Muse: Text-To-Image Generation via Masked Generative Transformers

### 1. 핵심 주장과 주요 기여 요약

Muse 논문은 **마스킹된 이미지 모델링(Masked Image Modeling)**을 기반으로 한 새로운 텍스트-이미지 생성 패러다임을 제시합니다. 핵심 주장은 다음과 같습니다:[1]

**핵심 주장**

Muse는 이산 토큰 공간에서의 마스킹 작업을 통해 사전학습된 대형 언어 모델(LLM)의 텍스트 임베딩으로 조건화된 이미지를 생성할 수 있다는 것입니다. 이 접근 방식은 확산 모델(Diffusion)이나 자기회귀 모델(Autoregressive) 대비 다음의 장점을 제공합니다:[1]

- **확산 모델 대비**: 이산 토큰 사용 및 샘플링 반복 감소로 인한 효율성 향상
- **자기회귀 모델 대비**: 병렬 디코딩으로 인한 추론 속도 개선[1]

**주요 기여**

1. **성능과 효율성의 우월성**: CC3M에서 FID 점수 6.06으로 새로운 SOTA 달성, COCO 제로샷 평가에서 FID 7.88 및 CLIP 점수 0.32 달성[1]

2. **추론 속도**: Imagen-3B 및 Parti-3B 대비 **10배 이상 빠르며**, Stable Diffusion v1.4 대비 약 **3배 빠름**[1]

3. **제로샷 편집 기능**: 모델 미세조정 없이 인페인팅(inpainting), 아웃페인팅(outpainting), 마스크-프리 편집을 직접 지원[1]

***

### 2. 문제 정의, 제안 방법, 모델 구조

**해결하는 문제**

기존 텍스트-이미지 생성 모델들은 다음의 한계를 가지고 있었습니다:[1]

- **확산 모델**: 픽셀 공간에서 작동하며 매우 많은 샘플링 반복이 필요 (수백 단계)
- **자기회귀 모델**: 순차적 생성으로 인해 추론 시간이 길어짐 (256 또는 4096 단계)
- 효율성과 품질의 트레이드오프 문제

**제안 방법**

Muse는 마스킹된 이미지 모델링을 통해 이를 해결합니다. 핵심 수식은 다음과 같습니다:

$$\ell_g = (1 + t)\ell_c - t\ell_u$$

이는 **분류기 없는 가이드(Classifier-Free Guidance, CFG)** 공식으로, 조건부 로짓 $\ell_c$와 무조건부 로짓 $\ell_u$를 보간하여 생성 품질과 텍스트 정렬을 개선합니다. 가이드 스케일 $t$를 샘플링 과정 중 선형적으로 증가시켜 초기 토큰의 다양성을 유지하면서 후기 토큰의 조건 영향력을 높입니다.[1]

**마스킹 전략**

Muse는 코사인 스케줄링 기반의 가변 마스킹 비율을 사용합니다:[1]

$$p(r) = \frac{2}{\pi}(1-r^2)^{-1/2}$$

여기서 $r \in $은 마스킹 비율이며, 예상 마스킹 비율은 0.64입니다. 이는 자기회귀 모델과 다르게 **임의의 토큰 부분집합에 대한 조건부 분포 $P(x_i|x_\Lambda)$**를 학습하게 하며, 이는 병렬 샘플링과 제로샷 편집을 가능하게 합니다.

**병렬 디코딩 알고리즘**

추론 시 코사인 스케줄을 기반으로 각 단계에서 가장 신뢰도 높은 마스킹 토큰들을 선택적으로 예측합니다:[1]

- **베이스 모델**: 256개 토큰을 **24 단계**에서 생성 (자기회귀 방식 256 단계 대비)
- **슈퍼레지올루션 모델**: 4096개 토큰을 **8 단계**에서 생성 (자기회귀 방식 4096 단계 대비)

**모델 구조**

Muse는 다단계 구조로 구성됩니다:[1]

1. **텍스트 인코더**: 사전학습된 **T5-XXL** 모델 (고정됨, 4.6B 파라미터)으로부터 4096 차원의 언어 임베딩 추출

2. **이미지 토큰화 (VQGAN)**: 이산 토큰으로 이미지 인코딩
   - 저해상도: 256×256 → 16×16 토큰 (f=16 다운샘플링 비율)
   - 고해상도: 512×512 → 64×64 토큰 (f=8 다운샘플링 비율)
   - 코드북 크기: 8192

3. **베이스 모델**: Masked Transformer (632M-3B 파라미터)
   - 구조: 48개 트랜스포머 레이어 (3B 모델 기준)
   - 은닉 차원: 2048, MLP 차원: 8192
   - 텍스트→이미지 크로스-어텐션 + 이미지 토큰 간 셀프-어텐션
   - 손실함수: 크로스-엔트로피 손실로 마스킹된 토큰 예측

4. **슈퍼레지올루션 모델**: 다축 트랜스포머 (32개 레이어)
   - 저해상도 16×16 토큰을 고해상도 64×64 토큰으로 변환
   - 텍스트 조건 유지 (선형 프로젝션을 통한 저해상도 토큰과의 연결)

5. **디코더 미세조정**: VQGAN 디코더의 잔차 레이어와 채널 증가로 세부 정보 향상 (인코더, 코드북, 트랜스포머 고정)

***

### 3. 성능 향상 및 한계

**성능 향상 결과**

정량적 평가:[1]

| 벤치마크 | 모델 | FID | CLIP |
|---------|------|-----|------|
| CC3M | Muse (632M+268M) | **6.06** | 0.26 |
| COCO 제로샷 | Muse-3B | **7.88** | **0.32** |
| COCO 제로샷 | Imagen-3.4B | 7.27 | 0.27 |
| COCO 제로샷 | Parti-20B | 7.23 | - |

정성적 평가:[1]

- **기수성(Cardinality) 이해**: 객체 개수 정확히 렌더링 (변화와 컨텍스트 이해)
- **공간 관계 이해**: "위", "왼쪽" 등 전치사 표현 정확히 반영
- **스타일 생성**: Rembrandt, 팝 아트 등 다양한 미술 양식 재현
- **텍스트 렌더링**: 단어와 구문을 이미지에 직접 렌더링
- **프롬프트 활용**: 긴 프롬프트의 모든 요소 통합 생성
- **인간 평가**: Stable Diffusion v1.4 대비 **2.7배 우수한 텍스트 정렬** (PartiPrompts 벤치마크)[1]

**한계**

Muse의 주요 한계는 다음과 같습니다:[1]

1. **장문 텍스트 렌더링**: 여러 단어로 된 구문 렌더링 시 단어 중복이나 부분 렌더링 문제
2. **높은 기수성(Cardinality)**: 객체 개수가 증가할수록 정확도 감소 (예: 10개 와인병 중 7개만 렌더링)
3. **다중 기수성**: "4마리 고양이와 3마리 개" 같은 복수의 기수성 표현에서 적어도 하나의 기수성이 잘못될 가능성
4. **세부 공간 관계**: 객체 간의 정교한 기하학적 관계 표현의 한계

***

### 4. 일반화 성능 향상 가능성 (중점)

**현재 일반화 성능 분석**

Muse의 일반화 능력은 여러 측면에서 강력함을 보입니다:[1]

1. **제로샷 성능**: CC3M으로 학습한 모델이 COCO에서 높은 성능 유지 (FID 7.88)
2. **LLM 활용의 효과**: T5-XXL의 풍부한 의미 표현이 미지의 객체와 개념에 대한 일반화 향상
3. **병렬 디코딩**: Markov 가정(많은 토큰이 다른 토큰이 주어질 때 조건부 독립)으로 **임의 부분집합 조건화** 지원

**일반화 향상 가능성**

2024년 최신 연구 기반으로 이 영역에서의 개선 방향은:[2][3][4][5]

1. **스케일링 법칙**: 모델 크기, 데이터 크기, 계산량 간 최적 스케일링 관계 이해
   - 텍스트-이미지 모델에서 **캡션 밀도와 다양성이 데이터셋 크기보다 중요**[5]
   - 트랜스포머 깊이 증가가 채널 수 증가보다 효율적[5]

2. **전이학습**: Transfer Guided Diffusion Process(TGDP) 등 새로운 전이학습 방법 개발[4][6]
   - 사전학습된 모델을 플러그-앤-플레이 선행지식으로 활용
   - 제한된 데이터의 타겟 도메인에 더 효과적인 적응[4]

3. **개선된 토큰화**: ViT-VQGAN 기반의 더 강력한 이미지 토큰화[7]
   - 순수 컨볼루션 기반 VQGAN보다 ViT 기반 토큰화가 더 효율적
   - 재구성 충실도 향상 (FID 17.04 → 4.17)[7]

4. **마스킹 전략의 최적화**: 
   - 계층별 마스킹 비율 조정을 통한 성능 향상
   - 적응형 마스킹으로 중요 영역에 집중[8]

5. **도메인 적응**: 
   - 다중 도메인 학습으로 일반화 향상
   - 합성 데이터와 실제 데이터 혼합 학습[5]

**구체적 개선 메커니즘**

Muse의 일반화를 강화하는 메커니즘:[2][1]

- **사전학습 LLM의 표현력**: Muse는 T5-XXL의 선형 매핑 가능성(linear mappability) 활용으로 언어와 비전 개념 간 효율적 전이[1]
- **마스킹 기반 학습의 유연성**: $P(x_i|x_\Lambda)$ 학습을 통해 다양한 부분집합 조건에 강건한 모델 구축[1]
- **계층적 생성**: 베이스→슈퍼레지올루션 구조가 저수준 의미→고수준 세부사항 순서로 학습하여 개념적 일반화 향상

***

### 5. 연구 영향 및 향후 고려사항

**학술적 영향**

Muse의 발표(2023년 1월)는 다음과 같은 후속 연구 방향을 촉발했습니다:[9][10][1]

1. **마스킹 기반 생성 모델의 재평가**: MaskGIT의 성공에 이어 Muse는 **마스킹이 확산보다 우월할 수 있음**을 입증[8]

2. **경량 복제본 출현**: aMUSEd (2024)는 Muse 파라미터의 10%만으로 유사한 성능 달성 시도[10]

3. **다중모달 확장**: MUSES (2024)는 3D 제어 가능한 이미지 생성으로 Muse 아이디어 확장[9]

**향후 연구 시 고려사항**

1. **기수성 및 공간 관계 해결**:
   - 구조화된 토큰 표현 (예: 그래프 기반 구조)
   - 명시적 카운팅 메커니즘 통합
   - 공간 관계 사전훈련 태스크 추가

2. **계산 효율성 극대화**:
   - 적응형 디코딩 단계 선택 (고정 단계 대신)
   - 동적 토큰 스케줄링으로 필요한 영역에만 집중
   - 토큰 프루닝(pruning) 기법 도입

3. **일반화 성능 강화**:
   - 다국어 및 문화 간 일반화 (현재 영어 중심)
   - 드물고 새로운 개념에 대한 강건성
   - 도메인 외 샘플(out-of-distribution) 견고성

4. **확장성 및 실용성**:
   - 저자원 환경에서의 미세조정 전략
   - 온디바이스 추론 최적화
   - 멀티모달 입력 (이미지, 스케치 등) 지원

5. **윤리 및 편향 완화**:
   - 학습 데이터 편향 분석 및 제거
   - 공정성 평가 메트릭 개발
   - 유해 콘텐츠 생성 방지 메커니즘

6. **LLM 토큰 인코딩의 최적화**:
   - T5-XXL 대신 더 효율적인 언어 인코더 탐색
   - 다국어 LLM 통합
   - 도메인별 특화 LLM 활용

**2024년 최신 동향 반영**

최근 연구는 다음 방향으로 진행 중입니다:[11][4][5]

- **DiT(Diffusion Transformer) 확산 모델의 성공**: U-ViT 기반 아키텍처가 더 효율적인 확산 생성 실현[11]
- **스케일링 법칙의 체계적 연구**: 모델-데이터-계산 간 최적 트레이드오프 정량화[5]
- **전이학습 개선**: 제한된 데이터에서 사전학습 모델 활용의 이론적 기초 확립[6][4]

***

## 결론

Muse는 **마스킹 기반 생성 모델이 확산과 자기회귀 방식 모두를 능가할 수 있음**을 입증한 중요한 논문입니다. 특히 **10배 이상의 속도 향상**, **제로샷 편집 기능**, **높은 텍스트-이미지 정렬**(CLIP 0.32)은 실용적 응용을 크게 확대했습니다.[1]

그러나 기수성 표현, 장문 텍스트 렌더링, 복합 공간 관계 등의 한계는 향후 연구 과제로 남아 있습니다. **일반화 성능은 LLM 활용, 적응형 마스킹, 계층적 생성 구조 등을 통해 지속적으로 개선될 여지가 크며**, 최신 스케일링 법칙 연구와 전이학습 기법들이 이를 가능하게 할 것으로 예상됩니다.[2][4][5]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/bf90ab49-86c6-49b4-a93b-5fe5479c98de/2301.00704v1.pdf)
[2](https://arxiv.org/pdf/2305.18455.pdf)
[3](http://arxiv.org/pdf/2412.17162.pdf)
[4](https://arxiv.org/abs/2405.16876)
[5](https://arxiv.org/abs/2404.02883)
[6](https://openreview.net/forum?id=6emETARnWi)
[7](https://arxiv.org/abs/2110.04627)
[8](https://velog.io/@eunjnnn/%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC-MaskGIT-Masked-Generative-Image-Transformer)
[9](https://arxiv.org/html/2408.10605v2)
[10](https://arxiv.org/abs/2401.01808)
[11](https://openreview.net/forum?id=iG7qH9Kdao)
[12](https://arxiv.org/pdf/2301.00704.pdf)
[13](http://arxiv.org/pdf/2105.13290.pdf)
[14](https://arxiv.org/pdf/2204.14217v1.pdf)
[15](http://arxiv.org/pdf/2203.13131.pdf)
[16](https://arxiv.org/html/2409.14704v1)
[17](https://arxiv.org/pdf/2406.07753.pdf)
[18](https://icml.cc/virtual/2023/poster/25125)
[19](https://proceedings.mlr.press/v202/chang23b/chang23b.pdf)
[20](https://arxiv.org/abs/2301.00704)
[21](https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf)
[22](https://ar5iv.labs.arxiv.org/html/2301.00704)
[23](https://www.scribd.com/document/701121695/2301-00704)
[24](https://proceedings.mlr.press/v202/chang23b.html)
[25](https://proceedings.neurips.cc/paper_files/paper/2024/file/98b2b307aa4aa323df2ba3a83460f25e-Paper-Conference.pdf)
[26](https://ostin.tistory.com/146)
[27](https://arxiv.org/html/2411.19339v2)
[28](http://arxiv.org/pdf/2503.06698.pdf)
[29](https://arxiv.org/html/2503.06132v1)
[30](https://arxiv.org/html/2411.16725v1)
[31](https://arxiv.org/html/2412.00665v1)
[32](https://arxiv.org/html/2410.02667v1)
[33](https://openreview.net/pdf/63ce2022df1b6352ef17e394bb00cd416cf9497c.pdf)
[34](https://proceedings.neurips.cc/paper_files/paper/2024/file/f782860c2a5d8f675b0066522b8c2cf2-Paper-Conference.pdf)
[35](https://www.rapidops.com/ai-tracker/vqgan/)
[36](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf)
[37](https://arxiv.org/html/2405.16876v2)
