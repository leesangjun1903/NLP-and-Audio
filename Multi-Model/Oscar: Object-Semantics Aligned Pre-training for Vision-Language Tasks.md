# Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks

## 핵심 주장과 주요 기여

Oscar 논문의 **핵심 주장**은 이미지에서 검출된 객체 태그를 "앵커 포인트(anchor points)"로 활용하여 비전-언어 모달리티 간의 의미적 정렬 학습을 크게 개선할 수 있다는 것입니다. 기존 VLP(Vision-Language Pre-training) 방법들이 단순히 이미지 영역 특징과 텍스트 특징을 연결하고 self-attention을 통해 무작정 정렬을 학습하는 반면, Oscar는 객체 태그를 매개체로 하여 보다 효율적이고 정확한 크로스 모달 표현 학습을 가능하게 합니다.[1]

**주요 기여**는 다음과 같습니다:[1]
- 비전-언어 이해 및 생성 작업을 위한 강력한 VLP 방법인 Oscar 제안
- 6개의 주요 V+L 벤치마크에서 새로운 SOTA 달성으로 기존 접근법 대비 상당한 성능 향상
- 객체 태그를 앵커 포인트로 활용한 크로스 모달 표현 학습의 효과성에 대한 광범위한 실험 및 분석 제공

## 해결하는 문제와 제안 방법

### 문제 인식

기존 VLP 방법들의 **두 가지 주요 문제점**을 지적합니다:[1]

1. **모호성(Ambiguity)**: Faster R-CNN을 통해 추출된 시각적 영역 특징들이 과도하게 샘플링되고 서로 겹치면서 시각적 임베딩에 모호성을 야기
2. **그라운딩 부족(Lack of grounding)**: 이미지 영역과 텍스트 간의 명시적 정렬 정보가 없어 약지도 학습 문제가 됨

### 제안 방법

Oscar는 **Word-Tag-Image 삼중체(triplet)** 구조를 도입합니다:[1]
- **w**: 텍스트의 단어 임베딩 시퀀스  
- **q**: 이미지에서 검출된 객체 태그의 단어 임베딩 시퀀스
- **v**: 이미지의 영역 벡터 집합

### 사전 훈련 목표 함수

두 가지 관점에서 설계된 손실 함수를 사용합니다:[1]

**1. Dictionary View - Masked Token Loss (MTL)**:

$$ L_{MTL} = -E_{(v,h) \sim D} \log p(h_i | h_{\\i}, v) $$

여기서 $$h = [w, q]$$이고, 15% 확률로 토큰을 마스킹하여 주변 토큰과 이미지 정보를 활용해 복원

**2. Modality View - Contrastive Loss**:

$$ L_C = -E_{(h',w) \sim D} \log p(y | f(h', w)) $$

여기서 $$h' = [q, v]$$이고, 50% 확률로 태그 시퀀스를 오염시켜 원본과 오염된 이미지 표현을 구분

**전체 사전 훈련 목표**:

$$ L_{Pre-training} = L_{MTL} + L_C $$

## 모델 구조

Oscar는 **Transformer 기반 아키텍처**를 사용하며, BERT의 초기화 매개변수를 활용합니다.[1]

**핵심 구조적 특징**:
- 입력을 modality view와 dictionary view 두 관점에서 해석
- 객체 태그가 언어와 시각 표현 공간 사이의 브리지 역할
- 위치 민감 영역 특징을 선형 투영으로 BERT와 같은 차원으로 변환
- OscarB (BERT-base, H=768)와 OscarL (BERT-large, H=1024) 두 변형 제공

## 성능 향상

Oscar는 **7개 downstream 작업**에서 광범위하게 평가되었으며, **6개 작업에서 새로운 SOTA를 달성**했습니다:[1]

### 주요 성능 개선
- **Image Retrieval**: R@1에서 5.8 포인트 향상
- **Text Retrieval**: R@1에서 6.9 포인트 향상  
- **Image Captioning**: CIDEr에서 10.7 포인트 향상
- **VQA**: 0.42 포인트 향상으로 73.82% 달성
- **NLVR2**: 0.87 포인트 향상

**효율성 측면**에서 Oscar base 모델이 기존 large 모델들을 능가하며, 6.5M 쌍으로 사전 훈련되어 UNITER(9.6M)나 LXMERT(9.18M)보다 적은 데이터로도 우수한 성능을 보였습니다.[1]

### Ablation Study 결과
객체 태그의 효과를 검증한 실험에서 태그를 사용한 경우가 baseline 대비 **수렴 속도가 2배 빨라지고 최종 성능도 크게 향상**되었습니다.[1]

## 일반화 성능 향상

Oscar의 **일반화 능력**은 특히 Novel Object Captioning (NoCaps) 작업에서 두드러집니다.[1]

### NoCaps에서의 강력한 일반화
- **Out-of-domain** 케이스에서 기존 SOTA 대비 훨씬 큰 성능 격차
- **Near-domain** 및 **out-of-domain** 상황에서 일관되게 우수한 성능
- 훈련 중 보지 못한 새로운 객체에 대한 캡션 생성 능력 향상

### 일반화 메커니즘
**사전 훈련된 언어 모델의 풍부한 의미 정보**를 활용하여 zero-shot 예측을 크게 개선합니다. 이는 DeViSE의 아이디어를 현대적 언어 모델 사전 훈련 시대에 맞게 재조명한 것으로, 사전 훈련된 언어 지식을 활용한 크로스 모달 전이 학습에서 의미 정렬과 샘플 효율성 향상에 매우 효과적임을 보여줍니다.[1]

### t-SNE 시각화를 통한 의미 공간 분석
Oscar는 **동일 객체의 시각적-텍스트 표현 간 거리를 크게 단축**시키고, **관련 의미를 가진 객체 클래스들을 더 가깝게 배치**하면서도 구별 가능하게 유지합니다. 이는 객체 태그가 크로스 모달 특징 학습에서 앵커 포인트 역할을 하여 정렬을 규제한다는 중요성을 입증합니다.[1]

## 한계점

논문에서 언급된 **주요 한계점**들:

1. **NLVR2 파인튜닝 아키텍처의 한계**: 현재 방식이 최적이 아니며 개선 여지가 있음을 인정[1]
2. **GQA에서의 제한적 성능**: NSM이 더 복잡한 추론 구조를 활용해 더 나은 성능을 보임[1]
3. **객체 검출기 의존성**: 더 정확한 객체 검출기 개발 시 성능 향상 가능성 시사[1]

## 향후 연구에 미치는 영향

### 방법론적 영향
1. **앵커 포인트 개념의 확산**: 객체 태그를 앵커 포인트로 활용하는 아이디어가 VLP 분야에서 새로운 연구 방향 제시
2. **효율적 사전 훈련**: 더 적은 데이터로도 우수한 성능을 달성할 수 있는 효율적 학습 방법론 확립
3. **멀티모달 임베딩 공간 설계**: 서로 다른 모달리티 간의 의미적 정렬을 위한 새로운 접근법 제안

### 향후 연구 시 고려사항

**기술적 개선 방향**:
- 보다 정교한 객체 검출 및 태깅 시스템 개발
- NLVR2와 같은 복합 추론 작업을 위한 아키텍처 최적화
- GQA 등 복잡한 시각적 추론을 위한 구조적 사전 지식 통합

**일반화 성능 강화**:  
- Zero-shot 및 few-shot 학습 능력 향상을 위한 언어 모델 활용 확대
- 도메인 간 전이 성능을 높이는 사전 훈련 전략 개발
- Novel object에 대한 이해 및 생성 능력 강화 방법론

**평가 및 분석**:
- 크로스 모달 정렬의 질을 측정하는 새로운 평가 지표 개발  
- 의미 공간에서의 표현 학습 품질에 대한 더 심층적 분석
- 다양한 도메인과 언어에서의 일반화 능력 검증

Oscar는 객체 태그를 매개로 한 효율적이고 효과적인 비전-언어 사전 훈련의 새로운 패러다임을 제시하며, 향후 멀티모달 AI 연구에 중요한 기여를 할 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7fe11e9f-e010-4726-9356-a9368602b712/2004.06165v5.pdf)
