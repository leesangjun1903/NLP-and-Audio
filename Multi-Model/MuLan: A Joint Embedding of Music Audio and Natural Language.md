# MuLan: A Joint Embedding of Music Audio and Natural Language

### 1. 핵심 주장 및 주요 기여[1]

**MuLan**은 음악 오디오를 제약 없는 자연어 음악 설명과 직접 연결하는 새로운 세대의 음향 모델의 첫 번째 시도입니다. 기존의 사전 정의된 온톨로지(고정된 음악 속성 또는 텍스트 쿼리)에 기반한 음악 태깅 및 콘텐츠 기반 검색 시스템과 달리, MuLan은 다음과 같은 혁신적 특징을 제시합니다.[1]

- **대규모 약한 감독 학습**: 4,400만 개의 음악 녹음(370,000시간)과 약한 관계의 자유형식 텍스트 주석으로 훈련
- **진정한 제로샷 기능**: 기존의 고정된 온톨로지를 포함하면서도 제로샷 기능 지원
- **범용 자연어 인터페이스**: 장르, 무드, 악기, 아티스트 등 임의의 음악 개념을 텍스트로 표현 가능

주요 기여는 크게 네 가지입니다: (1) 음악-텍스트 대규모 약한 감독 데이터셋 구축, (2) 두 개의 별개 임베딩 네트워크를 활용한 이중 타워 아키텍처 개발, (3) 전이 학습, 제로샷 음악 태깅, 교차 모달 검색 등에서 최첨단 성능 달성, (4) 음악 도메인의 언어 이해 능력 증진.

### 2. 문제점 정의, 제안 방법론 및 모델 구조

#### 2.1 해결하고자 하는 문제[1]

기존의 음악 검색 및 분류 시스템은 다음과 같은 한계가 있었습니다:

1. **고정된 온톨로지의 제약**: 사전에 정의된 제한된 음악 속성만 사용 가능
2. **데이터 부족**: 환경음 도메인과 달리 대규모 오디오-캡션 쌍 데이터의 부재
3. **제로샷 성능 부족**: 기존 음성-오디오 모델들의 제로샷 설정에서의 성능 부족
4. **불충분한 텍스트 다양성**: 음악을 설명하는 언어의 다양성을 충분히 포함하지 못함

MuLan은 이러한 문제들을 음악 도메인에 특화된 대규모 약한 감독 학습을 통해 해결합니다.

#### 2.2 학습 프레임워크 및 수식[1]

**기본 아키텍처**: 두 개의 분리된 임베딩 네트워크로 구성
- 오디오 임베딩 네트워크: $$f: \mathbb{R}^{F \times T} \rightarrow \mathbb{R}^d$$
- 텍스트 임베딩 네트워크: $$g: A^n \rightarrow \mathbb{R}^d$$

각 네트워크는 동일한 차원 $$d$$의 정규화된 임베딩 공간으로 끝나며, 가중치를 공유하지 않습니다.

**손실 함수**: Contrastive Multiview Coding (CMC) 손실을 사용합니다.[1]

$$L(B) = \sum_{i=1}^{B} -\log \frac{h[f(x^{(i)}), g(t^{(i)})]}{\sum_{j \neq i} h[f(x^{(i)}), g(t^{(j)})] + h[f(x^{(j)}), g(t^{(i)})]}$$

여기서 비평자 함수는 $$h[a, b] = \exp(a^T b / \tau)$$ (단, $$a, b \in \mathbb{R}^d$$, $$\tau \in (0,1]$$는 학습가능한 온도 하이퍼파라미터)입니다.

#### 2.3 모델 구조[1]

**오디오 인코더 (두 가지 옵션)**:
1. **ResNet-50**: 64개 멜 채널의 스펙트로그램을 입력받아 (64 × 1000) 패치로 처리. SpecAugment 적용. 평균 풀링 후 128차원 정규화 임베딩 생성
2. **AST (Audio Spectrogram Transformer)**: Vision Transformer 기반의 12개 Transformer 블록 (숨겨진 차원 768, 12개 주의 헤드). 128 × 1000 멜 스펙트로그램을 16×16 패치로 변환. [CLS] 토큰으로부터 최종 임베딩 추출

**텍스트 인코더**: BERT base-uncased[1]
- 12개 Transformer 블록 (숨겨진 차원 768, 12개 주의 헤드)
- 최대 토큰 길이 512
- [CLS] 토큰 임베딩을 128차원 공간으로 변환 및 정규화

**미니배치 구성 및 샘플링**: 각 미니배치는 다양한 데이터 소스로부터 처방된 비율로 구성 (SF:LF:PL:ASET = 2:2:1:1)

#### 2.4 데이터셋 마이닝[1]

총 4,400만 개의 30초 음악 클립으로부터 3가지 텍스트 소스 추출:

| 데이터 소스 | 설명 | 필터링 후 토큰수(B) | 평균 비디오당 주석수 |
|-----------|------|-----------------|------------|
| **단형(SF)** | 제목, 태그 | 5.4 | 29.6 |
| **장형(LF)** | 설명, 댓글 | 0.2 | 0.4 |
| **재생목록(PL)** | 재생목록 제목 | - | - |
| **AudioSet(ASET)** | 527개 모든 클래스 레이블 | - | 1.8 |

텍스트 필터링을 위해 700개 문장으로 미세조정된 BERT 기반 이진 분류기를 적용했습니다.

### 3. 성능 향상 및 실험 결과[1]

#### 3.1 제로샷 음악 태깅[1]

| 모델 | AudioSet Gen-25 | AudioSet Mu-141 | MTAT Top-50 | MTAT All-188 |
|------|-----------------|-----------------|------------|------------|
| M-AST (제로샷) | 0.840 | 0.909 | 0.778 | 0.776 |
| M-Resnet-50 (제로샷) | 0.840 | 0.899 | 0.782 | 0.772 |
| Hybrid [기준선] | 0.904 | 0.920 | 0.915 | 0.941 |
| JukeBox | - | - | 0.915* | - |

MTAT는 특히 "weird", "beats" 같은 다의성 태그와 "not rock", "no piano" 같은 부정 표현에서 성능 저하가 있었습니다. 이는 BERT의 알려진 부정 처리 한계입니다.

#### 3.2 전이 학습 (선형 프로브)[1]

| 모델 | AudioSet Gen-25 | AudioSet Mu-141 | MTAT Top-50 | MTAT All-188 |
|------|-----------------|-----------------|------------|------------|
| M-AST | 0.906 | 0.942 | 0.925 | 0.953 |
| M-Resnet-50 | 0.910 | 0.940 | 0.927 | 0.954 |

최첨단 전이 학습 성능을 달성하면서도 새로운 자연어 응용 프로그램 지원.

#### 3.3 텍스트 쿼리를 통한 음악 검색[1]

| 모델 | 제목 AUC | 제목 mAP | 설명 AUC | 설명 mAP |
|------|---------|---------|---------|---------|
| M-AST | 0.933 | 0.110 | 0.903 | 0.090 |
| M-Resnet-50 | 0.931 | 0.104 | 0.901 | 0.084 |
| AudioSet만 | 0.626 | 0.005 | 0.688 | 0.009 |

대규모 단형 태그 데이터 포함 시 AUC가 0.626에서 0.933으로 급상승했습니다.

#### 3.4 텍스트 삼중항 분류[1]

| 모델 | PlayList 정확도 | AudioSet 정확도 |
|------|----------------|-----------------|
| M-AST | 0.959 | 0.962 |
| M-Resnet-50 | 0.945 | 0.951 |
| SimCSE [기준선] | 0.950 | 0.938 |
| SBERT | 0.942 | 0.889 |
| BERT | 0.850 | 0.847 |

이들 결과는 제로샷 설정에도 불구하고 기존의 일반 목적 문장 임베딩 모델들을 능가합니다.

### 4. 모델의 일반화 성능 향상 가능성[1]

#### 4.1 데이터 다양성의 역할

실험에서 놀랍게도 **필터링되지 않은 데이터 사용이 필터링된 버전과 유사한 성능**을 보였습니다. 이는 대조 학습의 노이즈 강인성을 시사합니다. 필터링이 너무 공격적이어서 실제로 유용한 신호도 제거했을 가능성이 있습니다.

#### 4.2 텍스트 소스의 영향

텍스트 데이터 제거 실험 결과:
- **AudioSet만**: 제로샷 MTAT Top-50에서 0.753
- **AudioSet + SF**: 0.754
- **AudioSet + SF + LF**: 0.760
- **모든 데이터**: 0.782

더 많은 다양한 텍스트 소스 포함이 일반화 성능을 지속적으로 향상시킵니다.

#### 4.3 배치 크기와 대조 학습

대규모 배치 크기 (ResNet-50의 경우 B=6144, AST의 경우 B=5120)는 대조 손실 최적화에 중요하며, 이는 임베딩 공간의 품질을 향상시킵니다.

### 5. 한계점[1]

1. **부정 표현 처리 부족**: BERT의 알려진 한계로 "not rock", "no piano"와 같은 부정 표현에서 성능 저하
2. **다의성 태그**: "weird", "beats" 같은 다중 의미의 단어 처리 미흡
3. **제로샷-감독 갭**: 제로샷 설정에서 감독 학습과의 성능 격차 (예: MTAT Top-50에서 제로샷 0.782 vs 선형 프로브 0.927)
4. **약한 신호**: 많은 양의 노이즈가 있는 웹 데이터 사용으로 인한 신호 약화
5. **제한된 마이닝 범위**: 30초 클립만 사용하여 장형 음악 구조 모델링 제한

### 6. 최신 연구 기반 영향 및 향후 연구 방향[2][3][4]

#### 6.1 MuLan의 학계 영향

**인용 통계**: 2022년 8월 논문 발표 이후 237회 이상 인용되었습니다.

#### 6.2 후속 연구 발전

**MuQ-MuLan (2025년 1월)**: MuLan의 직접적 후속 연구로, 새로운 Mel Residual Vector Quantization (Mel-RVQ) 기반의 자감독 학습 모델을 제안하여:[2]
- MagnaTagATune 제로샷 음악 태깅에서 ROC-AUC **79.3** 달성 (원래 MuLan: 78.2)
- 훨씬 적은 데이터 (130K시간)로 학습하면서도 성능 향상
- 자감독 사전학습을 통해 일반화 성능 대폭 개선

**CLaMP 3 (2025년 2월)**: 다중 모달 및 다언어 일반화 문제 해결:[4]
- 악보, 연주 신호, 오디오 등 모든 주요 음악 모달리티를 다언어 텍스트와 동일 임베딩 공간에 정렬
- 2.31백만 개의 음악-텍스트 쌍 (M4-RAG) 데이터셋 구축
- 미학습 언어에 대한 강력한 다언어 일반화 능력 시연
- 음악 정보 검색(MIR) 다중 작업에서 최첨단 성능

**TTMR++ (2024년 10월)**: 메타데이터와 LLM을 활용한 텍스트-음악 검색 개선:[3]
- 미세조정된 LLaMA-7B를 통한 의사 캡션 생성
- 음악 메타데이터 지식 그래프 통합으로 아티스트-트랙 유사성 포함
- 1.3백만 개의 음악-텍스트 쌍으로 학습
- 제목, 설명, 트랙 및 아티스트 기반 검색 모두에서 성능 향상

#### 6.3 앞으로의 연구 방향

**1. 구조화된 텍스트 필터링 개선**: 논문에서 제안된 대로, 약한 신호와 절대 노이즈를 더 잘 구분하는 필터링 방법론 개발이 필수적입니다.

**2. 부정 및 다의성 표현 처리**: BERT의 한계를 극복하기 위해:
- 음악 도메인 특화 사전학습 모델 개발
- 부정 표현 처리 능력을 갖춘 텍스트 인코더 개선
- 도메인 특화 단어 임베딩 활용

**3. 다중 모달리티 통합**: CLaMP 3 같이 악보, 연주 신호, 비디오 등 추가 모달리티 포함으로 일반화 성능 향상

**4. 다언어 확장**: 전 세계의 음악 설명 다양성을 포함하는 다언어 모델 개발으로 교차 문화 음악 이해 능력 강화

**5. 자감독 학습 강화**: MuQ 같은 혁신적 자감독 목표(Mel-RVQ)를 통해 대규모 레이블 없는 데이터 활용도 증대

**6. 롱폼 음악 구조 모델링**: 현재 10-30초 클립 기반에서 전곡 수준의 장형 구조 이해로 확대

**7. 실시간 음악 검색 애플리케이션**: 개선된 모델을 라이브 스트리밍, 팟캐스트, 영상 콘텐츠 분석 등 실제 애플리케이션으로 배포

**8. 멀티태스크 학습**: 음악 태깅, 감정 분석, 장르 분류, 악기 식별 등 다양한 MIR 작업을 동시에 처리하는 통합 모델

#### 6.4 실용적 고려사항

- **계산 효율성**: 대규모 배치 학습의 메모리 요구사항 감소 방법 연구
- **오픈소스화**: 상용 제약이 있지만, 커뮤니티 기여 확대로 모델 접근성 향상
- **윤리적 고려**: 저작권 보호, 개인정보 보호, 편향성 제거에 대한 지속적 관심

### 결론

MuLan은 음악-텍스트 결합 임베딩 분야에서 획기적인 첫 시도로, 대규모 약한 감독 학습을 통해 제약 없는 자연어 인터페이스를 음악 오디오와 연결했습니다. 비록 부정 표현 처리, 다의성 해결, 제로샷-감독 갭 등의 한계가 있지만, 후속 연구들(MuQ-MuLan, CLaMP 3, TTMR++)이 이러한 문제들을 체계적으로 해결하고 있습니다. 특히 자감독 학습 강화, 다중 모달리티 통합, 다언어 확장은 향후 음악 정보 검색의 주요 방향이 될 것으로 예상됩니다.[3][4][2][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e6d9c673-8266-4db1-8a69-96a70db30366/2208.12415v1.pdf)
[2](http://arxiv.org/pdf/2501.01108.pdf)
[3](http://arxiv.org/pdf/2410.02271.pdf)
[4](https://aclanthology.org/2025.findings-acl.133/)
[5](https://journal.media-culture.org.au/index.php/mcjournal/article/view/3200)
[6](https://arxiv.org/pdf/2208.12415.pdf)
[7](http://arxiv.org/pdf/2410.03264.pdf)
[8](https://arxiv.org/pdf/2112.04214.pdf)
[9](http://arxiv.org/pdf/2407.19900.pdf)
[10](https://arxiv.org/html/2412.06660v1)
[11](https://arxiv.org/pdf/2210.03799.pdf)
[12](https://arxiv.org/abs/2208.12415)
[13](https://www.microsoft.com/en-us/research/video/a-cross-modal-audio-search-engine-based-on-joint-audio-text-embeddings/?locale=ko-kr)
[14](https://snu.elsevierpure.com/en/publications/mulan-a-joint-embedding-of-music-audio-and-natural-language)
[15](https://arxiv.org/abs/2502.10362)
[16](https://assemblyai.com/blog/recent-developments-in-generative-ai-for-audio)
[17](https://www.semanticscholar.org/paper/MuLan:-A-Joint-Embedding-of-Music-Audio-and-Natural-Huang-Jansen/df08b4b9a66dfa849dc8ac7c8fcf2900467f749e)
[18](https://arxiv.org/abs/1902.04397)
[19](https://zilliz.com/learn/top-10-most-used-embedding-models-for-audio-data)
[20](https://archives.ismir.net/ismir2022/paper/000067.pdf)
