
# SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing

## 1. 핵심 주장 및 주요 기여

**SpeechT5**는 T5(Text-To-Text Transfer Transformer)의 성공을 음성 영역으로 확장한 통합 모달 엔코더-디코더 프레임워크입니다. 이 논문의 핵심 주장은 다음과 같습니다:[1]

- 기존 음성 사전학습 모델들은 **단일 모달 학습**(음성 또는 텍스트만 사용)의 한계를 가지며, 모달리티 변환이 필요한 작업(예: ASR, TTS)에서 최적화되지 않음

- 모든 음성 처리 작업을 **"음성/텍스트 to 음성/텍스트"** 형식으로 통일함으로써 하나의 사전학습 모델로 다양한 작업을 처리 가능

- **크로스 모달 벡터 양자화(Cross-Modal Vector Quantization)** 기법을 통해 음성과 텍스트 표현을 공유 의미 공간으로 정렬

주요 기여는 다음과 같습니다:[1]

1. 음성 처리 작업을 위한 최초의 **통합 엔코더-디코더 프레임워크** 제안
2. 대규모 레이블 없는 음성 및 텍스트 데이터를 활용하는 크로스 모달 벡터 양자화 기법 개발
3. 6가지 음성 처리 작업(ASR, TTS, 음성 번역, 음성 변환, 음성 향상, 화자 인식)에서 최첨단 성능 달성

***

## 2. 문제 정의, 제안 방법 및 모델 구조

### 2.1 해결하는 문제

기존 음성 사전학습 방법들의 주요 문제점:[1]

- **모달리티 편중**: 음성 데이터만 활용하여 학습하므로 텍스트 정보의 중요성 간과
- **디코더 미활용**: 음성 인코더만 사전학습되어 음성-텍스트 생성 작업에 최적화되지 않음
- **작업 이질성**: 각 작업에 맞춘 별도의 모델 구축 필요

### 2.2 제안하는 방법

#### **모델 아키텍처**

SpeechT5는 다음 세 가지 주요 컴포넌트로 구성됩니다:[1]

1. **모달별 전/후 처리 네트워크(Pre/Post-nets)**
   - 음성 인코더 전 네트: wav2vec 2.0의 합성곱 특성 추출기
   - 음성 디코더 전 네트: 3개 완전 연결층 + x-vector 화자 임베딩
   - 음성 디코더 후 네트: 5개의 1차원 합성곱층 + 정지 토큰 예측
   - 텍스트 전/후 네트: 공유 임베딩 레이어

2. **공유 엔코더-디코더 백본**: 
   - 12개 Transformer 인코더 블록 + 6개 Transformer 디코더 블록
   - 모델 차원 768, FFN 차원 3,072, 12개 어텐션 헤드
   - 상대 위치 임베딩(Relative Position Embedding) 사용

3. **입출력 표현**:
   - 음성 입력: 원본 파형 $$X_s = (x_1^s, \ldots, x_{N_s}^s)$$
   - 음성 출력: 로그 Mel-필터뱅크 특성 $$X_f = (x_1^f, \ldots, x_{N_f}^f)$$
   - 텍스트 입출력: 문자 수준 시퀀스 $$X_t = (x_1^t, \ldots, x_{N_t}^t)$$

#### **사전학습 방법**

**(1) 음성 사전학습**

**양방향 마스크 예측(Bidirectional Masked Prediction)**:

$$L_{\text{mlm}}^s = \sum_{n \in M} \log p(z_n | \hat{H}, n)$$

여기서:
- $\hat{H}$: 마스크된 음성 특성 (8% 타임스텝 선택, 10 스텝 스팬 마스크)
- $M$: 마스크된 타임스텝 집합
- $z_n$: 클러스터링을 통해 생성된 음향 유닛 라벨

**시퀀스-투-시퀀스 생성 손실(L1 거리)**:

$$L_1^s = \sum_{n=1}^{N_f} \|y_n^f - x_n^f\|_1$$

여기서 $y_n^f$는 예측된 Mel-필터뱅크이고 $x_n^f$는 원본입니다.

정지 토큰 예측을 위한 이진 교차 엔트로피 손실 $L_{\text{bce}}^s$ 추가.

**(2) 텍스트 사전학습**

BART 방식의 텍스트 노이즈 제거:

$$L_{\text{mle}}^t = \sum_{n=1}^{N_t} \log p(y_n^t | y_{ < n}^t, \hat{X}^t)$$

여기서:
- 30%의 텍스트 스팬을 마스크 (Poisson 분포, λ=3.5)
- 각 스팬을 하나의 마스크 토큰으로 대체

**(3) 조인트 사전학습 - 크로스 모달 벡터 양자화**

**핵심 아이디어**: 음성과 텍스트 표현을 **공유 코드북**을 통해 정렬

**양자화 과정**:

$$c_i = \arg\min_{j \in [K]} \|u_i - c_j\|_2$$

여기서:
- $u_i$: 인코더 출력의 연속 표현
- $c_j$: 고정 크기 코드북 내 j번째 양자화 벡터
- $K$: 코드북 크기 (2개 코드북, 각 100 항목, 총 10,000 항목)

**컨텍스트와 양자화 표현 혼합**: 타임스텝의 10%를 양자화된 잠재 표현으로 무작위로 교체한 후 디코더의 크로스 어텐션에 사용.

**다양성 손실** (코드북 활용 최대화):

$$L_d = \frac{1}{K}\sum_{k=1}^{K} p_k \log p_k$$

여기서 $p_k$는 k번째 코드 선택의 평균 확률입니다.

**전체 사전학습 손실**:

$$L = L_{\text{mlm}}^s + L_1^s + L_{\text{bce}}^s + L_{\text{mle}}^t + \gamma L_d$$

여기서 $\gamma = 0.1$ (사전학습 중)

***

## 3. 성능 향상 결과

### 3.1 자동 음성 인식(ASR)

**LibriSpeech 100시간 데이터셋**:[1]

| 모델 | dev-clean | dev-other | test-clean | test-other |
|------|-----------|-----------|-----------|-----------|
| wav2vec 2.0 BASE | 6.1% | 13.5% | 6.1% | 13.3% |
| HuBERT BASE | 5.5% | 13.1% | 5.8% | 13.3% |
| **SpeechT5** | **4.3%** | **10.3%** | **4.4%** | **10.4%** |

**LM 융합 적용**:
- SpeechT5 (Transformer LM): 2.1% (clean) / 5.5% (other)
- wav2vec 2.0 BASE (Transformer LM): 2.2% / 6.3%

### 3.2 텍스트-음성 합성(TTS)

**LibriTTS 데이터셋**:[1]

| 모델 | Naturalness | MOS | CMOS |
|------|-------------|-----|------|
| Ground Truth | - | 3.87 ± 0.04 | - |
| Baseline | 2.76 | 3.56 ± 0.05 | 0 |
| **SpeechT5** | **2.91** | **3.65 ± 0.04** | **+0.29** |

### 3.3 음성 번역(ST)

**MUST-C 데이터셋**:[1]

| 모델 | EN-DE | EN-FR |
|------|-------|-------|
| ESPnet ST | 22.91 | 32.69 |
| Adapter Tuning | 24.63 | 34.98 |
| **SpeechT5** | **25.18** | **35.30** |

### 3.4 음성 변환(VC)

**CMU Arctic 데이터셋**:[1]

| 모델 | WER (bdl→slt) | WER (clb→slt) | MCD (bdl→slt) | MCD (clb→slt) |
|------|---------------|---------------|----------------|-----------------|
| VTN w/ TTS | 7.6% | 9.1% | 6.33 | 6.02 |
| **SpeechT5** | **7.8%** | **6.4%** | **5.93** | **5.87** |

### 3.5 음성 향상(SE)

**WHAM! 데이터셋**:[1]

| 모델 | WER |
|------|-----|
| 노이즈 음성 | 76.1% |
| Baseline | 10.9% |
| **SpeechT5** | **8.9%** (9% 상대 개선) |

### 3.6 화자 인식(SID)

**VoxCeleb1 데이터셋**:[1]

| 모델 | 정확도 |
|------|--------|
| HuBERT LARGE | 90.33% |
| SpeechNet (Multi-Task with TTS) | 87.90% |
| **SpeechT5** | **96.49%** |

***

## 4. 모델 일반화 성능 향상 분석

### 4.1 일반화 성능 향상 메커니즘

**1. 크로스 모달 학습의 시너지**

절제 연구(Ablation Study)에서 조인트 사전학습의 중요성이 입증되었습니다:[1]

| 변형 | ASR (clean) | VC (MCD) | SID (ACC) |
|------|-----------|----------|-----------|
| SpeechT5 | 4.4% | 5.93 | 96.49% |
| w/o Joint PT | 4.6% | 6.18 | 95.54% |
| w/o Text PT | 5.4% | 6.03 | 95.60% |
| w/o Speech PT | - | 6.49% | 38.61% |

음성 사전학습 제거 시 SID 성능이 95%에서 38%로 급락하여, **음성 인코더의 중요성**을 보여줍니다.

**2. 공유 의미 공간의 효과**

크로스 모달 벡터 양자화를 통해:
- 음성과 텍스트가 **동일 코드북으로 매핑**되어 모달리티 간 암묵적 정렬 학습
- 컨텍스트 상태와 양자화 표현의 **무작위 혼합** → 모델이 양쪽 정보를 모두 활용하도록 강제
- 이를 통해 음성-텍스트 변환 작업(ASR, TTS) 성능 대폭 향상

**3. 인코더-디코더 아키텍처의 이점**

기존 음성 인코더 기반 방식 vs SpeechT5:
- 디코더도 사전학습되어 **생성 작업에 최적화**
- 음성 변환, 음성 향상 등 **음성-음성 생성 작업**에서도 강력한 성능
- 절제 연구: CTC 없는 기준 모델 대비 **CTC 추가 시 20-30% WER 개선** → 디코더 품질의 중요성 증명

### 4.2 제한 사항 및 도전 과제

**1. 낮은 자원 환경에서의 성능**

- 제로샷(Zero-shot) TTS 성능이 매우 낮음 (필요한 향상: 화자 임베딩 최적화)
- 고자원 사전학습에 크게 의존

**2. 언어별 편향**

- 영어 중심 사전학습 데이터 (LibriSpeech)
- 다국어 태스크로 확장 시 성능 저하 가능성

**3. 구조적 한계**

- 동적 길이 변화에 취약 (SE 작업에서 PESQ/ESTOI로 평가 불가)
- 멀티스피커 작업에서 화자 정보 통합 방식 미흡

**4. 계산 복잡도**

- 6개의 모달별 전/후 처리 네트워크 추가로 인한 메모리 오버헤드
- 모바일/엣지 디바이스 배포 어려움

***

## 5. 연구 영향 및 향후 연구 방향

### 5.1 SpeechT5의 학술적 영향

**1. 후속 연구의 기초**

- **ArTST (Arabic Text and Speech Transformer)** : SpeechT5 아키텍처를 아랍어로 확장하여 ASR, TTS, 방언 식별 태스크 수행
- **Czech SpeechT5** : 체코어 멀티스피커 TTS에 SpeechT5 적용, 제로/퓨샷 학습 성능 평가
- **다국어 음성 처리의 선례**: 통합 프레임워크를 통한 크로스링구얼 작업 가능성 제시

**2. 크로스 모달 학습 패러다임**

- 음성-텍스트 정렬 기법의 새로운 표준 확립
- 벡터 양자화를 통한 모달리티 브릿징의 효과 실증
- 후속 연구에서 유사한 양자화 기반 접근 채택

**3. 음성 처리 벤치마크 확대**

- 단순 인코더 기반 평가 (SUPERB)에서 **엔코더-디코더 기반 평가로 확장**
- 생성 작업(TTS, VC, SE)의 중요성 재인식

### 5.2 최신 연구 트렌드 (2023-2025)

**1. 대규모 음성 언어 모델(SLM) 개발**

- **SpeechSSM** (2024 12월): 16분 이상의 장형 음성 생성을 위한 음성 언어 모델
  - SpeechT5의 한계(수십 초 생성만 가능) 극복
  - 선형시간 시퀀스 모델링 기반 아키텍처

- **BASE TTS** (2024 2월): 1억 개 파라미터 모델로 10만 시간 음성 학습
  - 신규 음성 토큰화 기법(화자 ID 분리)
  - SpeechT5보다 대규모 데이터 활용

**2. 멀티모달 일반화 능력 강화**

- **멀티모달 도메인 일반화** (2025): 
  - 음성-텍스트 결합 학습으로 **보이지 않은 도메인에 대한 강건성 향상**
  - Mixup 기반 데이터 증강 + 최적 수송 이론 응용

- **음성 기반 선호도 학습** (2024 9월):
  - Instruction-tuned 언어모델과 음성의 정렬
  - 텍스트 기반 선호도만으로 학습한 모델의 한계 극복

**3. 제어 가능한 음성 합성**

- **멀티모달 제어 음성 합성** (2025 6월):
  - 얼굴 + 텍스트 + 음성 결합 입력으로 스타일 제어
  - 단일 모달리티 방법보다 일반화 능력 향상

**4. 로버스트성 및 정렬 개선**

- **어텐션 기반 정렬 강화** (2024): 
  - T5-TTS 모델에서 정적 어텐션 프라이어 및 정렬 손실 추가
  - 문제가 있는 텍스트 입력에 대한 강건성 향상

### 5.3 향후 연구 시 고려 사항

**1. 아키텍처 개선**

- **파라미터 효율성**: 어댑터(Adapter) 모듈을 통한 경량화, 모바일 배포 용이화
- **계층적 구조 탐색**: 작업별 특화 모듈과 공유 백본의 최적 균형
- **비자동회귀 해독(Non-Autoregressive Decoding)**: 추론 속도 향상

**2. 데이터 및 사전학습**

- **장형(Long-Form) 음성 처리**: 현재 제한된 길이의 음성만 학습 가능
- **다국어/크로스링구얼 사전학습**: 영어 편향 극복
- **약한 지도 신호 활용**: 정렬되지 않은 음성-텍스트 쌍의 효율적 활용

**3. 평가 및 벤치마킹**

- **멀티태스크 벤치마크 확대**: 현재 제한된 벤치마크 (SUPERB)를 넘어 실제 응용 시나리오 중심
- **일반화 능력 평가**: 도메인 외(OOD) 성능, 저자원 언어, 노이즈 환경
- **효율성 메트릭**: 추론 시간, 메모리 사용량, 에너지 소비

**4. 응용 분야 확장**

- **음성 기반 멀티모달 이해**: 음성 + 비전 + 텍스트 결합 분석
- **실시간 음성 상호작용**: 스트리밍 처리, 저지연(Low-latency) 요구사항
- **접근성 기술**: 청각 장애인/난청 지원, 다양한 음성 스펙트럼 포괄

**5. 이론적 심화**

- **크로스 모달 정렬의 원리**: 왜 벡터 양자화가 효과적인가에 대한 이론적 분석
- **일반화 경계(Generalization Bounds)**: 멀티모달 학습의 이론적 기초 구축
- **적응적 학습**: 작업 간 전이학습(Transfer Learning) 메커니즘 규명

***

## 결론

**SpeechT5**는 음성 처리 분야에 **엔코더-디코더 통합 프레임워크** 및 **크로스 모달 벡터 양자화** 기법을 도입함으로써 획기적인 기여를 했습니다. T5의 철학(모든 작업을 통일된 형식으로 표현)을 음성 영역에 성공적으로 적용한 이 연구는 이후 다국어 음성 모델, 대규모 음성 언어 모델, 멀티모달 일반화 연구의 기초가 되었습니다.[2][3][1]

그러나 제로샷 성능, 장형 음성 처리, 멀티언어 적응 등에서 여전히 개선의 여지가 있습니다. 향후 연구는 **파라미터 효율성**, **도메인 외 일반화**, **실제 배포 환경 최적화**에 초점을 맞춰야 할 것으로 예상됩니다. 특히 2024-2025년의 트렌드는 더 큰 규모의 음음성 언어 모델과 강화된 제어 가능성으로 진화하고 있으며, SpeechT5의 기본 원리는 이러한 발전들의 토대로 계속 활용되고 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/2efa7b17-7c47-4c5a-8448-31f13d45c044/2110.07205v3.pdf)
[2](https://essd.copernicus.org/articles/16/3601/2024/)
[3](https://journal-stiayappimakassar.ac.id/index.php/srj/article/view/1359)
[4](https://ojs.transpublika.com/index.php/TIRES/article/view/1198)
[5](http://www.joaasr.com/index.php/joaasr/article/view/1175)
[6](https://esmed.org/MRA/mra/article/view/5389)
[7](https://journal-stiayappimakassar.ac.id/index.php/srj/article/view/946)
[8](https://kneesurgrelatres.biomedcentral.com/articles/10.1186/s43019-024-00241-6)
[9](https://tlcr.amegroups.com/article/view/91979/html)
[10](https://www.jmir.org/2025/1/e60148)
[11](https://respiratory-research.biomedcentral.com/articles/10.1186/s12931-024-02963-3)
[12](https://arxiv.org/pdf/2110.07205.pdf)
[13](https://aclanthology.org/2022.acl-long.393.pdf)
[14](http://arxiv.org/pdf/2412.18603.pdf)
[15](https://arxiv.org/html/2409.14672)
[16](https://zenodo.org/records/8092573/files/is2023-dysarthric-tts.pdf)
[17](https://arxiv.org/pdf/2310.16621.pdf)
[18](http://arxiv.org/pdf/2407.17167.pdf)
[19](https://arxiv.org/html/2402.08093v1)
[20](https://arxiv.org/pdf/2407.17167.pdf)
[21](https://arxiv.org/abs/2506.20945)
[22](https://aclanthology.org/2022.acl-long.105/)
[23](https://www.isca-archive.org/interspeech_2024/neekhara24_interspeech.pdf)
[24](https://openaccess.thecvf.com/content/ICCV2025/papers/Huang_Bridging_Domain_Generalization_to_Multimodal_Domain_Generalization_via_Unified_Representations_ICCV_2025_paper.pdf)
[25](https://arxiv.org/abs/2204.05409)
[26](https://github.com/microsoft/SpeechT5)
[27](https://www.arxiv.org/abs/2409.09611)
[28](https://arxiv.org/abs/2110.07205)
[29](https://arxiv.org/html/2410.18607v1)
