# VD-BERT: A Unified Vision and Dialog Transformer with BERT

**핵심 주장 및 주요 기여**  
VD-BERT는 시각적 대화(Visual Dialog) 과제를 해결하기 위해 사전학습된 BERT 언어 모델을 단일 스트림 Transformer 인코더에 확장하여 이미지와 다중턴 대화의 상호작용을 통합적으로 학습하는 프레임워크를 제안한다. 비전과 대화 정보를 동시에 양방향 주의(attention) 메커니즘을 통해 융합하고, 동일한 아키텍처로 응답 순위 판별(Discriminative)과 문장 생성(Generative) 두 설정을 모두 지원하는 것이 주요 기여이다.[1]

## 1. 해결하고자 하는 문제  
- **시각적 대화**는 이미지와 대화 이력을 모두 고려하여 다중 라운드 질문에 답해야 하며, 전통적인 VQA와 달리 긴 대화 문맥을 다뤄야 한다.  
- 기존 방법들은 주로 질문을 쿼리로 하여 이미지 영역이나 대화 이력만 선택적으로 주의(attend)하는 데 집중했으나, 대화-이미지-답변 간 복잡한 상호작용을 충분히 포착하지 못했다.

## 2. 제안 방법  
### 2.1 모델 구조  
- **입력 구성**: Faster R-CNN으로 추출한 36개 RoI 비전 피처와 이미지 캡션, 다중턴 대화를 단일 시퀀스로 병합하고, CLS/SEP/EOT/PRED 토큰을 삽입하여 BERT 기반 임베딩에 위치·세그먼트 정보를 추가한다.  
- **Transformer 인코더**: 12개 블록, 각 블록 내 멀티헤드 self-attention을 이용해 입력 시퀀스 전체 토큰 간 양방향 상호작용을 학습한다.  
- **디코더 통합**: Discriminative 설정은 NSP(next sentence prediction) 헤드를, Generative 설정은 seq2seq attention 마스크와 반복적 MLM(masked language modeling)으로 단일 인코더만으로 답변 생성 및 순위 매김을 수행한다.

### 2.2 훈련 목표식  
- **Visually Grounded MLM**: 텍스트 토큰 15%를 MASK한 뒤, 이미지 및 문맥 정보를 모두 활용해 복원하도록 학습하며, 비전-언어 융합을 강화한다.  
- **Visually Grounded NSP**: 입력 시퀀스 끝에 샘플 답변을 붙이고 올바른지(1)/아닌지(0)를 예측하도록 학습하여 답변 순위 판별력을 높인다.  

수식 요약  

$$ \mathcal{L}_{\text{MLM}} = \mathbb{E}_{(I,w)\sim D} \bigl[-\log P(w_m \mid I, w)\bigr] $$  

$$ \mathcal{L}_{\text{NSP}} = \mathbb{E}_{(I,w)\sim D} \bigl[-\log P(y \mid I, w)\bigr] $$

### 2.3 순위 최적화 모듈  
- VisDial v1.0의 0–1 실수형 밀집 레이블을 활용해 ListNet 순위 학습을 적용, NSP 점수 벡터 $$p$$와 실제 관련도 $$s$$의 확률 분포 차이를 최소화하는 손실을 추가한다.

## 3. 성능 향상  
- **사전학습 효과**: BERT 초기화가 무작위 초기화 대비 NDCG 약 +7점 향상.  
- **멀티턴 히스토리**: 전체 대화 이력을 활용할수록 Recall·MRR 지표 개선, 그러나 NDCG는 오히려 짧은 문맥이 다소 유리.  
- **밀집 레이블 미세조정**: ListNet 기반 fine-tuning으로 NDCG가 단일 모델 59.96→74.54로 대폭 상승, 앙상블 시 75.35를 달성하며 단일 모델 최상위 기록 수립. 반면 MRR·Recall 지표는 일부 감소.  

## 4. 한계  
- **지표 불일치**: 밀집 레이블 최적화가 NDCG 개선에는 기여하나, 실제 정답 순위 기반 지표(Recall, MRR)에서는 성능 저하를 초래함.  
- **대화 일반화**: 긴 대화 히스토리가 항상 성능 개선으로 이어지지 않으며, 특정 문맥에서는 과적합 위험 존재.  
- **사전학습 도메인 부조화**: 대규모 시각-언어 데이터로 사전학습된 모델(VLP) 대비, 언어 도메인 사전학습(BERT) 활용이 더 효과적인 것으로 관찰되어 다양한 시각-언어 코퍼스의 적합성 검증이 필요.

## 5. 일반화 성능 향상 가능성  
- **멀티도메인 사전학습**: 대화 중심의 시각-언어 데이터셋으로 추가 사전학습을 진행하여 멀티턴 문맥 일반화력을 강화할 수 있음.  
- **컨텍스트 적응 모듈**: 대화 길이에 따라 동적으로 히스토리 중요도를 조정하는 어텐션 게이트를 도입하면 불필요한 문맥 정보가 모델에 부담을 주는 것을 완화할 수 있다.  
- **대화식 정교화(fine-grained tuning)**: 질문 유형별(Yes/No·숫자·색상 등) 특화 손실 또는 어텐션 패턴 학습을 통해 특정 문맥·질문에 대한 일반화 성능을 제고할 여지가 있다.

## 6. 향후 연구 영향 및 고려사항  
향후 시각적 대화 연구는 **단일 스트림 인코더 기반의 통합적 멀티모달 학습** 방향을 더욱 확장하게 될 전망이다. VD-BERT가 제시한 사전학습된 언어 모델의 시각-대화 융합 방식은:

- **다양한 대화형 AI 시스템**: 로봇·멀티모달 에이전트 등 연속 대화 문맥을 다루는 응용에 직접 적용 가능하다.  
- **자기지도적 대화 예측 과제**: MLM·NSP와 유사한 시각-대화 사전학습 목표를 개발하여 언어·비전 간 더 촘촘한 상호작용을 끌어낼 수 있다.

연구 시 고려할 점은 **밀집 레이블과 실제 정답 간 불일치**로 인한 최적화 지표 간 상충 문제를 해결하기 위한 **복합 목표 함수 설계**이며, 이는 향후 시각적 대화 평가 체계 발전에도 기여할 수 있다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e6baee67-1a3d-44ac-b339-1cc3dd8e193b/2004.13278v3.pdf)
