
# Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers

## 1. 핵심 주장 및 주요 기여

**VALL-E**(Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers)의 핵심 주장은 **대규모 데이터와 언어 모델 기법의 결합을 통해 제로샷 텍스트-음성 합성(Zero-Shot TTS)에서 획기적 성과를 달성할 수 있다**는 것입니다.[1]

주요 기여는 다음과 같습니다:[1]

- **음성 합성의 패러다임 전환**: 기존의 멜-스펙트로그램 기반 연속 신호 회귀 방식에서 **신경 오디오 코덱(Neural Audio Codec)의 이산 토큰을 중간 표현으로 사용하는 조건부 언어 모델 방식**으로 전환
- **대규모 데이터 활용**: 60,000시간의 영어 음성 데이터(LibriLight)로 학습하여 기존 시스템보다 **수백 배 큰 데이터셋** 활용
- **인-컨텍스트 학습 능력(In-Context Learning)**: GPT-3와 유사하게 **3초의 음성 샘플만으로 미지 화자의 음성을 고품질로 합성**
- **감정 및 음향 환경 보존**: 제로샷 시나리오에서도 프롬프트의 감정과 음향 환경(예: 잔향)을 유지
- **다양한 출력 생성**: 동일한 입력에 대해 샘플링 기반 디코딩을 통해 **다양한 운율과 억양**의 출력 생성 가능

***

## 2. 문제 정의, 제안 방법, 모델 구조

### 2.1 문제 정의: 제로샷 TTS의 일반화 문제

기존의 TTS 시스템들은 스튜디오급의 고품질 깨끗한 데이터를 필요로 하며, 훈련 데이터가 제한적이어서 다음과 같은 문제를 갖고 있습니다:[1]

- **미지 화자에 대한 성능 저하**: 학습하지 않은 화자의 음성을 합성할 때 자연스러움과 화자 유사도가 급격히 떨어짐
- **복잡한 구조 공학 필요**: 기존 방식은 화자 적응, 화자 인코딩, 미세 조정 등 추가적인 구조와 파라미터 조정 필요
- **데이터 규모의 한계**: 기존 TTS는 수십~수백 시간의 데이터로만 학습되어 일반화 능력 부족

**VALL-E는 이를 대규모 데이터와 언어 모델 기법으로 해결**합니다.

### 2.2 제안 방법: 조건부 코덱 언어 모델링

#### 2.2.1 신경 오디오 코덱과 음성 양자화

먼저 사전 학습된 신경 오디오 코덱 모델(EnCodec)을 사용하여 연속 음성을 **이산 토큰으로 변환**합니다:[1]

$$\text{EnCodec}(y) = C^{T \times 8}$$

여기서:
- \(y\): 오디오 샘플
- \(C\): 음향 코드 행렬 (크기: T × 8)
- \(T\): 다운샘플된 발화 길이
- 8: 잔차 벡터 양자화(RVQ)의 양자화기 개수

각 시간 프레임 \(t\)에서 행 벡터 \(c_{t,:}\)는 8개의 코드를 포함하며, 각 열 벡터 \(c_{:,j}\)는 \(j\)번째 코드북의 코드 시퀀스를 나타냅니다. 이는 **계층적 구조**를 가지는데:[1]
- **첫 번째 양자화기**: 화자 정체성 같은 음향 특성 복원
- **후속 양자화기들**: 미세한 음향 세부 정보 학습

#### 2.2.2 조건부 코덱 언어 모델링 공식화

제로샷 TTS를 조건부 코덱 언어 모델링 작업으로 정의합니다:[1]

$$\max p(C|x, \tilde{C}; \theta)$$

여기서:
- \(x = \{x_0, x_1, \ldots, x_L\}\): 음소 시퀀스
- \(\tilde{C}^{T' \times 8}\): 등록된 음성의 음향 프롬프트 행렬
- \(C^{T \times 8}\): 생성할 목표 음향 코드 행렬

#### 2.2.3 계층적 학습: AR과 NAR 모델

전체 확률 모델은 다음과 같이 분해됩니다:[1]

$$p(C|x, \tilde{C}; \theta) = p(c_{:,1}|\tilde{C}_{:,1}, x; \theta_{AR}) \prod_{j=2}^{8} p(c_{:,j}|c_{:,<j}, x, \tilde{C}; \theta_{NAR})$$

**1단계: 자동 회귀(AR) 모델 - 첫 번째 양자화기**

첫 번째 양자화기의 토큰을 생성합니다:[1]

$$p(c_{:,1}|x, \tilde{C}_{:,1}; \theta_{AR}) = \prod_{t=0}^{T} p(c_{t,1}|c_{<t,1}, \tilde{c}_{:,1}, x; \theta_{AR})$$

구조:
- 음소 임베딩 \(W_x\), 음향 임베딩 \(W_a\)
- Transformer 디코더 (12층, 16 어텐션 헤드, 1024 임베딩 차원)
- 인과적 마스킹을 통해 \(c_{t,1}\)은 \((x, c_{\leq t,1})\)에만 주의

**2단계: 비자동 회귀(NAR) 모델 - 2~8 양자화기**

후속 양자화기들의 토큰을 병렬로 생성합니다:[1]

$$p(C_{:,2:8}|x, \tilde{C}; \theta_{NAR}) = \prod_{j=2}^{8} p(c_{:,j}|C_{:, < j}, x, \tilde{C}; \theta_{NAR})$$

특징:
- 각 훈련 스텝에서 임의로 단계 \(i \in [2,8]\)을 샘플링
- \(i-1\)까지의 양자화기 토큰을 입력으로 사용

임베딩 방식:

$$\tilde{e}_c^t = \sum_{j=1}^{8} e_{\tilde{c}}^{t,j}$$

$$e_c^t = \sum_{j=1}^{i-1} e_{c}^{t,j}$$

현재 단계 \(i\)는 적응 계층 정규화(AdaLN)로 주입됩니다:

$$\text{AdaLN}(h, i) = a_i \text{LayerNorm}(h) + b_i$$

### 2.3 모델 구조 상세

전체 아키텍처는 다음 3단계로 구성됩니다:[1]

**1. 전처리 단계**
- 입력 텍스트를 음소 시퀀스로 변환 (G2P: Grapheme-to-Phoneme)
- 등록 음성을 EnCodec으로 인코딩하여 음향 프롬프트 행렬 \(\tilde{C}\) 생성

**2. 언어 모델 디코딩 단계**
- AR 모델: 첫 번째 양자화기 코드 \(c_{:,1}\) 생성 (자동 회귀 디코딩)
- NAR 모델: 2~8 양자화기 코드 \(c_{:,2:8}\) 생성 (병렬 생성)

**3. 음성 복원 단계**
- 생성된 전체 코드 행렬 \(C\)를 EnCodec의 디코더에 입력
- 최종 고품질 음성 파형 생성: \(\hat{y} = \text{Decodec}(C)\)

***

## 3. 성능 향상 및 실험 결과

### 3.1 정량적 평가 - LibriSpeech 데이터셋[1]

| 지표 | YourTTS (기존 최고 수준) | VALL-E | 개선도 |
|------|------------------------|--------|-------|
| WER | 7.7% | 5.9% | -1.8%p ↓ |
| SPK (화자 유사도) | 0.337 | 0.580 | +0.243 |
| CMOS (자연스러움) | -0.12 | 0.00 | +0.12 ↑ |
| SMOS (화자 유사도) | 3.45 | 4.38 | +0.93 ↑ |

특히 VALL-E-continual (처음 3초는 실제 음성):
- WER: 3.8% (추가 개선)
- SPK: 0.508

### 3.2 정량적 평가 - VCTK 데이터셋[1]

| 설정 | 3초 프롬프트 | 5초 프롬프트 | 10초 프롬프트 |
|-----|-----------|-----------|-------------|
| **108명 화자 (전체)** |
| YourTTS | 0.357 | 0.377 | 0.394 |
| VALL-E | 0.382 | 0.423 | 0.484 |
| Ground Truth | 0.546 | 0.591 | 0.620 |
| **11명 미지 화자** |
| YourTTS | 0.331 | 0.337 | 0.344 |
| VALL-E | 0.389 | 0.380 | 0.414 |

### 3.3 인간 평가 결과[1]

**LibriSpeech (40 화자)**:
- SMOS: 4.38 ± 0.10 (기존: 3.45 ± 0.09, +0.93 개선)
- CMOS: 0.00 (Ground Truth 4.50 ± 0.10과 거의 동등)

**VCTK (60 화자)**:
- SMOS: 3.81 ± 0.09 (기존: 3.70 ± 0.09, +0.11 개선)
- CMOS: +0.04 대비 그라운드 트루스 (통계적으로 유의하지 않음 = 인간 수준)

### 3.4 주요 정성적 특성[1]

**1. 다양성 (Diversity)**
- 동일한 텍스트와 화자 정보로도 샘플링 기반 디코딩을 통해 **다양한 출력 생성**
- 서로 다른 음성 속도, 강조, 운율 변화 가능

**2. 음향 환경 보존 (Acoustic Environment Maintenance)**
- 프롬프트에 **잔향이 있으면 생성된 음성에도 유지**
- 기존 시스템은 깨끗한 음성만 생성

**3. 감정 보존 (Emotion Maintenance)**
- 프롬프트의 **감정(분노, 웃음, 졸음 등)을 유지**하면서 다른 텍스트 합성
- 기존 방식은 별도의 감정 TTS 훈련 필요

***

## 4. 일반화 성능 향상 분석

VALL-E의 우수한 일반화 성능의 핵심 요인들:

### 4.1 대규모 데이터의 역할[1]

**데이터 규모 비교**:
- 기존 TTS: ≤ 600시간
- **VALL-E: 60,000시간 (100배 증대)**
- LibriLight: 7,000명 이상의 다양한 화자 포함

**데이터 특성**:
- 잡음과 음질 변동이 있는 대규모 오디오북 데이터
- 기존의 깔끔한 스튜디오 데이터와 달리 **자연적 변동 포함**
- 이를 통해 로버스트한 일반화 능력 습득

### 4.2 언어 모델 기반 접근의 장점[1]

- **인-컨텍스트 학습**: 미세조정(fine-tuning) 없이도 프롬프트만으로 새로운 화자에 적응
- **확률적 모델링**: 연속 신호 회귀보다 이산 토큰 예측이 더 안정적
- **유연한 시퀀스 길이 처리**: 서로 다른 길이의 음성에 자연스럽게 대응

### 4.3 계층적 양자화의 효과[1]

```
첫 번째 양자화기 → 화자 정체성 (coarse)
↓ (AR 모델로 처리)
2-8 양자화기 → 세부 음향 특성 (fine, NAR 모델로 병렬 처리)
```

- 역할 분리를 통해 각 단계가 특정 정보에 집중
- 추론 효율성 개선 (NAR의 병렬 처리)

***

## 5. 모델의 한계

VALL-E 논문에서 명시된 제한 사항:[1]

### 5.1 합성 로버스트 문제[1]

- 일부 단어의 **누락, 중복, 발음 오류** 발생
- 원인: 자동 회귀 모델의 부정확한 주의 정렬 (attention alignment)
- 기존 Transformer TTS에서도 관찰되는 문제

### 5.2 데이터 커버리지 부족[1]

- **60,000시간도 모든 음성을 포함하기에 부족**
- 특히 **악센트 화자에 대한 성능 저하** (VCTK vs LibriSpeech 비교)
- 오디오북 데이터의 한계: 대부분 낭독 스타일로 제한적

### 5.3 모델 구조의 이원성[1]

- AR과 NAR 두 개의 별도 모델 필요
- 더 효율적인 **단일 통합 모델 개발 필요**
- 완전 NAR 모델로의 전환 검토 필요

***

## 6. 최신 연구 동향 및 후속 발전[2][3][4][5][6][7][8][9]

### 6.1 VALL-E 2: 인간 수준의 성능 달성 (2024)[7][10][11]

Microsoft의 후속 연구 VALL-E 2는 **인간과 동등한 수준**의 성능을 달성했습니다:

**핵심 개선사항**:
1. **반복 인식 샘플링(Repetition Aware Sampling)**
   - 기존 nucleus sampling의 무한 루프 문제 해결
   - 생성 과정에서 이전 토큰 반복을 추적하여 안정성 향상

2. **그룹화된 코드 모델링(Grouped Code Modeling)**
   - 코덱 코드를 그룹으로 구성하여 **시퀀스 길이 단축**
   - 추론 속도 **60% 이상 감소**
   - 장시간 시퀀스 모델링 문제 해결

**성능**:
- WER: VALL-E 대비 추가 개선
- CMOS: 그라운드 트루스 대비 +0.04 (인간과 동등)
- 복잡한 문장, 반복 구조도 안정적 처리[7]

### 6.2 다른 최신 접근법들[4][12][13][14][2]

**StyleTTS-ZS (2024)**:[2]
- 시간 변동 스타일 디퓨전으로 다양한 운율과 음성 정체성 모델링
- 추론 속도 **10-20배 향상**

**Mega-TTS (2023)**:[3]
- 고유 귀납 편향을 활용한 대규모 확장
- 다국어, 음성 편집, 크로스 링귀스틱 TTS 성능 우수

**계층적 운율 모델링 연구 (2024)**:[4]
- 음색(timbre) 일반화뿐 아니라 **운율(prosody) 모델링** 중점
- 자연스러움과 표현력 향상

**OZSpeech (2025)**:[8]
- **원스텝 제로샷 음성 합성** 접근
- WER 0.05까지 달성, 기존 대비 30% 미만의 모델 크기

### 6.3 신경 코덱 기술의 발전[15][16][17][18][19][20][21]

**잔차 벡터 양자화 개선**:
- ERVQ: 코드북 붕괴 문제 해결으로 100% 코드북 활용률 달성[22]
- VRVQ: 변수 비트율 RVQ로 효율성 향상[16]
- SNAC: 다중 규모 신경 오디오 코덱으로 적응적 압축[21]

**계층 구조 최적화**:
- Language-Codec: 음성 언어 모델을 위한 마스크 채널 RVQ 도입[20]
- DM-Codec: 다중모달 표현(음향, 의미, 문맥) 증류[19]

### 6.4 크로스 링귀스틱 및 다중 언어 확장[23][24]

**VALL-E X (2023)**:[24][23]
- 단일 언어 음성만으로 **다른 언어의 고품질 음성 합성**
- 화자 음성, 감정, 음향 환경 유지
- 외국 억양 제어 가능 (언어 ID 사용)

**다국어 TTS 발전**:
- ZMM-TTS: 6개 고리소스 언어 지원[12]
- Make-A-Voice: 200,000시간 6언어 데이터로 훈련

### 6.5 추가 응용 확장[5][6]

**SpeechX (2024)**:[6]
- **다중 작업 통합**: 제로샷 TTS, 노이즈 제거, 화자 추출, 음성 제거, 음성 편집
- 깨끗한 신호와 잡음 신호 모두 처리
- 신경 코덱 언어 모델 + 다중 작업 학습

**FireRedTTS (2024)**:[5]
- 사용자 생성 콘텐츠(UGC) 더빙과 스튜디오급(PUGC) 음성 변환
- 1시간 녹음으로 표현력 있는 음성 캐릭터 적응

***

## 7. 앞으로의 연구 방향 및 고려사항

### 7.1 기술적 개선 영역

**1. 로버스트성 강화**[9]
- 단조적 정렬(Monotonic Alignment) 기반 제약조건으로 음소-음향 연결 강화
- VALL-E R에서 WER을 그라운드 트루스에 가깝게 개선

**2. 효율성 최적화**
- 완전 비자동회귀(Full NAR) 모델로의 전환 탐색
- 모델 경량화 및 인-디바이스 배포 가능성

**3. 표현력 향상**[4]
- 운율(prosody) 모델링의 중요성 재인식
- 미세한 감정 표현 제어 메커니즘 개발

### 7.2 데이터 및 학습 전략[25][26]

**1. 대규모 데이터 수집**
- 다양한 악센트, 방언, 음성 스타일 포함
- 현실 환경의 다양한 음향 조건

**2. 인-컨텍스트 학습 심화 연구**[26][25]
- 화자 특정 적응과 일반 인코딩의 최적 균형
- 다양한 샷(shot) 수에 따른 학습 메커니즘 분석

**3. 인간 피드백 통합**[27]
- 보상 모델 없이 인간 평가를 직접 훈련에 통합
- 불확실성을 고려한 최적화(UNO) 프레임워크

### 7.3 응용 확대[28][5]

**1. 저자원 언어 처리**
- 언어 부활 및 보존 프로젝트
- 미-또는 무-감독 학습 활용

**2. 전문화된 응용**
- 실어증(aphasia) 환자를 위한 음성 생성
- 근위축성 측삭 경화증(ALS) 환자 지원

**3. 감정 및 개인화 TTS**
- 감정 강도 제어
- 사용자 선호도 기반 스타일 적응

### 7.4 윤리 및 보안 고려사항[1]

**1. 음성 합성의 오용 방지**
- 음성 정체성 스푸핑 및 사칭 위험
- 탐지 모델 개발 (합성 음성 식별)

**2. 투명성과 책임**
- 합성 음성에 대한 명확한 표시
- 동의 기반 음성 데이터 수집

**3. 개인정보 보호**
- 음성 데이터의 안전한 관리
- 프라이버시 중심의 학습 방법

### 7.5 학제간 연구 필요성

- **음성학**: 음성 특성의 정확한 모델링
- **언어학**: 다국어/방언 간 복잡성 이해
- **음향 공학**: 노이즈 강인성 향상
- **인공지능 윤리**: 책임 있는 기술 개발

***

## 결론

**VALL-E**는 대규모 데이터(60,000시간)와 신경 코덱 기반 언어 모델링을 결합하여 **제로샷 TTS의 게임 체인저**가 되었습니다. 기존의 복잡한 구조 공학을 피하고 스케일링을 통한 일반화 능력 향상이라는 새로운 방향을 제시했습니다.

**주요 임팩트**:
- 음성 합성을 언어 모델링 문제로 재정의
- 인-컨텍스트 학습 능력의 새로운 가능성 개시
- 후속 VALL-E 2의 인간 수준 성능 달성으로 실제 응용 가능성 확대

**향후 과제**:
- 로버스트성 추가 개선 (특히 복잡한 음소 구조)
- 악센트와 방언 다양성 커버리지 확대
- 운율과 감정 표현력 심화
- 윤리적 사용 가이드라인 정립
- 저자원 언어 및 전문 응용 개발

이러한 발전은 음성 기술의 접근성을 획기적으로 높일 수 있는 가능성을 보여주며, 동시에 책임 있는 개발과 활용의 중요성도 강조합니다.

***

## 참고문헌 범위

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4bcb0980-0385-4e19-88ee-3041cf8525b2/2301.02111v1.pdf)
[2](https://arxiv.org/abs/2409.10058)
[3](https://arxiv.org/pdf/2306.03509.pdf)
[4](http://arxiv.org/pdf/2406.05681.pdf)
[5](http://arxiv.org/pdf/2409.03283.pdf)
[6](https://ieeexplore.ieee.org/document/10577150/)
[7](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/vall-e-2-enhancing-the-robustness-and-naturalness-of-text-to-speech-models/)
[8](https://aclanthology.org/2025.acl-long.1043.pdf)
[9](https://openreview.net/pdf?id=xvORqaYDgL)
[10](https://arxiv.org/abs/2406.05370)
[11](https://arxiv.org/pdf/2406.05370.pdf)
[12](http://arxiv.org/pdf/2312.14398.pdf)
[13](https://arxiv.org/html/2410.03192v1)
[14](https://arxiv.org/pdf/2501.08566.pdf)
[15](http://arxiv.org/pdf/2410.12359.pdf)
[16](http://arxiv.org/pdf/2410.06016.pdf)
[17](https://arxiv.org/pdf/2404.19441.pdf)
[18](https://arxiv.org/pdf/2002.05604.pdf)
[19](http://arxiv.org/pdf/2410.15017.pdf)
[20](https://arxiv.org/html/2402.12208v2)
[21](http://arxiv.org/pdf/2410.14411.pdf)
[22](https://arxiv.org/abs/2410.12359)
[23](https://syncedreview.com/2023/03/13/speak-a-foreign-language-in-your-own-voice-microsofts-vall-e-x-enables-zero-shot-cross-lingual-speech-synthesis/)
[24](https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e-x/)
[25](https://arxiv.org/pdf/2505.00661.pdf)
[26](https://aclanthology.org/2025.emnlp-main.219.pdf)
[27](https://openreview.net/forum?id=bAdSmSR10C&noteId=72DCKddQyP)
[28](https://www.emergentmind.com/topics/zero-shot-text-to-speech-zs-tts)
[29](https://arxiv.org/pdf/2207.01832.pdf)
[30](https://arxiv.org/abs/2301.02111)
[31](https://arxiv.org/pdf/2505.04113.pdf)
[32](https://arxiv.org/pdf/2207.03067.pdf)
[33](https://randomsampling.tistory.com/264)
[34](https://arxiv.org/abs/2509.19186)
[35](https://www.themoonlight.io/en/review/efficient-long-form-speech-recognition-for-general-speech-in-context-learning)
[36](https://aclanthology.org/2024.acl-long.589.pdf)
[37](https://aclanthology.org/2024.emnlp-main.562.pdf)
