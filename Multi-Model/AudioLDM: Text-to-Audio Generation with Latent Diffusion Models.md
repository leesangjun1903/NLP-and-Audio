
# AudioLDM: Text-to-Audio Generation with Latent Diffusion Models

## 1. 핵심 주장 및 주요 기여

AudioLDM은 **텍스트 기반 오디오 생성의 품질과 계산 효율성을 동시에 달성**하는 혁신적인 시스템입니다. 이 논문의 핵심 주장은 기존 텍스트-오디오(TTA) 시스템이 마주한 두 가지 근본적인 문제를 해결한다는 데 있습니다.[1]

기존 방식들은 **오디오-텍스트 쌍 데이터의 품질 부족**으로 인한 성능 저하에 시달렸으나, AudioLDM은 대비 언어-오디오 사전학습(CLAP) 임베딩을 활용하여 **학습 단계에서 오디오 데이터만 사용**할 수 있도록 설계했습니다. 놀랍게도 이러한 접근법이 오디오-텍스트 쌍 데이터를 사용하는 것보다 더 우수한 성능을 제공합니다.[1]

**주요 기여**는 다음과 같습니다:[1]

- 텍스트-오디오 생성에 최초로 연속 잠재 확산 모델(LDM) 적용
- CLAP 임베딩을 활용한 텍스트 감독 없는 LDM 훈련 방법 개발
- 오디오 전용 데이터를 활용한 고품질, 고효율의 TTA 시스템 구축
- 영점 오디오 조작(style transfer, super-resolution, inpainting)의 첫 구현

***

## 2. 해결하고자 하는 문제, 제안 방법 및 모델 구조

### 2.1 핵심 문제

AudioLDM이 해결하는 문제는 세 가지로 정리됩니다:[1]

1. **데이터 품질 문제**: 대규모 고품질 오디오-텍스트 쌍 데이터의 부족
2. **계산 효율성**: 고차원 오디오를 직접 모델링할 경우의 높은 계산 비용
3. **기존 전처리의 한계**: 텍스트 전처리 과정에서 시공간 관계 정보 손실

### 2.2 제안 방법: CLAP 기반 조건화

AudioLDM의 핵심 혁신은 **학습과 샘플링 단계에서 서로 다른 임베딩을 사용**하는 것입니다.[1]

오디오 샘플 $$x$$와 텍스트 설명 $$y$$가 주어질 때, 텍스트 인코더 $$f_{\text{text}}(\cdot)$$와 오디오 인코더 $$f_{\text{audio}}(\cdot)$$가 각각 텍스트 임베딩 $$E_y \in \mathbb{R}^L$$과 오디오 임베딩 $$E_x \in \mathbb{R}^L$$을 추출합니다. 여기서 $$L$$은 CLAP 임베딩의 차원입니다.[1]

**학습 단계**에서 오디오 임베딩 $$E_x$$를 조건으로 사용하는 반면, **샘플링 단계**에서는 텍스트 임베딩 $$E_y$$를 조건으로 제공합니다. 이는 CLAP의 공동 임베딩 공간이 오디오와 텍스트 정보를 정렬하기 때문에 가능합니다.[1]

### 2.3 확산 모델의 수학적 기초

#### 순진행 과정(Forward Process)

주어진 노이즈 스케줄 $$0 < \beta_1 < \cdots < \beta_N < 1$$에서, 각 타임스텝 $$n \in [1, \ldots, N]$$에서의 전이 확률은:[1]

$$q(z_n | z_{n-1}) = \mathcal{N}(z_n; \sqrt{1-\beta_n}z_{n-1}, \beta_n I)$$

재매개변수화를 통해 다음과 같이 표현됩니다:[1]

$$q(z_n | z_0) = \mathcal{N}(z_n; \sqrt{\bar{\alpha}_n}z_0, (1-\bar{\alpha}_n)\epsilon)$$

여기서 $$\epsilon \sim \mathcal{N}(0, I)$$는 주입된 노이즈, $$\alpha_n = 1 - \beta_n$$, $$\bar{\alpha}\_n := \prod_{s=1}^{n} \alpha_s$$는 각 스텝에서의 노이즈 레벨입니다.[1]

#### 노이즈 추정 훈련 목표

모델 최적화를 위해 재가중된 노이즈 추정 훈련 목표를 사용합니다:[1]

$$\mathcal{L}_n(\theta) = \mathbb{E}_{z_0, \epsilon, n} \|\epsilon - \epsilon_\theta(z_n, n, E_x)\|_2^2$$

여기서 $$E_x$$는 CLAP의 사전학습된 오디오 인코더 $$f_{\text{audio}}(\cdot)$$가 생성한 오디오 웨이브폼의 임베딩입니다.[1]

#### 역과정(Reverse Process)

텍스트 임베딩 $$E_y$$로부터 오디오 사전 $$z_0$$을 단계적으로 생성합니다:[1]

$$p_\theta(z_{0:N} | E_y) = p(z_N) \prod_{n=n}^{N} p_\theta(z_{n-1} | z_n, E_y)$$

$$p_\theta(z_{n-1} | z_n, E_y) = \mathcal{N}(z_{n-1}; \mu_\theta(z_n, n, E_y), \sigma_n^2 I)$$

평균과 분산은 다음과 같이 매개변수화됩니다:[1]

$$\mu_\theta(z_n, n, E_y) = \frac{1}{\sqrt{\alpha_n}} \left( z_n - \frac{\beta_n}{\sqrt{1-\bar{\alpha}_n}} \epsilon_\theta(z_n, n, E_y) \right)$$

$$\sigma_n^2 = \frac{1-\bar{\alpha}_{n-1}}{1-\bar{\alpha}_n} \beta_n$$

여기서 $$\sigma_1^2 = \beta_1$$입니다.[1]

### 2.4 조건화 확대(Conditioning Augmentation)

데이터 부족 문제를 완화하기 위해 오디오 신호에 대해 Mixup 증강을 수행합니다:[1]

$$x_{1,2} = \lambda x_1 + (1-\lambda) x_2$$

여기서 $$\lambda$$는 Beta 분포 $$B(5,5)$$에서 샘플링된 $[0,1]$ 범위의 스케일 계수입니다. 텍스트 임베딩이 훈련 단계에서 필요 없으므로, 해당하는 텍스트 설명 $$y_{1,2}$$를 고려할 필요가 없습니다. 이는 음성 이벤트의 관계를 보존하면서 더 많은 훈련 데이터를 생성합니다.[1]

### 2.5 무분류기 지도(Classifier-Free Guidance)

제어 가능한 생성을 위해 무분류기 지도(CFG)를 도입합니다. 훈련 중 조건 $$E_x$$를 10% 확률로 무작위로 버려서 조건부 모델 $$\epsilon_\theta(z_n, n, E_x)$$와 무조건 모델 $$\epsilon_\theta(z_n, n)$$을 모두 훈련합니다.[1]

샘플링 단계에서 수정된 노이즈 추정을 사용합니다:[1]

$$\hat{\epsilon}_\theta(z_n, n, E_y) = w \epsilon_\theta(z_n, n) + (1-w) \epsilon_\theta(z_n, n, E_y)$$

여기서 $$w$$는 지도 스케일을 결정합니다.[1]

### 2.6 모델 구조

#### VAE 기반 압축

AudioLDM은 mel-spectrogram $$X \in \mathbb{R}^{T \times F}$$를 작은 연속 공간 $$z \in \mathbb{R}^{C \times T/r \times F/r}$$로 압축합니다. 여기서 $$r$$은 압축 레벨, $$C$$는 채널 수, $$T$$와 $$F$$는 시간-주파수 차원입니다.[1]

훈련 목표는 다음 세 가지 손실 함수를 포함합니다:[1]

- **재구성 손실**: 입력 mel-spectrogram과 재구성된 mel-spectrogram 간의 평균 절대 오차
- **적대적 손실**: PatchGAN 판별기를 통한 재구성 품질 개선
- **가우시안 제약 손실**: VAE 잠재 공간의 연속성과 구조화된 특성 강화

#### UNet 아키텍처

AudioLDM은 StableDiffusion의 UNet을 기반으로 하며, 4개의 인코더 블록, 1개의 중간 블록, 4개의 디코더 블록을 포함합니다. 기본 채널 수 $$c_u$$를 기준으로 인코더 블록의 채널 차원은 $$[c_u, 2c_u, 3c_u, 5c_u]$$입니다.[1]

마지막 3개의 인코더 블록과 첫 3개의 디코더 블록에 주의 메커니즘을 추가했습니다.[1]

***

## 3. 성능 향상 및 실험 결과

### 3.1 객관적 평가 지표

AudioLDM은 다음 지표로 평가되었습니다:[1]

- **Frechet Distance (FD)**: 생성된 샘플과 목표 샘플 간의 유사성 측정 (낮을수록 우수)
- **Inception Score (IS)**: 샘플 품질과 다양성 평가 (높을수록 우수)
- **Kullback-Leibler (KL) 발산**: 쌍 샘플 수준에서 측정
- **Frechet Audio Distance (FAD)**: VGGish를 사용한 유사성 측정

### 3.2 주요 결과

#### AudioCaps 데이터셋에서의 성능[1]

| 모델 | FD ↓ | IS ↑ | KL ↓ | 주요 특징 |
|-----|------|------|------|---------|
| Ground Truth | - | - | - | 참조 |
| DiffSound | 47.68 | 4.01 | 2.52 | 기존 방식 |
| AudioLDM-S | 29.48 | 6.90 | 1.97 | 소형 모델 |
| AudioLDM-L | 27.12 | 7.51 | 1.86 | 대형 모델 |
| **AudioLDM-L-Full** | **23.31** | **8.13** | **1.59** | 최최종 성능 |

AudioLDM-L-Full은 DiffSound 대비 FD에서 **50.9% 개선**을 달성했습니다.[1]

#### 주관적 평가[1]

인간 평가자 6명이 전체 품질(OVL)과 텍스트 관련성(REL)을 평가한 결과:

- **AudioLDM**: OVL ≈ 64-66, REL ≈ 64-66
- **DiffSound**: OVL = 45.00, REL = 43.83

AudioLDM은 DiffSound 대비 주관적 평가에서 **약 42% 향상**을 보였습니다.[1]

### 3.3 조건화 정보 분석

훈련 중 오디오 임베딩 $$E_x$$를 사용하는 것이 텍스트 임베딩 $$E_y$$를 사용하는 것보다 우수한 이유:[1]

1. **텍스트 모호성**: 같은 오디오에 대해 서로 다른 인간 주석자가 다양한 설명을 제공
2. **캡션 품질 편차**: BBC SFX 데이터셋의 "Boats: Battleships-5.25 conveyor space" 같은 비명확한 캡션
3. **오디오 임베딩의 우수성**: $$E_x$$는 오디오 신호에서 직접 추출되어 최적의 설명과 정렬

실험 결과, AudioLDM-L-Full에서 오디오 임베딩 사용 시 FD 23.31 vs 텍스트 임베딩 사용 시 FD 25.79를 기록했습니다.[1]

### 3.4 압축 레벨의 영향

VQ-VAE의 압축 레벨 $$r$$에 따른 성능:[1]

| 압축 레벨 | FD ↓ | IS ↑ | KL ↓ | 설명 |
|----------|------|------|------|------|
| r=4 | 29.48 | 6.90 | 1.97 | **기본값** |
| r=8 | 33.50 | 6.13 | 2.04 | 성능 저하 |
| r=16 | 34.32 | 5.68 | 2.09 | 더 큰 저하 |

$$r=4$$는 계산 효율성과 생성 품질의 최적 균형을 제공합니다.[1]

***

## 4. 모델의 일반화 성능 향상 가능성

### 4.1 현재 일반화 능력

AudioLDM의 일반화 성능은 **CLAP 임베딩의 품질**에 의존합니다. CLAP 모델의 영점 분류(zero-shot classification) 능력이 입증되었으므로, AudioLDM도 **보지 못한 오디오-텍스트 조합에 대해 상당한 일반화 능력**을 보유합니다.[1]

### 4.2 일반화 향상 전략

#### (1) 데이터셋 확장의 효과

AudioLDM-L-Full은 AudioCaps(AC) 단독 훈련에서 AudioSet(AS), Freesound, BBC SFX를 포함한 확장 데이터 훈련으로 전환했을 때 현저한 향상을 보였습니다:[1]

- **AC만 사용**: FD = 23.51
- **AC + AS + Freesound + BBC SFX**: FD = 23.31 (8,886시간 → 145시간 이후)

다양한 데이터 소스 활용이 모델의 음악, 음성, 음향 효과에 대한 표현 능력을 강화합니다.[1]

#### (2) 압축 레벨 최적화

VAE 압축 레벨은 일반화에 영향을 미칩니다. $$r=4$$에서 최적의 일반화 성능을 달성하며, 더 높은 압축은 정보 손실로 인한 일반화 성능 저하를 야기합니다.[1]

#### (3) 무분류기 지도의 역할

CFG 스케일 $$w$$의 조정이 일반화에 영향을 미칩니다:[1]
- $$w = 1.0$$: 약한 조건화, 높은 다양성
- $$w = 2.0-3.0$$: 최적 균형
- $$w > 3.0$$: 조건 과적합, 다양성 감소

### 4.3 교차 도메인 일반화

논문의 실험 결과, AudioLDM이 **다양한 음향 장면과 음성 유형에 걸친 일반화**를 입증했습니다:[1]

- AudioCaps 데이터셋의 음성, 음악, 음향 효과 생성
- AudioSet 평가에서도 일관된 성능 (FD = 24.26)

다만 **음악 생성**은 제한적이었습니다. AudioCaps에서 의도적으로 음악 관련 오디오를 제거했기 때문입니다.[1]

### 4.4 향후 일반화 향상 방향

#### (1) 고주파수 대역폭 문제

현재 AudioLDM의 샘플링 레이트는 16kHz로 제한되어, **음악 생성 품질 저하**를 초래합니다. 32kHz 또는 48kHz로 향상된 샘플링 레이트 적용이 필요합니다.[1]

#### (2) 엔드-투-엔드 최적화

AudioLDM의 모든 모듈(CLAP, VAE, LDM, Vocoder)이 **독립적으로 훈련**되어, 모듈 간 정렬 부족으로 인한 성능 저하가 발생합니다.[1]

미래 작업은 **엔드-투-엔드 미세조정**을 통해 서로 다른 모듈 간의 최적화를 이룰 수 있습니다.[1]

#### (3) CLAP 모델 개선

CLAP 임베딩 공간의 품질이 AudioLDM의 성능의 상한선을 결정합니다. 더욱 강력한 대비 언어-오디오 사전학습 모델 개발이 일반화 성능 향상을 직접적으로 추진할 것입니다.[1]

***

## 5. 주요 한계

### 5.1 기술적 한계

1. **샘플링 레이트 제한**: 16kHz는 음악 생성에 적합하지 않습니다[1]
2. **모듈 정렬 문제**: VAE 잠재 공간이 LDM 생성에 최적화되지 않을 수 있습니다[1]
3. **계산 비용**: 단일 GPU로 훈련 가능하지만, 대규모 데이터셋에서는 상당한 시간 소요[1]

### 5.2 평가상의 한계

객관적 평가 지표(FD, IS, KL)가 항상 인간의 지각과 일치하지 않습니다. 특히 미세한 음성 특성이나 음악적 표현력에서 한계가 있습니다.[1]

***

## 6. 텍스트-오디오 생성 분야의 영향과 미래 연구 방향

### 6.1 AudioLDM의 영향

AudioLDM이 발표된 이후(2023년 1월), 텍스트-오디오 생성 분야에서 **획기적인 발전**을 촉발했습니다:[2][3][4][5][6][7][8][9][10]

#### (1) **AudioLDM 2의 등장** (2023년 8월)[11][2]
AudioLDM의 저자들이 개발한 후속작으로, **"Language of Audio"(LOA) 개념**을 도입하여 음성, 음악, 음향 효과를 **통일된 프레임워크**로 처리합니다. 이는 AudioLDM의 모듈식 설계 문제를 어느 정도 해결했습니다.[11]

#### (2) **음악 특화 모델들의 발전** (2023년 8월 이후)[3][5]
- **MusicLDM**: AudioLDM 아키텍처를 음악 도메인에 적용, 비트-동기 Mixup 전략 도입[3]
- **Multi-Track MusicLDM**: 다중 악기 음악 생성으로 확장[5]

#### (3) **일반화 성능 연구** (2024년 이후)[12][13][14]
- **Leveraging Pre-trained AudioLDM**: AudioLDM의 사전학습 이점을 소규모 데이터셋에 적용하는 연구[12]
- **Latent CLAP Loss for Foley Sound Synthesis**: AudioLDM의 CLAP 손실 개선[13]
- **Domain Adaptation for ALM**: CLAP 기반 모델의 도메인 적응 방법 개발[14]

#### (4) **효율성 개선 연구** (2024년-2025년)[6][7][9]
- **EzAudio**: 효율적 확산 트랜스포머(DiT) 기반 고속 생성[6]
- **AudioLCM**: 잠재 일관성 모델로 2 반복 만에 고충실도 오디오 생성[7]
- **FlashAudio**: 직선 흐름(Rectified Flows) 기반 더 빠른 생성[9]

#### (5) **교차 모달 확장** (2024년 이후)[4][10][15]
- **AudioX**: 제어 가능한 다중 모달 입력(텍스트, 이미지, 음향) 통합 생성[4]
- **AudioComposer**: 세분화된 텍스트 설명을 위한 자동 데이터 시뮬레이션 파이프라인[10]
- **MM-LDM**: 비디오와 오디오의 다중 모달 확산 모델[15]

### 6.2 최신 연구 트렌드 (2024-2025)

#### (1) **통합 오디오 생성 모델**
- **UniAudio** (2024): 165,000시간의 오디오와 10억 개 파라미터로 훈련된 범용 오디오 기초 모델. 음성, 음악, 음향 효과를 **단일 모델**로 생성.[8]

#### (2) **샘플링 레이트 제어**
- **SRC-gAudio** (2024): 단일 모델 내에서 다양한 샘플링 레이트(8kHz~48kHz) 제어 생성 가능. AudioLDM의 16kHz 제한을 극복.[16]

#### (3) **CLAP 임베딩 공간의 이해 심화**
- **Timbre Representation in CLAP** (2025): CLAP 임베딩 공간에서 음색(timbre) 표현을 분석하여, 더욱 정밀한 제어 가능성 발견.[17]
- **Text2FX**: CLAP 임베딩을 활용한 텍스트-유도 오디오 효과 조작.[18]

#### (4) **확산 모델의 일반화 이론**
최근 이론 연구는 **확산 모델의 일반화 특성**을 밝혀냈습니다:[19][20]
- 확산 모델이 **저차원 다양체** 학습을 통해 고차원 데이터의 저주(curse of dimensionality)를 회피[19]
- **모델 재현성**: 서로 다른 확산 모델이 동일한 노이즈와 샘플링으로 유사한 출력 생성, 일반화 능력 근거[19]

### 6.3 향후 연구 고려사항

#### (1) **데이터 질과 다양성**
AudioLDM이 입증한 바와 같이, **오디오 임베딩 기반 학습의 우수성**이 확인되었습니다. 향후 연구는 **더욱 다양하고 고품질의 오디오 데이터셋** 구축에 주력해야 합니다.[12][1]

#### (2) **교차 모달 정렬 개선**
CLAP의 임베딩 정렬 성능을 더욱 향상시키면, 텍스트-오디오 생성의 **의미적 일관성**이 크게 개선될 것입니다. 특히 복합 음향 장면과 음악의 세밀한 제어가 가능해질 것입니다.[17]

#### (3) **실시간 생성 및 상호작용**
최근 **일관성 모델** 기반 방식들(AudioLCM, FlashAudio)의 발전으로 실시간 생성이 가능해지고 있습니다. 향후 **실시간 사용자 상호작용**을 지원하는 연구가 필요합니다.[7][9]

#### (4) **도메인 특화 모델과 범용 모델의 균형**
- **범용 모델**(UniAudio): 모든 오디오 유형을 처리하지만 세밀도 부족
- **특화 모델**(MusicLDM): 특정 도메인에서 뛰어나지만 확장성 한계

향후 연구는 **적응형 기초 모델** 개발을 통해 양자의 장점을 결합할 수 있습니다.[8][11]

#### (5) **윤리 및 안전성**
AudioLDM과 후속 모델들이 실제 응용으로 확대됨에 따라:[1]
- **가짜 오디오 생성**을 통한 정보 왜곡 위험
- **민감한 콘텐츠** 생성 제한
- **저작권** 문제 해결

이러한 윤리적 문제들이 향후 연구의 중요한 고려사항이 될 것입니다.[1]

#### (6) **평가 방법론 개선**
현재 평가 지표(FD, IS, KL)의 한계를 극복하기 위해, **인간 지각과 더 잘 일치하는 평가 프로토콜** 개발이 필요합니다. 특히 **음악적 표현력**, **음성의 자연스러움**, **환경음의 진정성** 평가 개선이 중요합니다.[1]

***

## 결론

AudioLDM은 텍스트-오디오 생성에 **잠재 확산 모델을 처음 성공적으로 적용**함으로써 해당 분야의 새로운 기준을 수립했습니다. CLAP 기반의 혁신적 조건화 방식과 오디오 임베딩 중심의 학습 전략은 **기존의 데이터 부족 문제를 우회**하면서도 **우수한 성능과 계산 효율성**을 동시에 달성했습니다.[1]

본 논문의 핵심 기여인 **모듈식 설계와 영점 오디오 조작** 능력은 이후 오디오 생성 분야의 발전을 촉발했으며, AudioLDM 2, MusicLDM, UniAudio 등 다양한 후속 연구로 이어졌습니다.[2][3][8][11]

향후 **고주파수 음질 개선**, **통합 다중 모달 생성**, **실시간 상호작용** 등의 방향으로 진화할 것으로 예상되며, 이 과정에서 **도메인 특화와 일반화의 균형**, **평가 방법론 개선**, **윤리적 안전성 확보**가 중요한 연구 과제로 부상할 것입니다.[9][10][8][11][19][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/225b3ef8-5dcb-4bec-bfef-068589f326c1/2301.12503v3.pdf)
[2](http://arxiv.org/pdf/2308.05734.pdf)
[3](https://arxiv.org/pdf/2308.01546.pdf)
[4](https://arxiv.org/html/2503.10522)
[5](https://arxiv.org/html/2409.02845v2)
[6](https://arxiv.org/abs/2409.10819)
[7](https://arxiv.org/abs/2406.00356)
[8](https://arxiv.org/pdf/2310.00704.pdf)
[9](https://arxiv.org/html/2410.12266v1)
[10](https://arxiv.org/html/2409.12560v2)
[11](https://arxiv.org/abs/2308.05734)
[12](https://arxiv.org/pdf/2303.03857.pdf)
[13](https://arxiv.org/pdf/2403.12182.pdf)
[14](http://arxiv.org/pdf/2402.09585.pdf)
[15](http://arxiv.org/pdf/2410.01594.pdf)
[16](https://arxiv.org/html/2410.06544v1)
[17](https://openreview.net/forum?id=KKzcDulLRm)
[18](https://arxiv.org/pdf/2409.18847.pdf)
[19](https://www.siam.org/publications/siam-news/articles/generalization-of-diffusion-models-principles-theory-and-implications/)
[20](https://academic.oup.com/nsr/article/11/12/nwae348/7810289)
[21](http://arxiv.org/pdf/2301.12503.pdf)
[22](https://audioldm.github.io)
[23](https://www.emergentmind.com/topics/text-to-audio-t2a-generation)
[24](https://openreview.net/pdf?id=rv5LuElUic)
[25](https://proceedings.mlr.press/v202/liu23f.html)
[26](https://www.inf.uni-hamburg.de/en/inst/ab/sp/research/diffusion-models.html)
[27](https://arxiv.org/abs/2301.12503)
[28](https://ui.adsabs.harvard.edu/abs/arXiv:2312.02683)
[29](https://jeongwooyeol0106.tistory.com/191)
[30](https://github.com/haoheliu/AudioLDM2)
[31](https://huggingface.co/papers/2301.12503)
[32](https://milvus.io/ai-quick-reference/how-are-audio-embeddings-integrated-into-multimodal-search-systems)
[33](https://www.sciencedirect.com/science/article/abs/pii/S0263224125027770)
[34](https://jeongwooyeol0106.tistory.com/163)
