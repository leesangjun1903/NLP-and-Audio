# Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond

### 1. 논문의 핵심 주장과 주요 기여

Qwen-VL은 Alibaba 팀이 개발한 **대규모 다용도 비전-언어 모델(Large Vision-Language Models, LVLMs)**로서, 이미지와 텍스트를 동시에 이해하고 처리할 수 있도록 설계되었습니다. 이 논문의 핵심 주장은 다음과 같습니다:[1]

**주요 기여:**
- **선도적 성능**: Qwen-VL은 유사한 모델 규모에서 이미지 캡셔닝, 시각적 질문 응답(VQA), 시각적 그라운딩 등 광범위한 벤치마크에서 기존 일반 모델들을 능가하는 성능을 달성했습니다[1]
- **세밀한 시각 이해**: 높은 해상도 입력과 세밀한 코퍼스를 통해 객체 그라운딩, 텍스트 인식, 문서 이해 등에서 뛰어난 능력을 보여줍니다[1]
- **다국어 지원**: 영어와 중국어를 포함한 다국어 데이터로 학습되어 자연스러운 다국어 상호작용을 지원합니다[1]
- **멀티 이미지 이해**: 여러 이미지를 동시에 처리하고 비교 분석할 수 있는 능력을 제공합니다[1]
- **공개 모델 커뮤니티 기여**: 기존 오픈소스 LVLM들의 부족한 최적화 문제를 해결하고, 연구 커뮤니티에 공개하여 향후 연구 촉진을 목표로 합니다[1]

***

### 2. 논문이 해결하는 문제와 제안 방법

#### 2.1 문제 정의

**해결하고자 하는 문제:**

1. **오픈소스 LVLM의 성능 격차**: 기존 오픈소스 LVLM들은 부족한 훈련과 최적화로 인해 상업 모델과 큰 격차를 보입니다[1]
2. **세밀한 시각 이해의 부족**: 대부분의 오픈소스 LVLM은 개체 그라운딩이나 텍스트 읽기 같은 세밀한 인식 능력이 부족합니다[1]
3. **복잡한 실제 시각 시나리오 대응**: 현실 세계의 복잡한 시각적 상황을 효과적이고 정확하게 이해할 필요성[1]

#### 2.2 제안된 방법

**모델 아키텍처:**

Qwen-VL은 세 가지 핵심 컴포넌트로 구성됩니다:[1]

1. **대규모 언어 모델(LLM)**: Qwen-7B의 사전 학습 가중치로 초기화됨[1]

2. **비전 인코더**: Vision Transformer(ViT) 아키텍처 기반
   - OpenCLIP의 ViT-bigG로 초기화[1]
   - 패치 분할 시 stride=14 사용

3. **위치 인식 비전-언어 어댑터(Position-aware Vision-Language Adapter)**:
   - 단일 계층의 교차 주의(cross-attention) 모듈로 구성[1]
   - 학습 가능한 벡터(임베딩)를 쿼리 벡터로, 비전 인코더의 이미지 특성을 키로 사용
   - 시각적 특성 시퀀스를 **고정 길이 256으로 압축**[1]

**어댑터의 수식 표현:**

$$\text{Adapter Output} = \text{CrossAttention}(Q, K, V)$$

여기서:
- $Q$: 학습 가능한 쿼리 벡터들 (길이 256)
- $K, V$: 비전 인코더에서 나온 이미지 특성

$$\text{Compressed Features} = \text{Attention}(Q, K, V) \cdot V^T$$

특히 중요한 점은 **2D 절대 위치 인코딩**이 쿼리-키 쌍에 포함되어 압축 과정에서 위치 정보 손실을 완화합니다.[1]

#### 2.3 입출력 인터페이스

**이미지 입력:**
- 이미지 특성 시퀀스 시작과 끝에 특수 토큰 `<img>`, `</img>` 추가[1]

**바운딩 박스 입출력:**
- 정규화 범위 [0, 1000)에서 문자열 형식으로 변환: `"(X_topleft, Y_topleft), (X_bottomright, Y_bottomright)"`[1]
- 바운딩 박스 구분 토큰: `<box>`, `</box>`
- 참조 토큰: `<ref>`, `</ref>`

***

### 3. 상세한 모델 구조 분석

#### 3.1 모델 파라미터

| 컴포넌트 | 파라미터 크기 |
|---------|------------|
| 비전 인코더(ViT) | 1.9B |
| VL 어댑터 | 0.08B |
| LLM | 7.7B |
| **전체 모델** | **9.6B**[1] |

#### 3.2 3단계 훈련 파이프라인

**Stage 1: 사전훈련 (Pre-training)**

데이터: 1.4억 개의 정제된 이미지-텍스트 쌍[1]
- LAION-en, LAION-zh, DataComp, Coyo, CC12M/CC3M, SBU, COCO 포함
- 원본 5억 개 데이터에서 28% 정제율

훈련 설정:[1]
- 입력 해상도: 224 × 224
- LLM 동결, 비전 인코더와 VL 어댑터만 최적화
- 최대 학습률: 2×10⁻⁴
- 배치 크기: 30,720
- 훈련 단계: 50,000 스텝 (약 15억 샘플 소비)
- 옵티마이저: AdamW ($\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-6}$)[1]

손실 함수:
$$L = -\sum_{i=1}^{N} \log P(y_i | x, I)$$

여기서 $P(y_i | x, I)$는 이미지 $I$와 컨텍스트 $x$가 주어졌을 때 토큰 $y_i$의 예측 확률

**Stage 2: 멀티태스크 사전훈련 (Multi-task Pre-training)**

데이터: 7가지 작업 동시 훈련[1]
- 캡셔닝: 19.7M
- VQA: 3.6M
- 그라운딩: 3.5M
- 참조 그라운딩: 8.7M
- 그라운디드 캡셔닝: 8.7M
- OCR: 24.8M
- 순수 텍스트 자동회귀: 7.8M

훈련 설정:[1]
- 입력 해상도: 448 × 448
- 전체 모델 훈련 (LLM 해제)
- 최대 학습률: 5×10⁻⁵
- 배치 크기: 4,096
- 훈련 단계: 19,000 스텝

**Stage 3: 지도 미세조정 (Supervised Fine-tuning)**

데이터: 350k 지침 튜닝 데이터[1]

훈련 설정:[1]
- 비전 인코더 동결, LLM과 어댑터 모듈만 최적화
- 최대 학습률: 1×10⁻⁵
- 배치 크기: 128
- 훈련 단계: 8,000 스텝

***

### 4. 성능 향상 및 벤치마크 결과

#### 4.1 이미지 캡셔닝 및 일반 VQA

| 작업 | 벤치마크 | Qwen-VL | 최고 일반 모델 | 성능 |
|------|---------|---------|------------|------|
| 캡셔닝 | Flickr30K (0-shot) | **85.8** CIDEr | 82.8 (InstructBLIP) | +3.0 |
| 캡셔닝 | Nocaps (0-shot) | **121.4** CIDEr | 103.9 (BLIP-2) | +17.5 |
| VQA | VQAv2 | **79.5** | 77.36 (Shikra) | +2.14 |
| VQA | OKVQA | **58.6** | 50.6 (Flamingo-80B) | +8.0 |
| VQA | GQA | **59.3** | 49.5 (InstructBLIP) | +9.8 |

#### 4.2 텍스트 지향 VQA (세밀한 이해)

| 벤치마크 | Qwen-VL | 이전 LVLM | 개선 |
|---------|---------|---------|------|
| TextVQA | **63.8** | 50.7 (InstructBLIP) | +13.1 |
| DocVQA | **65.1** | 62.2 (mPLUG-DocOwl) | +2.9 |
| ChartQA | **65.7** | 57.4 (mPLUG-DocOwl) | +8.3 |
| OCR-VQA | **75.7** | 71.3 (Pix2Struct) | +4.4 |

#### 4.3 참조 표현 이해 (그라운딩)

| 벤치마크 | Qwen-VL | Shikra-13B | 개선 |
|---------|---------|-----------|------|
| RefCOCO (val) | **89.36** | 87.83 | +1.53 |
| RefCOCO+ (val) | **83.12** | 82.89 | +0.23 |
| RefCOCOg (val) | **85.58** | 82.64 | +2.94 |
| GRIT | **78.22** | 69.03 | +9.19 |

#### 4.4 소수 샘플 학습 (Few-shot Learning)

Qwen-VL은 강력한 맥락 내 학습(in-context learning) 능력을 보여주며:[1]
- OKVQA, VizWiz, TextVQA, Flickr30k에서 유사 파라미터 모델(Flamingo-9B, IDEFICS-9B)을 능가
- Flamingo-80B와 유사한 성능 달성

#### 4.5 실제 사용자 행동 기반 평가

| 벤치마크 | 메트릭 | Qwen-VL-Chat | 최고 경쟁사 | 개선 |
|---------|--------|--------------|----------|------|
| TouchStone | 전체 점수 | **645.2** (영어), **401.2** (중국어) | 605.4 (mPLUG-Owl) | +39.8 (영어) |
| SEED-Bench | 정확도 | **58.2%** | 53.4% (InstructBLIP) | +4.8% |
| MME | 인식 + 인지 | **1487.58 + 360.71** | 1212.82 + 291.79 (InstructBLIP) | +274.76 + 68.92 |

***

### 5. 일반화 성능 향상 관련 핵심 분석

#### 5.1 모델 일반화 능력의 근원

**다양한 데이터 기반 학습:**
Qwen-VL은 1.4억 개의 정제된 이미지-텍스트 쌍으로 사전 훈련되어 광범위한 시각적 개념을 학습합니다. 데이터 정제 과정이 다음을 포함합니다:[1]
- 극단적 종횡비 제거
- 너무 작은 이미지 제거
- 낮은 CLIP 점수 제거
- 비영어/비중국어 텍스트 제거
- 이모지 문자 제거
- 비정상적 텍스트 패턴 정리

**멀티태스크 학습의 효과:**
Stage 2에서 7가지 작동 작업을 동시에 학습하므로:[1]
- 각 작업의 특화된 지식이 다른 작업으로 전이
- 모델이 다양한 시각-언어 이해 패턴 습득
- 한 작업의 개선이 다른 작업 성능도 향상

**구체적 수식 표현:**
$$L_{total} = \sum_{t=1}^{7} \lambda_t L_t$$

여기서 $L_t$는 작업 $t$의 손실, $\lambda_t$는 작업 가중치

#### 5.2 제로샷(Zero-shot) 및 소수샷(Few-shot) 학습 성능

**제로샷 성능:**
- ScienceQA에서 **67.1%** 정확도 (수정되지 않은 작업)
- VizWiz에서 **35.2%** VQA 점수

**소수샷 성능의 우수성:**
Figure 4에 따르면, Qwen-VL은 1, 4, 8, 16 샷 설정에서 지속적으로 성능 향상을 보여줍니다. 이는 모델이 컨텍스트에서 새로운 패턴을 빠르게 학습하는 능력을 나타냅니다.[1]

#### 5.3 높은 해상도의 영향

**해상도 증가로 인한 개선:**
- Stage 1: 224 × 224 해상도에서 학습
- Stage 2: 448 × 448로 업그레이드[1]

이로 인한 이점:

$$\text{Information Preservation Ratio} = 1 - \frac{\text{Down-sampling Loss}_{224}^2}{\text{Down-sampling Loss}_{448}^2}$$

448 × 448 해상도는 224 × 224보다 4배 많은 픽셀 정보를 유지하므로, 세밀한 텍스트 인식과 개체 위치 파악 능력이 크게 개선됩니다.[1]

#### 5.4 어댑터 설계의 일반화 효과

**쿼리 개수 선택:**
Figure 7에 따르면, 256개의 학습 가능한 쿼리를 선택한 이유:[1]
- 너무 적은 쿼리(64, 144): 초기 손실은 낮지만 수렴이 느림
- 최적 선택(256): 수렴 속도와 최종 성능의 균형

$$\text{Compression Ratio} = \frac{\text{Input Sequence Length}}{\text{Output Sequence Length}} = \frac{(448/14)^2}{256} = \frac{1024}{256} = 4$$

이 4배 압축은 계산 효율성을 유지하면서도 중요한 시각 정보를 보존합니다.

**위치 정보 보존:**
2D 절대 위치 인코딩이 쿼리-키 쌍에 포함되어, 특성 압축 중에도 공간적 위치 정보가 손실되지 않습니다. 이는 다음 수식으로 표현됩니다:[1]

$$\text{Position Encoding}_{2D}(i,j) = [\sin(i/10000^{2k/d}), \cos(i/10000^{2k/d}), \sin(j/10000^{2k/d}), \cos(j/10000^{2k/d})]$$

#### 5.5 다국어 학습의 일반화 이점

데이터 구성:[1]
- 영어 텍스트: 77.3%
- 중국어 텍스트: 22.7%

다국어 학습의 효과:
- 언어별 개념 표현의 공유로 더 강건한 표현 학습
- 언어 교차 지식 전이
- 새로운 언어나 도메인으로의 더 나은 적응성

#### 5.6 순수 텍스트 능력 보존

Table 11에 따르면, Qwen-VL은 멀티모달 훈련 후에도 순수 텍스트 작업에서 성능 저하 없음:[1]
- MMLÚ: 50.7 (Qwen-VL) vs 49.9 (Qwen-7B 중간)
- CMMLÚ: 49.5
- C-Eval: 51.1

이는 Stage 2와 3에서 순수 텍스트 데이터(7.8M)를 포함시킨 결과로, **재앙적 망각(catastrophic forgetting)** 문제를 해결합니다.[1]

***

### 6. 모델의 한계점

논문에서 명시적으로 언급된 한계점과 극복 가능성:

**현재 한계:**

1. **전문가 모델(specialist models)과의 성능 격차**
   - RefCOCO (val): Qwen-VL 89.36 vs ONE-PEACE 92.58 (차이: 3.22%)
   - OCR-VQA: Qwen-VL 75.7 vs PALI-X-55B 75.0 (경쟁)
   - 그러나 일반 모델 범주 내에서는 최고 성능 유지[1]

2. **모델 크기 확장의 필요성**
   - 현재 9.6B 파라미터는 55B 모델(PALI-X)보다 작음
   - 더 큰 모델 크기로 성능 향상 가능성 있음[1]

3. **비디오 이해 능력의 제한**
   - SEED-Bench의 비디오 작업에서 37.8% 정확도 (이미지: 65.4%)
   - 단순 프레임 샘플링으로만 달성[1]

#### 향후 개선 방향:[1]

1. **멀티모달 확장**: 음성, 비디오 등 추가 모달리티 통합
2. **확장성 향상**: 모델 크기, 훈련 데이터, 해상도 증대
3. **생성 능력 강화**: 고충실도 이미지 생성, 자연스러운 음성 생성

***

### 7. 모델 일반화 성능 향상의 메커니즘 종합

일반화 성능 향상을 위한 핵심 전략:

$$\text{Generalization Score} = \alpha \cdot \text{Data Diversity} + \beta \cdot \text{Task Diversity} + \gamma \cdot \text{Architecture Efficiency}$$

**각 요소의 기여:**
- **데이터 다양성** ($\alpha$): 1.4억 정제 샘플, 77.3% 영어 + 22.7% 중국어
- **작업 다양성** ($\beta$): 7가지 멀티태스크 학습
- **아키텍처 효율성** ($\gamma$): 256으로의 특성 압축으로 계산 효율성 유지

***

### 8. 최신 연구 기반 향후 연구 고려사항

#### 8.1 현재의 연구 동향 (2024-2025년)[2][3][4][5]

**1. 공간 추론 능력의 강화**
최근 연구(SPHERE 벤치마크)에서 현재 VLM들이 다차원 공간 추론에서 약점을 보입니다. Qwen-VL도 이 영역에서 개선이 필요합니다.[6]

**2. 모달리티 붕괴(Modality Collapse) 해결**
최신 설문(2025년 ACL)에 따르면, 많은 VLM이 텍스트 정보에 과도하게 의존하며 시각 정보를 무시하는 경향이 있습니다. Qwen-VL의 교차 주의 메커니즘이 이를 어느 정도 완화하지만, 추가 개선 필요합니다.[7]

**3. 혼합 전문가(Mixture-of-Experts) 구조의 도입**
최신 모델들(Llama 4, DeepSeek-VL2, Kimi-VL)이 MoE 디코더를 통합하여 효율성과 성능을 동시에 달성하고 있습니다.[8]

#### 8.2 Qwen-VL의 개선을 위한 제안[4][5][9]

**1. 세밀한 적응형 도메인 일반화**
$L_{domain} = L_{source} + \lambda \cdot L_{domain\_gap}$

의료 영상이나 원격 감지 같은 도메인에서의 성능 개선을 위해, 도메인 특화 적응 방법 도입.[9]

**2. 동적 프롬프트 튜닝(Dynamic Prompt Tuning)**
최신 연구(M2PT)에서 멀티모달 프롬프트 튜닝이 효과적임을 보여줍니다. 이는 0.09% 파라미터만 사용하면서도 경쟁력 있는 성능 달성.[10]

**3. 시각-언어 정렬 개선**
최신 FiVL 프레임워크는 주의 메커니즘의 투명성을 통해 시각-언어 정렬을 개선합니다. 비전-언어 정렬을 위한 명시적 손실 항 추가:[11]

$$L_{alignment} = -\sum_{v,l} \text{sim}(f_v, f_l) \cdot \log(\text{sim}(f_v, f_l))$$

#### 8.3 구체적인 연구 방향[5][12][4]

**1. 향상된 위치-인식 융합 메커니즘**
현재 Qwen-VL의 2D 위치 인코딩을 넘어, 적응형 공간 주의(Adaptive Spatial Attention)를 도입:

```math
\text{Spatial Attention}(i,j) = \frac{\exp(\text{Pos\_Score}(i,j))}{\sum_{i',j'} \exp(\text{Pos\_Score}(i',j'))}
```

**2. 다해상도 계층적 특성 처리**
- 저해상도: 의미론적 이해
- 고해상도: 세밀한 세부 정보

**3. 비전-언어 교차 엔트로피 손실**
최신 연구에서 제안된 손실 함수로 모달리티 붕괴 완화:

$$L = L_{vision} + L_{language} + \alpha \cdot L_{cross\_entropy}$$

***

### 결론

Qwen-VL은 **현저한 엔지니어링과 체계적인 3단계 훈련 전략**을 통해 비전-언어 이해의 새로운 기준을 설정했습니다. 특히 **위치-인식 어댑터를 통한 효율적인 특성 압축**, **다국어 멀티태스크 학습**, **높은 해상도 입력 처리**가 일반화 성능을 크게 향상시킵니다.[1]

향후 연구에서는 **공간 추론 강화**, **모달리티 붕괴 해결**, **MoE 구조 통합**, **동적 프롬프트 튜닝** 등을 통해 더욱 강건하고 효율적인 비전-언어 모델로 발전할 수 있을 것으로 예상됩니다. Qwen-VL의 공개 모델 정책과 체계적인 설계는 비전-언어 모델 연구의 대중화에 크게 기여할 것으로 기대됩니다.[3][2][1]

***

### 참고 문헌 표기

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e9793b14-9085-47bc-ae82-44db5cf6f2cf/2308.12966v3.pdf)
[2](https://arxiv.org/abs/2408.16500)
[3](https://arxiv.org/pdf/2408.12637.pdf)
[4](https://openreview.net/forum?id=YklIjgIZzn)
[5](https://www.sciencedirect.com/science/article/abs/pii/S1566253525006955)
[6](http://arxiv.org/pdf/2412.12693.pdf)
[7](https://aclanthology.org/2025.findings-acl.1256.pdf)
[8](https://huggingface.co/blog/vlms-2025)
[9](https://arxiv.org/abs/2401.01736)
[10](https://aclanthology.org/2024.emnlp-main.218.pdf)
[11](https://arxiv.org/html/2412.14672)
[12](https://www.arxiv.org/pdf/2507.23064.pdf)
[13](http://arxiv.org/pdf/2311.17091.pdf)
[14](https://arxiv.org/pdf/2308.12966.pdf)
[15](https://arxiv.org/html/2410.07112v2)
[16](https://arxiv.org/abs/2403.09027)
[17](https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf)
[18](https://www.emergentmind.com/topics/gated-cross-attention-mechanism)
[19](https://arxiv.org/html/2501.02189v6)
