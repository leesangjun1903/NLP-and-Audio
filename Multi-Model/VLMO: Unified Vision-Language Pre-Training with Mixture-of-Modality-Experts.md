
# VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

## 1. 핵심 주장 및 주요 기여

### 1.1 핵심 주장

VLMO(Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts)는 **단일 통합 모델이 두 가지 상이한 아키텍처의 장점을 모두 활용할 수 있다**는 혁신적 주장을 제시합니다. 기존 비전-언어 모델들은 다음 두 가지 선택에 직면했습니다:

1. **이중 인코더(Dual Encoder, CLIP/ALIGN 방식)**: 이미지와 텍스트를 별도로 인코딩하여 빠른 검색(O(N) 시간복잡도)이 가능하지만, 코사인 유사도만으로 상호작용하여 복잡한 추론 작업(VQA, Visual Reasoning)에서 성능 저하

2. **융합 인코더(Fusion Encoder, ViLBERT/LXMERT 방식)**: 교차-모달 어텐션으로 깊은 상호작용을 모델링하여 분류 작업에서는 우수하지만, 모든 이미지-텍스트 쌍을 함께 인코딩해야 하므로 검색 속도 느림(O(N²) 시간복잡도)

### 1.2 주요 기여

1. **통합 비전-언어 사전학습 모델 제안**: 동일한 모델이 세밀 조정 방식에 따라 이중 인코더로도, 융합 인코더로도 기능

2. **Mixture-of-Modality-Experts (MOME) Transformer 도입**: 각 Transformer 블록에 모달리티별 전문가(비전, 언어, 비전-언어 전문가)를 배치하고, 모달리티 간 공유 자기-어텐션으로 정렬

3. **단계적 사전학습(Stagewise Pre-Training) 전략**: 이미지 전용, 텍스트 전용 데이터를 활용하여 제한된 이미지-텍스트 쌍 데이터의 부족을 보완

4. **최첨단 성능 달성**: VQA, NLVR2, 이미지-텍스트 검색 등 다양한 벤치마크에서 이전 모델들을 상당히 초월

***

## 2. 해결 문제와 제안 방법

### 2.1 문제 정의

#### 문제 1: 아키텍처의 근본적 트레이드오프
기존 비전-언어 모델들은 **두 가지 아키텍처 중 하나를 선택**해야 했습니다:
- 이중 인코더: 검색 효율성 우수 ↔ 상호작용 능력 약함
- 융합 인코더: 상호작용 능력 우수 ↔ 검색 효율성 떨어짐

#### 문제 2: 사전학습 데이터의 비효율
- 이미지-텍스트 쌍 데이터는 제한적 (논문 기준 4M 이미지)
- 쌍 데이터의 텍스트가 단순하고 짧음 (평균 캡션 길이 제한)
- 이미지 전용, 텍스트 전용의 대규모 데이터가 미활용됨

### 2.2 Mixture-of-Modality-Experts (MOME) Transformer

#### 기본 구조

$$ H'_l = \text{MSA}(\text{LN}(H_{l-1})) + H_{l-1} \quad \cdots (1) $$

$$ H_l = \text{MoME-FFN}(\text{LN}(H'_l)) + H'_l \quad \cdots (2) $$

여기서:
- **MSA**: 다중-헤드 자기-어텐션 (모든 모달리티에서 공유)
- **MoME-FFN**: 입력 모달리티에 따라 선택되는 전문가 FFN
- **LN**: 층 정규화

#### 세 가지 모달리티 전문가

1. **비전 전문가(V-FFN)**: 이미지 전용 입력 처리
2. **언어 전문가(L-FFN)**: 텍스트 전용 입력 처리
3. **비전-언어 전문가(VL-FFN)**: 이미지-텍스트 쌍의 깊은 상호작용 (상위 2-3 레이어에만 사용)

#### 라우팅 메커니즘
- **하위 레이어(1~L-F)**: 입력 모달리티 타입에 따라 V-FFN 또는 L-FFN 선택 → 모달리티별 특성 학습
- **상위 레이어(L-F+1~L)**: VL-FFN 사용 → 이미지-텍스트 간 상호작용 강화

### 2.3 입력 표현 (Input Representations)

#### 이미지 표현

이미지 $v \in \mathbb{R}^{H \times W \times C}$를 패치 크기 $P \times P$로 분할:

$$ H_v^0 = [v_{[I\_CLS]}, V \cdot v_p^1, \ldots, V \cdot v_p^N] + V_{pos} + V_{type} $$

- 패치 개수: $N = HW/P^2$ (일반적으로 $P=16$)
- 선형 투영: $V \in \mathbb{R}^{(P^2C) \times D}$
- 위치 임베딩: $V_{pos} \in \mathbb{R}^{(N+1) \times D}$
- 타입 임베딩: $V_{type} \in \mathbb{R}^{D}$

#### 텍스트 표현

$$ H_w^0 = [w_{[T\_CLS]}, w_1, \ldots, w_M, w_{[T\_SEP]}] + T_{pos} + T_{type} $$

- WordPiece 토크나이제이션 (최대 길이 40)
- 위치 및 타입 임베딩 추가

#### 이미지-텍스트 표현

$$ H_{vl}^0 = [H_w^0; H_v^0] $$

### 2.4 사전학습 목표 함수

#### 1) 이미지-텍스트 대조 학습 (Image-Text Contrastive Learning)

배치의 N 이미지-텍스트 쌍에서 올바른 쌍을 찾는 작업:

$$ s^{i2t}_{i,j} = \hat{h}^{v\top}_i \hat{h}^w_j, \quad s^{t2i}_{i,j} = \hat{h}^{w\top}_i \hat{h}^v_j \quad \cdots (3) $$

$$ p^{i2t}_i = \frac{\exp(s^{i2t}_{i,i}/\sigma)}{\sum^N_{j=1} \exp(s^{i2t}_{i,j}/\sigma)}, \quad p^{t2i}_i = \frac{\exp(s^{t2i}_{i,i}/\sigma)}{\sum^N_{j=1} \exp(s^{t2i}_{i,j}/\sigma)} \quad \cdots (4) $$

손실함수:

$$ \mathcal{L}_{ITC} = -\frac{1}{N} \sum^N_{i=1} [\log p^{i2t}_i + \log p^{t2i}_i] $$

여기서 $\sigma$는 학습 가능한 온도 매개변수입니다.

#### 2) 마스크 언어 모델링 (Masked Language Modeling)

텍스트의 15%를 [MASK]로 교체한 후 예측:

$$ \mathcal{L}_{MLM} = -\sum_{t \in \text{masked}} \log P(w_t | w_{\neg t}, v) $$

#### 3) 이미지-텍스트 매칭 (Image-Text Matching)

이미지와 텍스트가 매칭되었는지 이진 분류:

$$ \mathcal{L}_{ITM} = -\log P(\text{match} | h_{[T\_CLS]}) $$

**혁신점**: 전역 하드 네거티브 마이닝(Global Hard Negative Mining)
- 기존(ALBEF): 단일 GPU 내 32개 예제에서만 마이닝
- VLMO: 모든 GPU의 1024개 예제에서 마이닝 → **1.84% 성능 개선**

### 2.5 단계적 사전학습 (Stagewise Pre-Training)

#### Stage 1: 비전 사전학습 (Vision Pre-Training)
- **데이터**: 대규모 이미지 전용 데이터
- **방법**: BEIT 마스크 이미지 모델링
- **학습**: V-FFN과 자기-어텐션 모듈
- **초기화**: BEIT-Base 가중치 사용

#### Stage 2: 언어 사전학습 (Language Pre-Training)
- **데이터**: Wikipedia, BookCorpus (텍스트 전용)
- **방법**: 마스크 언어 모델링
- **학습**: L-FFN만 학습 (V-FFN, 자기-어텐션은 동결)
- **이점**: 비전과 독립적으로 언어 표현 학습

#### Stage 3: 비전-언어 사전학습 (Vision-Language Pre-Training)
- **초기화**: Stage 1, 2의 사전학습된 모델 사용
- **학습**: 전체 모델 (공유 파라미터)
- **목표**: 이미지-텍스트 정렬 학습

**성능 개선**:
| 초기화 방식 | NLVR2 (dev) | Flickr30K (평균 R@) | 개선도 |
|----------|-----------|------------------|-------|
| 랜덤 | 낮음 | 낮음 | - |
| 이미지 전용 | 80.33% | 87.69% | - |
| **이미지 + 텍스트** | **82.09%** | **88.52%** | **+1.76%** |

***

## 3. 모델 구조

### 3.1 모델 규모

**VLMO-Base**:
- Transformer 레이어: 12
- 숨겨진 차원: 768
- 어텐션 헤드: 12
- FFN 중간 크기: 3,072
- VL-FFN 레이어: 상위 2개

**VLMO-Large**:
- Transformer 레이어: 24
- 숨겨진 차원: 1,024
- 어텐션 헤드: 16
- FFN 중간 크기: 4,096
- VL-FFN 레이어: 상위 3개

**VLMO-Large++**:
- VLMO-Large를 1.0B 노이즈 웹 이미지-텍스트 쌍으로 재훈련
- 배치 크기: 32K (단계별 훈련 조정)

### 3.2 세밀 조정 (Fine-Tuning) 전략

#### 분류 작업용 (VQA, NLVR2)
```
입력: 이미지-텍스트 쌍
인코딩: 융합 인코더 (모든 레이어의 모든 전문가 활성화)
출력: [T_CLS] 토큰 → 분류층 → 예측
```

#### 검색 작업용 (이미지-텍스트 검색)
```
이미지: V-FFN으로 인코딩 → [I_CLS] 특성 벡터
텍스트: L-FFN으로 인코딩 → [T_CLS] 특성 벡터
유사도: 점곱(dot product) 계산
시간복잡도: O(N)
```

***

## 4. 성능 향상

### 4.1 비전-언어 분류 작업

#### VQA 2.0 결과

| 모델 | 테스트-개발 | 테스트-표준 | 개선도 |
|------|----------|-----------|-------|
| UNITER-Base | 72.70 | 72.91 | - |
| ALBEF-Base | 74.54 | 74.70 | - |
| **VLMO-Base** | **76.64** | **76.89** | **+2.1%** |
| SimVLM-Large | 79.32 | 79.56 | - |
| **VLMO-Large** | **79.94** | **79.98** | **+0.6%** |
| **VLMO-Large++** | **82.88** | **82.78** | **+3.6%** |

#### NLVR2 결과

| 모델 | Dev | Test-P | 개선도 |
|------|-----|--------|--------|
| UNITER-Base | 77.18 | 77.85 | - |
| ALBEF-Base | 80.24 | 80.50 | - |
| **VLMO-Base** | **82.77** | **83.34** | **+2.5%** |
| SimVLM-Large | 84.13 | 84.84 | - |
| **VLMO-Large** | **85.64** | **86.86** | **+1.5%** |
| **VLMO-Large++** | **88.62** | **89.54** | **+3.0%** |

### 4.2 비전-언어 검색 작업

#### COCO (5K 테스트)

| 모델 | TR R@1 | TR R@5 | IR R@1 | IR R@5 | 개선도 |
|------|--------|--------|--------|--------|--------|
| UNITER-Large | 65.7 | 88.6 | 52.9 | 79.9 | - |
| ALBEF-Large | 73.1 | 91.4 | 56.8 | 81.5 | - |
| **VLMO-Large** | **78.2** | **94.4** | **60.6** | **84.4** | **+5.1/+2.9** |
| **VLMO-Large++** | **83.1** | **96.0** | **65.2** | **86.5** | **+10/+8.2** |

**주요 성과**: 이중 인코더로 사용하면서도 **융합 인코더 모델보다 우수한 성능**

#### Flickr30K (1K 테스트)

| 모델 | TR R@1 | IR R@1 |
|------|--------|--------|
| ALBEF-Base | 94.3 | 82.8 |
| **VLMO-Base** | **92.3** | **79.3** |
| VLMO-Large | **95.3** | **84.5** |
| **VLMO-Large++** | **96.8** | **88.1** |

### 4.3 비전 작업 성능

| 모델 | ImageNet (acc@1) | ADE20K (mIoU) |
|------|-----------------|---------------|
| ViT-Base | 83.6 | - |
| BEIT-Base | 85.2 | 52.8 |
| **VLMO-Base** | **85.5** | **53.4** |

***

## 5. 일반화 성능 향상 분석

### 5.1 단계적 사전학습의 효과

**NLVR2 개발 세트**:
- 이미지 전용 사전학습: 80.33%
- 이미지 + 텍스트 사전학습: **82.09%** (+1.76%)

**Flickr30K 검색**:
- 이미지 전용: 87.69%
- 이미지 + 텍스트: **88.52%** (+0.83%)

**분석**: 텍스트 전용 사전학습이 비전-언어 작업에 예상 이상의 도움을 주는 것으로 나타남. 이는 비전-언어 쌍 데이터의 낮은 품질을 고품질 텍스트 데이터로 보완하는 효과로 해석됩니다.

### 5.2 MOME Transformer 구조의 기여

| 구성 | NLVR2 (dev) | Flickr30K (평균) |
|------|-----------|------------|
| 표준 Transformer | 78.81% | 93.37% |
| MOME (VL-FFN 제거) | 79.58% | 94.50% |
| **완전한 MOME** | **80.13%** | **95.17%** |

**분석**:
- 모달리티 전문가 전체: +1.32% (NLVR2)
- VL-FFN 추가: +0.55% (깊은 상호작용의 가치)
- 공유 자기-어텐션: +1.21% (모달리티 정렬 효과)

### 5.3 전역 하드 네거티브 마이닝

| 마이닝 방식 | NLVR2 (dev) | Test-P |
|----------|----------|--------|
| 로컬(32개) | 77.70% | 77.95% |
| **전역(1024개)** | **79.54%** | **79.48%** |

**개선도**: +1.84% (정보성 높은 음성 샘플 활용의 중요성)

### 5.4 공유 자기-어텐션의 효과

| 어텐션 타입 | NLVR2 (dev) | Flickr30K (평균) |
|----------|-----------|------------|
| 별도 어텐션 | 78.92% | 94.63% |
| **공유 어텐션** | **80.13%** | **95.17%** |

***

## 6. 모델의 한계

### 6.1 기술적 한계

1. **계산 복잡도 미해결**
   - VL-FFN 사용 상위 레이어에서 여전히 O(N²) 시간복잡도
   - 극대규모 배치에서 메모리 효율성 제한

2. **전문가 선택의 정적 특성**
   - 모달리티 전문가 선택이 사전정의된 규칙에 따름
   - 동적 라우팅 메커니즘 부재 → 학습 가능한 게이트 방식 미도입

3. **비전 특성 추출의 단순성**
   - ViT 스타일의 선형 패치 임베딩만 사용
   - 계층적 특성 추출(hierarchical features) 미활용

### 6.2 성능 관련 한계

1. **공간 추론 능력 부족** (논문 이후 연구에서 발견)
   - VLM의 공간 관계 이해 능력이 제한적
   - VLMO도 이 문제에서 완전히 자유롭지 않을 가능성

2. **다국어 지원 부재**
   - 영어 중심의 사전학습
   - 다언어로의 전이 성능 미검증

3. **노이즈 데이터에 대한 견고성**
   - VLMO-Large++는 1.0B 노이즈 이미지로 훈련했지만, 극한의 노이즈 환경에서의 성능 미평가

### 6.3 데이터 관련 한계

1. **이미지-텍스트 쌍 데이터 필수**
   - 단계적 사전학습이 도움이 되지만, 최종 단계에서는 쌍 데이터 필수
   - 쌍 데이터의 일반화 능력 제한

2. **대규모 검색 최적화 미흡**
   - 매우 큰 규모 검색(수억 개 이미지)에서의 성능/메모리 트레이드오프 미분석

### 6.4 통합 아키텍처의 트레이드오프

1. **각 작업별 최적화 부재**
   - 완벽한 이중 장점이 아닌 타협의 산물
   - 각 작업에 특화된 모델에 비해 약간의 성능 저하 가능성

2. **모델 크기 오버헤드**
   - 세 종류의 전문가(V-FFN, L-FFN, VL-FFN) 유지로 메모리 오버헤드 발생
   - 단일 FFN 모델 대비 매개변수 증가

***

## 7. 논문의 영향 및 향후 연구 방향

### 7.1 학계에 미친 영향

#### 아키텍처 설계의 패러다임 전환

VLMO 이후 관련 연구들의 변화:

1. **혼합-전문가 기반 멀티모달 모델 확산**
   - FuseMoE (NeurIPS 2024): 불완전한 다중 모달 데이터 처리로 확장
   - 라플라스 게이팅으로 수렴성 개선
   - VLMO 스타일의 혼합-전문가가 표준 패러다임으로 인정

2. **효율적 VLM 개발 가속화**
   - EfficientVLM (2023): VLMO의 모달리티별 전문가 개념으로 경량화
   - 매개변수 70% 감소에도 성능 향상 (VQA +4.9%, NLVR2 +5.6%)
   - VLMO의 설계 원리가 효율성 개선의 핵심 도구로 활용

3. **의료 영상 VLM에의 영향**
   - 2024-2025년 의료 VLM들(Llama3-Med, LViT, Med-VLFM)이 단계적 사전학습 채택
   - 도메인 특화 이미지/텍스트 데이터로 재훈련

#### 사전학습 전략의 발전

- **SimVLM(2022)의 약감시 학습**과 **VLMO의 단계적 사전학습**이 상호 보완적임 입증
- 최신 모델들이 두 전략을 통합하여 사용

#### 벤치마킹 표준화

- VLUE 벤치마크(2022) 확립에 영향
- 효율성-성능 트레이드오프 분석의 중요성 인식

### 7.2 실무 응용

1. **멀티모달 검색 시스템**
   - E-커머스, 소셜 미디어 콘텐츠 검색
   - 대규모 이미지 검색에 VLMO 스타일의 이중 인코더 적용

2. **멀티태스크 학습 플랫폼**
   - 단일 모델로 분류와 검색 모두 지원
   - 배포 비용 절감, 유지보수 효율성 증대

### 7.3 향후 연구 고려사항

#### 단기 연구 (1-2년)

1. **동적 라우팅 메커니즘**
   - 현재: 고정 레이어 기반 선택
   - 개선: 입력에 따른 학습 가능한 게이트 (TOP-K 라우팅)
   - 기대 효과: 효율성 +15-20%

2. **공간 추론 능력 강화**
   - 상대 위치 인코딩 개선 (PEVL 방식)
   - 공간 관계를 명시적으로 학습하는 사전학습 작업 추가
   - 기대 효과: VQA +2-3%, NLVR2 +1-2%

3. **다국어 지원**
   - 100+ 언어 사전학습 데이터 통합
   - 참고: mBERT, XLM-R의 다국어 접근법

#### 중기 연구 (2-4년)

1. **생성 기능 통합**
   - 이미지 캡셔닝, 시각적 질문 답변 생성
   - 참고: UniLM, BLIP-2 아키텍처

2. **도메인별 특화 모델**
   - 의료(CT, X-ray), 산업(제조, 농업), 원격감지 영상
   - 도메인 특화 코퍼스로 재사전학습

3. **효율성 개선**
   - 지식 증류: 더 작은 모델로 압축
   - 양자화: INT8 이상
   - 목표: 엣지 디바이스 배포 (100M 매개변수 이하)

#### 장기 연구 (4년 이상)

1. **멀티모달 확장**
   - 비디오, 음성, 3D 데이터 통합
   - 논문 결론에서 명시된 미래 방향

2. **개방형 세계 이해(Open-World Understanding)**
   - 훈련 데이터에 없는 개념 학습
   - 제로샷 성능 향상

3. **설명 가능성 강화**
   - 의사결정 프로세스의 투명성
   - 의료, 금융 등 규제 분야 진출

### 7.4 VLMO 이후의 최신 발전 (2023-2025)

#### 관련 최신 모델들

| 모델 | 년도 | 주요 혁신 | VLMO와의 관계 |
|------|------|---------|----------|
| EfficientVLM | 2023 | 모달리티 전문가로 경량화 | VLMO 설계 개념 활용 |
| FuseMoE | 2024 | 불완전 모달 데이터 처리 | 혼합-전문가 확장 |
| NEVLP | 2024 | 노이즈 견고 사전학습 | 데이터 품질 개선 |
| 의료 VLM 시리즈 | 2024-25 | 단계적 사전학습 | VLMO 전략 채택 |
| FiVL | 2025 | 비전-언어 정렬 강화 | 공유 어텐션 개념 확장 |

#### 새로운 패러다임

1. **약감시 학습(Weak Supervision)**
   - 웹 데이터 노이즈 처리 개선
   - VLMO의 단계적 접근을 보완

2. **프롬프트 학습 통합**
   - 고정 모델에서 프롬프트만 학습
   - 매개변수 효율적 세밀 조정

3. **LLM 통합**
   - BLIP-2, LLaVA: 비전 인코더 + LLM
   - VLMO의 통합 설계와 상이한 접근

### 7.5 실무 적용 시 고려사항

#### 배포 환경별 최적화

| 환경 | 추천 모델 | 특징 |
|------|---------|------|
| 클라우드 | VLMO-Large++ | 최고 성능 |
| 서버 | VLMO-Large | 균형잡힌 성능 |
| 엣지/모바일 | EfficientVLM | 경량화 |

#### 작업별 전략

- **검색 중심**: 이중 인코더 모드 활성화
- **분류 중심**: 융합 인코더 모드 활성화
- **혼합 작업**: VLMO의 통합 설계 활용

#### 데이터 준비

1. **이미지-텍스트 쌍**: 최소 1M 권장
2. **도메인 특화 데이터**: 추가 수집 시 성능 크게 향상
3. **데이터 품질**: VLMO-Large++가 1.0B 노이즈 이미지로도 우수 성능 입증

***

## 8. 결론

VLMO는 비전-언어 사전학습의 **패러다임 전환점**을 제시합니다. 기존의 이분법적 선택(이중 vs 융합)을 넘어, **모달리티별 전문화와 공유 어텐션을 결합**한 새로운 통합 아키텍처를 제안함으로써 다양한 다운스트림 작업에 효과적으로 대응할 수 있는 길을 열었습니다.

### 핵심 가치

1. **아키텍처 혁신**: MOME Transformer로 단일 모델의 다목적 활용
2. **학습 전략 개선**: 단계적 사전학습으로 일반화 성능 향상
3. **성능 우수성**: 모든 주요 벤치마크에서 SOTA 달성
4. **실용성**: 검색과 분류 작업의 효율성을 동시에 확보

### 해결 과제

- 공간 추론 능력 강화
- 동적 라우팅 메커니즘 도입
- 다국어 지원 확대
- 생성 기능 통합

VLMO가 제시한 혼합-전문가 기반의 모달리티별 특화 아키텍처는 앞으로의 멀티모달 AI 연구에서 표준 설계 원리로 자리잡을 것으로 예상되며, 의료, 산업, 커머스 등 다양한 분야의 실무 응용에서 그 가치가 지속적으로 입증되고 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/804d20fe-5b63-44ca-8694-e9e1182afb4f/2111.02358v2.pdf)
[2](https://dl.acm.org/doi/10.1145/3580305.3599209)
[3](https://ieeexplore.ieee.org/document/10580842/)
[4](https://ieeexplore.ieee.org/document/10389628/)
[5](https://link.springer.com/10.1007/s11633-023-1431-y)
[6](https://arxiv.org/abs/2401.03955)
[7](https://arxiv.org/abs/2311.16673)
[8](https://arxiv.org/abs/2310.08879)
[9](https://arxiv.org/abs/2311.01043)
[10](https://pels.umsida.ac.id/index.php/PELS/article/view/1395)
[11](https://dl.acm.org/doi/10.1145/3628797.3628950)
[12](http://arxiv.org/pdf/2108.10904v3.pdf)
[13](https://arxiv.org/pdf/2409.09582.pdf)
[14](https://arxiv.org/html/2412.14672)
[15](https://aclanthology.org/2023.acl-short.32.pdf)
[16](https://aclanthology.org/2023.emnlp-main.568.pdf)
[17](https://arxiv.org/pdf/2205.15237.pdf)
[18](http://arxiv.org/pdf/2207.01772v1.pdf)
[19](http://arxiv.org/pdf/2205.11169.pdf)
[20](https://openreview.net/forum?id=nYpPAT4L3D)
[21](https://proceedings.neurips.cc/paper_files/paper/2024/file/7d62a85ebfed2f680eb5544beae93191-Paper-Conference.pdf)
[22](https://arxiv.org/pdf/2404.07214.pdf)
[23](https://proceedings.neurips.cc/paper_files/paper/2024/file/a03037317560b8c5f2fb4b6466d4c439-Paper-Conference.pdf)
[24](https://www.sciencedirect.com/science/article/pii/S0968090X25002918)
[25](https://aclanthology.org/2023.findings-acl.873.pdf)
[26](https://pmc.ncbi.nlm.nih.gov/articles/PMC12411343/)
[27](https://arxiv.org/html/2503.22517v1)
[28](https://papers.neurips.cc/paper_files/paper/2022/file/d46662aa53e78a62afd980a29e0c37ed-Paper-Conference.pdf)
[29](https://formative.jmir.org/2024/1/e32690/)
