# ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

## 1. 핵심 주장과 주요 기여

**ViLT(Vision-and-Language Transformer)**는 기존 VLP(Vision-and-Language Pre-training) 모델들의 복잡한 시각적 임베딩 파이프라인을 혁신적으로 단순화한 모델입니다.[1]

### 핵심 주장
ViLT는 기존 VLP 모델들이 의존하던 **CNN 백본과 영역 감독(region supervision)**을 완전히 제거하고, 단순한 패치 투영(patch projection)만으로도 경쟁력 있는 성능을 달성할 수 있음을 증명했습니다. 이는 VLP 분야에서 처음으로 시각적 입력을 텍스트와 동일한 방식으로 처리하는 단일화된 접근법을 구현한 것입니다.[1]

### 주요 기여
- **아키텍처 단순화**: 트랜스포머 모듈이 별도의 깊은 시각적 임베더 대신 시각적 특징을 직접 추출하고 처리하도록 설계[1]
- **효율성 혁신**: 기존 VLP 모델 대비 **수십 배 빠른 속도**를 달성하면서도 유사하거나 더 나은 성능 제공[1]
- **훈련 기법 도입**: 전체 단어 마스킹(whole word masking)과 이미지 증강 기법을 VLP에 최초로 성공적으로 적용[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 해결 대상 문제
기존 VLP 모델들은 두 가지 근본적인 문제를 가지고 있었습니다:[1]

1. **효율성/속도 문제**: 입력 특징 추출에 필요한 계산량이 멀티모달 상호작용 단계보다 훨씬 많음
2. **표현력 제한**: 시각적 임베더의 표현력과 사전 정의된 시각적 어휘에 상한이 제한됨

### 제안 방법

#### 모델 구조
ViLT는 **단일 스트림 접근법**을 따르며, 다음과 같은 수식으로 표현됩니다:[1]

**텍스트 임베딩:**

$$ \bar{t} = [t_{class}; t_1^T; \cdots; t_L^T] + T^{pos} $$

**이미지 임베딩:**

$$ \bar{v} = [v_{class}; v_1^V; \cdots; v_N^V] + V^{pos} $$

**결합된 입력:**

$$ z^0 = [\bar{t} + t_{type}; \bar{v} + v_{type}] $$

**Transformer 레이어 처리:**

$$ \hat{z}^d = MSA(LN(z^{d-1})) + z^{d-1}, \quad d = 1 \ldots D $$

$$ z^d = MLP(LN(\hat{z}^d)) + \hat{z}^d, \quad d = 1 \ldots D $$

**풀링된 표현:**

$$ p = \tanh(z_0^D W_{pool}) $$

#### 핵심 혁신사항

**1. 패치 투영(Patch Projection)**
- 입력 이미지 $$I \in \mathbb{R}^{C \times H \times W}$$를 패치로 분할하여 $$v \in \mathbb{R}^{N \times (P^2 \cdot C)}$$로 변환
- 선형 투영 $$V \in \mathbb{R}^{(P^2 \cdot C) \times H}$$을 통해 임베딩
- 32×32 패치 투영으로 단 2.4M 파라미터만 사용[1]

**2. 사전 훈련 목표**
- **Image Text Matching (ITM)**: 이미지-텍스트 쌍의 일치 여부 판별
- **Masked Language Modeling (MLM)**: 마스킹된 텍스트 토큰 예측
- **Word Patch Alignment (WPA)**: IPOT(Inexact Proximal Point Method for Optimal Transport)를 사용한 단어-패치 정렬[1]

## 3. 성능 향상 및 한계

### 성능 향상
ViLT는 다음과 같은 성능을 달성했습니다:[1]

**추론 속도**: 약 15ms로 기존 모델들 대비 수십 배 향상
- UNITER-Base: ~900ms
- Pixel-BERT-R50: ~60ms
- ViLT-B/32: ~15ms

**다운스트림 태스크 성능**:
- VQAv2: 71.26% (test-dev)
- NLVR2: 76.13% (test-P)
- Flickr30K Text Retrieval R@1: 83.5%

### 한계점
1. **VQA 성능**: 객체 탐지기를 사용하는 다른 VLP 모델들에 비해 VQA 점수가 다소 낮음[1]
2. **시각적 마스킹**: Masked Patch Prediction(MPP)이 성능 향상에 기여하지 못함[1]
3. **데이터 규모**: 대규모 정렬된 비전-언어 데이터셋의 부족으로 더 큰 모델 훈련에 제약[1]

## 4. 일반화 성능과 관련 내용

### 일반화 성능 향상 요소

**1. 이미지 증강 기법**
ViLT는 RandAugment를 파인튜닝 시 적용하여 일반화 성능을 개선했습니다. 색상 반전과 cutout을 제외한 모든 정책을 사용하여 시각적 정보의 손실을 최소화했습니다.[1]

**2. 전체 단어 마스킹**
전체 단어 마스킹은 VLP에서 특히 중요한데, 이는 모델이 부분적으로 마스킹된 토큰에만 의존하지 않고 다른 모달리티의 정보를 충분히 활용하도록 합니다. 예를 들어, "giraffe"가 ["gi", "##raf", "##fe"]로 토큰화될 때, 모든 토큰을 마스킹해야 모델이 이미지 정보를 적극 활용합니다.[1]

**3. 아키텍처의 단순성**
단순한 패치 투영 방식은 복잡한 CNN 백본이나 객체 탐지 모듈에 비해 **overfitting 위험을 줄이고** 다양한 도메인에 대한 일반화 능력을 향상시킵니다.[1]

### 모달리티 간 상호작용 집중
ViLT는 계산 자원의 대부분을 **모달리티 간 상호작용**에 집중시킴으로써, 단순한 융합 방식으로는 학습하기 어려운 복잡한 비전-언어 태스크에서도 효과적인 성능을 보입니다.[1]

## 5. 미래 연구에 미치는 영향과 고려사항

### 연구에 미치는 영향

**1. 패러다임 전환**
ViLT는 VLP 연구의 방향을 **"단일 모달 임베더 성능 향상"**에서 **"트랜스포머 내부의 모달리티 상호작용"**으로 전환시키는 계기를 제공했습니다.[1]

**2. 효율성 중시**
실제 응용에서 중요한 추론 속도와 계산 효율성을 VLP 모델 설계의 핵심 고려사항으로 부각시켰습니다.[1]

**3. 단순성의 가치**
복잡한 아키텍처보다 단순하고 효율적인 설계가 더 나은 성능을 낼 수 있음을 증명했습니다.[1]

### 향후 연구 시 고려사항

**1. 확장성(Scalability)**
대규모 트랜스포머의 성능 확장성을 고려하여 ViLT-L(Large), ViLT-H(Huge) 등의 변형 모델 개발이 필요합니다. 다만 정렬된 비전-언어 데이터셋의 부족이 주요 제약사항입니다.[1]

**2. 시각적 입력을 위한 마스킹 모델링**
MRM의 성공을 고려할 때, 패치 기반의 더 정교한 마스킹 목표 개발이 필요합니다. 대안적 클러스터링(alternating clustering)이나 동시 클러스터링(simultaneous clustering) 방법의 적용을 고려해야 합니다.[1]

**3. 증강 전략**
텍스트와 시각적 입력 모두에 적합한 증강 전략의 탐구가 필요합니다. 특히 가우시안 블러와 같은 기법들의 VLP에서의 효과를 검증해야 합니다.[1]

**4. 영역 감독 없는 연구**
영역 감독을 사용하지 않는 더 정교한 시각적 모달리티를 위한 마스킹 목표 개발이 중요한 연구 방향입니다.[1]

ViLT는 VLP 분야에서 효율성과 성능의 균형을 재정의하며, 향후 연구가 복잡한 구성요소의 단순한 조합보다는 **모달리티 간 의미 있는 상호작용**에 집중해야 함을 시사합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3eacd098-fdf1-4020-a0a2-003dd2a22ab7/2102.03334v2.pdf)
