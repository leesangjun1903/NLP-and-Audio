# Imagen : Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding

## 1. 핵심 주장과 주요 기여

**Imagen**은 Google Brain 팀이 발표한 혁신적인 텍스트-이미지 생성 모델로, 다음의 핵심 발견과 기여를 제시합니다.[1]

### 1.1 핵심 주장

Imagen의 가장 중요한 발견은 **텍스트 전용 코퍼스에서 사전학습된 대규모 언어 모델(예: T5-XXL, 4.6B 파라미터)이 텍스트-이미지 생성을 위한 텍스트 인코더로 매우 효과적이라는 것**입니다. 특히 흥미로운 점은 언어 모델 인코더의 크기를 증가시키는 것이 이미지 확산 모델의 크기를 증가시키는 것보다 **샘플 품질과 이미지-텍스트 정렬 개선에 훨씬 더 효과적**이라는 사실입니다.[1]

이는 다중 모달 이미지-텍스트 데이터로 학습된 CLIP과 같은 기존의 텍스트 인코더와 대조적입니다. 논문의 실험 결과에 따르면, DrawBench 벤치마크에서 인간 평가자들은 모든 11개 카테고리에서 T5-XXL 인코더를 CLIP보다 선호했습니다.[1]

### 1.2 주요 기여

1. **대규모 사전학습 언어 모델의 효과성 발견**: 텍스트 인코더 크기 증가가 이미지 확산 모델보다 성능에 더 큰 영향을 미침
2. **동적 임계값 처리(Dynamic Thresholding)**: 고(高)guidance 가중치 사용으로 인한 포화 픽셀 문제를 해결
3. **효율적 U-Net 아키텍처**: 메모리 효율성을 개선하고 수렴 속도를 2-3배 향상
4. **최첨단 성능**: COCO 데이터셋에서 제로샷 FID-30K 7.27 달성 (기존 방법 및 DALL-E 2 초과)
5. **DrawBench 벤치마크 도입**: 텍스트-이미지 모델의 다차원 평가를 위한 포괄적인 평가 세트[1]

***

## 2. 해결하는 문제 및 제안 방법

### 2.1 연구 문제

텍스트-이미지 생성 분야에서 다음의 주요 문제들이 존재했습니다:

- **제한된 텍스트 이해**: 기존 모델들은 복잡하고 구성적인 텍스트 프롬프트를 충분히 이해하지 못함
- **포토리얼리즘 부족**: 높은 품질의 포토리얼 이미지 생성에 제약
- **평가 메트릭의 한계**: COCO 데이터셋이 모델 간 차이를 충분히 드러내지 못함
- **Guidance 가중치의 한계**: 큰 guidance 가중치 사용 시 이미지 품질 저하

### 2.2 제안된 방법론

#### 2.2.1 모델 아키텍처: 계단식 확산 모델

Imagen은 세 개의 조건부 확산 모델로 구성됩니다:[1]

1. **기저 모델 (Base Model)**: 64×64 텍스트-이미지 생성 모델
2. **첫 번째 초해상도 모델**: 64×64 → 256×256
3. **두 번째 초해상도 모델**: 256×256 → 1024×1024

#### 2.2.2 수학적 공식

**확산 모델의 학습 목표**:

$$E_{x,c,\epsilon,t} \left[ w_t \| \hat{x}_\theta(\alpha_t x + \sigma_t \epsilon, c) - x \|_2^2 \right]$$

여기서:
- $x$: 데이터
- $c$: 조건 신호(텍스트 임베딩)
- $\epsilon \sim \mathcal{N}(0, I)$: 가우시안 노이즈
- $\alpha_t, \sigma_t, w_t$: 시간 $t$의 함수
- $\hat{x}_\theta$: 학습된 신경망[1]

**분류기 자유 Guidance**:

조건부 생성 시 x-예측 조정:

$$\tilde{\epsilon}_\theta(z_t, c) = w\epsilon_\theta(z_t, c) + (1-w)\epsilon_\theta(z_t)$$

여기서 $w$는 guidance 가중치입니다.[1]

**동적 임계값 처리**:

각 샘플링 스텝에서:

$$s = \text{percentile}(|\hat{x}_0^t|, p)$$

$s > 1$이면:

$$\hat{x}_0^t = \text{clip}(\hat{x}_0^t, -s, s) / s$$

이 방법은 고guidance 가중치 사용 시 픽셀 포화를 적극적으로 방지합니다.[1]

#### 2.2.3 텍스트 인코더

Imagen은 **동결된(frozen)** 사전학습 텍스트 인코더를 사용합니다. 탐색된 옵션들:

- **BERT**: 작은 크기(최대 340M), 텍스트 전용 코퍼스
- **T5**: 더 큰 크기(최대 11B, T5-XXL 4.6B), C4 코퍼스
- **CLIP**: 이미지-텍스트 대조 학습 기반[1]

텍스트 조건화 방법:

- 풀링된 텍스트 임베딩 벡터와 시간스탭 임베딩 결합
- 다중 해상도(32×32, 16×16, 8×8)에서 **교차 주의(Cross-Attention)**를 통한 전체 텍스트 임베딩 시퀀스 활용[1]

#### 2.2.4 노이즈 조건화 증강 (Noise Conditioning Augmentation)

초해상도 모델은 낮은 해상도 이미지의 노이즈 수준을 인식하도록 학습:

```math
x_{lr}^{\text{aug}} = x_{lr} + \text{aug\_level} \times \epsilon
```

이는 고품질 생성을 위해 **매우 중요**합니다.[1]

#### 2.2.5 Efficient U-Net 아키텍처

표준 U-Net 대비 개선사항:[1]

1. **파라미터 재배치**: 고해상도 블록에서 저해상도 블록으로 파라미터 이동
2. **스킵 연결 스케일링**: 스킵 연결을 $1/\sqrt{2}$로 스케일하여 수렴 속도 향상
3. **다운샘플링/업샘플링 순서 역전**: forward pass 속도 개선

이 아키텍처는 2-3배 빠른 추론 속도를 달성합니다.[1]

***

## 3. 모델 구조의 상세 분석

### 3.1 기저 모델 (64×64)

**아키텍처 구성**:
- U-Net 기반 확산 모델
- 채널 멀티플라이어:[2][3][4][1]
- 자기 주의 해상도:[5][6][7]
- 텍스트 교차 주의 해상도:[6][7][5]
- ResNet 블록 수: 3
- 임베딩 차원: 512
- 헤드당 채널: 64[1]

**텍스트 조건화**:
- 풀링된 텍스트 임베딩을 타임스탭 임베딩과 함께 추가
- 텍스트 임베딩에 계층 정규화 적용
- 교차 주의를 통한 컨텍스트 임베딩 시퀀스 활용[1]

### 3.2 초해상도 모델 (64→256, 256→1024)

**64×64 → 256×256 초해상도 모델**:
- 효율적 U-Net 아키텍처
- 낮은 해상도 이미지 + 노이즈 증강 입력
- 자기 주의와 교차 주의 모두 포함
- 400M 파라미터[1]

**256×256 → 1024×1024 초해상도 모델**:
- 자기 주의 계층 제거 (메모리 효율성)
- 텍스트 교차 주의는 유지 (중요도 높음)
- 64×64 → 256×256 크롭 기반 학습
- 600M 파라미터[1]

### 3.3 훈련 설정

**모델 크기**:
- 기저 64×64 모델: 2B 파라미터
- 초해상도 모델들: 각각 600M, 400M 파라미터[1]

**최적화**:
- 배치 크기: 2048
- 훈련 스텝: 2.5M
- 기저 모델: Adafactor 최적화
- 초해상도: Adam 최적화
- 학습률: 1e-4 (선형 워밍업 포함)
- 노이즈 스케줄: 코사인 스케줄[1]

**컴퓨팅 자원**:
- 기저 모델: 256개 TPU-v4 칩
- 초해상도 모델: 128개 TPU-v4 칩 각각[1]

**훈련 데이터**:
- 내부 데이터셋: ~460M 이미지-텍스트 쌍
- 공개 LAION 데이터셋: ~400M 이미지-텍스트 쌍[1]

***

## 4. 성능 향상 분석

### 4.1 벤치마크 성능

**MS-COCO 제로샷 평가**:[1]

| 모델 | FID-30K | 평가 기준 |
|------|---------|---------|
| Imagen | **7.27** | SOTA 달성 |
| DALL-E 2 | 10.39 | 동시대 최고 모델 |
| GLIDE | 12.24 | 이전 SOTA |
| Make-A-Scene | 7.55 | COCO 학습 모델 |
| Latent Diffusion | - | 비교 모델 |

**인간 평가** (COCO 검증 세트, 200개 쌍):

- **포토리얼리즘**: Imagen 39.2% 선호도 (원본 이미지 50% 기준)
- **이미지-텍스트 정렬**: 91.4±0.44 점 (원본 91.9±0.42와 거의 동일)
- **사람 제외 카테고리**: 포토리얼리즘 43.6% 증가 (사람 생성 품질 저하 시사)[1]

### 4.2 DrawBench 성능

DrawBench는 11개 카테고리, 총 200개 프롬프트로 구성:

1. **색상 (Colors)**: 지정된 색상의 객체 생성
2. **수량 (Counting)**: 정확한 개수의 객체 생성
3. **상충 관계 (Conflicting interactions)**: 상충되는 상호작용
4. **설명 (Description)**: 복잡한 텍스트 프롬프트 이해
5. **위치 관계 (Positional)**: 공간적 위치 지정
6. **텍스트 (Text)**: 이미지 내 텍스트 생성
7. **희귀 단어 (Rare Words)**: 드문 단어 이해
8. **맞춤법 오류**: 오타 포함 프롬프트 이해
9. DALL-E, Marcus et al., Reddit에서 수집한 도전적 프롬프트[1]

**비교 결과**:

Imagen 대 다른 모델의 인간 평가 선호도:

- **Imagen vs DALL-E 2**:
  - 이미지-텍스트 정렬: Imagen 강하게 우위
  - 이미지 충실도: Imagen 모든 카테고리 우위

- **Imagen vs GLIDE**:
  - 8/11 카테고리에서 이미지-텍스트 정렬 우위
  - 10/11 카테고리에서 이미지 충실도 우위

- **Imagen vs VQ-GAN+CLIP, Latent Diffusion**: 
  - 모두 현저히 우위[1]

### 4.3 어블레이션 연구 결과

#### 4.3.1 텍스트 인코더 크기의 영향

**핵심 발견**: 텍스트 인코더 스케일링은 U-Net 스케일링보다 **훨씬 더 효과적**

- T5-Small → T5-XXL: 상당한 FID 및 CLIP 점수 개선
- 300M U-Net → 2B U-Net: 상대적으로 더 작은 개선[1]

**수치적 증거**:
- 텍스트 인코더 크기 증가 → FID 개선: 25% 이상
- 이미지 U-Net 크기 증가 → FID 개선: 5% 이상[1]

#### 4.3.2 동적 임계값의 효과

정적 임계값(Static Thresholding)과 비교:

- **정적**: 고guidance 가중치에서도 작동하나 포화된 이미지 생성
- **동적**: 픽셀 포화 방지로 포토리얼리즘과 이미지-텍스트 정렬 모두 향상
- 고guidance 가중치(w > 5)에서 **특히 중요**[1]

#### 4.3.3 텍스트 조건화 방식의 영향

테스트된 방식:

1. **평균 풀링 (Mean Pooling)**: 기본선
2. **주의 풀링 (Attention Pooling)**: 약간의 개선
3. **교차 주의 (Cross-Attention)**: **FID와 CLIP 모두에서 현저히 우수**[1]

**결론**: 전체 텍스트 임베딩 시퀀스에 대한 교차 주의가 필수적

#### 4.3.4 노이즈 조건화 증강의 효과

- 증강 없음: 낮은 CLIP/FID 점수, guidance 가중치에 따른 변동 적음
- 증강 포함: 높은 CLIP/FID 점수, 더 큰 변동 범위 (다양성 증가)[1]

#### 4.3.5 Efficient U-Net 아키텍처의 이점

표준 U-Net 대비:

- **수렴 속도**: 2-3배 더 빠름
- **메모리 효율성**: 상당한 감소
- **추론 속도**: 2-3배 향상
- **성능**: 동일하거나 우수[1]

### 4.4 계층적 성능 분석

모델은 계층적으로 성능 향상:

1. **64×64 기저 모델**: 전체 의미론적 이해 담당
2. **64→256 초해상도**: 세부 사항 추가, 아티팩트 제거
3. **256→1024 초해상도**: 최종 포토리얼 이미지 생성[1]

각 초해상도 모델은 노이즈 증강을 통해 이전 단계의 아티팩트를 복구할 수 있습니다.

***

## 5. 일반화 성능 향상 가능성 (중점 분석)

### 5.1 대규모 언어 모델의 일반화 능력

**핵심 통찰**:

T5와 같은 대규모 언어 모델은 텍스트 전용 데이터(약 800GB)에서 학습되어 매우 광범위한 텍스트 분포에 노출됩니다. 이는 다음을 가능하게 합니다:

1. **조합론적 이해**: 훈련 중 보지 못한 단어 조합의 의미 파악
2. **드문 개념 처리**: 소수의 예제에서 나타난 개념도 인코딩
3. **계층적 의미**: 복잡한 구조의 텍스트 이해[1]

### 5.2 현재 제약 및 한계

논문과 후속 연구들이 밝혀낸 중요한 제약:

#### 5.2.1 수량 인식의 부족

**최근 연구 (2025)**: T2ICountBench 벤치마크를 통한 평가 결과:[8][9]

- 모든 최신 확산 모델이 객체 수 지정 실패
- 수량 증가에 따라 정확도 급격히 감소
- 프롬프트 개선만으로는 해결 불가능
- **핵심 원인**: 확산 모델의 수치 이해 한계[9][8]

**Imagen의 상황**:
- CLIP 점수는 높으나 정확한 수량 생성 어려움
- DrawBench의 "Counting" 카테고리에서 상대적 약점[1]

#### 5.2.2 구성적 일반화의 한계

**최근 연구 (2023-2024)**: 합성 작업 기반 연구:[10][11]

구성적 일반화에서:

1. **메모리 vs 생성화**: 모델은 먼저 훈련 데이터를 암기한 후 점진적으로 일반화
2. **곱셈적 출현**: 구성 능력은 갑자기 나타나며, 이는 구성 작업 성능에 곱셈적으로 영향
3. **드문 개념 조합**: 훈련 데이터에서 드문 개념의 조합 생성에는 **훨씬 더 많은 최적화 스텝 필요**[11][10]

**의미**: Imagen도 분포 외(Out-of-Distribution) 개념 조합에서 성능 저하

#### 5.2.3 사람 생성 품질 저하

**인간 평가 결과**:[1]

- 사람 포함 이미지: 39.2% 포토리얼리즘 선호도
- 사람 제외 이미지: 43.9% 포토리얼리즘 선호도
- **차이**: 4.7% 포인트의 명확한 성능 저하

**원인**: 훈련 데이터의 사람 이미지 표현 편향 및 복잡도[1]

#### 5.2.4 사회적 편향 및 자원 편향

논문에서 확인된 편향:[1]

1. **인종 편향**: 밝은 피부톤 우선 생성 경향
2. **성별 편향**: 직업 표현에서 서방 성별 고정관념 반영
3. **문화적 편향**: 특정 지역/문화 과도 표현 또는 과소 표현

**최근 연구 강조** (2024-2025):[12][13][14]

- **직업 편향**: Stable Diffusion에서 "농구" 제시 시 95%가 아프리카계 미국인 남성 생성
- **피부톤 편향**: 의료 이미지 생성 시 밝은 피부톤 과다 표현
- **장애 편향**: 장애인 표현의 부재 또는 부정적 표현[13][14][12]

### 5.3 일반화 개선 방향

#### 5.3.1 프롬프트 증강 및 재작성

**최근 동향 (2025)**:[15][16]

거대 언어 모델(LLM)을 사용한 프롬프트 재작성:

1. **프롬프트 정제 프레임워크**: 사용자 입력 개선
2. **직접 선호도 최적화 (DPO)**: 감독 미세조정 없이 개선
3. **이동성**: 한 모델에서 훈련한 재작성기가 다른 모델로 전이 가능

**효과**: 이미지-텍스트 정렬, 시각적 품질, 미학 모두 향상[16][15]

#### 5.3.2 풍부한 인간 피드백

**Google의 최신 연구 (2025)**:[17]

RichHF-18K 데이터셋을 통한:

1. **세분화된 피드백**: 단순 등급이 아닌 위치 기반 아티팩트 정보
2. **Rich Automatic Human Feedback (RAHF)**: 예측 모델 훈련
3. **이중 개선**: 로라 미세조정 + 영역 인페인팅

**결과**: Muse 모델 미세조정 시 아티팩트 현저히 감소[17]

#### 5.3.3 구성적 생성 모델

**방향**: 확산 모델의 구성적 이해 강화:[18][10]

- 개념 그래프 기반 학습
- 출력 분포 외 일반화를 위한 명시적 구조 강화[10][18]

### 5.4 언어 모델 크기 스케일링의 한계

**중요한 발견** (본 논문):[1]

T5-XXL(4.6B)이 최대 측정된 모델이나:

- 더 큰 언어 모델(예: PaLM 540B)의 효과 미측정
- 텍스트 이해에는 계속 개선 여지 있음
- 하지만 다른 병목(U-Net 용량, 컴퓨팅)이 존재할 수 있음[1]

**비용-이익 분석**:
- 언어 모델 스케일링 오프라인 계산 가능 (효율적)
- U-Net 스케일링은 훈련 시간 직접 증가[1]

***

## 6. 모델의 한계 분석

### 6.1 데이터 관련 한계

**훈련 데이터의 문제**:[1]

1. **수집 방식**: 웹 스크래핑 데이터에 기반
2. **감시 부족**: 포괄적 데이터 감시 전 없음
3. **LAION-400M 감시 발견**: 포르노 콘텐츠, 인종 혐오, 유해 고정관념 포함

**결과**: 표현 해악, 문화적 배제, 편향 재생산[1]

### 6.2 기술적 한계

1. **사람 생성 성능**: 사람이 포함된 이미지 품질 저하
2. **수량 정확도**: 정확한 객체 수 생성 실패
3. **텍스트 생성**: 이미지 내 텍스트 렌더링 제한
4. **미세한 공간 관계**: 복잡한 위치 관계 이해 제약[1]

### 6.3 사회적 영향 및 윤리적 우려

**식별된 편향** (내부 평가):[1]

1. **피부톤 편향**: 밝은 피부톤 편향
2. **성별 편향**: 직업 표현에서 서방 성별 고정관념
3. **모드 드롭**: 특정 개념 조합 누락 → 사회적 해악 복합

**논문의 윤리적 결정**:[1]

- 공개 데모 미공개
- 코드 공개 보류
- 책임 있는 공개(responsible externalization) 탐색 필요

***

## 7. 앞으로의 연구에 미치는 영향과 고려사항

### 7.1 Imagen이 미친 영향

#### 7.1.1 기술적 영향

1. **언어 모델 텍스트 인코더의 확립**: 후속 모델들이 대규모 사전학습 언어 모델 활용 추세[19][20]

2. **동적 임계값의 광범위 채용**: 다양한 확산 모델에서 고guidance 샘플링 표준 기법[21]

3. **계단식 확산 모델 확립**: 고해상도 생성의 효과적 패러다임 확립[22]

4. **구조화된 평가의 중요성 강조**: DrawBench 이후 세분화된 벤치마크 개발 가속화[23][19]

#### 7.1.2 산업적 영향

- Google Imagen 모델의 Vertex AI 등 상업 플랫폼 통합
- 텍스트 인코더의 스케일링이 ROI 최적화 도구로 인식[24]
- 후속 모델들의 설계 철학에 영향 (예: Stable Diffusion 구조)[22]

### 7.2 현재 진행 중인 연구 방향

#### 7.2.1 편향 및 공정성 연구

**Fair Diffusion (2023-2024)**:[25][12]

- 사용자 지시사항 기반 편향 완화
- BiasPainter: 자동화된 편향 평가 프레임워크
- 직업, 성별, 피부톤 편향 측정 및 완화[25][12]

**Gaussian Harmony (2023)**:[26]

- 확산 모델의 얼굴 생성에서 속성 균형
- 나이, 성별, 인종 등 민감한 속성의 공정한 분포[26]

#### 7.2.2 구성적 이해 강화

**Exploring Diffusion Models on Synthetic Tasks (2023)**:[10]

- 개념 그래프 기반 체계적 연구
- 구성 능력의 곱셈적 출현 발견
- 분포 외 생성화를 위한 필요한 최적화 스텝 증가 식별[10]

**최신 진전**: Stable Diffusion 3-m을 포함한 확대된 평가:[27]

- 더 큰 모델이 더 강력한 구성 생성 능력 보유
- 하지만 판별 구성 작업에서 성능 상충 관찰[27]

#### 7.2.3 수량 및 정확한 제어

**T2ICountBench (2025)**:[8][9]

- 객체 수량 정확도의 체계적 평가
- 모든 최신 모델의 한계 확인
- **해결 방향**: 수치 이해를 위한 특화된 학습[9][8]

#### 7.2.4 프롬프트 최적화

**최신 접근** (2025):[28][15][16]

1. **입력측 스케일링**: 추론 중 LLM 기반 프롬프트 개선[15][16]
2. **프롬프트 역변환**: 이미지에서 하드 프롬프트 역산[28]
3. **프롬프트 주입 학습**: 특정 개념 학습을 위한 최적 프롬프트 발견[28]

### 7.3 향후 연구 시 고려할 점

#### 7.3.1 데이터 윤리 및 투명성

**권장사항**:

1. **데이터 감시 강화**: 훈련 전 포괄적 감시
2. **데이터시트 문서화**: 데이터셋의 한계 명시
3. **개방적 공개**: 사용 제약 명확히
4. **데이터 선택의 민주화**: 모델 개발 과정의 이해관계자 참여[1]

#### 7.3.2 편향 평가 방법론 발전

**필요 영역**:

1. **다차원 편향 평가**: 피부톤, 성별, 직업, 문화, 나이 등 포괄적 평가[12][13][25]
2. **세분화된 메트릭**: NPMI(정규화된 점별 상호정보) 확대 적용[1]
3. **사회적 영향 평가**: 표현 해악, 스테레오타입 강화 측정[29]

#### 7.3.3 기술적 개선의 균형

**다층 접근**:

1. **구조적 개선**: 모델 설계의 편향 완화 (예: Gaussian Harmony)
2. **훈련 데이터 개선**: 대표성 있는 데이터 큐레이션
3. **사후 처리 개선**: 생성 후 편향 완화 기법
4. **사용자 제어 강화**: 공정성 관련 지시사항 적용 가능성[17]

#### 7.3.4 평가 패러다임의 진화

**확장된 평가**:

1. **자동화 메트릭의 한계 인식**: FID, CLIP 점수만으로 불충분
2. **인간 평가의 체계화**: 표준화된 평가 프레임워크 필요
3. **문맥 평가**: 실제 사용 시나리오 기반 평가[17]
4. **다문화 평가 팀**: 다양한 배경의 평가자 포함[14][1]

#### 7.3.5 모델 일반화 한계의 이해

**인식 필요**:

1. **구성적 일반화의 한계**: 분포 외 개념 조합에서 성능 저하 예상[10]
2. **스케일링의 수확 감소**: 언어 모델 크기 증가 효과의 점진적 감소
3. **병목 전환**: 언어 모델 개선 후 이미지 모델이 새로운 병목
4. **근본적 한계**: 훈련 데이터에 없는 개념의 생성화 불가능[10][1]

***

## 8. 결론

**Imagen**은 텍스트-이미지 생성 분야에 여러 중요한 기여를 했습니다:

1. **패러다임 전환**: 대규모 사전학습 언어 모델의 효과성 입증
2. **기술 혁신**: 동적 임계값과 효율적 U-Net 아키텍처 도입
3. **성능 달성**: 최첨단 FID 점수 달성
4. **평가 기준 제시**: DrawBench를 통한 체계적 평가

그러나 **일반화 성능 향상**에 있어 다음의 제약이 존재합니다:

- **수량 이해의 부족**: 정확한 객체 수 생성 실패
- **구성적 한계**: 드문 개념 조합의 분포 외 일반화 어려움
- **편향의 영속**: 훈련 데이터의 사회적 편향 재생산
- **사람 이미지 생성 한계**: 특정 인구통계 집단에서 성능 저하

향후 연구는 이러한 한계를 인식하면서도 **다층적 접근**을 통해 개선해야 합니다: 데이터 윤리, 기술 혁신, 공정성 평가, 모델 투명성의 균형 있는 발전이 필수적입니다.

***

## 참고문헌 ID

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/1d69cc0f-e8c3-4586-bf8a-e64e3f21f06e/2205.11487v1.pdf)
[2](https://humgenomics.biomedcentral.com/articles/10.1186/s40246-025-00716-x)
[3](https://www.nature.com/articles/s41433-025-04006-7)
[4](https://www.acpjournals.org/doi/10.7326/M14-0538)
[5](https://arxiv.org/html/2412.00122v1)
[6](https://arxiv.org/abs/2205.11487)
[7](https://journals.library.columbia.edu/index.php/bioethics/article/view/14119)
[8](https://arxiv.org/abs/2503.06884)
[9](https://openreview.net/forum?id=kL3pz7YSQF)
[10](https://proceedings.neurips.cc/paper_files/paper/2023/file/9d0f188c7947eacb0c07f709576824f6-Paper-Conference.pdf)
[11](https://ntt-review.jp/archive/ntttechnical.php?contents=ntr202509ra1.html)
[12](https://arxiv.org/pdf/2401.00763.pdf)
[13](https://ai.jmir.org/2024/1/e58275)
[14](https://arxiv.org/html/2407.01556v1)
[15](https://openreview.net/forum?id=0bwjkwSTuk)
[16](https://arxiv.org/abs/2510.12041)
[17](https://research.google/blog/rich-human-feedback-for-text-to-image-generation/)
[18](https://cs.stanford.edu/people/jure/pubs/compositional-iclr24.pdf)
[19](https://dl.acm.org/doi/pdf/10.1145/3640543.3645173)
[20](https://arxiv.org/abs/2209.10948)
[21](https://arxiv.org/html/2501.03495v2)
[22](https://www.sciencedirect.com/science/article/abs/pii/S0141938223002020)
[23](https://papers.nips.cc/paper_files/paper/2023/file/1a675d804f50509b8e21d0d3ca709d03-Paper-Conference.pdf)
[24](https://syncedreview.com/2022/06/01/googles-imagen-text-to-image-diffusion-model-with-deep-language-understanding-defeats-dall-e-2/)
[25](https://arxiv.org/pdf/2302.10893.pdf)
[26](https://arxiv.org/html/2312.14976)
[27](https://arxiv.org/html/2505.17955v3)
[28](http://openaccess.thecvf.com/content/CVPR2024/papers/Mahajan_Prompting_Hard_or_Hardly_Prompting_Prompt_Inversion_for_Text-to-Image_Diffusion_CVPR_2024_paper.pdf)
[29](https://arxiv.org/pdf/2311.18345.pdf)
[30](https://arxiv.org/pdf/2211.01324.pdf)
[31](http://arxiv.org/pdf/2205.11487.pdf)
[32](http://arxiv.org/pdf/2404.10763.pdf)
[33](https://arxiv.org/html/2402.11487)
[34](https://proceedings.neurips.cc/paper_files/paper/2024/file/be30024e7fa2c29cac7a6dafcbb8571f-Paper-Conference.pdf)
[35](https://arxiv.org/html/2303.07909v3)
[36](https://mednext.zotarellifilhoscientificworks.com/index.php/mednext/article/view/396)
[37](https://journal.media-culture.org.au/index.php/mcjournal/article/view/3123)
[38](https://www.mdpi.com/2673-2688/5/4/112)
[39](https://arxiv.org/abs/2508.06503)
[40](https://www.researchprotocols.org/2023/1/e48544)
[41](https://arxiv.org/abs/2510.12167)
[42](https://revistaczambos.utelvtsd.edu.ec/index.php/home/article/view/119)
[43](https://rme.gums.ac.ir/article-1-1475-en.html)
[44](https://academic.oup.com/humrep/article/doi/10.1093/humrep/deaf097.238/8170966)
[45](https://arxiv.org/pdf/2309.07277.pdf)
[46](https://pmc.ncbi.nlm.nih.gov/articles/PMC11485237/)
[47](http://arxiv.org/pdf/2410.10160.pdf)
[48](https://dl.acm.org/doi/10.1609/aaai.v38i13.29403)
[49](https://www.nature.com/articles/s41598-025-99623-3)
[50](https://www.sciencedirect.com/science/article/pii/S0001299824000461)
