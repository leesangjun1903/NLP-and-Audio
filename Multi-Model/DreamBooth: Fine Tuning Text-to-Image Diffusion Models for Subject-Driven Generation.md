# DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation

### 1. 핵심 주장과 주요 기여

DreamBooth는 텍스트-이미지 확산 모델(Diffusion Models)을 극히 적은 수의 이미지(3-5장)로 개인화하여, **특정 주제(Subject)를 고유한 식별자와 바인딩**함으로써 주제 기반 이미지 생성을 가능하게 하는 방법론을 제시합니다.[1]

**핵심 기여는 다음과 같습니다:**

- **주제 기반 개인화 프레임워크**: 사용자가 제공한 소수의 이미지로부터 특정 주제를 모델의 출력 영역에 임베딩하는 기법
- **클래스별 사전 보존 손실함수(Class-specific Prior Preservation Loss)**: 언어 드리프트(Language Drift) 문제를 해결하면서 동시에 모델의 다양성을 유지
- **새로운 평가 데이터셋 및 프로토콜**: 주제 충실도(Subject Fidelity)와 프롬프트 충실도(Prompt Fidelity)를 측정하는 벤치마크 제시[1]

### 2. 해결 문제, 제안 방법, 모델 구조 및 성능 분석

#### 2.1 핵심 문제점

기존의 대규모 텍스트-이미지 모델(DALL-E 2, Imagen)은 뛰어난 의미적 선행 지식을 가지고 있지만, **특정 주제의 정확한 외형을 재현하면서도 새로운 맥락에서 그것을 생성하는 능력이 부족**합니다. 예를 들어, 사용자의 반려견을 "a dog"라는 프롬프트로 생성하면 다른 개처럼 보이는 이미지가 만들어집니다.[1]

#### 2.2 제안 방법론

**2.2.1 텍스트-이미지 확산 모델의 기초**

확산 모델은 가우시안 분포에서 샘플링한 노이즈를 점진적으로 제거하는 역 과정을 학습합니다. 훈련 목표는:

$$\mathbb{E}_{x,c,\epsilon,t}\left[w_t\|\hat{x}_\theta(\alpha_t x + \sigma_t \epsilon, c) - x\|_2^2\right]$$

여기서 $x$는 실제 이미지, $c = \Gamma(P)$는 텍스트 프롬프트 $P$로부터 텍스트 인코더 $\Gamma$를 통해 생성된 조건 벡터, $\alpha_t, \sigma_t, w_t$는 노이즈 스케줄과 샘플 품질을 제어하는 항입니다.[1]

**2.2.2 주제 개인화 전략**

사용자가 제공한 주제의 이미지들과 "A [V] [class noun]" 형태의 텍스트 프롬프트를 사용하여 사전 학습된 모델을 미세조정합니다. 여기서:
- **[V]**: 희귀 토큰(Rare Token) 식별자 - 모델의 기존 지식과 간섭을 최소화하기 위해 토크나이저의 낮은 확률 범위에서 선택
- **[class noun]**: 주제의 클래스 명사(예: "dog", "cat", "teapot")

**희귀 토큰 선택의 중요성**: 일반적인 영어 단어를 사용하면 모델이 그 원래 의미를 해제한 후 주제와 다시 연결해야 하는 추가 학습 부담이 생깁니다. 따라서 T5-XXL 토크나이저의 5000-10000 범위에서 선택한 희귀 토큰을 역토크나이징하여 3글자 이하의 유니코드 시퀀스를 사용합니다.[1]

**2.2.3 클래스별 사전 보존 손실함수(Prior Preservation Loss, PPL)**

DreamBooth의 핵심 혁신은 언어 드리프트 문제를 해결하기 위한 손실함수입니다:

$$\mathbb{E}_{x,c,\epsilon,\epsilon',t}\left[w_t\|\hat{x}_\theta(\alpha_t x + \sigma_t\epsilon, c) - x\|_2^2 + \lambda w_{t'}\|\hat{x}_\theta(\alpha_{t'} x_{pr} + \sigma_{t'}\epsilon', c_{pr}) - x_{pr}\|_2^2\right]$$

여기서:
- 첫 번째 항: 주제 이미지에 대한 재구성 손실
- 두 번째 항: 클래스 사전 보존 손실 - 동결된 사전 학습 모델로부터 생성된 클래스 샘플 $x_{pr} = \hat{x}(z_{t_1}, c_{pr})$에 대한 감독
- $\lambda$: 두 항 간의 상대적 가중치 (일반적으로 $\lambda = 1$)[1]

**작동 원리**: 미세조정 과정에서 모델이 주제를 학습하면서도, 동시에 클래스 전체에 대한 이해를 유지하도록 강제합니다. 모델 스스로가 생성한 샘플을 감독 신호로 사용하는 "자생적(Autogenous)" 접근 방식입니다.[1]

**2.2.4 초해상도(Super-Resolution) 모듈 미세조정**

Imagen의 경우, 기본 모델 외에도 $64\times64 \to 256\times256$ 및 $256\times256 \to 1024\times1024$ 초해상도 모듈을 미세조정해야 합니다. 중요한 통찰은 **노이즈 증강 수준을 낮추는 것**입니다. 원래 학습에서 사용한 $10^{-3}$이 아닌 $10^{-5}$를 사용하면 주제의 세밀한 세부사항이 보존되고 환각 현상이 줄어듭니다.[1]

#### 2.3 훈련 설정 및 성능

**훈련 파라미터**:
- 반복 횟수: ~1000회
- 학습률: Imagen의 경우 $10^{-5}$, Stable Diffusion의 경우 $5\times10^{-6}$
- 클래스 샘플 생성 수: ~1000개 (더 적은 수도 가능)
- 훈련 시간: TPUv4에서 약 5분 (Imagen), NVIDIA A100에서 약 5분 (Stable Diffusion)[1]

**정량적 성능 지표**:

DreamBooth는 세 가지 평가 지표를 사용합니다:[1]

| 지표 | 설명 | 계산 방법 |
|------|------|---------|
| **DINO** (주제 충실도) | 생성된 이미지와 실제 이미지 간의 ViT-S/16 DINO 임베딩 코사인 유사도 평균 | 자기지도 학습 기반 (클래스 내 차이 구분 가능) |
| **CLIP-I** (주제 충실도) | CLIP 임베딩 기반 코사인 유사도 | 감독 학습 기반 (세부사항 구분 능력 낮음) |
| **CLIP-T** (프롬프트 충실도) | 프롬프트와 생성 이미지 간의 CLIP 임베딩 유사도 | 텍스트 정렬도 측정 |

**Textual Inversion 비교 결과**:[1]

| 방법 | DINO ↑ | CLIP-I ↑ | CLIP-T ↑ |
|------|--------|----------|----------|
| DreamBooth (Imagen) | 0.696 | 0.812 | 0.306 |
| DreamBooth (Stable Diffusion) | 0.668 | 0.803 | 0.305 |
| Textual Inversion | 0.569 | 0.780 | 0.255 |

**사용자 연구 결과**: 72명의 사용자 평가에서 DreamBooth가 주제 충실도 68% vs 22%, 프롬프트 충실도 81% vs 12%로 Textual Inversion을 압도적으로 능가했습니다.[1]

#### 2.4 주요 절제 연구(Ablation Study)

**클래스 명사 중요성**:

| 설정 | DINO ↑ | CLIP-I ↑ |
|------|--------|----------|
| 올바른 클래스 명사 | 0.744 | 0.853 |
| 클래스 명사 없음 | 0.303 | 0.607 |
| 잘못된 클래스 명사 | 0.454 | 0.728 |

올바른 클래스 명사를 사용하지 않으면 모델이 주제를 제대로 학습하지 못하고 수렴이 어려워집니다.[1]

**사전 보존 손실의 효과**:

PPL 없이 미세조정하면 언어 드리프트로 인해 모델이 같은 클래스의 다른 인스턴스를 생성하는 능력을 잃습니다. PPL을 추가하면:
- 사전 보존 지표: 0.664 → 0.493 (낮을수록 좋음, 드리프트 감소)
- 다양성: 0.371 (유지)
- 주제 충실도: DINO 0.712 → 0.684 (미세한 감소, 하지만 언어 드리프트 방지)[1]

### 3. 일반화 성능 향상 가능성

#### 3.1 현재의 일반화 문제점

논문에서 제시된 **한계 사항**:

1. **희귀한 맥락 생성 실패**: 모델이 학습 데이터에서 드물게 나타나는 맥락을 생성하지 못함[1]
2. **맥락-외형 얽힘(Context-Appearance Entanglement)**: 프롬프트에 포함된 맥락이 주제의 외형을 변화시킴 (예: 가방의 색상이 맥락에 따라 변함)[1]
3. **과적합**: 프롬프트가 원래 이미지의 설정과 유사할 때 실제 이미지와 매우 유사한 이미지 생성 (다양성 감소)[1]
4. **주제 복잡성에 따른 성능 차이**: 강아지, 고양이 같은 일반적인 주제는 잘 학습되지만, 드문 객체는 여러 변형을 지원하지 못함[1]

#### 3.2 입력 이미지 수의 영향

실험 결과 3-5개의 이미지가 최적이며:[1]
- 1-2개 이미지: 일반적인 주제(Corgi 개)의 경우 가능하지만 희귀한 객체(배낭)는 어려움
- 4개 이미지: 대부분 주제에서 최적 성능
- 5개 이상: 추가 개선 미미 (테이블 5, 6 참조)[1]

#### 3.3 앞으로의 개선 방향

**최신 연구(2023-2025)에서 제시된 개선 방법들:**

**1. AttnDreamBooth (2024)**: DreamBooth와 Textual Inversion의 한계를 분석하여, 임베딩 정렬 학습을 개선했습니다. 새로운 프롬프트에 통합할 때 Textual Inversion은 과적합 경향을, DreamBooth는 개념 간과 경향을 보이는 문제를 해결합니다.[2]

**2. DreamBlend (2024)**: 조기 체크포인트의 프롬프트 충실도와 후기 체크포인트의 주제 충실도를 결합하기 위해 교차주의(Cross Attention) 안내를 제안합니다. 이 방식은 과적합을 최소화하면서 도전적인 프롬프트에서 향상된 성능을 제공합니다.[3]

**3. DreamBoothDPO (2025)**: 직접 선호도 최적화(Direct Preference Optimization) 프레임워크를 적용하여, 개념 충실도와 프롬프트 정렬 간의 트레이드오프를 자동으로 조정합니다. 외부 품질 지표를 사용하여 합성 데이터셋을 생성합니다.[4]

**4. Preserve and Personalize (ICLR 2026 투고)**: Lipschitz 정규화를 기반으로 하는 새로운 정규화 목표를 제안하여, 사전 학습된 모델의 출력 분포를 명시적으로 보존합니다. 이는 분포 드리프트(Distributional Drift)를 방지하고 다양성과 일관성을 유지합니다.[5]

**5. SSR-Encoder (CVPR 2024)**: 마스크 없이도 선택적 주제 표현을 인코딩하여, 여러 주제 간의 얽힘 문제를 해결합니다. 미세 조정 없는 방법으로도 SOTA 성능을 달성합니다.[6]

**6. Proxy-Tuning (2025)**: 자동회귀(Autoregressive) 모델에도 주제 기반 생성을 적용합니다. 미세조정된 AR 모델이 확산 모델 감독자를 능가하는 "약-강 일반화" 현상을 발견했습니다.[7]

**7. LoRA 기반 개선**: 블록 단위 LoRA(Parameter-Efficient Fine-Tuning)를 통해 세밀한 미세조정을 수행하여, 훈련 시간을 단축하면서도 효율성을 높입니다.[8]

### 4. 모델 아키텍처 및 구조

#### 4.1 기본 구조

DreamBooth의 아키텍처는 기존 텍스트-이미지 확산 모델을 기반으로 하며, 주요 구성 요소는:

```
사용자 이미지 (3-5개) 
    ↓
[텍스트 프롬프트 생성] → "A [V] [class noun]"
    ↓
┌─────────────────────────────────────────┐
│  사전학습 텍스트-이미지 확산 모델 미세조정  │
├─────────────────────────────────────────┤
│ • 텍스트 인코더 미세조정                  │
│ • U-Net (노이즈 예측) 미세조정            │
│ • 초해상도 모듈 미세조정                  │
└─────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────┐
│  손실함수 계산 (두 항)                    │
├─────────────────────────────────────────┤
│ L_subject: 주제 이미지 재구성 손실        │
│ L_prior: 클래스 사전 보존 손실            │
│ L_total = L_subject + λ × L_prior        │
└─────────────────────────────────────────┘
    ↓
[미세조정된 개인화 모델]
    ↓
[프롬프트 입력] → "A [V] [class noun] [context]"
    ↓
[새로운 맥락에서 주제 이미지 생성]
```

#### 4.2 Imagen 기반 구현 (논문 주요 실험)

Imagen 모델의 경우:

1. **기본 텍스트-이미지 모델**: $64\times64$ 해상도에서 작동
   - T5-XXL 텍스트 인코더로부터 조건 벡터 생성
   - U-Net 기반 노이즈 예측기
   
2. **초해상도 모듈 1**: $64\times64 \to 256\times256$
   - 노이즈 증강 수준: 원래 $10^{-3}$에서 $10^{-5}$로 감소
   
3. **초해상도 모듈 2**: $256\times256 \to 1024\times1024$
   - 선택적 미세조정 (복잡한 세부사항의 주제에만)[1]

#### 4.3 Stable Diffusion 기반 구현

Stable Diffusion의 경우 단일 모델 아키텍처:
- 인코더-디코더 기반 잠재 확산 모델
- 낮은 차원 잠재 공간에서 노이즈 제거 과정 수행
- 학습률: $5\times10^{-6}$ (Imagen보다 낮음)
- U-Net과 텍스트 인코더 모두 미세조정[1]

### 5. 한계와 실패 사례

논문에서 제시된 구체적인 **실패 모드**:[1]

1. **경우 a**: 희귀한 맥락의 부정확한 생성 - 모델의 선행 지식이 약하거나 훈련 데이터에서 주제-맥락 조합의 확률이 낮음

2. **경우 b**: 맥락-외형 얽힘 - 배낭의 색상이 맥락(예: 밤하늘)에 따라 변함

3. **경우 c**: 과적합 - 프롬프트가 원본 환경과 유사할 때 실제 이미지와 거의 동일한 이미지 생성

추가 한계:
- 주제별 학습 난이도 차이
- 생성된 이미지의 fidelity 변동성
- 의미론적 수정의 강도에 따른 주제 특징 환각[1]

### 6. 앞으로의 연구 시 고려사항

#### 6.1 개선 필요 영역

**1. 일반화 성능 강화**

- **분포 드리프트 제어**: Lipschitz 정규화를 통한 사전 학습 분포 보존[5]
- **적응형 손실 가중치**: 다양한 주제에 대해 동적으로 $\lambda$ 조정[4]
- **다중 주제 처리**: 단일 모델에서 여러 주제를 동시에 학습할 때의 얽힘 문제 해결[6]

**2. 효율성 개선**

- **LoRA와의 결합**: 전체 모델 미세조정 대신 저랭크 어댑터 활용으로 훈련 시간 단축[8]
- **메모리 효율성**: 매개변수 효율 미세조정 방법 적용으로 리소스 최적화[9]
- **추론 속도**: 압축된 모델을 통한 빠른 이미지 생성

**3. 세밀한 제어**

- **주제-맥락 분리**: 교차주의 메커니즘을 통한 더 정교한 제어[3]
- **스타일 보존**: 예술적 스타일의 일관성 유지 개선
- **표현 조작**: 표정, 자세 등의 독립적인 제어

#### 6.2 새로운 응용 영역

- **의료 영상 합성**: 개인화된 의료 이미지 생성으로 진단 데이터 다양화 (프라이버시 보호)[10]
- **중국 산수화 생성**: 문화적 예술 스타일 보존[11]
- **이상 이미지 교정**: 생성 모델을 통한 결함 있는 이미지 자동 수정[12]
- **AR/VR 콘텐츠**: 사용자 특화 가상 환경 생성

#### 6.3 윤리 및 안전성 고려사항

- **가짜 탐지**: 개인화된 생성 모델로 인한 거짓 정보 확산 위험 관리
- **개인정보 보호**: 사용자 이미지 데이터의 안전한 처리
- **편향성 분석**: 주제별 생성 품질의 공정성 평가

#### 6.4 아키텍처 진화

- **자동회귀 모델 적용**: Diffusion뿐 아니라 AR 기반 생성 모델로의 확장[7]
- **멀티모달 학습**: 텍스트-이미지-음성 통합 개인화
- **지속 학습**: 시간에 따른 주제 특징의 진화 대응

### 결론

DreamBooth는 기존 텍스트-이미지 생성 모델에 **개인화 능력**을 부여한 획기적인 연구로, 단순하면서도 효과적인 **클래스별 사전 보존 손실함수**를 통해 언어 드리프트 문제를 해결했습니다. 특히 희귀 토큰 기반 식별자 설계와 초해상도 모듈의 노이즈 감소 전략은 세밀한 주제 특징 보존을 가능하게 합니다.[1]

최신 연구들은 DreamBooth의 기본 프레임워크를 기반으로 하면서, **분포 드리프트 제어**, **선호도 최적화**, **교차주의 안내** 등을 통해 지속적으로 성능을 개선하고 있습니다. 앞으로의 연구는 **일반화 성능 강화**, **효율성 극대화**, **다중 주제 처리**, 그리고 **윤리적 안전성 확보**에 초점을 맞춰야 할 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b819df20-d9fc-4ca8-bc3d-278fb3d4d9ec/2208.12242v2.pdf)
[2](https://arxiv.org/abs/2406.05000)
[3](https://openaccess.thecvf.com/content/WACV2025/papers/Ram_DreamBlend_Advancing_Personalized_Fine-Tuning_of_Text-to-Image_Diffusion_Models_WACV_2025_paper.pdf)
[4](https://arxiv.org/html/2505.20975v1)
[5](https://openreview.net/forum?id=2ge1Y6DWPw)
[6](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.pdf)
[7](https://arxiv.org/abs/2503.10125)
[8](https://huggingface.co/papers/2403.07500)
[9](http://arxiv.org/pdf/2402.15179.pdf)
[10](https://pmc.ncbi.nlm.nih.gov/articles/PMC11387006/)
[11](http://arxiv.org/pdf/2408.08561.pdf)
[12](https://arxiv.org/html/2409.16174v1)
[13](http://arxiv.org/pdf/2208.12242.pdf)
[14](https://arxiv.org/html/2502.20667)
[15](https://arxiv.org/pdf/2401.13974.pdf)
[16](http://arxiv.org/pdf/2407.05312.pdf)
[17](https://arxiv.org/html/2407.05312v1)
[18](https://iclr.cc/virtual/2025/session/31972)
[19](https://www.hexianghu.com/pdf/chen2023suti.pdf)
[20](https://neurips.cc/virtual/2023/session/74071)
[21](https://cvpr.thecvf.com/Conferences/2023/AcceptedPapers)
[22](http://arxiv.org/pdf/2503.11880.pdf)
[23](https://arxiv.org/pdf/2306.07967.pdf)
[24](http://arxiv.org/pdf/2404.19245.pdf)
[25](https://arxiv.org/pdf/2402.16141.pdf)
[26](http://arxiv.org/pdf/2406.09044.pdf)
[27](https://arxiv.org/pdf/2402.17263.pdf)
[28](https://arxiv.org/html/2405.11236)
[29](https://openreview.net/forum?id=kuCY0mW4Q3)
[30](https://www.sciencedirect.com/science/article/abs/pii/S095070512500807X)
[31](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dreambooth/)
[32](https://merl.com/publications/docs/TR2025-025.pdf)
[33](https://jang-inspiration.com/dreambooth)
[34](https://www.nature.com/articles/s41598-024-75599-4)
[35](https://proceedings.neurips.cc/paper_files/paper/2024/file/7eb6233e02f7d9efbb84acd839a996fb-Paper-Conference.pdf)
[36](https://ostin.tistory.com/127)
[37](https://aclanthology.org/2024.emnlp-main.372.pdf)
