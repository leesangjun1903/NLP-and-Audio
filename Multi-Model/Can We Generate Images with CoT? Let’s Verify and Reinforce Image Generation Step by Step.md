
# Can We Generate Images with CoT? Let’s Verify and Reinforce Image Generation Step by Step

## 1. 핵심 주장 및 기여도 요약

본 논문은 **Chain-of-Thought(CoT) 추론이 자동회귀 이미지 생성을 향상시킬 수 있는지**를 처음으로 종합적으로 조사한 연구입니다. 기존에 LLM 및 멀티모달 모델에서 입증된 CoT 기법을 이미지 생성에 적용하여 다음을 달성했습니다.

| 연구 성과 | 수치 |
|----------|------|
| GenEval 벤치마크 개선 | +24% |
| Stable Diffusion 3 대비 우위 | +15% |
| 테스트 시간 검증 (ORM) 개선 | +10% |
| 반복적 DPO 개선 | +11% |
| PARM 단독 개선 | +14% |

논문의 **핵심 기여**는 세 가지입니다:

1. **CoT 추론 전략의 자동회귀 이미지 생성 적용**: 테스트 시간 계산 확장, 선호도 정렬, 통합 기법의 유효성을 최초로 입증
2. **PARM 및 PARM++ 모델 제안**: 이미지 생성 특성에 맞춤화된 보상 모델로 기존 ORM/PRM의 한계 극복
3. **체계적 비교 분석**: ORM vs PRM, 테스트 시간 검증 vs 선호도 정렬, 그리고 이들의 상호보완성 규명

## 2. 해결하고자 하는 문제와 제안하는 방법

### 2.1 문제 정의

자동회귀 이미지 생성(AR-IG)은 다음 토큰 예측 패러다임을 사용하여 이미지를 생성합니다:

$$p(x_{1:T} | c) = \prod_{t=1}^{T} p(x_t | x_{1:t-1}, c)$$

여기서 $x_t$는 $t$번째 단계의 토큰, $c$는 텍스트 조건입니다.

**주요 문제점**:
- 초기 단계 이미지가 너무 흐려 평가 불가능
- 후기 단계 이미지가 거의 동일하여 판별 어려움
- 생성 경로의 불안정성으로 인한 일관성 부족
- 기존 ORM/PRM 보상 모델의 이미지 도메인 부적응

### 2.2 제안하는 방법론

#### 2.2.1 테스트 시간 검증: ORM과 PRM

**Outcome Reward Model (ORM)**: 최종 이미지만 평가

$$\text{Best-of-N} = \arg\max_{y_i \in \{y_1, ..., y_N\}} r_{ORM}(x, y_i)$$

학습 프로세스:
- 13K 고유 텍스트 프롬프트로 50개 이미지 생성
- 288K 랭킹 데이터셋 구성 ('yes'/'no' 라벨)
- LLaVA-OneVision을 기반으로 미세조정

**Process Reward Model (PRM)**: 중간 단계별 평가

$$r_{PRM}(x, y_{1:t})$$

300K 단계별 랭킹 데이터셋 자동 생성 방식:
- 단계 $i$에서 이미지 고정
- 남은 $18-i$ 단계에 대해 4개 경로 생성
- 최종 이미지 중 하나라도 'yes'면 단계 $i$를 'yes'로 라벨

#### 2.2.2 선호도 정렬: Direct Preference Optimization (DPO)

표준 RLHF 목적함수를 암묵적 보상으로 재매개변수화:

$$\mathcal{L}_{DPO}(\pi_\theta; \pi_{ref}) = -\mathbb{E}_{(x,y_w,y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right]$$

여기서:
- $\pi_\theta$: 학습 정책, $\pi_{ref}$: 참조 정책
- $y_w$: 선호 이미지, $y_l$: 비선호 이미지
- $\beta = 0.5$: 정규화 강도

**반복적 DPO**: 초기 DPO 후 새로운 정렬 모델로 업데이트된 랭킹 데이터 생성 (7K → 8K 샘플)

#### 2.2.3 PARM: 잠재력 평가 보상 모델

기존 ORM/PRM의 한계를 극복하기 위해 세 가지 적응적 작업 제안:

**작업 1: 명확도 판단**

$$C_t = \begin{cases} \text{yes} & t \geq 11 \\ \text{no} & t < 10 \end{cases}$$

- 조기 단계의 흐린 이미지 배제

**작업 2: 잠재력 평가**

$$P(y_t) = \begin{cases} \text{yes} & \text{if } \exists \text{ path leads to high-quality image} \\ \text{no} & \text{otherwise} \end{cases}$$

- 현재 단계에서 최종 고품질 이미지 도달 가능성 판정

**작업 3: 최상의 N' 선택**

$$y^* = \arg\max_{y_i \in \{\text{potential paths}\}} r_{ORM}(x, y_i)$$

- 남은 경로 중 최적 선택

**PARM 학습 데이터** (400K 인스턴스):
- 명확도 데이터: 120K
- 잠재력 데이터: 80K  
- 최상 선택 데이터: 200K

#### 2.2.4 PARM++: 반사 메커니즘 추가

최종 이미지 선택 후 반사 평가:

$$R(y|x) = \begin{cases} \text{yes} & \text{if image aligns with prompt} \\ \text{no + reason} & \text{otherwise} \end{cases}$$

불일치 시 자가 수정:
$$y^{(i+1)} = \pi(y^{(i)}, x, \text{reason}^{(i)})$$

최대 3회 반복, 총 520K 학습 인스턴스 (반사 평가용 120K 추가)

## 3. 모델 구조 및 성능 향상

### 3.1 모델 아키텍처

| 컴포넌트 | 설명 | 데이터셋 규모 |
|---------|------|------------|
| **기준 모델** | Show-o (자동회귀 이미지 생성) | - |
| **ORM** | LLaVA-OneVision(7B) 기반 | 288K |
| **PRM** | 단계별 평가 LLaVA | 300K |
| **PARM** | 3단계 적응형 평가 | 400K |
| **PARM++** | PARM + 반사 메커니즘 | 520K |
| **텍스트 인코더** | T5/LLaVA 계열 | 사전학습 활용 |
| **VQ 토크나이저** | 이미지 → 이산 토큰 | 16배 다운샘플 |

### 3.2 성능 향상 상세 분석

**GenEval 벤치마크 결과**:

| 방법 | 단일 객체 | 두 객체 | 개수 | 색상 | 위치 | 속성 | 전체 |
|-----|--------|--------|------|------|------|------|------|
| 기준선 | 0.95 | 0.52 | 0.49 | 0.82 | 0.11 | 0.28 | **0.53** |
| Fine-tuned ORM | 0.99 | 0.72 | 0.65 | 0.84 | 0.25 | 0.33 | **0.63** |
| 반복 DPO | 0.98 | 0.72 | 0.53 | 0.84 | 0.40 | 0.46 | **0.65** |
| PARM | 0.99 | 0.77 | 0.68 | 0.86 | 0.29 | 0.45 | **0.67** |
| PARM + 반복 DPO | 0.98 | 0.83 | 0.64 | 0.84 | 0.59 | 0.62 | **0.74** |
| **PARM++ + 반복 DPO** | 0.99 | 0.86 | 0.67 | 0.84 | 0.66 | 0.64 | **0.77** |

**주요 통찰**:
- **두 객체 생성**: +34%p 개선 (0.52 → 0.86)
- **위치 관계**: +55%p 개선 (0.11 → 0.66)
- **속성 결합**: +36%p 개선 (0.28 → 0.64)

### 3.3 테스트 시간 계산 확장의 효과

Best-of-N 선택 전략에서 N의 증가에 따른 성능:

$$\text{Performance}(N) \approx f(N)$$

- **Fine-tuned ORM**: N=1(63%) → N=20(63%) → N=50(62%)
- **PARM**: N=1(67%) → N=20(67%) → N=50(68%)

PARM이 테스트 시간 계산에 더 나은 확장성을 보임

## 4. 모델 일반화 성능

### 4.1 다중 자동회귀 모델에서의 검증

논문에서 다른 AR 모델에 대한 플러그-앤-플레이 적용을 확인:

| 모델 | 기준선 | Fine-tuned ORM | DPO | PARM | 개선율 |
|-----|--------|------------|-----|------|--------|
| **LlamaGen-3B** | 0.32 | 0.39 | 0.41 | 0.46 | +43.8% |
| **Janus-Pro-7B** | 0.80 | 0.89 | 0.83 | 0.91 | +13.8% |

**일반화의 의의**:
- Show-o의 적응형 생성 순서와 다른 토큰-바이-토큰 모델 모두에 적용
- 재교육 없이 기존 모델에 통합 가능
- 모델 아키텍처 비종속성 확인

### 4.2 도메인 외 일반화 분석

**강점**:
1. 단계별 명확도 판단이 다양한 모델 구조에 일관되게 작동
2. 보상 모델이 모델 독립적으로 이미지-텍스트 정렬 평가
3. DPO 기반 정렬이 도메인 특화 학습보다 우수

**제한점**:
1. 고해상도(1024×1024 이상) 이미지에서의 성능 미확인
2. 특정 도메인(예술, 의료)에서의 적용 결과 부재
3. 매우 복잡한 구성의 프롬프트에 대한 성능 한계

## 5. 주요 한계 및 개선 가능성

### 5.1 현재 한계

| 한계 | 원인 | 영향 |
|-----|------|------|
| **초기 단계 평가** | 흐린 이미지로 인한 신뢰도 부족 | 조기 탈락 가능성 |
| **후기 단계 판별** | 유사한 시각적 결과 | 세밀한 선택 어려움 |
| **자가 수정 부작용** | 모델이 새로운 오류 도입 가능성 | -2% 기준선 성능 감소 |
| **계산 비용** | Best-of-N과 반복 실행 | 배포 환경의 트레이드오프 |
| **데이터 의존성** | 288K-520K 학습 샘플 필요 | 새로운 도메인 적용 난제 |

### 5.2 향후 연구 방향

논문에서 암시된 개선 가능성:

1. **더 강건한 중간 표현**
   - 잠재 공간에서의 이미지 품질 평가
   - 다중해상도 표현 활용

2. **적응형 반사 메커니즘**
   - 오류 유형별 맞춤형 수정 전략
   - 사람-루프(human-in-the-loop) 피드백 통합

3. **통일된 멀티모달 정렬**
   - 더 강력한 기초 모델(예: GPT-4V 수준) 활용
   - 비전-언어 정렬의 정밀도 개선

4. **효율성 개선**
   - 테스트 시간 계산 최소화
   - 조기 중단 전략(early stopping)

## 6. 2020년 이후 관련 최신 연구 비교

### 6.1 CoT 기반 이미지 생성 연구계의 진화

| 연도 | 논문 | 주요 기여 | vs 본 논문 |
|-----|------|----------|----------|
| 2023 | DPO (Rafailov et al.) | RLHF 대체 기법 제안 | 기반 기술 |
| 2024 | LlamaGen | AR 이미지 생성 기준선 | 비교 모델 |
| 2024 | Show-o | 통합 멀티모달 모델 | 기준선 모델 |
| 2025 | ReasonGen-R1 | SFT + GRPO 기반 CoT | 유사 접근 |
| 2025 | ShortCoTI | 효율적 CoT 시퀀스 | 보완 기술 |
| 2025 | Visual-CoG | 단계별 보상 신호 | 통합 비슷 |
| 2025 | EARL | RL 기반 이미지 편집 | RL 활용 비교 |

### 6.2 핵심 기술 비교

**CoT 활용 방식**:
- **본 논문**: 단계별 검증 + 후처리 반사
- **ReasonGen-R1**: 텍스트 기반 사전 추론 + GRPO 정렬
- **Visual-CoG**: 다단계 보상 신호(시맨틱, 프로세스, 결과)

**보상 모델 설계**:
- **본 논문**: 적응형 명확도→잠재력→선택 (PARM)
- **Visual-CoG**: 단계별 일관된 보상
- **기존 방법**: ORM(결과만) 또는 PRM(모든 단계)

**효율성**:
- **본 논문**: Best-of-20 + 최대 3회 반사
- **ShortCoTI**: 프롬프트 길이 54% 감소
- **ReasonGen-R1**: 비슷한 계산 비용, 더 높은 복잡성

### 6.3 성능 비교

$$\text{GenEval 점수}$$

| 모델 | 점수 | 출시년 |
|-----|------|--------|
| Stable Diffusion 2.1 | 0.50 | 2022 |
| Stable Diffusion XL | 0.55 | 2023 |
| Stable Diffusion 3 | 0.62 | 2024 |
| Show-o (기준선) | 0.53 | 2024 |
| **본 논문 (PARM++)** | **0.77** | 2025 |
| Janus-Pro (기준선) | 0.80 | 2025 |

**분석**: 본 논문의 접근법이 기존 최고 수준(SD3 0.62) 대비 +24% 개선, Janus-Pro와 비슷한 수준 달성

## 7. 연구의 영향과 향후 고려사항

### 7.1 학문적 영향

**패러다임 전환**:
- **종전**: 이미지 생성 = 한 번의 생성 작업
- **현재**: 이미지 생성 = 검증-강화 루프

이는 LLM 분야의 CoT 혁신이 생성형 비전 모델에서도 재현될 수 있음을 입증합니다.

**방법론적 기여**:
1. **적응형 보상 메커니즘**: 도메인 특화 평가 방식의 새로운 패러다임
2. **단계별 선호도 정렬**: AR 생성 과정에 DPO 적용의 실증적 검증
3. **반사 기반 자가 수정**: 생성형 모델의 자율 오류 수정 메커니즘

### 7.2 실무 적용 고려사항

**배포 환경에서의 트레이드오프**:

$$\text{Quality Gain} = \frac{\Delta \text{(성능)}}{\text{추가 계산 비용}}$$

- Best-of-20 검증: 20배 메모리/계산
- 반복 반사(최대 3회): 최대 3배 추가 시간
- **트레이드오프**: 품질 향상(+24%) vs 계산 증가(약 60배)

**비용-효과 분석**:
- 고품질 요구 애플리케이션: 전체 파이프라인 추천
- 실시간 애플리케이션: PARM만 또는 N=5 선택 권장
- 엣지 디바이스: DPO 정렬만으로 +11% 달성 가능

### 7.3 향후 연구 시 고려할 핵심 포인트

**1. 데이터 효율성**
- 현재 400K-520K 학습 인스턴스 필요
- 자동 주석(Math-Shepherd 방식) 활용했으나, 새 도메인 적용 시 비용 증가
- **개선 방향**: 적은 데이터로 더 강력한 보상 모델 학습

**2. 모델 견고성**
- PARM++의 자가 수정이 -2% 성능 감소 야기
- 다중 모달리티 입력(이미지+텍스트) 처리 시 안정성 문제 가능
- **개선 방향**: 멀티태스크 학습으로 일관성 유지

**3. 확장성**
- Show-o, LlamaGen, Janus-Pro 테스트했으나 매우 큰 모델(10B+)은 미지수
- 고해상도(4K 이상) 이미지 생성에서의 성능 미평가
- **개선 방향**: 모델 크기/이미지 해상도에 따른 체계적 연구

**4. 일반화 도메인**
- GenEval은 객체 중심 평가에 특화
- 예술, 애니메이션, 의료, 건축 등 특수 도메인에서 성능 불명확
- **개선 방향**: 도메인별 맞춤형 평가 메트릭 개발

**5. 계산 효율성**
- 테스트 시간 계산 60배 증가는 실무 배포에 제약
- ShortCoTI(프롬프트 54% 단축) 같은 병렬 최적화 필요
- **개선 방향**: 동적 N 선택, 조기 중단 전략, 증류(distillation)

### 7.4 산업 적용 시나리오

**1. 고품질 콘텐츠 생성 (E-commerce, 광고)**
- 추천: 전체 PARM++ + 반복 DPO 파이프라인
- 추가 비용 정당화: 품질 향상으로 클릭률/전환율 증가

**2. 실시간 인터랙티브 애플리케이션 (채팅봇, AR)**
- 추천: DPO 정렬만 적용 (추가 비용 최소)
- 또는 N=5 Best-of-N (적정 균형)

**3. 교육/연구 목적**
- 추천: PARM 단독 (강력한 일반화)
- 도메인 특화 반사 메커니즘 개발 가능

**4. 다국어/문화 적응**
- 추천: 도메인 특화 DPO 미세조정
- 로컬 선호도 데이터 수집으로 기준선 개선

***

## 결론

본 논문 "Can We Generate Images with CoT?"는 **이미지 생성 분야에 CoT 추론 패러다임을 처음으로 체계적으로 도입한 획기적 연구**입니다. 테스트 시간 검증, 선호도 정렬, 그리고 특화된 보상 모델(PARM/PARM++)의 조합으로 기존 최고 수준 모델을 +24% 초과하는 성능을 달성했습니다.

**핵심 강점**:
- 포괄적인 실증 연구로 CoT 전략의 이미지 생성 적용 가능성 입증
- 도메인 특화 설계(PARM)로 기존 일반 모델의 한계 극복
- 다중 AR 모델에서 검증된 일반화 성능

**주요 한계**:
- 높은 계산 비용(60배)으로 인한 배포 제약
- 특수 도메인에서의 성능 미평가
- 자가 수정 메커니즘의 불안정성

향후 연구는 **계산 효율성 개선**, **도메인 특화 평가 메트릭 개발**, **더 강력한 멀티모달 기초 모델 활용**에 중점을 두어야 하며, 이는 생성형 AI의 다음 단계 발전을 주도할 것으로 기대됩니다.

<span style="display:none">[^1_1][^1_10][^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_2][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_3][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_4][^1_40][^1_41][^1_42][^1_43][^1_5][^1_6][^1_7][^1_8][^1_9]</span>

<div align="center">⁂</div>

[^1_1]: 2501.13926v2.pdf

[^1_2]: https://arxiv.org/abs/2505.24875

[^1_3]: https://arxiv.org/abs/2510.05593

[^1_4]: https://arxiv.org/abs/2502.16965

[^1_5]: https://arxiv.org/abs/2501.13926

[^1_6]: https://arxiv.org/abs/2505.17017

[^1_7]: https://ieeexplore.ieee.org/document/11093840/

[^1_8]: https://arxiv.org/abs/2508.01119

[^1_9]: https://arxiv.org/abs/2508.18032

[^1_10]: https://arxiv.org/abs/2509.24251

[^1_11]: https://arxiv.org/abs/2508.13382

[^1_12]: https://arxiv.org/html/2501.13926v1

[^1_13]: https://arxiv.org/html/2411.10180v1

[^1_14]: https://arxiv.org/html/2501.04699v1

[^1_15]: http://arxiv.org/pdf/2503.16194.pdf

[^1_16]: http://arxiv.org/pdf/2406.06525v1.pdf

[^1_17]: https://arxiv.org/html/2503.19312v1

[^1_18]: http://arxiv.org/pdf/2502.13329.pdf

[^1_19]: http://arxiv.org/pdf/2503.10639.pdf

[^1_20]: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Lets_Verify_and_Reinforce_Image_Generation_Step_by_Step_CVPR_2025_paper.pdf

[^1_21]: https://www.edge-ai-vision.com/2023/01/from-dall·e-to-stable-diffusion-how-do-text-to-image-generation-models-work/

[^1_22]: https://cameronrwolfe.substack.com/p/direct-preference-optimization

[^1_23]: https://huggingface.co/papers/2501.13926

[^1_24]: https://arxiv.org/html/2303.07909v3

[^1_25]: https://arxiv.org/abs/2305.18290

[^1_26]: https://arxiv.org/abs/2303.07909

[^1_27]: https://dalpo0814.tistory.com/62

[^1_28]: https://github.com/AlonzoLeeeooo/awesome-text-to-image-studies

[^1_29]: https://gudong0918.tistory.com/146

[^1_30]: https://liner.com/review/can-we-generate-images-with-cot-lets-verify-and-reinforce

[^1_31]: https://dmqa.korea.ac.kr/uploads/seminar/[이진우] Enhancing Prompt Understanding in Diffusion Model2.pdf

[^1_32]: https://openaccess.thecvf.com/content/CVPR2025/papers/Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization_CVPR_2025_paper.pdf

[^1_33]: https://arxiv.org/abs/2305.18295

[^1_34]: https://arxiv.org/html/2504.15176v1

[^1_35]: https://arxiv.org/html/2510.05593v1

[^1_36]: https://arxiv.org/abs/2309.04109

[^1_37]: https://arxiv.org/abs/2410.18013

[^1_38]: https://arxiv.org/html/2505.24875v2

[^1_39]: https://arxiv.org/html/2403.04279v1

[^1_40]: https://arxiv.org/html/2508.10711v1

[^1_41]: https://arxiv.org/html/2509.25771v1

[^1_42]: https://arxiv.org/html/2405.19316v2

[^1_43]: https://research.google/blog/mobilediffusion-rapid-text-to-image-generation-on-device/

