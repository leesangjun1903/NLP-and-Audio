# Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation

### 1. 핵심 주장과 주요 기여

**논문의 핵심 주장**

"Diffused Heads"는 **음성 구동 인물 얼굴 생성 분야에서 GAN 기반 방법을 뛰어넘는 확산 모델(Diffusion Model)의 우월성**을 최초로 입증한다. 기존의 GAN 기반 접근 방식들이 학습 불안정성, 모드 붕괴(mode collapse), 얼굴 왜곡, 추가 감시 신호 의존성 등의 문제를 안고 있는 반면, 제안된 방법은 단 하나의 신원 이미지와 음성 녹음만으로 자연스러운 머리 움직임, 표정, 눈 깜빡임을 포함한 현실적인 인물 비디오를 생성할 수 있다.[1]

**주요 기여 세 가지**[1]

첫째, **음성 구동 인물 얼굴 생성을 위한 최초의 확산 모델 기반 솔루션**을 제시한다. 기존 방법들과 달리 적대적 훈련(adversarial training) 대신 변분적 접근(variational approach)을 사용하므로 판별자 안정화가 불필요하다.

둘째, **모션 프레임(motion frames)과 음성 임베딩(audio embeddings)을 통한 생성 이미지의 일관성 유지 메커니즘**을 도입한다. 이는 자연스럽지 않은 시퀀스 문제를 해결하고 음성-영상 동기화를 보장한다.

셋째, **일반화 성능의 견고함**을 입증한다. 신원 이미지와 음성 녹음의 소스에 관계없이 안정적으로 작동하는 모델의 강인성을 구현한다.

***

### 2. 문제 정의, 제안 방법, 모델 구조, 성능 개선 및 한계

#### 2.1 해결하고자 하는 문제

**기존 GAN 기반 방법의 한계**

- **훈련 불안정성**: GAN은 광범위한 아키텍처 탐색과 매개변수 조정이 필요하고 수렴이 어렵다.
- **제한된 표현력**: 추가 마스크나 구동 프레임 같은 참조 비디오 기반 감시 신호에 의존하면서 원래의 머리 움직임과 얼굴 표정 생성 능력이 제한된다.
- **모드 붕괴**: GAN의 생성기가 데이터 분포의 작은 부분만 학습하여 다양성이 부족하다.
- **얼굴 왜곡**: 특히 큰 머리 움직임이 있을 때 일관된 신원을 유지하기 어렵고 신원 일관성을 위해 사전 훈련된 얼굴 인증 모델이 필요하다.

**해결해야 할 핵심 문제들**

1) 추가 감시 신호 없이도 자연스러운 머리 움직임 생성
2) 정확한 음성 동기화와 함께 표정 다양성 보장
3) 신원 일관성 유지
4) 모드 붕괴 없는 다양한 비디오 생성

#### 2.2 제안하는 방법 및 수식

**기본 확산 모델 프레임워크**

표준 확산 모델은 데이터 분포 $$q(x_0)$$에서 샘플 $$x_0$$를 점진적으로 가우시안 노이즈로 변환하는 순방향 프로세스를 정의한다:[1]

$$q(x_{1:T} | x_0) = \prod_{t=1}^{T} q(x_t | x_{t-1})$$

여기서 $$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$이고, $$\{\beta_t\}_{t=1}^T$$는 사전 정의된 노이즈 스케줄이다.

**중요한 성질**: 충분히 큰 $$T$$와 $$\beta_t \in $$에 대해, $$x_T$$는 isotropic 가우시안 분포에 가까워진다 ($$x_T \sim \mathcal{N}(0, I)$$).[1]

**단일 단계 접근**: 임의의 시간 단계에서 중간 상태에 직접 접근 가능하다:[1]

$$q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, \sqrt{1-\bar{\alpha}_t}I)$$

여기서 $$\bar{\alpha}\_t = \prod_{s=1}^{t} (1-\beta_s)$$이다.

**역방향 프로세스**: Bayes 정리를 이용하여 다루기 쉬운 후진 분포를 정의한다:[1]

$$q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I)$$

여기서:

$$\tilde{\mu}_t = \frac{\sqrt{1-\beta_{t-1}}}{1-\bar{\alpha}_t}\beta_t x_0 + \frac{\sqrt{1-\beta_t}}{1-\bar{\alpha}_t}(1-\beta_t)x_t$$

$$\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$$

**변분 하한(VLB) 학습 목표**:[1]

$$\mathcal{L}_{simple} = \mathbb{E}_{t,x_0,\epsilon_t} [\|\epsilon_t - \hat{\epsilon}_\theta(x_t, t)\|^2]$$

및

$$\mathcal{L}_{vlb} = \mathcal{L}_0 + \sum_{t=1}^{T-1} \mathcal{L}_t + \mathcal{L}_T$$

여기서:

$$\mathcal{L}_t = \mathbb{D}_{KL}(q(x_t | x_{t-1}, x_0) \| p_\theta(x_t | x_{t-1}))$$

**입력 구성 (신원 유지)**:[1]

$$x_k^{in,t} = x_k^t \oplus x_0^{id} \oplus x_k^{motion}$$

여기서 $$x_k^t$$는 잡음이 추가된 타겟 프레임, $$x_0^{id}$$는 신원 프레임, $$x_k^{motion}$$는 모션 프레임이고, $$\oplus$$는 채널 차원에서의 연결을 나타낸다.

**모션 프레임 정의**:[1]

$$x_k^{motion} = x_k^{m_x} \oplus \cdots \oplus x_{k-1}$$

여기서 $$m_x$$는 모션 프레임의 개수다. 일반적으로 $$m_x = 2$$가 최적이다.

**음성 조건화 메커니즘**:[1]

$$h^{s+1} = \gamma_y^s \odot \text{GN}(h^s) \oplus b_y^s$$

여기서 $$\gamma_y^s, b_y^s = \text{MLP}(y_k)$$이고, GN은 그룹 정규화이다. 이는 시간 인코딩뿐만 아니라 음성 임베딩으로도 숨겨진 상태를 스케일링하고 시프트한다.

**모션 오디오 임베딩**:[1]

$$y_k^{motion} = y_{k-m_y} \oplus \cdots \oplus y_k \oplus \cdots \oplus y_{k+m_y}$$

여기서 $$m_y$$는 한쪽 측의 추가 오디오 임베딩 개수이다.

**립 싱크 손실 함수**:[1]

$$\mathcal{L}_{ls} = \mathbb{E}_{t,x_0,\epsilon_t}[\|\overline{\epsilon}_t - \overline{\hat{\epsilon}}_\theta(x_t, t)\|^2]$$

여기서 $$\overline{\epsilon}\_t$$와 $$\overline{\hat{\epsilon}}_\theta$$는 입에 대한 얼굴 랜드마크로 자른 버전들이다. 최종 손실 함수는:[1]

```math
\mathcal{L} = (1-\lambda_{vlb}-\lambda_{ls})\mathcal{L}_{simple} + \lambda_{vlb}\mathcal{L}_{vlb} + \lambda_{ls}\mathcal{L}_{ls}
```

실험에서 $$\lambda_{ls} = 0.2$$일 때 최적의 성능을 보였다.

#### 2.3 모델 구조

**UNet 기반 아키텍처**:[1]

모델은 기본 아키텍처로 2D UNet을 사용하며, 다음과 같은 특성을 가진다:

- **입력 블록**: 256-512-768 채널을 사용하며, 각각 2개의 ResNet 계층 포함
- **주의 계층**: 중간 블록에만 4개 헤드와 64개 헤드 채널을 가진 주의 계층 1개 사용 (초기 실험에서 많은 주의 계층은 품질을 악화시킴)
- **정보 주입**: 시간 임베딩 $$t_s, t_b = \text{MLP}(t)$$는 그룹 정규화를 통해 주입됨

**조건화 메커니즘**:

1) **신원 유지**: 모든 생성 단계에서 고정된 신원 프레임 연결
2) **시간 정보**: 모션 프레임과 모션 오디오 임베딩으로 과거 및 미래 정보 제공
3) **음성 정보**: 사전 훈련된 음성 인코더에서 추출한 임베딩으로 조건화

**샘플링 과정**:[1]

생성 시에는 확산 모델의 역방향 프로세스를 따른다:

$$x_{t-1} \sim \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t I)$$

**효율성 개선**: DDIM (Denoising Diffusion Implicit Models) 또는 타임스텝 리스페이싱을 사용하여 샘플링 시간을 약 5배 감소시킨다.

**그레이스케일 모션 프레임**: 생성 중 오류 누적을 완화하기 위해, 신원 정보를 색상에서 얻는 것을 방지하기 위해 모션 프레임을 그레이스케일로 변환한다.

#### 2.4 성능 향상

**정량적 평가 결과** (표 1):[1]

모델은 LRW와 CREMA 데이터셋에서 다음과 같은 성능을 달성했다:

| 지표 | SDA | MakeItTalk | Wav2Lip | PC-AVS | EAMM | Diffused Heads |
|------|-----|-----------|---------|--------|------|-----------------|
| **FVD (LRW)** ↓ | 198.84 | 269.29 | 366.14 | 153.12 | 172.18 | **71.88** |
| **FID (LRW)** ↓ | 61.95 | 7.57 | 2.83 | 11.96 | 9.28 | **3.94** |
| **Blinks/sec (LRW)** | 0.52 | 0.09 | 0.03 | 0.20 | 0.03 | **0.35** |
| **WER (LRW)** ↓ | 0.77 | 0.99 | 0.51 | 0.64 | 0.95 | **0.77** |

**주요 성능 지표 해석**:

- **FVD (Fréchet Video Distance)**: 연속 프레임 간의 부자연스러운 동역학을 페널티하여 시간적 일관성을 측정. Diffused Heads가 가장 우수 (71.88)
- **FID (Fréchet Inception Distance)**: 생성된 비디오의 전체 품질 측정. 최고 성능 (3.94)
- **눈 깜빡임**: 자연스러운 표정 생성의 지표로, Diffused Heads가 0.35로 높은 값 달성 (지표가 높을수록 좋음)
- **OFM (Optical Flow Magnitude)**: 연속 프레임 간의 광흐름 평균값으로 매끄러움 측정

**인간 평가 (Turing 테스트)**:[1]

140명의 참가자에게 30개 비디오(Diffused Heads 10개, PC-AVS 10개, 실제 영상 10개)를 보여주고 실제 여부를 판단하도록 함:

| 방법 | 점수 |
|------|------|
| **Diffused Heads** | **68.72** |
| PC-AVS | 34.95 |
| Real videos | 64.00 |

이는 Diffused Heads가 실제 영상에 매우 근접하고 기존 최고 성능 방법(PC-AVS)을 크게 능가함을 보여준다.

**정성적 성능**:[1]

- 실제 영상과 구별하기 어려운 비디오 생성
- 자연스러운 표정, 눈 깜빡임, 찡그림 생성
- 프레임 간 부드러운 모션 유지
- 신원 보존 우수
- 아티팩트 최소화
- 머리카락, 안경 등 어려운 객체의 정확한 생성
- 측면 뷰 등 어려운 각도에서도 우수한 성능

#### 2.5 모델의 한계

**생성 길이 제한**:[1]

확산 모델의 자기회귀적 특성으로 인해 8-9초 이상의 비디오 생성 시 초기 품질 유지가 어렵다. 추가 포즈 입력이나 시각적 가이드가 없으므로 장시간 생성이 불가능하다.

**생성 속도**:[1]

확산 모델의 특성상 다른 생성 모델(예: GAN)에 비해 생성 시간이 길다. 현재로서는 실시간 응용 프로그램에 사용할 수 없다.

**립 싱크 품질**:[1]

사전 훈련된 립 읽기 전문가 모델을 훈련에 사용하지 않아 WER(Word Error Rate)이 비교 방법들(예: Wav2Lip)에 비해 다소 낮다. 그러나 여전히 허용 가능한 수준이고 AV Confidence 값이 실제 영상과 유사하다.

**평가 메트릭 문제**:[1]

새로운 메트릭(OFM, F-MSE)을 제안했지만, 음성 구동 얼굴 애니메이션 작업에 대한 표준화된 평가 메트릭의 부족이 여전히 문제다.

***

### 3. 모델의 일반화 성능 향상 가능성

#### 3.1 현재 일반화 성능 분석

**교차 데이터셋 일반화**:[1]

논문의 일반화 섹션(5.5)에서 다음을 입증했다:

- **신원 프레임 소스 다양화**: CREMA와 LRW에서의 신원 이미지 사용
- **음성 소스 다양화**: AVSpeech 데이터셋의 한국 여성, 독일 남성 음성
- **완전 도메인 외(Out-of-domain) 테스트**: DALL-E 2로 생성한 아바타 이미지와 개인 녹음

**핵심 발견**: 모델이 훈련 데이터 분포 외의 데이터에서도 우수하게 작동하며, 생성된 프레임이 자연스럽고 립 움직임과 표정이 현실적이다. 특히 훈련 중 인물 얼굴만 사용했음에도 아바타 이미지도 성공적으로 처리한다.

#### 3.2 일반화 성능 향상 메커니즘

**1) 모션 프레임과 신원 분리**

Ablation 연구(표 3)에서 모션 프레임을 그레이스케일로 변환하는 것의 중요성이 드러난다. 이는 다음을 의미한다:[1]

- 신원 정보를 신원 프레임에서 주로 추출
- 색상 정보를 제거한 모션 프레임은 움직임 정보만 전달
- 복잡한 데이터셋(여러 참가자)에서 일반화 능력 향상

**2) 모션 오디오 임베딩의 양방향 구조**

$$y_k^{motion} = y_{k-m_y} \oplus \cdots \oplus y_k \oplus \cdots \oplus y_{k+m_y}$$

이 구조는:[1]

- 과거와 미래의 음성 맥락 제공
- 다양한 음성 속도와 발음 패턴에 대한 강건성 증대
- 보이지 않은 화자의 음성 특성에 대한 적응

**3) 변분적 접근의 강점**

GAN 대신 Denoising Diffusion Probabilistic Model을 사용하면서:[1]

- 모드 붕괴 문제 제거
- 데이터 분포의 전체 지원을 모델이 학습할 수 있음
- 더 광범위한 얼굴 표정과 머리 움직임 생성 가능

#### 3.3 향상 가능성에 대한 분석

**제약조건에 기인한 일반화 제한**

현재 일반화의 주요 제약:

1) **단일 얼굴 참조 제한**: 신원 프레임 1개만 사용하므로, 극단적인 각도나 드물게 나타나는 표정에 미흡할 수 있음
2) **오토레그레시브 오류 누적**: 프레임을 순차적으로 생성하므로, 생성된 프레임의 오류가 후속 프레임에 누적됨[1]
3) **음성-영상 도메인 공동 적응 부재**: 특정 악센트나 독특한 음성 특성에 대한 미세 조정이 필요할 수 있음

**일반화 성능 향상을 위한 가능성**

향후 개선 방향:

1) **다중 신원 프레임 활용**: 여러 각도의 초기 얼굴 이미지를 사용하면 입체적 구조 정보 활용 가능
2) **3D 모델 통합**: 3D Morphable Model (3DMM) 파라미터를 명시적으로 학습하면 기하학적 제약 추가 가능[2][3][4][5][6]
3) **메타-러닝 접근**: Few-shot 적응을 통해 새로운 화자에 대한 신속한 조정 가능
4) **계층적 확산 구조**: 추상적 수준(3DMM 파라미터)과 구체적 수준(픽셀)에서의 계층적 처리[7][4]
5) **Transformer 기반 아키텍처**: 최근 연구들이 Diffusion Transformer (DiT)를 사용하여 시간적 일관성 개선[4][8][5][9]

***

### 4. 논문이 앞으로의 연구에 미치는 영향과 고려사항

#### 4.1 연구 커뮤니티에 미치는 영향

**패러다임 전환**

Diffused Heads는 **음성 구동 얼굴 생성에서 GAN 기반 방법으로부터 확산 모델 기반 방법으로의 근본적인 전환**을 주도했다. 이는 다음과 같은 연쇄 효과를 발생시켰다:[10][1]

1) **확산 모델 채택 확대**: 2023년 이후 발표된 수십 개의 후속 연구가 확산 모델을 기반으로 함[8][5][6][11][12][13][14][15][16][17][18][19][20][2][7][4]

2) **성능 벤치마크 재설정**: Diffused Heads의 성과가 새로운 비교 기준점으로 확립되어, 이후 모든 방법들이 이를 기준으로 성능을 평가함

3) **평가 메트릭 개선 논의**: OFM(Optical Flow Magnitude)과 F-MSE(Frame-wise Mean Square Error) 같은 새로운 메트릭 제안으로 음성 구동 얼굴 생성 평가의 국제화 추진[1]

#### 4.2 후속 연구의 확산 모델 발전 방향

**2023-2025년 주요 후속 연구 동향**

**1) 확산 트랜스포머(DiT) 기반 방법의 부상**

최근 연구들이 U-Net 대신 Diffusion Transformer를 사용하고 있다:[5][9][20][4]

- **MoDiT (2025)**: 3D Morphable Model (3DMM) 계수와 Diffusion Transformer 결합으로 시간적 안정성 개선[5]
- **Talking-DiSSM (2025)**: State-Space Models (SSM)으로 선형 복잡도 시간 모델링, 이차 복잡도의 Self-Attention 대체[8]
- **MoDiTalker (2024)**: 동작 분리를 통해 고충실도 얼굴 생성과 시간적 일관성 동시 달성[21]

**수학적 개선**: Transformer의 이차 주의 복잡도 $$O(T^2)$$를 SSM의 선형 복잡도 $$O(T)$$로 개선[8]

**2) 3DMM 매개변수를 명시적으로 활용하는 경향**

최근 방법들이 3D 형태 정보를 명시적으로 사용:[6][11][13][14][20][2][4][5]

- **KDTalker (2025)**: 비지도 암묵적 3D 키포인트를 활용하여 다양한 머리 포즈 생성[11]
- **DAE-Talker (2024)**: Diffusion Autoencoder의 데이터 주도적 잠재 표현으로 3D 기하 제약 없이 성능 향상[16]
- **Cafe-Talk (2025)**: 3D 인물 얼굴 생성에 다중 모드 조건화 (음성, 감정, 액션 유닛) 통합[6]

**3) 감정 표현 및 제어 가능성의 확장**

단순 음성 구동을 넘어 감정, 액션 유닛(Action Units) 등을 포함한 다양한 제어 가능성 추가:[12][7][6]

- **DREAM-Talk (2023)**: 감정 표현과 정확한 립 싱크를 동시에 달성하는 두 단계 확산 프레임워크[12]
- **MF-ETalk (2025)**: 다중 모드 특징 지도를 사용한 감정 인식 얼굴 생성[7]
- **Cafe-Talk (2025)**: 거친 감정 조건화와 세밀한 Action Unit 조건화의 단계적 학습[6]

**4) 의미론적 분리(Semantic Disentanglement) 추구**

최근 연구가 얼굴의 다양한 요소(입술, 머리 포즈, 표정)를 분리하여 독립적으로 제어:[2][4][11]

- **DisentTalk (2025)**: 3DMM 표현 매개변수를 의미 있는 부분공간으로 분해하는 데이터 주도적 분리 프레임워크[2]
- **FD2Talk (2024)**: 얼굴 분해 확산 모델로 일반화된 얼굴 생성 달성[22]

**5) 전신 및 다인물 생성으로의 확장**

단일 얼굴을 넘어 전신 움직임과 다인물 상황으로 확대:[23][9]

- **AnyTalker (2025)**: 다인물 비디오 생성으로 확장, 신원 인식 주의 메커니즘으로 임의 개수의 신원 처리[9]
- **JWB-DH-V1 (2025)**: 10,000개 신원을 포함한 대규모 벤치마크로 전신 얼굴-음성 생성 평가[23]

#### 4.3 향후 연구 시 반드시 고려할 점

**1) 시간적 일관성(Temporal Consistency)의 중요성**

Diffused Heads의 주요 한계인 8-9초 이상 생성 불가 문제는 여전히 해결되지 않은 과제다.[1]

**개선 방향**:
- 모션 선행(motion priors)의 활용으로 장시간 생성 안정화[14]
- 양방향 State-Space Model로 전역 시간적 일관성 보장[8]
- 계층적 확산 전략으로 다단계 세분화[4][5]

**2) 계산 효율성(Computational Efficiency)**

현재 확산 모델은 실시간 응용이 불가능하다.[1]

**필요한 개선**:
- 디스틸레이션(Distillation)을 통한 샘플링 단계 감소[24]
- 더 효율적인 시간 모델(예: SSM 사용)[8]
- 조건부 생성 최적화로 샘플링 단계 수 감소[25]

**3) 일반화 및 적응성(Generalization and Adaptation)**

도메인 외(out-of-domain) 일반화는 개선되었지만, 특정 악센트나 언어에 대한 세밀한 조정이 필요할 수 있다.[26][2][1]

**개선 방향**:
- 다언어/다악센트 데이터로 학습[2]
- Few-shot 메타-러닝 적응[26]
- 대규모 벤치마크 데이터셋 구축[23]

**4) 평가 메트릭의 표준화**

현재 FVD, FID 외에 음성 구동 애니메이션 특화 메트릭이 부족하다.[27][1]

**필요한 개선**:
- 감정/표현성 평가 메트릭 개발
- 시간적 일관성 전문 메트릭 설계
- 자동 립 싱크 정확도 측정 도구 개선

**5) 다중 모드 제어의 활성화**

단순 음성 구동을 넘어 감정, 제스처, 텍스트 등 다양한 입력 신호 통합 필요:[7][12][6]

**개선 방향**:
- 음성, 텍스트, 이미지 등 다중 모드 조건화 통합
- 감정 표현과 음성 동기화의 균형 맞춤
- 사용자 지정 제어(customizable control) 메커니즘 개발

**6) 신원 보존과 얼굴 왜곡 문제**

극단적인 각도나 대규모 머리 움직임에서의 안정성 개선:[11][5][1]

**개선 방향**:
- 암묵적 3D 키포인트를 명시적 기하 제약과 결합[11]
- 3D 형태 인코딩으로 신원 보존 강화[5]
- 다각도 학습 데이터 확충[28]

**7) 데이터셋 다양성 및 균형**

현재 주요 벤치마크(CREMA, LRW)의 제한성 극복:

**필요한 개선**:
- 더 큰 규모의 다양한 데이터셋 구축[23][2]
- 다양한 인종, 성별, 나이 분포의 균형 잡힌 표현[23]
- 도메인 특화 데이터셋(예: 의료, 교육용 아바타)[23]

**8) 윤리적 고려사항**

얼굴 합성의 발전은 deepfake 등의 악용 가능성을 증가시킨다.

**필수 고려사항**:
- 위조된 콘텐츠 탐지 기술 병행 개발
- 합성된 비디오의 워터마킹 및 인증 메커니즘
- 법적/규제 프레임워크와의 조화

***

### 5. 2020년 이후 관련 최신 연구 탐색

#### 5.1 시간대별 연구 동향

**2020-2022: GAN 지배와 확산 모델 초기 진입**

이 기간의 주요 연구:[18][19][29][30]

- **Wav2Lip (2020)**: 임 읽기 모델 기반 음성-입술 동기화
- **MakeItTalk (2020)**: 스타일 벡터 기반 표현 생성
- **CodeTalker (2023)**: 이산 동작 선행을 통한 3D 얼굴 애니메이션[19]
- **논문 발표 2023.01**: Diffused Heads 발표로 확산 모델 전환점 마련[10]

**2023: 확산 모델로의 전환점**

- **DiffTalk (2023)**: 잠재 확산 모델로 일반화된 얼굴 애니메이션[15]
- **DREAM-Talk (2023)**: 감정 표현과 입술 동기화 동시 달성[12]
- **GSmoothFace (2023)**: 세밀한 3D 얼굴 가이드로 부드러운 생성[31]

**2024: 특화 및 제어성 강화**

- **MoDiTalker (2024)**: 동작 분리로 고충실도 달성[21]
- **ControlTalk (2024)**: 암묵적 얼굴 키포인트 편집으로 제어성 향상[26]
- **PortraitTalk (2024)**: 맞춤형 원샷 음성 구동 생성[13]
- **DAE-Talker (2024)**: 확산 오토인코더의 데이터 주도적 표현[16]
- **IF-MDM (2024)**: 암묵적 얼굴 동작 확산 모델로 실시간화 시도[18]
- **FD2Talk (2024)**: 얼굴 분해 확산 모델로 일반화 강화[22]

**2025: 다중성과 효율성 추구**

최신 연구들이 다음을 중점으로 함:[3][20][9][4][7][5][6][11][2][8][23]

1) **효율성 개선** - Transformer와 SSM 기반 구조로 샘플링 가속
2) **제어성 확장** - 감정, 제스처, 언어 등 다중 조건화
3) **확장성** - 다인물, 전신, 다양한 도메인으로 확대
4) **의미론적 분리** - 얼굴 요소들의 독립적 제어

#### 5.2 최신 주요 연구 상세 분석

**1. DisentTalk (2025): 의미론적 분리 확산 모델**[2]

```
주요 기여:
- 3DMM 표현 매개변수를 의미 있는 부분공간으로 분해
- 계층적 잠재 확산 아키텍처로 3DMM 파라미터 공간에서 작동
- 지역별 주의 메커니즘으로 공간 정밀성과 시간 일관성 보장

기술적 혁신:
- Semantic Disentanglement Framework
- Region-aware Attention Mechanisms
- 새로운 중국 고해상도 얼굴 데이터셋(CHDTF) 소개
```

**2. MF-ETalk (2025): 다중 모드 특징 기반 감정 생성**[7]

주요 특징:
- Action Units (AUs)를 사용한 얼굴 표현 분해
- 비선형 관계 모델링을 위한 잔여 인코더
- 계층적 다중 모드 특징 융합 모듈

**3. Talking-DiSSM (2025): 상태 공간 모델 기반**[8]

혁신점:
- 양방향 State-Space Models (Bi-SSM)으로 선형 복잡도 달성
- Batch-overlapped sampling으로 배치 간 상관관계 구축

$$\text{Temporal Complexity: } O(T) \text{ vs } O(T^2) \text{ (Self-Attention)}$$

**4. MoDiT (2025): 3D 계수 학습**[5]

특징:
- 3D Morphable Model 계수를 명시적으로 학습
- 계층적 잡음 제거 전략
- 자연스러운 눈 깜빡임 모델링

**5. Cafe-Talk (2025): 다중 모드 조건화**[6]

기술:
- 거친(coarse-grained) 및 세밀한(fine-grained) 제어 조건 통합
- 두 단계 훈련 파이프라인으로 조건 분리
- 자연언어 사용자 입력 지원

**6. AnyTalker (2025): 다인물 생성**[9]

혁신:
- 신원 인식 주의 메커니즘으로 임의 개수의 신원 처리
- 단일 인물 비디오만으로 다인물 학습

#### 5.3 분야의 현황 및 과제

**현재 SOTA (State-of-the-Art) 경향**

1) **아키텍처**: UNet → Transformer → State-Space Model 진화
2) **조건화**: 음성 단독 → 다중 모드 (음성+감정+제스처+텍스트)
3) **3D 활용**: 암묵적 → 명시적 3D 정보 활용
4) **확장성**: 단일 신원 → 다인물, 전신 생성

**여전한 과제**

| 과제 | 현황 | 연구 방향 |
|------|------|---------|
| **시간적 일관성** | 8-9초 제약[1] | 모션 선행, SSM 활용[8][14] |
| **실시간성** | 매우 느림[1] | 디스틸레이션, 효율적 아키텍처[24][25] |
| **입술 동기화 정확도** | WER 높음[1] | 입술 읽기 전문가 통합[2][7][32] |
| **극단적 각도 안정성** | 불안정[1] | 다각도 3D 학습[11][28] |
| **일반화** | 개선 중[1][26] | 메타-러닝, 대규모 데이터셋[23] |
| **평가 메트릭** | 표준화 부족[1] | 새로운 메트릭 개발[27] |

***

### 결론

**Diffused Heads**는 2023년 발표되어 음성 구동 얼굴 생성 분야에 **패러다임 전환**을 일으킨 획기적인 논문이다. GAN의 한계를 극복하는 확산 모델의 우월성을 최초로 입증함으로써, 이후 100개 이상의 후속 연구를 촉발했다.[17][20][3][13][14][15][19][4][9][21][16][18][10][11][12][7][5][6][2][8][23][1]

**주요 기여의 지속적 영향**:

1) **모션 프레임과 음성 임베딩의 설계**는 최근 연구들이 계속 개선하고 활용하고 있는 핵심 요소이다.

2) **확산 모델 기반 접근**은 현재 모든 최신 연구의 기본 토대가 되어있으며, 특히 Transformer 기반 구조로의 진화를 가능하게 했다.

3) **일반화 성능에 대한 강조**는 후속 연구들이 도메인 외 적응성과 메타-러닝에 집중하도록 영감을 주었다.

**향후 연구의 중요 과제**:

- **시간적 일관성**: 8-9초 이상의 장시간 생성 안정화
- **실시간 처리**: 계산 효율성 극적 개선
- **다중 제어**: 음성을 넘어 감정, 제스처, 언어 등 통합
- **확장성**: 다인물, 전신, 크로스 도메인 적응
- **평가 표준화**: 음성 구동 애니메이션 특화 메트릭 개발

이 논문이 제시한 기초 위에서 2024-2025년 연구들은 **Transformer 기반 아키텍처, 3D 명시적 표현, 의미론적 분리, 다중 모드 조건화, 효율적 시간 모델**로의 진화를 이루고 있으며, 이러한 발전이 궁극적으로 **실시간 고품질 음성 구동 인물 생성의 실용화**를 향해 나아가고 있다.

***

## 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/de601414-7431-494c-9cbd-85318c62b8b4/2301.03396v2.pdf)
[2](https://ieeexplore.ieee.org/document/11209940/)
[3](https://link.springer.com/10.1007/s00371-025-03907-8)
[4](https://ieeexplore.ieee.org/document/10887982/)
[5](https://arxiv.org/abs/2507.05092)
[6](https://arxiv.org/abs/2503.14517)
[7](https://www.mdpi.com/2079-9292/14/13/2684)
[8](https://dl.acm.org/doi/10.1145/3742790)
[9](https://www.semanticscholar.org/paper/20bcc79275aac5d07b5bc9cf0375ac043e96bd9f)
[10](https://openaccess.thecvf.com/content/WACV2024/papers/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.pdf)
[11](https://arxiv.org/abs/2503.12963)
[12](https://arxiv.org/html/2312.13578v1)
[13](https://arxiv.org/html/2412.07754v1)
[14](https://arxiv.org/html/2502.09533v1)
[15](https://arxiv.org/pdf/2301.03786.pdf)
[16](https://arxiv.org/pdf/2303.17550.pdf)
[17](https://arxiv.org/pdf/2404.10667.pdf)
[18](https://arxiv.org/html/2412.04000v2)
[19](https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.pdf)
[20](https://arxiv.org/pdf/2502.17198.pdf)
[21](https://arxiv.org/html/2403.19144)
[22](https://arxiv.org/html/2408.09384v1)
[23](https://arxiv.org/abs/2507.20987)
[24](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)
[25](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Upscale-A-Video_Temporal-Consistent_Diffusion_Model_for_Real-World_Video_Super-Resolution_CVPR_2024_paper.pdf)
[26](https://arxiv.org/abs/2406.02880)
[27](https://arxiv.org/abs/2504.16081)
[28](https://arxiv.org/html/2409.16990)
[29](http://arxiv.org/pdf/2107.04806.pdf)
[30](https://openreview.net/pdf?id=sgDFqNTdaN)
[31](https://arxiv.org/html/2312.07385)
[32](https://www.krafton.ai/en/vision-animation/5810/)
[33](https://www.emergentmind.com/topics/latent-video-diffusion-model)
[34](https://www.semanticscholar.org/paper/a677b65bf1e1f9881bccc90fe08261e11f79fab3)
[35](https://arxiv.org/html/2510.21864v1)
[36](https://github.com/harlanhong/awesome-talking-head-generation)
[37](https://academic.oup.com/jsm/article/doi/10.1093/jsxmed/qdae167.029/7919055)
[38](https://www.semanticscholar.org/paper/396c88f8afb8591266a77bc93a2633668caf423e)
[39](https://arxiv.org/pdf/2402.18122.pdf)
[40](https://aclanthology.org/2025.emnlp-main.40.pdf)
[41](https://www.trinitydynamics.net/urltrinitydynamics-net-2024-av-trends-and-2025-predictions/)
[42](https://github.com/JosephPai/Awesome-Talking-Face)
[43](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06048.pdf)
[44](https://dl.acm.org/doi/10.1145/3696445)
[45](https://openreview.net/pdf/c037185b821dbd738b57a6885c2937263b1a4ddd.pdf)
