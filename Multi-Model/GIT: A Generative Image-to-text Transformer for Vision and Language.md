# GIT: A Generative Image-to-text Transformer for Vision and Language

### 1. 핵심 주장과 주요 기여 요약

GIT(Generative Image-to-text Transformer)는 비전-언어 통합을 위한 획기적인 접근법을 제시합니다. 이 논문의 핵심 주장은 **극도로 단순화된 아키텍처로도 복잡한 구조와 외부 모듈에 의존하는 기존 방식을 능가할 수 있다**는 것입니다.[1]

**주요 기여:**

**아키텍처 측면**: 기존의 다중 모달 인코더-디코더와 복잡한 구조를 하나의 이미지 인코더와 텍스트 디코더로 단순화했습니다. 이는 이미지 캡셔닝, 비디오 캡셔닝, VQA(Visual Question Answering) 등 다양한 작업을 통합된 언어 모델링 작업으로 처리할 수 있음을 시사합니다.[1]

**성능 성과**: 이미지/비디오 캡셔닝 및 질의응답을 포함한 14개 벤치마크에서 새로운 최첨단(SOTA) 성능을 달성했습니다. 특히 TextCaps에서 처음으로 인간 성능(125.5)을 초과하여 138.2 CIDEr를 기록했습니다.[1]

**외부 모듈 제거**: 기존 방식이 필요로 했던 객체 탐지기, 객체 태그, OCR 엔진 등의 외부 모듈이 제거되었으며, 엔드-투-엔드 학습이 가능해졌습니다.[1]

**일반화 능력**: 비디오 도메인에 특화되지 않았음에도 비디오 작업에서 우수한 성능을 보였으며, 제너레이션 기반 이미지 분류와 장면 텍스트 인식을 포함한 새로운 패러다임을 제시했습니다.[1]

***

### 2. 해결하는 문제 및 동기

**문제 정의:**[1]

1. **아키텍처 복잡성**: 기존 비전-언어 모델들은 다층 복잡한 구조(마스킹된 언어 모델링, 이미지-텍스트 매칭 등)를 채택했으며, 이는 사전학습과 파인튜닝 간의 작업 불일치를 야기합니다.[1]

2. **외부 의존성**: 많은 접근법들이 Faster R-CNN 기반의 객체 탐지기에 의존하여 이미지 특성을 추출했으며, 씬텍스트 관련 작업을 위해 OCR 엔진을 필요로 했습니다.[1]

3. **작업 일관성 부재**: 다양한 하류 작업(캡셔닝, VQA 등)을 처리하기 위해 각각 맞춤형 적응이 필요했습니다.[1]

**동기:**[1]

생성 모델은 사전학습과 파인튜닝에서 일관된 네트워크 아키텍처를 제공할 수 있다는 관찰에서 출발합니다. 이미지를 입력받아 전체 텍스트 설명을 출력하는 극도로 단순한 구조(이미지 인코더 + 텍스트 디코더)로도 큰 규모의 데이터와 모델 사이즈 스케일링을 통해 강력한 성능을 달성할 수 있다는 가설에 기반합니다.[1]

***

### 3. 제안 방법 (수식 포함)

#### 3.1 네트워크 아키텍처[1]

**이미지 인코더**: 대조적 사전학습된 모델(예: Florence)을 기반으로 하는 Swin 유형의 비전 트랜스포머를 사용합니다. 이는 2D 특성 맵을 평탄화하여 특성 리스트로 변환하고, 선형 레이어와 레이어 정규화를 거쳐 D 차원으로 투영됩니다.[1]

**텍스트 디코더**: 텍스트 설명을 예측하기 위한 트랜스포머 모듈입니다. 텍스트는 토큰화되고 D 차원으로 임베딩된 후, 위치 인코딩과 레이어 정규화가 적용됩니다.[1]

**입력 구성**: 이미지 특성과 텍스트 임베딩이 연결되어 트랜스포머 모듈의 입력이 되며, seq2seq 어텐션 마스크가 적용되어 텍스트 토큰이 이전 토큰과 모든 이미지 토큰에만 의존하도록 제한됩니다.[1]

#### 3.2 사전학습 목적함수[1]

이미지-텍스트 쌍 $$(I, y_1, ..., y_N)$$에 대해 언어 모델링(LM) 손실을 적용합니다:

$$\mathcal{L} = \frac{1}{N-1}\sum_{i=1}^{N-1} CE(y_i, p(y_i|I, y_j, j=0,...,i-1))$$

여기서:
- $$CE$$는 라벨 평활화(label smoothing = 0.1)를 포함한 교차 엔트로피 손실
- $$y_0$$는 BOS 토큰, $$y_{N-1}$$는 EOS 토큰
- $$p(y_i|...)$$는 이전 토큰들과 이미지가 주어졌을 때 현재 토큰의 확률[1]

**LM vs MLM 선택 이유**:[1]
- MLM은 각 반복에서 약 15%의 토큰만 예측하므로, 모든 토큰 예측을 위해 약 6.7 에포크 필요
- LM은 각 반복에서 모든 토큰을 예측하므로 계산 효율이 높음
- 대규모 데이터 학습 시 에포크 수가 제한적(2 에포크)이므로 LM이 적합

#### 3.3 파인튜닝 방식[1]

**이미지 캡셔닝**: 사전학습과 동일한 LM 작업으로 파인튜닝[1]

**VQA**: 질문과 정답을 새로운 특수 캡션으로 연결하되, LM 손실은 정답과 EOS 토큰에만 적용됩니다:[1]
$$\text{입력: } [Q, \text{BOS}], \quad \text{손실: } \sum_{i=|Q|+1}^{|Q|+|A|+1} CE(y_i, p(y_i|...))$$

**비디오 처리**: 여러 프레임을 샘플링하여 각각 이미지 인코더로 독립 인코딩 후, 학습 가능한 시간 임베딩(초기값 0)을 추가하고 연결합니다:[1]
$$\text{Video}_{\text{rep}} = \text{Concat}(f(F_1) + t_1, f(F_2) + t_2, ..., f(F_k) + t_k)$$
여기서 $$F_i$$는 i번째 프레임, $$f$$는 이미지 인코더, $$t_i$$는 시간 임베딩[1]

#### 3.4 이미지 분류를 위한 제너레이션 기반 접근[1]

전통적 분류와 달리, 클래스 이름을 이미지 캡션으로 해석하고 자동회귀 방식으로 예측합니다. 이는 사전 정의된 어휘에 의존하지 않으므로 새로운 카테고리 추가 시 매개변수 재학습이 필요하지 않습니다.[1]

***

### 4. 모델 구조 상세 분석

#### 4.1 Seq2Seq 어텐션 마스크[1]

일반적인 단방향 마스크와 달리, GIT는 seq2seq 마스크를 적용하여:
- 텍스트 토큰은 이전 텍스트 토큰 + 모든 이미지 토큰에 접근
- 이미지 토큰은 다른 이미지 토큰끼리 상호작용 가능
- 이는 이미지 토큰 간 정보 공유를 통해 더 나은 표현 학습 가능[1]

#### 4.2 초기화 전략[1]

**이미지 인코더**: 대조적 사전학습 모델(Florence CoSwin)로 초기화[1]
**텍스트 디코더**: 무작위 초기화 (BERT 초기화는 이미지 신호를 이해하지 못하므로 불필요)[1]

이는 별도의 어휘사전 초기화 없이도 효과적이며, 다양한 설계 선택 탐색을 가능하게 합니다.[1]

#### 4.3 모델 변형[1]

논문에서 제시된 모델 변형:

| 변형 | 이미지 인코더 | 사전학습 데이터 | 에포크 | 파라미터 |
|------|-------------|-------------|-------|---------|
| GITB | CLIPViT-B16 | 10M (4M 이미지) | 30 | 129M |
| GITL | CLIPViT-L14 | 20M (14M 이미지) | 30 | 347M |
| GIT | FlorenceCoSwin | 800M (0.8B 이미지) | 2 | 681M |
| GIT2 | DaViT | 12.9B (10.5B 이미지) | 2 | 5.1B |

***

### 5. 성능 향상

#### 5.1 이미지 캡셔닝 성과[1]

| 벤치마크 | GIT 성과 | 이전 SOTA | 향상도 |
|---------|---------|---------|--------|
| COCO test-std | 148.8 CIDEr | 138.7 | **+10.1** |
| nocaps | 123.4 CIDEr | 120.6 | **+2.8** |
| TextCaps | 138.2 CIDEr | 109.7 | **+28.5** |
| VizWiz-Captions | 114.4 CIDEr | 94.1 | **+20.3** |

**특히 주목**: TextCaps에서 처음으로 인간 성능(125.5)을 초과[1]

#### 5.2 VQA 성과[1]

| 벤치마크 | GIT | Flamingo (80B) | 개선사항 |
|---------|-----|---|---------|
| VizWiz-VQA | 67.5 | 65.4 | **+2.1** |
| OCR-VQA | 68.1 | - | 새로운 SOTA |
| ST-VQA | 69.6 | - | LaTr와 동등 |
| TextVQA | 59.75 | 54.1 | **+5.6** |

#### 5.3 비디오 작업 성과[1]

**비디오 캡셔닝**:
- MSVD: 180.2 CIDEr (이전: 120.6) - **+59.6**
- MSRVTT: 73.9 CIDEr (이전: 60) - **+13.9**
- VATEX: 93.8 CIDEr (이전: 86.5) - **+7.3**

**비디오 QA**:
- MSVD-QA: 56.8% (이전: 48.3) - **+8.5**
- TGIF-Frame: 72.8% (이전: 69.5) - **+3.3**

#### 5.4 이미지 분류[1]

ImageNet-1K에서 88.79% top-1 정확도 달성 (Florence와 비교하면 약 1.2% 낮음, 이는 생성 모델의 증가된 난이도 때문)

#### 5.5 장면 텍스트 인식[1]

6개 표준 벤치마크 평균:
- TextCaps 파인튜닝: 89.9% 정확도
- MJST 데이터 파인튜닝: 92.9% 정확도 (이전 SOTA: 91.9)

***

### 6. 모델 및 데이터 스케일링 분석[1]

#### 6.1 스케일링 관찰[1]

**COCO 벤치마크**:
- 기본 모델은 4M → 14M 이미지에서 성능 향상
- 14M → 0.8B 이미지로 증가하면 성능 감소
  - 이유: COCO의 이미지 분포가 처음 20M 데이터와 더 유사; 기본 모델의 용량 부족[1]

**TextCaps 및 VizWiz-QA**:
- 모든 모델 변형이 더 많은 데이터에서 significant하게 향상
- 더 큰 백본(이미지 인코더)이 0.8B 데이터에서 더 큰 개선 달성[1]

#### 6.2 텍스트 디코더 스케일링의 한계[1]

초기 가설과 달리, 더 큰 디코더(12 → 24 레이어)는 성능 향상을 보이지 않음:

| 레이어 수 | COCO | nocaps | VizWiz |
|----------|------|--------|---------|
| 6 | 138.9 | 136.4 | 119.3 |
| 12 | 138.9 | 136.0 | 118.1 |
| 24 | 139.1 | 134.6 | 115.4 |

**가능한 이유**:[1]
1. 텍스트는 이미지보다 학습이 쉬워 작은 디코더도 충분
2. 이미지 인코더가 객체 인식, 디코더가 자연스러운 문장 생성을 담당
3. 더 큰 디코더는 학습 난이도 증가로 성능 저하

#### 6.3 씬 텍스트 데이터 분석[1]

사전학습 데이터 분석:
- CC12M의 15%, 수집 이미지의 31%가 씬 텍스트 설명 포함
- 네트워크가 점진적으로 씬 텍스트 읽기 학습 (외부 OCR 없이)[1]

***

### 7. 일반화 성능 향상 분석

#### 7.1 제너레이션 기반 분류의 일반화[1]

**Zero-shot 성능**:
- Exact match: 1.93% (제한적)
- Contains 메트릭: 40.88% (모델이 클래스 개념 포착)
- Vocabulary-prior: 33.48% (어휘 제약 시 성능 저하)[1]

**Few-shot 성능의 개선**:
- 1-shot: 64.54% (exact) → 66.76% (contains)
- 5-shot: 79.79% (exact) → 80.95% (contains)
- Flamingo(80B)의 77.3%보다 GIT(0.7B)의 80.95% 5-shot 성능이 우수[1]

**적응성 이점**:
- Flamingo는 인-컨텍스트 학습으로 매 테스트 이미지마다 support 예제 필요
- GIT는 가벼운 파인튜닝으로 일회성 학습 후 추론 시 추가 비용 없음[1]

#### 7.2 아키텍처 선택과 일반화[1]

**Cross-attention vs Self-attention 비교**:

| 아키텍처 | 소규모 데이터(10M) | 대규모 데이터(0.8B) |
|---------|-------------|------------|
| Cross-attention | 더 우수 | 약간 낮음 |
| Self-attention | 약간 낮음 | **더 우수** |

**해석**:[1]
- 소규모: 이미지 토큰 간 정보 공유의 이점 제한
- 대규모: 충분한 학습으로 self-attention을 통한 이미지 토큰 상호작용이 더 효과적
- 이미지-텍스트 상호정보 최대화 가능[1]

#### 7.3 도메인 간 전이 학습[1]

**Karpathy split (COCO) → nocaps (OOD)**:
- nocaps에서 높은 성능: 125.5 CIDEr (Karpathy에서 144.8)
- 모델이 일반적인 개념 학습, 특정 분포에 과적합 아님[1]

**Novel object 인식 (nocaps)**:
- Out-of-domain 성능 유지: 127.1 CIDEr
- COCO 훈련 데이터에 없는 객체도 정확히 설명 가능[1]

***

### 8. 한계 및 과제

#### 8.1 명시적 한계[1]

**생성 모델의 난이도**:
- VQAv2에서 78.81% (Florence 80.36% 대비 -1.5%)
- 이유: 각 정답이 최소 2개 토큰 정확예측 필요 (정답 + EOS)
- 판별 모델은 1회 예측만 필요[1]

**이미지 분류 성능 저하**:
- ImageNet-1K: 88.79% vs Florence 90.05% (-1.2%)
- 제너레이션 방식의 증가된 복잡성[1]

**제어 및 인-컨텍스트 학습 불가**:
- 생성된 캡션에 대한 명시적 제어 불가
- 매개변수 업데이트 없는 인-컨텍스트 학습 미지원[1]

#### 8.2 아키텍처 적응 한계[1]

**더 큰 디코더의 비효과성**: 텍스트 디코더 스케일링이 성능 향상을 가져오지 못함 (섹션 6.2)

**VQAv2의 성능 정체**: 비디오 QA 작업에서 데이터 규모 증가가 제한적 개선만 제공 (도메인 갭 존재)

***

### 9. 최신 연구 기반 미래 영향 및 고려사항

#### 9.1 일반화 성능 향상의 미래 방향[2][3][4][5]

**지속적 학습(Continual Learning) 관점**:[2]
최근 연구(2024)는 비전-언어 모델의 지속적 학습 시 이전 지식 망각 문제를 제기합니다. GIT의 단순 아키텍처는 이 문제 해결에 유리할 수 있으며, 선택적 지식 증류(Selective Dual-Teacher Knowledge Distillation)와 결합하면 제너레이션 모델도 새로운 작업 학습 시 이전 성능 유지 가능.[2]

**개념 기반 프롬프트 학습**:[3]
2024년 연구는 시각적 개념(색상, 형태, 크기 등)의 전이 가능성을 강조합니다. GIT의 대규모 사전학습이 이러한 개념을 충분히 포착한다면, 파인튜닝 시 개념 수준의 특성 제어로 일반화 향상 가능.[3]

**매개변수 효율적 전이 학습**:[6][7]
2024-2025년 연구는 어댑터(Adapter)와 프롬프트 기반 방식의 발전을 보입니다. GIT의 간단한 구조는 경량 어댑터 추가로 새로운 도메인 적응이 용이하며, 교차-모달 정보(cross-modal cues) 활용으로 더욱 효율화 가능.[7][6]

**과적합 완화**:[4]
2024년 LOBG 연구는 VLM의 파인튜닝 시 과적합 문제를 체계적으로 분석합니다. GIT가 제너레이션 기반 접근을 사용하므로, 구조적 토폴로지 보존(Structural Topology Preservation) 손실을 추가하면 특성 공간 재구조화로 일반화 성능 향상 가능.[4]

#### 9.2 멀티모달 스케일링 법칙[8][9]

**스케일링 법칙의 통합성**:[9]
최근 2025년 연구(ICCV)는 멀티모달 모델이 텍스트 LLM과 유사한 스케일링 법칙을 따름을 보입니다. 특히 early-fusion (GIT의 구조와 유사)과 late-fusion의 성능 격차는 모델 규모 증가에 따라 수렴되지만, 계산 효율성 측면에서 early-fusion이 우월함. 따라서 GIT의 아키텍처는 미래 초대규모 멀티모달 모델 개발의 효율적 기반 제공 가능.[9]

**모달리티 간 토큰화 효율**:[8]
멀티모달 모델의 성능은 모달리티별 압축률과 토큰화 효율에 기반하는 새로운 가설이 제기되었습니다. GIT의 이미지 토큰 연결 전략이 최적화된다면, 더 적은 계산으로 더 나은 성능 달성 가능.[8]

#### 9.3 도메인 적응 및 부호화 불변성[10][11]

**VLM 일반화의 체계적 분류**:[10]
2023-2025년 종합 리뷰는 비전-언어 모델의 일반화를 다양한 설정(비지도 도메인 적응, 도메인 일반화, few-shot 적응 등)으로 분류합니다. GIT의 극도로 단순한 구조는 이 모든 설정에서 경량 모듈화 확장이 용이하며, 특히 few-shot 적응에서 우수성 입증.[10][1]

**멀티모달 도메인 적응**:[11]
2024년 NeurIPS 논문은 텍스트-이미지 확산 모델을 활용한 멀티모달 도메인 적응을 제시합니다. GIT의 생성 능력과 확산 모델 결합 시, 깊이, 적외선, 이벤트 등 새로운 모달리티로의 적응 가능성 증대.[11]

#### 9.4 로보틱스 및 구체화된 AI로의 확장[12][13]

**비전-언어-행동 모델**:[13][12]
최근 2024-2025년 연구는 VLM을 로봇 제어로 확장하는 비전-언어-행동(VLA) 모델을 집중 분석합니다. GIT의 생성 방식은 자연스러운 지시(instruction)와 행동 예측을 통합할 수 있으며, 크로스-구현(cross-embodiment) 데이터 활용으로 다양한 로봇 간 전이 학습 가능.[12][13]

***

### 10. 향후 연구 시 고려사항

#### 10.1 아키텍처 개선[1]

1. **텍스트 디코더 스케일링**: 현재 6 레이어 고정의 한계 극복. 더 정교한 스케일링 전략(조건부 스케일링, 적응적 병목) 탐색[1]

2. **혼합 데이터 원본 통합**: 텍스트만으로도 학습 가능한 구조로 발전. GPT-3 같은 텍스트 전용 LLM 데이터 활용으로 디코더 성능 강화[1]

3. **이중 모달리티 인-컨텍스트 학습**: 현재 한계인 매개변수 업데이트 없는 인-컨텍스트 학습 해결. 적응적 프롬프팅 메커니즘 개발[1]

#### 10.2 데이터 효율성[1]

1. **씬 텍스트 인식 최적화**: 15-31% 비율의 씬 텍스트 데이터를 더 효율적으로 활용. 합성 씬 텍스트 데이터 자동 생성 및 선택적 학습[1]

2. **저자원 시나리오**: 현재 0.8B 데이터의 축소 버전에서의 성능 분석 필요. 데이터 효율성 개선으로 엣지 기기 배포 가능성[1]

3. **도메인 특화 사전학습**: 의료, 산업 이미징 등 특정 도메인의 작은 데이터셋으로 사전학습 후 강력한 성능 달성 가능성 탐색[1]

#### 10.3 공정성 및 사회적 영향[1]

1. **바이어스 완화**: TextCaps 분석에서 성별/피부색 바이어스 격차 지속 (성별: 0.7-2.1, 피부색: 2.3-5.3). 균형잡힌 학습 데이터 구성 필요[1]

2. **독성 언어 제어**: 대규모 데이터 사전학습 시 독성 콘텐츠 포함 가능성. 생성된 캡션 안전성 평가 및 필터링 메커니즘 필수[1]

3. **접근성 보장**: 시각장애인 지원의 원래 목표 달성을 위해 실제 배포 환경에서의 성능/공정성 검증 필수[1]

#### 10.4 최신 연구 통합[5][3][4][9][2][8]

1. **지속적 학습**: 선택적 증류 프레임워크와 결합으로 새로운 작업 학습 시 기존 성능 유지[2]

2. **개념 기반 제어**: 시각 개념 캐시 활용으로 생성된 캡션의 명시적 제어 가능성[3]

3. **효율적 적응**: 다양한 PETL 방법과의 호환성 탐색. 매개변수 효율성과 성능 균형 최적화[6][7]

4. **과적합 완화**: LOBG 같은 기법으로 파인튜닝 일반화 향상[4]

5. **스케일링 최적화**: 멀티모달 스케일링 법칙 기반 계산 효율적 모델 크기 결정. 모달리티별 토큰화 효율 최대화[9][8]

***

### 11. 결론

GIT는 **단순성, 확장성, 일반화 능력의 최적 조합**을 제시하는 선도적 연구입니다. 이미지 인코더 + 텍스트 디코더라는 극도로 단순한 구조로 14개 벤치마크에서 새로운 SOTA를 달성하고, 최초로 TextCaps에서 인간 성능을 초과했습니다.[1]

**핵심 통찰**: 복잡한 아키텍처와 외부 모듈 없이도, 규모 있는 데이터와 효율적인 학습으로 우수한 일반화 성능을 달성할 수 있습니다.[1]

**미래 전망**: 최신 연구들(2024-2025)은 GIT의 아키텍처가 지속적 학습, 효율적 적응, 멀티모달 스케일링, 로봇 제어 등 다양한 분야로 확장 가능함을 시사합니다. 특히 early-fusion의 계산 효율성이 멀티모달 모델 개발의 미래 방향으로 재확인되고 있습니다.[7][5][6][12][3][4][8][9][2]

향후 연구는 디코더 스케일링, 인-컨텍스트 학습, 데이터 효율성 개선, 공정성 보장에 집중해야 하며, 이는 더욱 강력하고 실용적인 비전-언어 모델 개발의 기초가 될 것입니다.[3][4][8][9][2][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8ad93e61-dbbb-4f3c-ba99-4fad03fdf3cd/2205.14100v5.pdf)
[2](https://arxiv.org/abs/2403.09296)
[3](https://arxiv.org/abs/2401.07457)
[4](https://arxiv.org/abs/2410.10247)
[5](https://arxiv.org/abs/2503.07065)
[6](https://link.springer.com/10.1007/s00530-025-01878-3)
[7](https://ieeexplore.ieee.org/document/10688369/)
[8](https://arxiv.org/html/2409.06754v1)
[9](https://openaccess.thecvf.com/content/ICCV2025/papers/Shukor_Scaling_Laws_for_Native_Multimodal_Models_ICCV_2025_paper.pdf)
[10](https://arxiv.org/html/2506.18504v1)
[11](https://openreview.net/forum?id=5BwWgyvgwR)
[12](https://arxiv.org/abs/2412.14058)
[13](https://arxiv.org/abs/2406.20095)
[14](https://ieeexplore.ieee.org/document/10655598/)
[15](https://ojs.aaai.org/index.php/AAAI/article/view/32444)
[16](https://arxiv.org/abs/2409.03868)
[17](http://arxiv.org/pdf/2311.15569.pdf)
[18](https://arxiv.org/html/2504.00691v1)
[19](https://arxiv.org/html/2403.09394v1)
[20](http://arxiv.org/pdf/2311.17091.pdf)
[21](https://arxiv.org/html/2406.10995v2)
[22](http://arxiv.org/pdf/2309.01479.pdf)
[23](https://arxiv.org/html/2411.04549v1)
[24](https://www.sciencedirect.com/science/article/abs/pii/S0950705125010317)
[25](https://gwang-kim.github.io/datid_3d/)
[26](https://arxiv.org/abs/2411.11223)
[27](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf)
[28](https://transformer-circuits.pub/2024/scaling-monosemanticity/)
[29](https://github.com/junha1125/Vision-Language-Model-in-ECCV-2024)
