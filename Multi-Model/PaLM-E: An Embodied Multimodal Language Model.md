# PaLM-E: An Embodied Multimodal Language Model

## 1. 핵심 주장 및 주요 기여
**핵심 주장**  
PaLM-E는 대규모 언어 모델(LLM) PaLM에 시각·물리 센서 입력을 직접 주입하여, 단일 모델이 로봇 계획, 시각질문응답(VQA), 이미지 캡셔닝, 언어 추론 등 다양한 작업을 통합적으로 수행할 수 있음을 입증한다.[1]

**주요 기여**  
1. **임베디드 멀티모달 입력**: 이미지, 상태 벡터, 3D 신(scene) 표현(OSRT)을 PaLM의 토큰 임베딩 공간에 삽입.[1]
2. **범용성**: TAMP, 테이블 푸시, 모바일 조작 등 서로 다른 로봇 환경에서 단일 모델로 전이학습 및 제로/원샷 일반화 달성.[1]
3. **전이 학습 효과**: 인터넷 규모의 비전·언어 데이터와 로봇 데이터의 혼합 학습 시 로봇 성능이 크게 향상됨(최대 2배).[1]
4. **스케일링 및 언어 보존**: 모델 크기(12B→84B→562B) 증가 시 멀티모달 학습 후에도 언어 성능 저하(망각) 최소화.[1]

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 향상 및 한계

### 문제 정의  
기존 LLM은 텍스트 기반 추론에는 강력하나, 실제 로봇 제어·계획 문제에서는 시각·물리 센서 정보를 직접 처리할 수 없어 지리적·기하학적 제약을 고려한 장기 계획 수립이 어려움.[1]

### 제안 방법  
- **멀티모달 문장(Multimodal Sentences)**: 텍스트 토큰 사이에 연속 관측값(이미지 임베딩 φ_viT, 상태 임베딩 φ_state, OSRT φ_OSRT)을 삽입.[1]
- **학습 손실**: 디코더-Only LLM의 자동회귀 교차엔트로피 손실을 이용해 prefix 이후 토큰 예측 최적화.[1]
- **프롬프트 예시**:  
  ```
  Obj1 is <obj1>. Obj2 is <obj2>. Given <img>. Q: How to grasp the green object?
  ```
- **수식 요약**:  

$$
    p(w_{1:L}) = \prod_{l=1}^L p_\text{LM}(w_l\mid w_{1:l-1})
  $$  

$$
    x_i = \begin{cases}
      \gamma(w_i), & \text{if } w_i \in \text{텍스트 토큰}\\
      \phi_j(O_j)_i, & \text{if } w_i \text{은 관측값 임베딩}
    \end{cases}
  $$  

### 모델 구조  
- **LLM**: PaLM (8B, 62B, 540B 파라미터)  
- **비전 인코더**: ViT-4B, ViT-22B(사전학습) 및 토큰러너(ViT+TL)  
- **3D 장면 인코더**: OSRT (무감독 슬롯 기반)  
- **엔티티 레이블링**: 객체 토큰(&lt;obj j&gt;)을 통해 계획 단계에서 참조 가능.[1]

### 성능 향상  
- **TAMP 환경**: 전체 데이터 혼합 시 1% 학습 데이터만으로 계획 성공률 31.8%→74.3% 증가.[1]
- **Language-Table**: 10개 샘플만으로 장기 계획 성공률 20%→80% 달성.[1]
- **모바일 조작**: 폐루프 실패 감지 F1 0.62→0.91, 능동 적합도 예측 F1 0.63→0.91.[1]
- **OK-VQA**: PaLM-E-562B는 66.1%로 최첨단 비 finetuned 모델 능가.[1]
- **언어 망각 완화**: PaLM-E-562B는 NLG 성능 3.9%만 손실, PaLM-12B는 87.3% 손실.[1]

### 한계  
- **실시간 저지연 적용**: 생략된 네트워크 최적화로 로봇 제어 주기 내 지연 가능성  
- **고해상도 이미지 처리**: 대규모 ViT 연산 비용  
- **안전성·로버스트니스**: 예측 오류 시 장애 복구 메커니즘 필요  
- **데이터 편향**: 인터넷-스케일 V&L 데이터 편향이 로봇 도메인으로 전이될 위험

## 3. 일반화 성능 향상 관련 핵심 내용
1. **전이 학습(Transfer Learning)**  
   - 비전·언어 인터넷 데이터와 로봇 데이터를 혼합 학습 시, 로봇 도메인 소량 샘플로도 성능 대폭 향상.[1]
2. **제로/원샷 일반화**  
   - 100개 원샷 튜닝 후, 미학습 객체·질문에 대해 제로샷 해결.[1]
3. **모델 크기 스케일링**  
   - LLM 파라미터 수 증가가 멀티모달 일반화 능력 및 언어 망각 억제에 기여.[1]
4. **OSRT 활용**  
   - 3D 객체 중심 표현이 공간 관계 일반화에 효율적, 적은 로봇 데이터로도 높은 성공률 달성.[1]

## 4. 향후 연구 영향 및 고려사항
PaLM-E는 **범용 멀티모달 LLM**을 통해 로봇 제어와 VQA, 캡셔닝, 언어 이해를 단일 모델로 통합하는 가능성을 제시했다.  
- **영향**: 로봇학습, 자율 에이전트 설계, 멀티모달 추론·계획 연구에 기반 제공  
- **고려사항**:  
  1. **실시간 최적화**: 임베딩·추론 속도 개선  
  2. **안전성 검증**: 예측 오차 복구·검증 메커니즘  
  3. **데이터 다양성**: 도메인-특화·대표성 높은 로봇 데이터 증대  
  4. **지속적 학습**: 온라인·생성적 피드백 기반 적응 학습

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/924bd33c-b89e-4387-b7db-94efd7fa4dc2/2303.03378v1.pdf)
