# Language Is Not All You Need: Aligning Perception with Language Models

## **1. 핵심 주장과 주요 기여**

이 논문의 핵심 주장은 **언어만으로는 충분하지 않으며, 인공지능의 진정한 일반화를 위해서는 시각적 인식(perception)을 언어 모델과 결합해야 한다**는 것입니다.[1]

**주요 기여는 다음과 같습니다:**

### **KOSMOS-1 모델 개발**
- **Multimodal Large Language Model (MLLM)**으로서 언어와 시각 정보를 통합 처리[1]
- **Zero-shot** 및 **Few-shot** 학습이 가능한 범용 멀티모달 인터페이스 구현[1]
- **In-context learning**과 **instruction following** 기능을 멀티모달 환경으로 확장[1]

### **Cross-modal Transfer 능력 입증**
- 언어에서 멀티모달로, 멀티모달에서 언어로의 **양방향 지식 전이** 가능성 확인[1]
- 시각적 상식 추론에서 언어 전용 모델 대비 **상당한 성능 향상** 달성[1]

### **새로운 평가 벤치마크 도입**
- **Raven IQ Test** 데이터셋을 통한 **비언어적 추론 능력** 평가 프레임워크 제시[1]

## **2. 해결하고자 하는 문제와 제안 방법**

### **문제 정의**
기존 대규모 언어 모델(LLM)은 **텍스트 기반 작업에서는 뛰어난 성능**을 보이지만, **이미지나 오디오 같은 멀티모달 데이터를 직접 처리할 수 없다**는 근본적 한계가 있습니다.[1]

### **제안 방법**

#### **모델 구조**
**KOSMOS-1은 다음과 같은 아키텍처를 갖습니다:**

1. **Transformer 기반 Causal Language Model**을 백본으로 사용[1]
2. **Vision Encoder**: 사전 훈련된 CLIP ViT-L/14 모델을 이미지 임베딩 생성에 활용[1]
3. **Resampler**: 이미지 임베딩 수를 줄이는 attentive pooling 메커니즘[1]
4. **MAGNETO**: 더 나은 훈련 안정성을 위한 Transformer 변형 사용[1]
5. **XPOS**: 긴 컨텍스트 모델링을 위한 상대적 위치 인코딩[1]

#### **훈련 목적 함수**
**다음 토큰 예측(Next-token prediction)** 방식을 사용하며, **로그 우도를 최대화**하는 것을 목표로 합니다:

$$ \mathcal{L} = -\sum_{t} \log P(x_t | x_{ < t}) $$

여기서 **텍스트 토큰만이 손실 계산에 포함**되며, 이미지 임베딩은 컨텍스트 정보로만 활용됩니다.[1]

#### **입력 표현 방식**
특수 토큰을 사용하여 **멀티모달 입력을 통합 시퀀스로 변환**:
- `<s>` 시퀀스 시작, `</s>` 시퀀스 종료  
- `<image>` 이미지 시작, `</image>` 이미지 종료
- 예시: `<s> paragraph <image> Image Embedding </image> paragraph </s>`[1]

## **3. 모델의 일반화 성능 향상**

### **Cross-modal Transfer Learning**
**KOSMOS-1의 가장 주목할 만한 특징은 모달리티 간 지식 전이 능력입니다:**

#### **언어에서 멀티모달로의 전이**
- **Language-only instruction tuning**이 **멀티모달 작업 성능을 크게 향상**시킴[1]
- VQAv2에서 **4.3점**, VizWiz에서 **1.3점** 성능 향상 달성[1]

#### **멀티모달에서 언어로의 전이**
- **시각적 상식 추론 작업**에서 언어 전용 모델 대비 **현저한 성능 향상**:
  - RELATIVESIZE: **1.5% 향상** (92.7% → 94.2%)[1]
  - MEMORYCOLOR: **14.7% 향상** (61.4% → 76.1%)[1]
  - COLORTERMS: **9.7% 향상** (63.4% → 73.1%)[1]

### **Few-shot Learning 능력**
- **멀티모달 chain-of-thought prompting**을 통해 **5.8점 성능 향상** 달성[1]
- **In-context learning**을 통한 **적응적 학습 능력** 보유[1]

### **Zero-shot 일반화**
- **ImageNet에서 38.1% 정확도** 달성 (제약 조건 하)[1]
- **자연어 설명을 통한 분류**에서 **90.0% 정확도** 달성[1]

## **4. 모델의 한계**

### **성능적 한계**
- **Raven IQ Test에서 26% 정확도**로 성인 평균과는 **여전히 큰 격차** 존재[1]
- **일부 언어 작업에서 언어 전용 모델 대비 약간 낮은 성능** (zero-shot, one-shot 설정)[1]

### **구조적 한계**
- **1.6B 파라미터 모델**로 상대적으로 **작은 규모**[1]
- **CLIP 모델 의존성**: 사전 훈련된 CLIP 인코더에 **강하게 의존**[1]
- **오직 텍스트 토큰만 손실에 포함**되어 **시각적 표현 학습이 간접적**[1]

## **5. 미래 연구에 미치는 영향과 고려사항**

### **연구 분야에 미치는 영향**

#### **새로운 연구 방향 제시**
- **멀티모달 대화형 AI** 개발의 새로운 패러다임 제시[1]
- **OCR-free 문서 이해** 분야의 혁신적 접근법 제안[1]
- **로보틱스 분야**에서의 통합 인터페이스 가능성 시사[1]

#### **평가 방법론 혁신**
- **비언어적 추론 능력 평가**를 위한 새로운 벤치마크 제시[1]
- **Cross-modal transfer** 평가 프레임워크 구축[1]

### **앞으로 연구 시 고려할 점**

#### **모델 확장성**
- **모델 크기 확장** 및 **음성 모달리티 통합** 필요성[1]
- **더욱 효율적인 멀티모달 아키텍처** 개발 요구

#### **훈련 데이터 품질**
- **고품질 멀티모달 데이터셋** 구축의 중요성
- **데이터 필터링 및 전처리** 기법 개선 필요

#### **평가 체계 개선**
- **더욱 포괄적인 멀티모달 벤치마크** 개발 필요
- **실제 응용 환경에서의 성능 평가** 체계 구축

#### **윤리적 고려사항**
- **편향성 문제** 및 **안전성 확보** 방안 마련
- **멀티모달 AI의 사회적 영향** 연구 필요

이 논문은 **언어와 시각을 통합한 AI 시스템의 가능성**을 보여주며, **AGI(Artificial General Intelligence) 달성**을 위한 중요한 이정표를 제시했습니다. 특히 **cross-modal transfer learning**의 효과를 실증적으로 입증함으로써, 향후 멀티모달 AI 연구의 핵심 방향을 제시했다고 평가할 수 있습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/22de4ad1-46c2-466c-a674-7e84f481e04c/2302.14045v2.pdf)
