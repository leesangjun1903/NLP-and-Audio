# OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe

### 1. 핵심 주장과 주요 기여

**OpenMMReasoner**는 멀티모달 대규모 언어 모델(LMRM)의 투명하고 확장 가능한 학습 레시피를 제시하는 논문입니다. 핵심 주장은 **데이터 품질과 학습 설계의 체계적 최적화를 통해 멀티모달 추론 능력을 대폭 향상시킬 수 있다**는 것입니다.[1]

주요 기여는 다음과 같습니다:[1]

**첫째, 데이터 큐레이션 인사이트**: 멀티모달 추론을 위한 고품질 SFT(지도 학습 미세 조정)와 RL(강화 학습) 데이터 구축에 대한 첫 번째 체계적 연구를 제시했습니다. 특히 **질문 다양성뿐 아니라 답변 다양성**이 데이터 품질의 중요한 축임을 발견했습니다.

**둘째, 강력한 SFT 레시피**: 단계별 검증과 함께 874,000개 샘플 규모의 콜드스타트 데이터셋을 개발했습니다. 이는 교사 모델 선택, 답변 다양성 스케일링, 도메인 혼합을 통해 구성되었습니다.

**셋째, 고급 RL 레시피**: GSPO, GRPO, DAPO 등 여러 RL 알고리즘을 비교 분석하여 가장 적합한 알고리즘을 선택했습니다. 74,000개 샘플 규모의 RL 데이터셋과 함께 안정적인 RL 파이프라인을 구축했습니다.[1]

결과적으로 Qwen2.5-VL-7B 기준 **11.6% 성능 향상**을 달성했으며, 9개의 멀티모달 추론 벤치마크에서 최첨단 방법들을 지속적으로 능가했습니다.[1]

***

### 2. 해결 문제, 제안 방법, 모델 구조

#### 2.1 해결하는 핵심 문제

기존 멀티모달 추론 연구의 주요 한계는 **투명성 부족**입니다. 많은 연구가 SFT와 RL 단계를 제시하지만, 데이터 큐레이션 과정의 세부사항이나 포괄적인 소거 분석(ablation analysis)을 제공하지 않습니다. 이로 인해 재현성이 제한되고, 멀티모달 추론 시스템이 실제로 어떻게 구축되는지에 대한 이해가 부족했습니다. OpenMMReasoner는 이 투명성 격차를 해소하면서 **확장 가능하고 통합된 SFT-RL 레시피**를 제공합니다.[1]

#### 2.2 제안 방법: 두 단계 학습 레시피

**2.2.1 SFT(지도 학습 미세 조정) 단계**

SFT 단계는 세 가지 주요 단계로 구성됩니다:

**데이터 소싱 및 포매팅**: 약 103,000개의 원본 질문-답변 쌍을 LLaVA-CoT, OpenVLThinker, WeMath2.0 등 공개 데이터셋에서 수집합니다. 모든 샘플은 일관된 추론 형식으로 정규화됩니다.[1]

**데이터 증류 및 스케일링**: 강력한 교사 모델(Qwen3-VL-235B-Instruct)을 선택하여 각 질문에 대해 검증된 답변 추적을 생성합니다. 표 2에 따르면, 더 강력한 교사 모델이 평균 벤치마크에서 최소 4.5점의 성능 향상을 제공합니다.[1]

중요한 발견은 **답변 다양성의 중요성**입니다. 표 3에 따르면, 검증된 답변의 개수를 1개에서 8개로 증가시키면 평균 벤치마크 성능이 50.5에서 55.2로 향상됩니다:[1]

| 샘플 전략 | 평균 성능 | MathVision | MathVerse | MathVista |
|---------|---------|-----------|----------|----------|
| 1× 샘플링 | 50.5 | 25.5 | 41.1 | 69.2 |
| 2× 샘플링 | 52.7 | 30.8 | 54.4 | 72.9 |
| 4× 샘플링 | 52.9 | 30.8 | 55.3 | 72.6 |
| 8× 샘플링 | 55.2 | 34.6 | 57.1 | 73.7 |

**길이 및 난이도 필터링 분석**: 표 4에 따르면, 과도한 필터링은 답변 다양성 감소로 인해 성능을 저하시킵니다. 따라서 최종 레시피에서는 필터링 정책을 채택하지 않았습니다.[1]

**도메인 혼합**: 일반적인 추론 데이터에 수학 특화 데이터(MMR1, MiroMind-M1)를 추가합니다. 표 5에 따르면 이러한 도메인 혼합은 전체 성능을 55.2에서 56.3으로 향상시킵니다.[1]

최종 SFT 데이터셋은 세 단계를 거쳐 진화합니다: 103k 원본 질문 → 583k 검증된 일반 추론 샘플 → 874k 혼합 SFT 레시피.[1]

**2.2.2 RL(강화 학습) 단계**

RL 단계는 SFT에서 구축된 강력한 추론 기초를 추가로 개선합니다.[1]

**RL 알고리즘 비교**: 세 가지 알고리즘을 평가합니다:

- **GRPO (Group Relative Policy Optimization)**의 목적 함수:[1]

$$J_{GRPO} = E_{q,a \sim D, o_i \sim G} \left[ \frac{1}{G} \sum_{i=1}^G \min\left(\frac{r_{i,t} A_{i,t}}{\text{clip}(r_{i,t}, 1-\epsilon, 1+\epsilon) A_{i,t}}\right) \right]$$

여기서 $r_{i,t}$는 확률 비율, $A_{i,t}$는 정규화된 이점입니다.

- **DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization)**의 목적 함수:[1]

$$J_{DAPO} = E_{q,a \sim D, o_i \sim G} \left[ \frac{1}{G} \sum_{i=1}^G \min\left(r_{i,t} A_{i,t}, \text{clip}(r_{i,t}, \epsilon_{low}, \epsilon_{high}) A_{i,t}\right) \right]$$

단계별 손실 대신 샘플 수준의 손실을 사용하여 엔트로피 붕괴를 해결합니다.

- **GSPO (Group Sequence Policy Optimization)**의 목적 함수:[1]

$$J_{GSPO} = E_{q,a \sim D, o_i \sim G} \left[ \frac{1}{G} \sum_{i=1}^G \min\left(s_i A_{i,t}, \text{clip}(s_i, 1-\epsilon, 1+\epsilon) A_{i,t}\right) \right]$$

여기서 $s_i$는 시퀀스 레벨 중요성 비율입니다.

표 7에 따르면 **GSPO가 가장 높은 안정성, 탐색 능력, 전체 효율을 보입니다**. 더 빠른 수렴, 더 높은 보상, 더 안정적인 동작을 달성합니다.[1]

**보상 함수**: 작업 정확성과 출력 포매팅을 균형있게 조합합니다:[1]

$$R = (1 - \alpha_{fmt}) R_{acc} + \alpha_{fmt} R_{fmt}$$

여기서 $R_{acc}$는 작업 목적의 정확성, $R_{fmt}$는 답변 형식의 일관성을 측정하며, $\alpha_{fmt} = 0.1$입니다.

**2.2.3 추론 효율성 고려**

OpenVisionReasoner(OVR)와 비교하여, 과도한 추론 길이를 줄이는 것이 중요함을 발견했습니다. DAPO의 과다 길이 페널티 전략을 채택하여 추론 효율성과 성능 사이의 균형을 맞춥니다. 그림 6에 따르면 OpenMMReasoner는 OVR보다 훨씬 적은 토큰을 사용하면서 우수한 정확도를 달성합니다.[1]

***

### 3. 일반화 성능 향상

#### 3.1 데이터 다양성의 역할

**답변 다양성의 중요성**: 표 3은 동일한 질문 소스를 사용하면서 검증된 답변의 개수를 증가시키면 일관된 성능 향상을 보입니다. 이는 **다양한 해결 경로에 대한 노출이 모델의 추론 능력을 강화**함을 시사합니다.[1]

**도메인 혼합의 효과**: 표 5에 따르면 이미지 기반 수학 추론(ImgMath), 텍스트 기반 수학 추론(TxtMath), 그리고 둘 다를 혼합하면 성능이 지속적으로 향상됩니다. 특히 이미지-텍스트 혼합은 평균 성능을 56.3으로 증가시킵니다.[1]

#### 3.2 교사 모델 선택의 역할

표 2에 따르면 교사 모델 선택이 중요합니다. 더 강력한 교사 모델(Qwen3-VL-235B-Instruct)을 선택하면 데이터 효율성을 유지하면서 더 높은 품질의 감독을 제공합니다.[1]

#### 3.3 텍스트-멀티모달 추론 전이

흥미로운 발견은 **멀티모달 RL 훈련이 텍스트 기반 추론 능력도 향상시킨다**는 것입니다. 그림 5에 따르면 RL 훈련 중 AIME24, AIME25, AMC23과 같은 순수 텍스트 벤치마크의 검증 성능이 지속적으로 증가합니다.[1]

표 8에 따르면 텍스트 추론 능력이 시각 추론과 함께 개선됩니다:[1]

| 모델 | AIME24 | AIME25 | GPQA Diamond |
|------|--------|--------|--------------|
| 베이스라인 | 6.7 | 6.7 | 31.8 |
| ColdStart | 16.7 | 14.6 | 35.4 |
| RL | 27.1 | 22.1 | 38.9 |

이는 **추론 능력이 멀티모달 데이터를 통해 학습될 때 도메인을 초월하여 전이된다**는 것을 나타냅니다.

#### 3.4 안정적인 RL 훈련의 핵심 요소

그림 7에 따르면 **롤아웃 개수와 롤아웃 온도**가 RL 훈련 안정성에 매우 중요합니다:[1]

- 더 높은 롤아웃 온도(예: 1.4)는 상당한 불안정성을 야기합니다.
- 16개 롤아웃 구성이 8개보다 일관되게 더 높은 보상과 더 부드러운 동역학을 생성합니다.
- 특히 DAPO에서 8개 설정은 심각한 후기 불안정성을 보입니다.

***

### 4. 성능 결과

표 6은 다양한 벤치마크에서의 포괄적 평가 결과를 보여줍니다. OpenMMReasoner(OMR-7B)는 대부분의 기준선을 능가합니다:[1]

| 모델 | MathVista | MathVision | WeMath | MMMU | LogicVista |
|------|----------|-----------|--------|------|-----------|
| Qwen2.5-VL-7B | 69.2 | 25.5 | 47.9 | 51.8 | 37.9 |
| OVR-7B | 72.1 | 51.8 | 54.6 | 54.8 | 50.2 |
| OMR-7B (SFT) | 74.8 | 36.6 | 47.9 | 54.4 | 39.3 |
| OMR-7B (RL) | **79.5** | **43.6** | **57.8** | **57.8** | **44.1** |

SFT 단계만으로도 OVR을 능가하며, RL 단계를 거쳐 추가적 개선을 달성합니다.

***

### 5. 한계 및 제약사항

논문의 한계점들은 다음과 같습니다:[1]

**첫째, 단일 모델 가족 중심**: 연구는 주로 Qwen2.5-VL-Instruct에 초점을 맞추고 있으며, 다른 모델 아키텍처에 대한 평가가 제한적입니다.

**둘째, 이미지 도메인 중심**: 비디오, 오디오, 시간적 스트림 등 다른 모달리티에 대한 확장이 제한적입니다.

**셋째, 스케일링 한계의 미개척**: 추가 스케일링 하에서의 모델 성능 상한선이 아직 식별되지 않았습니다.

**넷째, 과다 길이 문제**: 추론 단계가 과도하게 길어질 수 있는 경향이 있습니다.

***

### 6. 앞으로의 연구 방향 및 영향

#### 6.1 이 연구가 미치는 영향

OpenMMReasoner는 **멀티모달 추론 연구를 위한 투명한 기준점(benchmark)**을 제시합니다. 다음과 같은 측면에서 중요한 영향을 미칩니다:[1]

**데이터 품질 중심 패러다임**: 단순 규모 증대보다 **데이터 다양성(질문 다양성과 답변 다양성)**이 더 중요함을 입증했습니다. 이는 향후 멀티모달 모델 개발에서 데이터 큐레이션의 중요성을 강조합니다.

**재현성 강화**: 모든 코드, 파이프라인, 데이터를 공개함으로써 학계의 재현성을 크게 향상시킵니다.

**알고리즘 비교 분석**: GSPO가 다른 RL 알고리즘보다 우수함을 체계적으로 입증하여 향후 멀티모달 RL 연구를 안내합니다.

#### 6.2 최신 관련 연구 탐색

최근 멀티모달 추론 분야의 진화를 살펴보면:

**1. RL 기반 접근법의 다양화**

MoDoMoDo (Multi-Domain Data Mixtures for Multimodal LLM RL)는 여러 데이터셋에서 최적의 혼합 전략을 학습하여 MLLM의 일반 추론 능력을 향상시킵니다. OpenMMReasoner의 데이터 혼합 아이디어를 확장하여, **데이터 혼합 예측 및 최적화 전략**을 제시합니다.[2]

ViGoRL (Visually Grounded Reinforcement Learning)은 각 추론 단계를 특정 시각 좌표로 고정시키는 새로운 패러다임을 제시합니다. 이는 OpenMMReasoner의 단순 보상 함수를 **공간적으로 그라운드된 감독**으로 확장할 가능성을 시사합니다.[3]

**2. 멀티모달 추론의 도메인 특화**

MuCR (Multimodal Causal Reasoning Benchmark)는 멀티모달 인과 추론 벤치마크를 도입하여, OpenMMReasoner가 다루지 않은 **도메인별 추론 능력 평가**의 필요성을 제기합니다. 특히 멀티모달 데이터에서 시각 단서를 효과적으로 식별하는 것이 교차 모달 일반화의 핵심임을 강조합니다.[4]

**3. 안정성과 일반화 개선**

MBPO (Modality-Balancing Preference Optimization)는 모달리티 불균형 문제를 해결하기 위해 **적대적 음성 마이닝**을 사용합니다. OpenMMReasoner의 보상 함수 설계 원칙을 발전시켜, LLM 편향에 의해 유도된 거부된 응답을 생성합니다.[5]

ViSurf (Visual Supervised-and-Reinforcement Fine-Tuning)는 SFT와 RLVR을 **단일 단계로 통합**하는 새로운 패러다임을 제시합니다. 이는 OpenMMReasoner의 2단계 접근법보다 효율적일 가능성을 제시하며, 다음 단계 연구 방향을 제시합니다.[6]

**4. 멀티모달 추론의 체계적 이해**

MARS2 2025 Challenge는 멀티모달 추론에서 **여러 추론 작업 간의 시너지 효과**를 평가하는 새로운 도전 과제를 제시합니다. OpenMMReasoner가 달성한 도메인 혼합의 이점을 더 체계적으로 평가하려는 노력입니다.[7]

최근 종합 리뷰 논문은 LLM 추론의 시간 스케일링, 지도 학습 미세 조정, 증류 등 다양한 방법론을 분석하여, OpenMMReasoner의 2단계 레시피가 **일반적인 추론 향상 패턴 중 하나**임을 보여줍니다.[8]

#### 6.3 향후 연구 시 고려할 점

**1. 알고리즘 혁신**: GSPO 외에도 더 안정적인 RL 알고리즘 개발이 필요합니다. 특히 **엔트로피 붕괴** 문제를 근본적으로 해결하는 알고리즘이 중요합니다.

**2. 멀티모달 확장**: 비디오, 오디오, 시간적 정보를 포함한 **풍부한 모달리티 통합**이 필수적입니다. 이는 더 복잡한 추론 과제를 가능하게 합니다.

**3. 효율성-성능 트레이드오프**: OpenMMReasoner의 토큰 효율성 개선이 중요하지만, 더 근본적으로 **계산 효율성과 성능의 파레토 프론티어**를 탐색해야 합니다.

**4. 도메인 특화 vs 일반화**: MuCR과 MARS2 같은 도메인별 평가를 통해, OpenMMReasoner의 일반화 능력이 **도메인별 특화에 얼마나 영향을 받는지** 파악해야 합니다.

**5. 데이터 선택 최적화**: 현재 균등한 도메인 혼합 외에도, **동적 데이터 선택 및 커리큘럼 학습** 전략을 포함한 더 정교한 데이터 큐레이션이 필요합니다.

**6. 평가 메트릭 고도화**: 현재의 정확도 중심 평가를 넘어, **추론 과정의 해석가능성, 오류 분석, 실패 케이스의 특성화** 등을 포함한 다차원적 평가가 필요합니다.

**7. 교사-학생 상호작용**: 표 2의 결과를 기반으로, 다양한 강점의 교사 모델을 혼합하거나 **동적으로 선택하는 전략**이 데이터 효율성을 더욱 개선할 수 있습니다.

***

### 결론

OpenMMReasoner는 멀티모달 추론 연구에 **투명성, 재현성, 체계성**을 가져온 중요한 기여입니다. 답변 다양성의 중요성, 효과적인 교사 모델 선택, GSPO의 우수성, 도메인 혼합의 효과 등 실질적 인사이트를 제공합니다. 특히 데이터 품질 중심의 패러다임과 SFT-RL 2단계 레시피는 향후 멀티모달 모델 개발의 기준점이 될 것입니다. 

그러나 단일 모델 가족, 이미지 중심, 스케일링 한계 등의 제약이 있으며, 최근 연구들(MoDoMoDo, ViGoRL, MBPO, ViSurf 등)은 이를 보완하기 위한 다양한 방향을 제시하고 있습니다. 향후 연구는 효율성-성능 트레이드오프의 최적화, 다양한 모달리티 통합, 도메인 특화성과 일반화의 균형, 그리고 더 정교한 알고리즘 개발에 집중해야 할 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ddc863c6-db19-4b0b-8fea-ff6577313c94/2511.16334v1.pdf)
[2](https://arxiv.org/abs/2509.24385)
[3](https://arxiv.org/abs/2509.21100)
[4](https://arxiv.org/abs/2505.24871)
[5](https://ieeexplore.ieee.org/document/11097694/)
[6](https://ieeexplore.ieee.org/document/11125732/)
[7](https://ieeexplore.ieee.org/document/11094475/)
[8](https://arxiv.org/abs/2505.19510)
[9](https://arxiv.org/abs/2505.11926)
[10](https://arxiv.org/abs/2507.13152)
[11](https://arxiv.org/abs/2506.08022)
[12](https://arxiv.org/html/2503.22732v1)
[13](https://arxiv.org/pdf/2309.05519.pdf)
[14](https://arxiv.org/pdf/2411.14432.pdf)
[15](https://arxiv.org/html/2402.05889)
[16](http://arxiv.org/pdf/2402.06599.pdf)
[17](https://arxiv.org/pdf/2305.14628.pdf)
[18](https://arxiv.org/pdf/2401.06805.pdf)
[19](http://arxiv.org/pdf/2502.02871.pdf)
[20](https://arxiv.org/html/2509.14142v1)
[21](https://arxiv.org/abs/2507.20766)
[22](https://arxiv.org/html/2511.16334v1)
[23](https://arxiv.org/pdf/2503.18071.pdf)
[24](https://openreview.net/forum?id=4OsgYD7em5)
[25](https://huggingface.co/papers/2511.16334)
[26](https://aclanthology.org/2025.findings-acl.288.pdf)
[27](https://huggingface.co/papers/2510.10606)
[28](https://arxiv.org/html/2509.06948v1)
[29](https://openaccess.thecvf.com/content/ICCV2025W/MARS2/papers/Xu_MARS2_2025_Challenge_on_Multimodal_Reasoning_Datasets_Methods_Results_Discussion_ICCVW_2025_paper.pdf)
[30](https://openreview.net/forum?id=psJiUopUt7)
[31](https://visually-grounded-rl.github.io)
[32](https://www.themoonlight.io/en/review/reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example)
