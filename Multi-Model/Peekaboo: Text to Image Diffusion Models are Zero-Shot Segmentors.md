# Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors

## 1. 핵심 주장과 주요 기여 (Core Claims and Key Contributions)

### 1.1 핵심 주장
**Peekaboo**는 텍스트-이미지 생성을 위해 사전학습된 확산 모델(Stable Diffusion)이 명시적인 분할 정보 없이도 뛰어난 **픽셀 수준 위치 정보(pixel-level localization information)**를 내재적으로 포함하고 있다는 것을 보여준다. 이러한 숨겨진 지식을 추론 시간 최적화를 통해 추출하면, 별도의 학습 없이도 **영어 텍스트 프롬프트로 이미지의 특정 영역을 분할할 수 있다**는 것이 핵심 주장이다.

### 1.2 주요 기여
1. **새로운 비감독 분할 메커니즘**: 의미론적 분할(semantic segmentation)과 지칭 분할(referring segmentation) 모두에 적용 가능한 최초의 무학습 기법
2. **사전학습 확산 모델에 내재된 위치 정보 입증**: Stable Diffusion 같은 생성 모델이 학습 과정 중 위치 정보가 없음에도 불구하고 강력한 공간적 이해력 보유
3. **기초 모델로서의 확산 모델 활용**: 무학습 분할을 위한 오프더셀프(off-the-shelf) 기초 모델 활용 방법론 제시
4. **RGBA 이미지 생성의 선도**: RGB 이미지로만 학습된 확산 모델로부터 투명도 채널이 있는 이미지 생성

---

## 2. 해결하고자 하는 문제

### 2.1 배경 문제
- **높은 주석 비용**: 의미론적 분할은 픽셀 수준의 정확한 레이블 필요로 막대한 수작업 주석 비용 발생
- **고정된 범주의 한계**: 기존 지도 학습 기반 방법은 사전 정의된 클래스로만 분할 가능
- **자연어의 유연성 부재**: 이전 비감독 방법들은 복잡한 자연어 프롬프트를 처리하지 못함
- **계산 비용**: 최근 확산 모델 기반 분할 방법이 있지만 5.3일 × 32개 V100 GPU와 같은 막대한 학습 비용 필요

### 2.2 구체적 문제점
표 1에서 보여지듯이:
- **GroupViT** (그룹 비전 트랜스포머): 학습 필요, RefCOCO에서 mIoU 0.112로 낮은 성능
- **Clippy**: 복잡한 자연어 프롬프트에 약함
- 기존 무학습 방법들이 개방형 어휘(open-vocabulary) 지칭 분할에서 실패

### 2.3 근본적 질문
> **"사전학습된 텍스트-이미지 확산 모델이 분할 정보에 노출되지 않았는데, 자연언어로 표현된 의미 있는 공간 영역을 이해하고 그라운딩할 수 있는가?"**

---

## 3. 제안하는 방법론

### 3.1 기본 개념 및 직관

Peekaboo의 핵심 아이디어:
- **입력**: 이미지 $\mathbf{x}$, 텍스트 프롬프트 $p$, 랜덤 배경색 $\mathbf{b}$
- **목표**: 학습 가능한 알파 마스크 $\alpha$를 최적화하여 텍스트 프롬프트와 일치하는 영역을 찾기

### 3.2 알파 블렌딩 (Alpha Compositing)

합성 이미지는 다음과 같이 생성:

$$\hat{\mathbf{x}} = \alpha \odot \mathbf{x} + (1 - \alpha) \odot \mathbf{b}$$

여기서:
- $\alpha$: 학습 가능한 알파 마스크 ($\in [0, 1]$)
- $\mathbf{x}$: 원본 이미지
- $\mathbf{b}$: 랜덤 RGB 색상의 배경
- $\odot$: 원소 단위 곱셈(element-wise multiplication)

**직관**: 특정 픽셀이 알파 값이 높으면 원본 이미지를 더 보여주고, 낮으면 배경을 더 보여준다. 텍스트 프롬프트와 관련 있는 영역일수록 합성 이미지에서 중요하므로, 확산 모델의 기울기가 그 영역에서 강할 것이다.

### 3.3 잠재 점수 증류 손실 (Latent Score Distillation Loss)

**Score Distillation Sampling (SDS)** 원리 적용. 잠재 공간에서 작동하는 버전:

$$L_s = \text{MSE}\left(\epsilon, D\left(\tilde{\mathbf{z}}, T(p)\right)\right)$$

여기서:
- $\epsilon \sim \mathcal{N}(0, 1)$: 가우시안 노이즈
- $\mathbf{z}$: VAE 인코더로 인코딩된 합성 이미지의 잠재 벡터
- $\tilde{\mathbf{z}}$: 확산 과정을 통해 노이즈가 추가된 $\mathbf{z}$
- $D$: 확산 U-Net 네트워크
- $T(p)$: 텍스트 프롬프트 $p$의 임베딩
- $\text{MSE}$: 평균 제곱 오차

**의미**: 확산 모델이 예측한 노이즈와 실제 노이즈의 차이를 최소화. 합성 이미지가 텍스트 조건부 분포와 잘 맞을수록 예측 노이즈가 작아짐.

### 3.4 알파 정규화 손실 (Alpha Regularization Loss)

$$L_\alpha = \sum_{i} \alpha_i$$

**목적**: 최소 알파 마스크 강제. 모든 픽셀을 포함하는 trivial 해결책 방지.

### 3.5 총 손실 함수 (Peekaboo Loss)

$$L_p = L_s + \lambda_\alpha L_\alpha$$

여기서 $\lambda_\alpha = 0.05$ (정규화 계수)

### 3.6 알파 마스크 파라미터화

#### 3.6.1 래스터 양측 필터링 (Raster Bilateral Parametrization) - 주요 방법

1. **초기화**: 학습 가능한 행렬 초기화
2. **양측 필터 적용**: 
   - 이미지의 색상 정보와 공간 거리를 고려한 비선형 필터
   - 색상이 비슷하고 공간적으로 가까운 픽셀에 높은 가중치 부여
   - 이미지 경계 보존
   
3. **시그모이드 클리핑**: 출력을 [0, 1] 범위로 정규화

```math
\alpha_{\text{final}} = \sigma(\text{bilateral\_blur}(\alpha_{\text{learnable}}))
```

#### 3.6.2 다른 파라미터화 방법들

- **Raster (베이스라인)**: 양측 필터 없음, 성능 저하
- **Fourier**: 푸리에 특성 네트워크 사용, 때로 환각 문제 발생
- **Fourier Bilateral**: 푸리에 + 양측 필터, 개선된 성능
- **Depth Bilateral**: MIDAS 깊이 맵으로 안내, 최고 성능 (mIoU 0.551)

### 3.7 최적화 프로세스 (Algorithm 1)

```
1. 학습 가능한 알파 마스크 초기화: α ~ N(0,1)
2. SGD 옵티마이저 설정 (learning rate: 1e-5)
3. 200 반복에 대해:
   - 합성 이미지 생성: x̂ = α⊙x + (1-α)⊙b
   - 손실 계산: L_p = L_s + L_α
   - 역전파: α에 대한 기울기 계산
   - 옵티마이저 스텝 실행
4. 수렴한 α 반환
```

---

## 4. 모델 구조

### 4.1 전체 아키텍처

```
입력 이미지 x
        ↓
[시각 인코더 (VAE)]
        ↓
잠재 벡터 z
        ↓
[확산 노이즈 추가]
        ↓
노이즈 잠재 벡터 z̃
        ↓
─────────────────────────────────────
        ↓                          ↓
[확산 U-Net] ←[텍스트 임베더]
    (고정)         (고정)
        ↓
[예측 노이즈]
        ↓
[손실 계산] ← [학습 가능한 알파 마스크]
        ↓
[기울기 기반 최적화]
        ↓
최종 알파 마스크 α
```

### 4.2 핵심 컴포넌트

| 컴포넌트 | 역할 | 상태 |
|---------|------|------|
| Stable Diffusion | 텍스트 기반 이미지 생성 및 조건부 예측 | 고정 (Frozen) |
| VAE 인코더 | 이미지를 잠재 공간으로 압축 | 고정 |
| 텍스트 인코더 (CLIP) | 자연어를 의미론적 임베딩으로 변환 | 고정 |
| 학습 가능한 알파 마스크 | 분할 영역을 나타내는 행렬 | **학습 가능** |
| 양측 필터 | 이미지 경계 보존하며 마스크 평활화 | 고정 (이미지 가이드) |

### 4.3 구조적 혁신

**역방향 작동**: 
- 전통적인 이미지 생성 확산 모델의 역 방향으로 활용
- 노이즈 예측 기울기를 역으로 사용하여 마스크 최적화
- DreamFusion의 점수 증류 개념을 픽셀 분할에 적용

---

## 5. 성능 향상 및 한계

### 5.1 정량적 성능 결과

#### 5.1.1 지칭 분할 (Referring Segmentation on RefCOCO)

| 방법 | Prec@0.2 | Prec@0.4 | Prec@0.6 | Prec@0.8 | mIoU |
|------|----------|----------|----------|----------|------|
| Random (베이스라인) | 0.141 | 0.022 | 0.003 | 0.000 | 0.102 |
| GroupViT | 0.212 | 0.075 | 0.020 | 0.002 | 0.112 |
| LSeg (준지도) | 0.512 | 0.212 | 0.051 | 0.008 | 0.235 |
| **Peekaboo (RGB Bilateral)** | **0.318** | **0.099** | **0.018** | **0.002** | **0.163** |
| **Peekaboo (Depth Bilateral)** | **0.359** | **0.135** | **0.037** | **0.003** | **0.204** |

**성과**: 
- GroupViT보다 높은 성능 (학습 불필요)
- LSeg 대비 84% 수준의 성능으로 학습 없이도 경쟁력 있는 결과 달성
- **비용**: 0일의 학습 시간, 0 GPU 필요

#### 5.1.2 의미론적 분할 (Semantic Segmentation on VOC-C)

| 방법 | Prec@0.2 | Prec@0.4 | Prec@0.6 | Prec@0.8 | mIoU |
|------|----------|----------|----------|----------|------|
| Random | 0.670 | 0.198 | 0.032 | 0.012 | 0.281 |
| Clippy | 0.757 | 0.459 | 0.263 | 0.049 | 0.539 |
| GroupViT | 0.862 | 0.778 | 0.602 | 0.205 | 0.578 |
| LSeg | 0.952 | 0.897 | 0.758 | 0.311 | 0.678 |
| **Peekaboo (RGB Bilateral)** | **0.892** | **0.709** | **0.331** | **0.130** | **0.520** |
| **Peekaboo (Depth Bilateral)** | **0.929** | **0.707** | **0.455** | **0.187** | **0.551** |

**성과**:
- Clippy 능가 (Depth Bilateral 변형)
- LSeg 대비 81% 수준의 성능 달성
- 간단한 단일 단어 프롬프트에서는 더 높은 성능

### 5.2 정성적 강점

1. **개방형 어휘 능력**: "Emma Watson", "Harry Styles", "Darth Vader" 등 문화 참고사항 이해 가능
2. **입도 조절**: 같은 객체에 대해 다양한 수준의 세분화 가능
   - "man" → 전체 인간
   - "man with blonde hair in blue shirt" → 세부 영역
3. **복합 프롬프트 처리**: "giant metal creature with shiny red eyes", "bartender at center in gray shirt and blue jeans"
4. **실시간 응용**: 로봇 공학 등에서 자연언어로 임의 객체 상호작용 가능

### 5.3 심각한 한계

#### 5.3.1 할루시네이션 (Hallucination)
- **문제**: 배경 텍스처로부터 객체 모양을 환각
- **원인**: 확산 모델의 강한 선행 지식(prior)
- **예시**: 
  - 이미지에 없는 팔까지 분할
  - 배경의 패턴으로 객체 모양 생성
- **빈도**: 단순한 단일 단어 프롬프트에서 더 자주 발생

#### 5.3.2 불필요한 부분 포함
- **문제**: 관심 영역 외의 부분 오포함
- **예시**: 칼 분할 시 빵 슬라이스를 손잡이로 사용
- **원인**: 텍스트 조건부 분포와 이미지 내용의 불일치

#### 5.3.3 수렴 실패
- 일부 경우 알파 마스크가 제대로 수렴하지 않음
- 객체의 외형 부분의 윤곽을 놓치고 유지

#### 5.3.4 중심 편향 (Center Bias)
- 생성된 객체가 주로 이미지 중앙에 위치하는 경향
- Stable Diffusion의 학습 데이터(LAION-5B)의 편향 반영
- 비중심 객체 분할 시 성능 저하

#### 5.3.5 근본적 한계
> **"본 방법은 거대한 사전학습 확산 모델에 의존한다. 일반적인 학술 환경에서 이러한 모델 학습의 자원 제약으로 인해, Peekaboo를 기반으로 한 향후 연구는 공개적으로 이용 가능한 확산 모델에 의존해야 한다."**

**추가 문제점**:
- 확산 모델의 모든 편향과 결함이 전이됨
- 인터넷 규모 데이터셋의 문제점 상속

---

## 6. 모델의 일반화 성능 향상 가능성

### 6.1 현재 일반화 능력

#### 6.1.1 데이터셋 간 전이 (Cross-Dataset Transfer)
- **RefCOCO 학습 없음**: 새로운 스타일의 지칭 표현 처리
- **VOC 클래스 외 추론**: 사전 정의된 20개 클래스 외의 객체 분할 가능
- **하이브리드 프롬프트**: "man with blonde hair in blue shirt and brown pants" 같은 복합 표현

#### 6.1.2 도메인 적응성 (Domain Adaptation)
- **합성 이미지**: 생성 AI 이미지에서도 작동
- **실제 사진**: 카메라로 촬영한 다양한 조건의 사진 처리
- **미시적 이미지**: 팬픽션 캐릭터, 가상 객체 분할 가능

### 6.2 일반화 향상의 근거

#### 6.2.1 스케일 우위
- **학습 데이터**: LAION-5B (50억 개 이미지-텍스트 쌍)
- **모델 크기**: 거대 비전-언어 모델 (billions of parameters)
- **범위**: 자연언어와 시각 개념의 광범위한 커버리지

#### 6.2.2 텍스트 조건부 분포의 강력함
- Stable Diffusion이 "X의 이미지"라는 표현을 본다면, 해당 객체 X와 관련된 시각적 특징들이 자동으로 포함됨
- 이 과정에서 위치 정보도 자연스럽게 학습됨

#### 6.2.3 비감독 학습의 이점
- **학습 편향 없음**: 특정 데이터셋의 주석 편향 미포함
- **오버피팅 위험 낮음**: 새로운 도메인에 적응하기 용이
- **분포 시프트 견디기**: 사전학습 모델의 강력한 일반화 능력

### 6.3 향상 가능성의 경로

#### 6.3.1 파라미터화 개선 (이미 입증됨)
```
Raster (mIoU 0.340) 
  ↓
Fourier (mIoU 0.454)
  ↓
Bilateral Fourier (mIoU 0.470)
  ↓
RGB Bilateral (mIoU 0.520)
  ↓
Depth Bilateral (mIoU 0.551) [최고 성능]
```

**교훈**: 이미지 구조를 활용한 파라미터화가 성능을 11-27% 향상

#### 6.3.2 확산 모델 선택 (예상 가능성)
- 더 큰 확산 모델 사용 → 더 정확한 위치 정보
- 다중 모달 모델 (vision-language 특화) → 더 나은 의미 이해
- 미세조정된 확산 모델 (특정 도메인) → 도메인 특화 성능

#### 6.3.3 손실 함수 설계 (확장 가능성)
현재 손실: $L_p = L_s + \lambda_\alpha L_\alpha$

**개선 방향**:
- 연경계 보존 정규화 추가
- 다중 스케일 손실 (coarse-to-fine)
- 형태 선행(shape prior) 포함
- 적응형 정규화 계수

#### 6.3.4 사전 처리 및 사후 처리
- **CRF (조건부 무작위장) 정제**: 경계 정확성 향상
- **모프로지컬 연산**: 작은 노이즈 제거
- **다중 마스크 앙상블**: 여러 무작위 초기화로 강건성 증가

### 6.4 도메인 특화 일반화

#### 6.4.1 의료 영상
- **가능성**: 의료 확산 모델 (RadiologyGPT 등) 사용 시 가능
- **이점**: 해부학적 정확성 향상
- **도전**: 모델 접근성 제한

#### 6.4.2 자율주행
- **응용**: 도로, 보행자, 신호등 실시간 분할
- **확산 모델**: 자동주행 생성 모델 (BEV Diffusion 등) 활용 가능
- **일반화**: 날씨, 시간대, 도시 변화에 강건

#### 6.4.3 로봇 비전
- **강점**: "빨간색 물체 집기", "나를 따라가는 사람" 같은 복잡한 지시
- **일반화**: 새로운 환경에서 자연언어 지시 이해
- **성능**: 학습 없이도 새 객체/장면 대응

### 6.5 미래 개선을 위한 벤치마크 제안

| 평가 지표 | 현황 (Peekaboo) | 개선 가능 |
|----------|-----------------|----------|
| RefCOCO mIoU | 0.204 | → 0.35+ |
| VOC-C mIoU | 0.551 | → 0.70+ |
| 도메인 외 일반화 | 제한적 | → 강화 필요 |
| 환각 오류율 | ~15-20% | → <5% |
| 추론 속도 | 200 iter (느림) | → 가속화 |
| 계산 요구사항 | 단일 GPU 필요 | → CPU 가능 |

---

## 7. 논문이 미치는 영향과 향후 연구 고려사항

### 7.1 학문적 영향

#### 7.1.1 개념적 기여
1. **"숨겨진 분할자로서의 생성 모델"** 패러다임 확립
   - 확산 모델의 새로운 용도 발견
   - 생성과 인식의 연결 고리 제시

2. **무학습 접근의 가능성 입증**
   - 기존: 모든 다운스트림 태스크는 학습 필요
   - Peekaboo: 사전학습 모델만으로도 복잡한 분할 가능

3. **추론 시간 최적화의 효과성**
   - DreamFusion 개념의 새로운 적용 영역
   - 미분 가능한 함수로 표현 가능한 문제라면 확산 기울기 활용 가능

#### 7.1.2 기술적 영향
1. **Score Distillation Sampling의 일반화**
   - 3D 생성 (DreamFusion) → 2D 분할 (Peekaboo)
   - 향후 다양한 시각 작업 적용 가능성

2. **양측 필터의 창의적 활용**
   - 신경망 파라미터화 시 이미지 구조 통합
   - 제약 조건 없는 최적화에도 구조적 선행 적용 가능

3. **개방형 어휘 시스템의 구축**
   - 무한한 텍스트 표현 → 임의의 객체 분할
   - CLIP 기반 방법 대비 더 정교한 공간 이해

### 7.2 실무적 응용

#### 7.2.1 즉각 적용 가능한 분야

**이미지 편집 및 합성**
- 사용자 친화적 인터페이스: 자연언어로 영역 선택
- 생산성 향상: 수동 마스킹 시간 단축
- 예: "파란 옷 입은 사람"만 색상 조정

**콘텐츠 창작**
- 게임 개발: 투명도 채널 있는 캐릭터/아이템 자동 생성
- 영상 제작: 배경 분리 없이 자연언어 기반 마스킹
- 그래픽 디자인: 자동 레이아웃 생성

**접근성 향상**
- 시각장애인을 위한 이미지 설명: "노란색 자동차는 왼쪽에, 빨간 신호등은 오른쪽에"
- 비전-언어 시스템의 기초로 활용

#### 7.2.2 향후 응용 분야

**자율주행**
```
프롬프트: "교차로의 보행자"
출력: 분할된 보행자 영역 → 회피 경로 계획
이점: 새로운 도시/환경에서 재학습 불필요
```

**로봇 비전**
```
명령: "왼쪽의 빨간 상자를 집기"
처리: 자연언어 → 분할 → 그리퍼 제어
일반화: 새로운 객체 이름만 추가하면 작동
```

**의료 진단**
```
의사의 언어: "종양 주변 영역"
시스템: 해부학적 정확성 유지하며 자동 분할
학습 용도: 새로운 질병/해부학 구조에 즉시 적응
```

**원격 감지**
```
질문: "홍수 피해 지역"
응답: 위성 이미지에서 자동 분할
이점: 재난 대응에 실시간 활용
```

### 7.3 향후 연구 시 고려할 중요한 점

#### 7.3.1 근본적 한계 극복

**1. 할루시네이션 문제 해결**
```
현재 접근:
- 문제: "dog" → 배경 텍스처로부터 강아지 모양 생성
- 원인: 확산 모델의 과도한 선행

제안 방향:
- 이미지 기반 제약: 보이는 픽셀만 포함하도록 강제
  L_constraint = KL(α, observed_mask_prior)
- 배경 불변성: 배경 변화에 강건한 손실
  L_robustness = sum_k ||α_original - α_noisy_bg||²
- 형태 학습: 사전학습된 형태 인식기 활용
```

**2. 수렴 안정성 강화**
```
개선 방안:
- 적응형 학습률: 초기 큰 스텝 → 수렴 시 작은 스텝
- 조기 종료: 검증 손실 기반 자동 중지
- 다중 초기화: 앙상블로 최적 결과 선택
- 시간 스케줄: 확산 타임스텝을 점진적으로 감소
```

**3. 중심 편향 제거**
```
전략:
- 데이터 균형: 비중심 객체에 가중치 부여
- 위치 프롬프트: "오른쪽의 X" 같은 공간 표현 지원
- 모델 미세조정: 도메인 특화 확산 모델 사용
```

#### 7.3.2 성능 벤치마크 확대

현재 평가의 한계:
- VOC-C: 128x128 이상의 단일 객체만 (인위적 제한)
- RefCOCO: 복잡한 장면에는 여전히 어려움
- 새로운 평가 지표 필요

**제안하는 벤치마크**:
```python
# 1. 일반화 벤치마크
- 데이터셋: Pascal VOC → COCO → ADE20K (난이도 증가)
- 메트릭: 도메인 외 mIoU, FBIoU

# 2. 건강성 벤치마크 (Robustness)
- 이미지 왜곡: 회전, 스케일, 노이즈
- 프롬프트 변형: 동의어, 번역, 모호한 표현
- 메트릭: 성능 드롭 측정

# 3. 효율성 벤치마크
- 추론 시간: 반복 수 vs 성능 곡선
- 메모리 사용: GPU/CPU 메모리 프로파일
- 처리량: 초당 이미지 수

# 4. 안전성 벤치마크
- 편향: 특정 대상에 대한 분할 성공률 (인종, 성별, 나이)
- 환각률: 없는 객체 생성 빈도
- 실패 모드: 체계적 오류 분류
```

#### 7.3.3 방법론적 확장

**A. 다중 객체 동시 분할**
```
현재: 단일 프롬프트 → 단일 분할
개선: "이미지의 모든 사람과 동물"
접근: 
- 다중 선택적 쿼리 (Multi-query) 활용
- 마스크 혼합 전략 (mask blending)
```

**B. 도메인 적응형 파라미터화**
```
관찰: Depth Bilateral이 RGB Bilateral보다 우수
확장:
- Semantic Bilateral: 의미론적 맵 활용
- Temporal Bilateral (비디오): 프레임 간 일관성
- Multimodal Bilateral: 적외선/깊이 센서 포함
```

**C. 대화형 분할**
```
흐름:
사용자: "주황색 고양이를 분할해"
시스템: 불충분한 정보 감지
피드백: "더 밝은 주황색"
개선된 분할

기술: 대화형 최적화 루프
```

#### 7.3.4 윤리 및 편향 고려

**1. 확산 모델 상속 편향**
```
문제: LAION-5B의 인물 표현 불균형
- 특정 인종/성별 과소 표현
- 직업 스테레오타입 (의사 = 남성)

해결 방안:
- 편향 감지: Fair-diffusion 같은 도구 활용
- 프롬프트 정제: 중립 표현 장려
- 도메인 특화: 균형 잡힌 데이터셋의 확산 모델 사용
```

**2. 투명성과 설명 가능성**
```
사용자: 왜 특정 영역이 분할되었나?
설명 방법:
- Attention map 시각화 (어느 텍스트 토큰이 기여?)
- 기울기 기반 설명성 (어느 픽셀이 중요?)
- 대안 프롬프트: "~가 아닌" 형태의 음수 사례
```

**3. 오용 방지**
```
위험성:
- 신원 확인: 사진에서 특정인 자동 분할
- 합성 생성: 조작된 컨텐츠 제작 용이

대응:
- 사용자 교육: 윤리적 가이드라인 제시
- 감시 메커니즘: 의심스러운 패턴 탐지
- 워터마킹: AI 생성 콘텐츠 표시
```

#### 7.3.5 계산 효율성

**현재 병목**:
- 확산 모델 추론이 비싼 작업
- 각 반복마다 U-Net 호출 필요
- 200 반복 × 여러 확산 스텝 = 높은 지연

**최적화 방향**:
```
1. 가속화 기법
- 증류(Distillation): 빠른 확산 모델 학습
- 병렬화: 배치 처리로 여러 이미지 동시 처리
- 캐싱: 동일 프롬프트의 반복 계산 저장

2. 경량 모델
- 더 작은 확산 모델 (DistilledSD 등)
- 모바일 배포: Edge computing 지원

3. 적응형 최적화
- 초반 큰 스텝 후 정밀 최적화
- 신뢰도 기반 조기 종료
```

#### 7.3.6 자연언어 인터페이스 발전

**1. 복잡한 문법 지원**
```
현재: "파란 옷"
확장:
- 관계 표현: "X의 왼쪽에 있는 Y"
- 부정 표현: "배경이 아닌 모든 것"
- 정량 표현: "가장 큰 X", "처음 세 개의 사람"
```

**2. 다국어 지원**
```
특성:
- CLIP의 다국어 능력 활용
- 문화 특화 표현 이해 (이모지, 방언)
- 번역 오류 견디기
```

**3. 문맥 이해**
```
이전 분할 결과를 활용한 점진적 개선
예: "이미지의 모든 사람" → "그 중 가장 큰 사람만"
```

#### 7.3.7 멀티모달 확장

**비디오 분할**
```
현재: 정적 이미지
확장: 프레임 간 일관성
- 시간적 정규화: 인접 프레임의 마스크 유사성 강제
- 광학 흐름: 움직임 예측으로 안내
```

**3D 객체 분할**
```
포인트 클라우드 → "빨간색 의자" 분할
기술: 3D 확산 모델 (점수 증류 또는 보편적 3D 표현)
```

**음성-기반 분할**
```
음성 입력: "이 사람을 분할해"
프로세스: 음성 인식 → 자연언어 분할
응용: 시각 장애인 지원
```

---

## 8. 관련 최신 연구 동향 (2020년 이후)

### 8.1 2023-2025년 주요 진전

#### 8.1.1 생성 모델 기반 분할

**DiffSegmenter (2023)**
- 개선: Peekaboo의 제한 극복을 위한 BLIP 기반 프롬프트 설계
- 성과: 더 정교한 카테고리 필터링
- mIoU: COCO-Stuff에서 Peekaboo 대비 향상

**MaskDiffusion (2024)**
- 혁신: 냉동 Stable Diffusion으로 개방형 어휘 분할
- 접근: 직접 마스크 적응화 (Peekaboo의 알파 마스크와 유사하나 구현 다름)
- 특성: 추가 학습 또는 주석 불필요

**Open-Vocabulary SAM (2024)**
- 통합: Segment Anything (SAM) + CLIP
- 강점: 대화형 분할 + 개방형 어휘
- 일반화: 다양한 프롬프트 (점, 박스, 텍스트)

#### 8.1.2 도메인 일반화

**DGInStyle (2024)**
- 문제: 도메인 시프트에서의 분할 성능 저하
- 해결책: 확산 모델로 스타일 제어 가능한 학습 데이터 생성
- 결과: 자율주행 벤치마크에서 SOTA

**DomainFusion (2024)**
- 기법: 확산 모델의 잠재 공간으로부터 도메인 불변 특성 추출
- 응용: 도메인 일반화 (훈련 도메인 외의 테스트 성능 향상)
- 성과: 생성 이미지 97% 감소하고도 경쟁력 있는 성능

#### 8.1.3 의료 이미지 분할

**MambaDiff (2025)**
- 혁신: Mamba 구조로 확산 모델 강화
- 응용: 3D 의료 이미지 분할 (뇌, 간 등)
- 강점: 적은 파라미터로 높은 성능
- 성과: BraTS 2024, LiTS, MSD 데이터셋에서 SOTA

**Polyp-DDPM (2024)**
- 응용: 의료 합성 데이터 생성 → 분할 성능 향상
- 특성: 조건부 생성 (마스크 조건)
- 결과: FID 78.47 (기존 대비 >5 포인트 개선)

#### 8.1.4 약감독 및 무감독 분할

**Diffusion-Guided WSSS (2024)**
- 문제: 약한 주석 (이미지 레이블만) 상황에서의 분할
- 해결책: DDPM의 의미론적 정보로 부족한 감독 보강
- 기술: Locality Fusion Cross Attention (LFCA) 모듈

**Instance Segmentation with Step Noisy Perception (2025)**
- 혁신: 확산 모델의 노이즈 단계를 명시적으로 활용
- 방법: Step Noisy Perception 코딩 (SNP)
- 대상: 인스턴스 분할 (COCO, LVIS)
- 성과: 장꼬리(long-tail) 분포 처리 개선

#### 8.1.5 비디오 분할

**VidSeg (2024-2025)**
- 목표: 학습 없는 비디오 의미론적 분할
- 방법: Peekaboo의 시간 확장 (프레임 일관성)
- 특징: 조건부 확산 모델 활용
- 응용: 동적 장면에서의 객체 추적

#### 8.1.6 효율성 및 가속화

**AP-Adapter (2024)**
- 문제: 자동 프롬프트 최적화가 새로운 확산 모델에 일반화되지 않음
- 해결책: 다중 모델 데이터셋으로 학습한 어댑터
- 영향: 모델 일반화 능력 향상
- 관련성: Peekaboo도 여러 확산 모델에 적응 필요

### 8.2 기술적 진화 경로

```
2020-2021: CLIP 등장
    ↓
[Vision-Language 모델의 강화]
    ↓
2022: DreamFusion (3D 생성)
    │
    ├→ Score Distillation Sampling 도입
    │
2023: Peekaboo (2D 분할)
    │
    ├→ 무학습 분할 방법론
    └→ 추론 시간 최적화 확산
        
2023-2024: 후속 확산 분할 방법론 폭발
    │
    ├→ DiffSegmenter (프롬프트 설계)
    ├→ MaskDiffusion (마스크 정제)
    ├→ 의료/도메인 특화 모델
    └→ 효율성/가속화
    
2024-2025: 통합 및 확장
    │
    ├→ SAM과의 결합 (Open-Vocabulary SAM)
    ├→ 비디오로의 확장 (VidSeg)
    ├→ 3D로의 확장 (DiffAtlas)
    └→ 효율성 향상 (Mamba 통합)
```

### 8.3 개방형 어휘 분할의 진화

| 방법 | 년도 | 학습 필요 | 가능 도메인 | 주요 혁신 |
|------|------|---------|----------|---------|
| Zero-Shot CLIP Seg | 2022 | ✓ (선택적) | 한정 | CLIP 기본 분할 |
| GroupViT | 2022 | ✓ | 제한적 | 그룹 토큰 학습 |
| ZegFormer | 2022 | ✗ | 제한적 | 픽셀-세그먼트 분리 |
| **Peekaboo** | **2023** | **✗** | **광범위** | **추론 최적화** |
| DiffSegmenter | 2023 | ✗ | 광범위 | 프롬프트 설계 |
| MaskDiffusion | 2024 | ✗ | 광범위 | 마스크 정제 |
| Open-Vocab SAM | 2024 | ✓ (소량) | 매우 광범위 | SAM+CLIP 통합 |

### 8.4 벤치마크 성능 비교 (mIoU)

#### VOC 의미론적 분할 (무학습)
```
2022: ZegFormer (SegFormer + CLIP) = ~62%
2023: Peekaboo (Depth Bilateral) = 55.1%
2024: MaskDiffusion 변형 = ~60%+
```

#### RefCOCO 지칭 분할 (무학습)
```
2022: 이전 방법 < 10%
2023: Peekaboo (Depth Bilateral) = 20.4%
2024: DiffSegmenter 변형 = ~25%+
```

### 8.5 향후 예상 추세 (2025-2026)

**1. 효율성 우선**
- 실시간 처리: 모바일/엣지 디바이스 지원
- 메모리 효율: 단일 GPU에서 고해상도 처리

**2. 정확성 향상**
- 세분화 수준: 더 높은 IoU 달성
- 환각 감소: 의료/보안 응용 가능

**3. 상호작용성**
- 반복적 개선: 사용자 피드백 기반 마스크 조정
- 명령 이해: "점차 확대", "경계 조정" 같은 동적 명령

**4. 멀티모달 통합**
- 음성/텍스트 혼합
- 제스처 기반 상호작용
- 다중 센서 퓨전 (RGB + 깊이 + 열화상)

**5. 실세계 배포**
- 자율주행: 실시간 의미론적 이해
- 로봇: 자연언어 지시 이해
- 의료: 진단 보조 시스템

---

## 결론

**Peekaboo**는 단순하면서도 강력한 아이디어로 다음을 입증했다:

1. **생성 모델의 숨겨진 능력**: 분할 정보 없이 학습된 확산 모델도 강력한 위치 정보를 내포
2. **무학습의 가능성**: 사전학습 모델의 추론 시간 최적화만으로 복잡한 시각 작업 수행 가능
3. **개방형 어휘의 달성**: 임의의 자연언어 표현으로부터 즉시 분할 가능

이러한 개념은 후속 연구자들에게 다음 방향을 제시했다:
- 다양한 파라미터화 기법의 탐색
- 도메인 특화 모델로의 적용
- 멀티모달 및 3D 확장
- 효율성 개선을 통한 실세계 배포

한계(할루시네이션, 중심 편향, 수렴 불안정)를 극복하기 위해서는 파라미터화, 손실 함수, 확산 모델 선택, 사후 처리 등에서의 지속적인 혁신이 필요하며, 향후 연구자들은 이 기초 위에서 더욱 강건하고 효율적인 시스템을 구축할 수 있을 것으로 예상된다.

---

## 수식 요약

### 핵심 수식 모음

1. **알파 합성**: $$\hat{\mathbf{x}} = \alpha \odot \mathbf{x} + (1 - \alpha) \odot \mathbf{b}$$

2. **잠재 점수 증류 손실**: $$L_s = \text{MSE}\left(\epsilon, D\left(\tilde{\mathbf{z}}, T(p)\right)\right)$$

3. **알파 정규화 손실**: $$L_\alpha = \sum_{i} \alpha_i$$

4. **Peekaboo 전체 손실**: $$L_p = L_s + \lambda_\alpha L_\alpha$$

5. **알파 마스크 파라미터화 (RBP)**:

```math
\alpha_{\text{final}} = \sigma(\text{bilateral\_blur}(\alpha_{\text{learnable}}, \mathbf{x}))
```

여기서 $\sigma$는 시그모이드 함수, bilateral_blur는 이미지 $\mathbf{x}$ 가이드하의 양측 필터

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/7b521af3-6d6a-4a60-ac50-cd06f046337a/2211.13224v2.pdf)
[2](https://journal.stemfellowship.org/doi/10.17975/sfj-2024-004)
[3](https://ieeexplore.ieee.org/document/10678598/)
[4](https://arxiv.org/abs/2508.07514)
[5](https://arxiv.org/abs/2406.17541)
[6](https://link.springer.com/10.1007/s10489-025-06673-1)
[7](https://www.sciltp.com/journals/ijndi/2024/4/674)
[8](https://ieeexplore.ieee.org/document/11164500/)
[9](https://www.sciltp.com/journals/ijndi/2025/1/972)
[10](https://iopscience.iop.org/article/10.1149/MA2024-02674691mtgabs)
[11](https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-025-24473-7)
[12](https://arxiv.org/pdf/2312.03048.pdf)
[13](https://arxiv.org/html/2309.02773v3)
[14](http://arxiv.org/pdf/2309.14303.pdf)
[15](http://arxiv.org/pdf/2403.11194.pdf)
[16](http://arxiv.org/pdf/2403.14291.pdf)
[17](https://arxiv.org/pdf/2303.08888.pdf)
[18](https://arxiv.org/html/2411.16308v1)
[19](https://arxiv.org/abs/2307.00773)
[20](https://www.nature.com/articles/s41598-025-90631-x)
[21](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Decoupling_Zero-Shot_Semantic_Segmentation_CVPR_2022_paper.pdf)
[22](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05806.pdf)
[23](https://pure.kaist.ac.kr/en/publications/diffusion-guided-weakly-supervised-semantic-segmentation)
[24](https://paperswithcode.com/paper/decoupling-zero-shot-semantic-segmentation)
[25](https://www.sciencedirect.com/science/article/abs/pii/S1566253525001022)
[26](https://arxiv.org/abs/2409.15117)
[27](https://www.sciencedirect.com/science/article/abs/pii/S0262885625000447)
[28](https://proceedings.neurips.cc/paper_files/paper/2024/file/b2077e6d66da612fcb701589efa9ce88-Paper-Conference.pdf)
[29](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VidSeg_Training-free_Video_Semantic_Segmentation_based_on_Diffusion_Models_CVPR_2025_paper.html)
[30](https://arxiv.org/abs/2309.02773)
[31](https://journals.lww.com/10.1097/RLI.0000000000001245)
[32](https://iopscience.iop.org/article/10.1149/MA2025-02432165mtgabs)
[33](https://arxiv.org/abs/2303.05105)
[34](https://arxiv.org/html/2405.05791v1)
[35](https://arxiv.org/html/2410.02369)
[36](https://arxiv.org/pdf/2402.04031.pdf)
[37](http://arxiv.org/pdf/2503.06748.pdf)
[38](https://arxiv.org/html/2407.03548v1)
[39](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Open-Vocabulary_Semantic_Segmentation_Models_From_Natural_Language_Supervision_CVPR_2023_paper.pdf)
[40](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf)
[41](https://vcg-team.github.io/DiffSegmenter-webpage/)
[42](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06014.pdf)
[43](https://arxiv.org/html/2403.11194v1)
[44](https://arxiv.org/html/2404.07448v3)
[45](https://arxiv.org/abs/2211.13224)
[46](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Open-vocabulary_Object_Segmentation_with_Diffusion_Models_ICCV_2023_paper.pdf)
[47](https://openreview.net/pdf?id=e8PVEkSa4Fq)
[48](https://dl.acm.org/doi/10.1109/TIP.2025.3551648)
[49](https://arxiv.org/abs/2403.11194)
