
# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks

## 1. 핵심 주장과 주요 기여[1]

**Florence-2**는 Microsoft Azure AI에서 개발한 혁신적인 비전 기초 모델(Vision Foundation Model)으로, **단일 모델로 다양한 컴퓨터 비전 및 비전-언어 작업**을 수행할 수 있도록 설계되었습니다. 이 논문의 핵심 주장은 다음과 같습니다:[1]

기존의 대규모 비전 모델들은 전이 학습(Transfer Learning)에는 탁월하지만, **단순한 지시사항으로 다양한 작업을 수행하는 능력**에서는 부족하다는 점입니다. 이는 **공간적 계층 구조(Spatial Hierarchy)**와 **의미론적 세분성(Semantic Granularity)**의 복잡성을 다루지 못하기 때문입니다.[1]

### 주요 기여:

1. **통일된 프롬프트 기반 표현**: 텍스트 프롬프트를 통해 이미지 캡셔닝, 객체 탐지, 그라운딩, 세분화(Segmentation) 등 다양한 작업을 텍스트 형태의 출력으로 생성[1]

2. **대규모 고품질 데이터셋 개발**: **FLD-5B** 데이터셋 - 126백만 개 이미지에 54억 개의 포괄적인 시각 주석[1]

3. **시퀀스-투-시퀀스 구조**: 모든 작업을 통일된 언어 모델링 목표 하에서 처리[1]

4. **우수한 제로샷 및 파인튜닝 성능**: 기존의 더 큰 모델들을 능가하는 성능[1]

***

## 2. 해결하는 문제와 제안 방법

### 2.1 주요 문제점[1]

컴퓨터 비전에서 보편적 표현(Universal Representation) 달성의 핵심 도전:

- **공간적 계층 구조**: 이미지 수준의 개념부터 픽셀 수준의 세부 사항까지 다양한 규모에서의 공간 정보 이해
- **의미론적 세분성**: 고수준 캡션에서 세밀한 설명까지 다양한 의미 수준의 이해
- **주석 데이터 부족**: 포괄적인 멀티태스크 학습을 위한 대규모 고품질 주석 데이터의 부재
- **통일된 아키텍처 부재**: 다양한 비전 작업을 단일 아키텍처로 수행할 수 없음[1]

### 2.2 제안 방법

#### A. 포괄적 멀티태스크 학습[1]

Florence-2는 세 가지 서로 다른 학습 목표를 통합합니다:

1. **이미지 수준 이해 작업**: 이미지 분류, 캡셔닝, VQA를 통해 고수준의 의미 학습
2. **영역/픽셀 수준 인식 작업**: 객체 탐지, 세분화, 참조 표현 이해를 통해 세부 위치 정보 학습
3. **세밀한 시각-의미 정렬 작업**: 텍스트 구절과 이미지 영역의 정렬을 통해 로컬 세부 사항 학습[1]

#### B. 최적화 목표[1]

모든 작업에 대해 표준 언어 모델링 크로스-엔트로피 손실(Cross-Entropy Loss)을 사용합니다:

$$L = -\sum_{i=1}^{|y|} \log P_\theta(y_i|y_{ < i}, x)$$

여기서 $$\theta$$는 네트워크 파라미터이고, $$|y|$$는 타겟 토큰의 개수입니다[1].

### 2.3 모델 구조[1]

Florence-2는 다음과 같은 구조로 구성됩니다:

**1. 비전 인코더 (Vision Encoder)**
- **DaViT**(Dual Attention Vision Transformers) 사용
- 입력 이미지 $$I \in \mathbb{R}^{H \times W \times 3}$$를 평탄화된 시각 토큰 임베딩 $$V \in \mathbb{R}^{N_v \times D_v}$$로 변환
- $$N_v$$: 시각 토큰의 개수, $$D_v$$: 토큰 차원

**2. 멀티모달 인코더-디코더 (Multi-modality Encoder-Decoder)**
- 표준 Transformer 기반 아키텍처
- 프롬프트 텍스트 임베딩 $$T_{prompt} \in \mathbb{R}^{N_t \times D}$$를 얻음
- 시각 토큰 $$V'$$(선형 투영 후)과 프롬프트 임베딩을 연결: $$X = [V', T_{prompt}]$$
- LayerNorm을 통해 차원 정렬[1]

**3. 토큰 표현**
- **텍스트**: 일반 텍스트는 그대로 유지
- **박스 표현** (x₀, y₀, x₁, y₁): 객체 탐지, 밀집 영역 캡셔닝
- **쿼드 박스 표현** (x₀, y₀, ..., x₃, y₃): 텍스트 탐지 및 인식
- **폴리곤 표현** (x₀, y₀, ..., xₙ, yₙ): 참조 세분화
- 1,000개의 양자화 빈(Bins)을 사용하여 위치 토큰 표현[1]

***

## 3. FLD-5B 데이터셋과 데이터 엔진[1]

### 3.1 데이터셋 규모

- **126백만 개 이미지**
- **5.4억 개 텍스트 주석**
- **13억 개 영역-텍스트 쌍**
- **36억 개 텍스트-구절-영역 삼중항**

### 3.2 데이터 파이프라인[1]

**1단계: 이미지 수집**
- ImageNet-22k, Object 365, Open Images, Conceptual Captions, LAION에서 다양한 이미지 수집

**2단계: 초기 주석(전문 모델 사용)**
- 공개 데이터셋에서 학습한 오프라인 모델과 클라우드 플랫폼의 온라인 서비스 활용
- 텍스트 주석: 이미지-텍스트 모델 사용
- 영역-텍스트 쌍: DINO 객체 탐지기 및 Azure OCR API 활용
- 텍스트-구절-영역: Grounding DINO와 SAM 모델 활용[1]

**3단계: 데이터 필터링 및 향상**
- **텍스트 필터링**: SpaCy 기반 파싱 도구를 사용하여 과도한 객체 포함 텍스트 제거
- **영역 필터링**: 신뢰도 임계값 기반 필터링 및 비최대 억제(NMS) 적용
- 복잡도 측정을 통한 텍스트 품질 평가[1]

**4단계: 반복적 데이터 정제(Iterative Refinement)**
- 멀티태스크 모델을 반복적으로 학습하면서 초기 주석의 오류 수정
- 부정확한 라벨은 모델의 더 나은 예측으로 교체
- 초기에 충분한 데이터가 없던 작업(예: 상세 설명)도 반복 학습된 모델로 주석[1]

### 3.3 주석 통계[1]

| 주석 유형 | 텍스트 유형 | 이미지 개수 | 평균 토큰 | 평균 영역 |
|---------|----------|---------|---------|---------|
| 텍스트 | 간단 | 2.35억 | 7.95 | - |
| 텍스트 | 상세 | 1.26억 | 31.65 | - |
| 텍스트 | 매우 상세 | 1.26억 | 70.53 | - |
| 영역-텍스트 | 구절 | 1.26억 | - | 5.42 |
| 텍스트-구절-영역 | 간단 | 2.35억 | 7.95 | 4.27 |

***

## 4. 성능 향상 및 실험 결과[1]

### 4.1 제로샷 성능[1]

Florence-2-L (7.71억 파라미터)의 제로샷 성능:

- **COCO 캡션**: 135.6 CIDEr (Flamingo 80B: 84.3)
- **Flickr30k 그라운딩**: 84.4 Recall@1 (Kosmos-2: 78.7)
- **RefCOCO 참조 표현**: 56.3 Accuracy
- **RefCOCO+ 참조 표현**: 61.6 Accuracy
- **RefCOCO 세분화**: 35.8% mIoU[1]

### 4.2 파인튜닝 성능[1]

공개 데이터셋으로 파인튜닝 후 성능:

- **COCO 캡션** (Karpathy 테스트): 143.3 CIDEr
- **TextVQA**: 73.5% 정확도
- **TextCaps**: 151.1 CIDEr
- **RefCOCO 세분화**: 80.5% mIoU[1]

### 4.3 다운스트림 작업 성능[1]

**Mask R-CNN으로 COCO 객체 탐지:**
- Florence-2 사전학습: 53.6 AP
- ImageNet 지도 학습 사전학습: 46.7 AP
- **성능 향상: 6.9 AP 포인트**
- **훈련 효율성 4배 향상**

**DINO로 COCO 객체 탐지:**
- Florence-2 사전학습: 59.2 AP
- ImageNet 사전학습: 53.7 AP
- **성능 향상: 5.5 AP 포인트**

**UperNet으로 ADE20K 의미 세분화:**
- Florence-2 사전학습: 54.9 mIoU
- FCMAE 사전학습: 52.1 mIoU
- **성능 향상: 2.8 mIoU 포인트**
- **훈련 효율성 4배 향상**[1]

***

## 5. 일반화 성능 향상 가능성[1]

### 5.1 멀티태스크 전이 분석[1]

다양한 수준의 작업으로 사전학습된 모델의 성능 비교:

| 사전학습 구성 | COCO 캡션 | COCO 탐지 | Flickr30k | RefCOCO RES |
|------------|---------|---------|---------|----------|
| 이미지 수준만 | 128.8 | 0.1 | 62.0 | 28.4 |
| 이미지+영역 수준 | 134.6 | 29.7 | 79.1 | 18.2 |
| 이미지+영역+픽셀 수준 | 133.4 | 28.3 | 78.1 | 31.6 |

**핵심 발견**: 이미지-영역-픽셀 수준 모두에서 학습한 모델이 모든 다운스트림 작업에서 가장 우수한 성능을 보임[1]

### 5.2 데이터 스케일링 효과[1]

데이터 크기에 따른 제로샷 성능:

| 데이터 크기 | COCO 캡션 | COCO 탐지 | Flickr30k | RefCOCO RES |
|---------|---------|---------|---------|----------|
| 0.12M | 102.8 | 16.1 | 74.0 | 15.9 |
| 0.36M | 114.3 | 18.7 | 75.8 | 16.6 |
| 1.2M | 118.1 | 18.9 | 76.3 | 19.3 |
| 12M | 118.7 | 19.7 | 76.3 | 18.6 |

**결론**: 데이터 규모 증가에 따라 제로샷 성능이 지속적으로 향상되는 경향을 보임[1]

### 5.3 모델 스케일링 효과[1]

Florence-2-B (2.32억 파라미터) vs Florence-2-L (7.71억 파라미터):

| 모델 | COCO 캡션 | COCO 탐지 | Flickr30k | RefCOCO RES |
|-----|---------|---------|---------|----------|
| Base | 133.0 | 34.7 | 83.6 | 34.6 |
| Large | 135.6 | 37.5 | 84.4 | 35.8 |

더 큰 모델이 모든 작업에서 성능 향상을 보임[1]

### 5.4 비전 백본 학습의 중요성[1]

비전 인코더를 고정(Frozen)하는 경우의 성능 영향:

| 설정 | COCO 캡션 | COCO 탐지 | Flickr30k | RefCOCO RES |
|-----|---------|---------|---------|----------|
| 인코더 고정 | 120.0 | 6.9 | 66.3 | 9.9 |
| 인코더 학습 | 118.7 | 19.7 | 76.3 | 18.6 |

**핵심 발견**: 비전 인코더를 고정하면 영역 및 픽셀 수준 작업에서 심각한 성능 저하 발생. 이는 기존 이미지 수준 작업 중심의 사전학습이 영역/픽셀 수준 기술을 충분히 제공하지 못함을 시사[1]

***

## 6. 한계(Limitations)[1]

논문에서 명시되거나 암시된 주요 한계:

1. **계산 비용**: 5.4억 개의 주석을 생성하기 위해 전문 모델과 클라우드 서비스에 상당한 계산 자원 소비

2. **주석 품질**: 자동 생성된 주석이 노이즈를 포함할 수 있으며, 반복 정제 과정도 완벽한 정확도를 보장하지 못함

3. **특정 도메인 성능**: 웹 기반 이미지로 학습되어 특정 도메인(의료 이미지 등)에서의 성능이 제한될 수 있음

4. **모델 크기**: 파라미터 수가 상대적으로 적지만, 여전히 실제 배포 환경에서는 압축이 필요할 수 있음

5. **복잡한 추론**: 다중 단계의 추론이 필요한 복잡한 시각적 이해 작업에서는 제한사항 존재

***

## 7. 앞으로의 연구에 미치는 영향[2][3][4]

### 7.1 즉각적인 영향[2]

**Florence-VL의 개발**: Florence-2의 생성적 비전 인코더가 멀티모달 대형 언어 모델(MLLMs)의 기반이 되어 더욱 강력한 시각-언어 모델 개발을 촉발[2]

**의료 이미지 분석**: Florence-2의 기초 위에 의료 분야 특화 모델들이 개발되고 있으며, 상대적으로 작은 모델 크기로도 경쟁력 있는 성능 달성[3]

### 7.2 최신 연구 동향(2024-2025)[5][6][7]

**1. 통일 모델의 필요성 재검토**[5]

최신 연구는 "통일 시각-언어 모델이 정말 필요한가?"라는 근본적인 질문을 제기하고 있습니다. 통일 모델이 혼합 데이터 학습을 통해 이해와 생성 작업 간 상호 이익을 제공하는지 체계적으로 검증하고 있습니다.[5]

**2. 파운데이션 모델 벤치마킹**[6]

최신 연구(NeurIPS 2024)에서 VLM 기반 파운데이션 모델들의 제로샷 성능이 기존의 몇샷(Few-shot) 탐지 방식을 크게 능가함을 입증했습니다. 예를 들어 GroundingDINO는 COCO에서 48 AP를 달성하며 기존 최고 성능 몇샷 탐지기(33 AP)를 크게 앞섬.[6]

**3. 도메인 특화 적응**[4]

Florence-2를 기반으로 비정형 환경(unstructured environments)의 객체 탐지를 위한 미세 조정 방법론이 개발되고 있어, 일반화 가능한 기초 모델에서 특정 응용 분야로의 효과적인 전이 학습 경로를 제시.[4]

### 7.3 향후 연구 시 고려할 점[6][5][1]

#### A. 데이터 질 vs 규모의 균형[1]

데이터 스케일링 실험 결과, 단순히 데이터 크기 증가만으로는 성능 향상이 포화될 수 있습니다. 향후 연구는:
- 자동 주석 방법의 더 나은 품질 보증 메커니즘 개발
- 노이즈에 강건한 학습 알고리즘 개발에 주력해야 함

#### B. 계층적 멀티태스크 학습의 최적화[1]

현재 모델이 이미지-영역-픽셀 세 수준 모두에서 학습할 때 최고 성능을 발휘하지만:
- 이들 작업 간 최적의 가중치 배분 방법
- 작업 간 간섭(negative transfer) 최소화 전략 연구 필요

#### C. 도메인 적응 및 특화[3][4]

- 의료, 위성 이미지, 산업 검사 등 특정 도메인의 특성을 반영한 효율적 미세 조정 방법 개발
- 제한된 도메인 데이터로도 강력한 성능을 유지하는 메커니즘 연구

#### D. 효율성 개선[1]

- 4배 훈련 효율성 달성에도 불구하고, 모바일/엣지 디바이스 배포를 위한 더한 압축 및 경량화 기술 필요
- 추론 시 계산량 감소 방법론 개발

#### E. 통일 모델의 한계 극복[5]

- 이해 작업과 생성 작업 간 트레이드오프 분석
- 특정 작업에서 전문화된 모델과의 성능 격차 해소

#### F. 제로샷 성능 추가 향상[6]

- 현재 제로샷 성능이 좋지만, 실제 응용에서 몇샷 적응(few-shot adaptation) 성능을 더욱 개선하는 연구 필요
- 텍스트 프롬프트 최적화 및 컨텍스트 학습(In-Context Learning) 기법 개발

#### G. 해석가능성 및 신뢰성[3]

- 모델의 의사 결정 과정 해석 방법 개발
- 특히 의료 등 높은 신뢰성이 요구되는 분야에서의 검증 기준 수립

#### H. 다국어 및 문화 다양성[1]

- FLD-5B 데이터가 주로 영문 텍스트 중심
- 다양한 언어 및 문화적 맥락을 반영하는 멀티리거얼 모델 개발

***

## 결론

Florence-2는 **공간적 계층 구조와 의미론적 세분성**을 동시에 다루는 통일된 비전 기초 모델로서 컴퓨터 비전 분야에 패러다임 전환을 가져왔습니다. FLD-5B 데이터셋의 개발과 자동화된 주석 생성 파이프라인은 **대규모 고품질 학습 데이터 확보의 새로운 방향**을 제시했습니다.[1]

특히 **멀티태스크 학습의 일반화 성능**은 이미지 수준, 영역 수준, 픽셀 수준 모두에서의 종합적 학습이 모든 하위 작업에 긍정적 영향을 미침을 입증했습니다. 다운스트림 작업에서 4배 훈련 효율성 향상과 6.9 AP의 성능 개선은 사전학습의 중요성을 재확인시켜 줍니다.[1]

향후 연구는 **도메인 특화 적응, 효율성 개선, 데이터 품질 보증, 해석가능성 강화** 등의 방향으로 진행될 것으로 예상되며, Florence-2의 개방형 공개(오픈소스)가 커뮤니티 기반 발전을 촉진하고 있습니다.[8][9]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e19cf349-0058-4d16-a3ab-e122f5f898dc/2311.06242v1-abcugdoem.pdf)
[2](http://arxiv.org/pdf/2412.04424.pdf)
[3](https://arxiv.org/html/2503.03278)
[4](http://arxiv.org/pdf/2503.04918.pdf)
[5](https://arxiv.org/html/2505.23043v1)
[6](https://proceedings.neurips.cc/paper_files/paper/2024/hash/22b2067b8f680812624032025864c5a1-Abstract-Datasets_and_Benchmarks_Track.html)
[7](https://viso.ai/computer-vision/florence-2/)
[8](https://www.ikomia.ai/blog/microsoft-florence-2-unified-vision-ai)
[9](https://docs.openvino.ai/2024/notebooks/florence2-with-output.html)
[10](http://arxiv.org/pdf/2311.06242.pdf)
[11](http://arxiv.org/pdf/2111.11432.pdf)
[12](https://bjo.bmj.com/content/bjophthalmol/early/2024/06/04/bjo-2024-325459.full.pdf)
[13](https://arxiv.org/pdf/2405.14137.pdf)
[14](https://aclanthology.org/2023.findings-emnlp.40.pdf)
[15](https://cvpr.thecvf.com/virtual/2024/poster/30529)
[16](https://arxiv.org/abs/2409.00106)
[17](https://openreview.net/forum?id=E01k9048soZ)
[18](https://arxiv.org/abs/2406.03907)
[19](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf)
