# Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning

### 1. 핵심 주장 및 주요 기여

**Agent0-VL**은 비전-언어 에이전트의 자율적 진화를 실현하는 혁신적인 프레임워크입니다. 논문의 핵심 주장은 **도구 기반 검증이 자기보상 학습의 평가 신뢰성을 근본적으로 개선할 수 있다**는 것입니다. 기존의 순수 텍스트 기반 자기평가 방식이 복잡한 시각 추론 단계 검증에 실패하고 평가 환각(evaluation hallucination)에 빠지는 문제를 해결하기 위해, Agent0-VL은 도구 호출을 추론뿐만 아니라 자기평가와 자기수리 단계에까지 통합합니다.[1]

**주요 기여:**
- **통합된 이중 역할 아키텍처**: 단일 대규모 비전-언어 모델(LVLM) 내에서 **Solver**(다단계 도구 통합 추론 수행)와 **Verifier**(구조화된 피드백 생성 및 도구 기반 보상 신호 제공)가 협력하는 아키텍처[1]
- **자기진화 추론 사이클(SERC)**: 도구 기반 검증과 강화학습이 결합되어 추론과 평가 분포를 공동으로 정렬하는 폐쇄 루프 시스템[1]
- **외부 보상 제로 진화**: 인간 주석이나 외부 보상 모델 없이 자기생성 보상만으로 지속적 성능 향상 달성[1]

***

### 2. 해결 대상 문제 및 제안 방법

#### 2.1 핵심 문제

기존 비전-언어 에이전트는 두 가지 근본적 제약을 가집니다:[1]

1. **평가 능력의 한계**: 텍스트 기반 반성만으로는 복잡한 다단계 계산, 공간 추론, 정밀한 물리/기하학적 계산을 검증할 수 없음
2. **신뢰할 수 없는 평가 프로세스**: 과도한 텍스트 추론으로 인해 모델이 언어 지름길에 의존하고 세밀한 시각 이해를 우회하며 언어 선입견에 의존, 결국 평가 환각 유발

#### 2.2 이론적 기초: 부분 관측 마르코프 결정 프로세스(POMDP)

Agent0-VL의 추론 과정은 POMDP 프레임워크로 형식화됩니다:[1]

$$M = (S, A, O, T, R, \gamma)$$

여기서:
- **상태 공간** $s_t \in S$: 텍스트 추론 맥락, 시각 특성, 과거 도구 입력/출력 추적을 인코딩하는 잠재 멀티모달 추론 상태
- **행동 공간** $a_t \in A$: 텍스트 추론 단계 $a_t^{\text{text}}$ 또는 구조화된 도구 호출 $a_t^{\text{tool}}$ (Python 코드 실행)
- **관찰 공간** $o_t \in O$: 외부 도구 또는 환경으로부터의 피드백 (수치 결과 또는 텍스트 검색)
- **전이 함수** $T(s_{t+1}|s_t, a_t, o_t)$: 생성된 응답과 해당 도구 피드백에 따른 상태 진화
- 할인 계수 $\gamma$

각 시간 단계에서 에이전트는 궤적을 생성합니다:[1]

$$\tau: s_1, a_1, o_1, s_2, a_2, o_2, \ldots, s_T, a_T, o_T$$

여기서 각 전이는 하나의 추론-도구 피드백 상호작용을 인코딩합니다.

#### 2.3 통합 정책 공식화

**공유 정책**은 역할 지시자 $m \in \{S, V\}$를 통해 두 역할을 제어합니다:[1]

$$\pi_\theta(a_t|s_t, m) = \begin{cases} \pi_\theta^S(a_t|s_t) & m = S \text{ (Solver)} \\ \pi_\theta^V(a_t|s_t, a_t', o_t) & m = V \text{ (Verifier)} \end{cases}$$

**Solver**는 다단계 추론을 수행하며 신뢰할 수 있는 증거로 계산과 시각 인식을 위해 외부 도구를 선택적으로 호출합니다:[1]

$$a_t \sim \pi_\theta^S(a_t|s_t, b_{t-1})$$

여기서 $b_t$는 누적된 추론 맥락과 멀티모달 정보를 인코딩하는 잠재 신념 상태입니다.

**Verifier**는 생성형 검증 모드로 전환하여 각 추론 단계를 평가하고 피드백 튜플을 출력합니다:[1]

$$V_t = (\text{score}_t, \text{conf}_t, \text{critique}_t)$$

여기서:
- $\text{score}_t \in [-1, 1]$: 사실적 정확성 측정
- $\text{conf}_t \in $: 인식론적 확실성 추정[1]
- $\text{critique}_t$: 잠재적 추론 결함을 설명하는 자연언어 반성

#### 2.4 도구 기반 검증 및 보상 메커니즘

**프로세스 수준 보상**은 의미론적 신뢰성, 도구 기반 검증, 교차 역할 정규화를 통합합니다:[1]

$$r_t^{\text{proc}} = \lambda_{\text{tool}} r_t^{\text{tool}} + \text{score}_t \cdot \text{conf}_t - \lambda_{\text{div}} D_{\text{KL}}(V_t || E_t)$$

여기서:
- $\lambda_{\text{tool}}$: 도구 기반 정확성 스케일
- $r_t^{\text{tool}}$: 도구 실행의 정확성
- $\text{score}_t \cdot \text{conf}_t$: 의미론적 신뢰성
- $D_{\text{KL}}(V_t || E_t)$: Verifier와 Solver 분포 간 KL 발산으로 안정성 강화

#### 2.5 신뢰도 기반 선택적 자기수리

검증 후 모델은 신뢰도 점수에 따라 자기수리 수행 여부를 결정합니다. 수리 게이트는 다음과 같이 정의됩니다:[1]

$$g_t = \sigma(\beta(c - \text{conf}_t))$$

여기서:
- $c$: 신뢰도 임계값
- $\sigma$: 시그모이드 함수
- $\beta$: 게이팅 온도 제어

활성화되면 Verifier는 지역 수리 지시사항 $\zeta_t = f(s_t, a_t, V_t)$를 발행하고 Solver는 보정된 세그먼트를 재생성합니다:[1]

$$\tilde{a}_t \sim \pi_\theta^S(a_t|s_t, \zeta_t, m = S)$$

단계별 보상은 수리 비용을 포함하도록 조정됩니다:[1]

$$r_t = r_t^{\text{proc}} \cdot g_t - C_{\text{repair}}$$

여기서 $C_{\text{repair}}$는 불필요한 수리에 대한 페널티입니다.

#### 2.6 자기진화 추론 사이클(SERC) 및 GRPO 최적화

**전체 궤적 반환**은 최종 결과 보상과 누적 단계별 프로세스 보상을 결합합니다:[1]

$$g_i = \lambda_{\text{out}} r_{\text{out}} + \sum_{t=1}^T \gamma^{t-1} r_t$$

여기서 $\lambda_{\text{out}}$는 두 보상 유형의 균형을 조절합니다.

**그룹 상대 정책 최적화(GRPO)**는 PPO 변형으로 생성 작업용으로 설계되었습니다. G개 궤적 그룹 $\{\tau_i\}_{i=1}^G$에 대해 정규화된 이점을 계산합니다:[1]

$$A_i = \frac{g_i - \text{mean}(g_1, \ldots, g_G)}{\text{std}(g_1, \ldots, g_G) + \epsilon}$$

정책은 KL 정규화를 통해 안정성을 유지하면서 최적화됩니다:[1]

$$\mathcal{L}_{\text{EDLP}} = \mathbb{E}_i \left[\min(r_i A_i, \text{clip}(r_i, 1-\delta, 1+\delta) A_i)\right] - \lambda_{\text{KL}} D_{\text{KL}}(\pi_\theta || \pi_{\text{old}})$$

여기서 $r_i = \pi_\theta(\tau_i) / \pi_{\text{old}}(\tau_i)$는 중요도 샘플링 비율입니다.

***

### 3. 모델 구조 및 아키텍처

#### 3.1 통합 이중 역할 아키텍처

Agent0-VL은 단일 LVLM 내에서 두 가지 보완적 역할 간에 전환합니다:[1]

```
입력: 질의(I, q) + 이미지

├─ [Solver 모드]
│  ├─ 다단계 추론 생성
│  ├─ 필요시 도구 호출 (Python 코드 실행)
│  ├─ 도구 출력 통합
│  └─ 정교화된 추론 궤적 생성
│
├─ [Verifier 모드]
│  ├─ 각 단계별 검증 수행
│  ├─ 도구 기반 교차 검증
│  ├─ score, confidence, critique 생성
│  └─ 단계별 보상 및 수리 신호 계산
│
├─ [자기수리 모드]
│  ├─ 낮은 신뢰도 단계 식별
│  ├─ 수정된 추론 패치 생성
│  ├─ 도구 재실행 (검증용)
│  └─ 보정된 추론 체인 재생성
│
└─ 출력: 최종 답변 + 증거
```

#### 3.2 모델 구성

**베이스 모델**: Qwen2.5-VL-7B 및 Qwen3-VL-8B[1]

**파라미터 공유**: Solver와 Verifier는 동일한 모델 파라미터를 공유하며 역할 토큰으로 구분[1]

**훈련 단계**:
1. **감독 학습(SFT)**: 200k 샘플로 도구 사용과 검증 형식 학습[1]
2. **초기 안정화**: 2에포크 외부 보상으로 사전 훈련 (자기진화 전 구조화된 탐색 활성화)[1]
3. **자기진화 RL**: 40k 데이터로 GRPO 기반 자기보상 학습[1]

**훈련 하이퍼파라미터**:[1]
- 학습률(SFT): $1 \times 10^{-5}$
- 학습률(RL): $5 \times 10^{-7}$
- 배치 크기: 128(SFT), 256(RL)
- KL 발산 계수: $\lambda_{\text{KL}} = 0.001$
- 엔트로피 계수: $\lambda_{\text{ent}} = 0.01$
- 신뢰도 임계값: $c = 0.7$
- 수리 패널티: $\gamma_{\text{repair}} = 0.05$

***

### 4. 성능 평가 및 성과

#### 4.1 벤치마크 전반 성능

Agent0-VL-7B는 모든 오픈소스 베이스라인을 일관되게 능가합니다:[1]

| 벤치마크 | Base | Agent0-VL-7B | 향상도 |
|---------|------|--------------|-------|
| MathVerse | 46.3% | 53.1% | +6.8% |
| MathVision | 25.1% | 37.3% | +12.2% |
| MathVista | 67.8% | 75.6% | +7.8% |
| WeMath | 62.1% | 71.7% | +9.6% |
| HallBench | 65.0% | 72.9% | +7.9% |
| ChartQA | 83.5% | 87.3% | +3.8% |
| MMMU | 58.6% | 61.1% | +2.5% |
| **평균** | **58.3%** | **65.6%** | **+12.5%** |

Agent0-VL-7B는 Qwen2.5-VL-7B-TIR(도구 통합 베이스)에 비해 **10.3% 향상**을 달성했습니다.[1]

더욱 강력한 베이스 모델인 Qwen3-VL-8B를 사용한 Agent0-VL-8B는 **6.1% 향상**을 달성하고 GPT-4o를 포함한 폐쇄소스 시스템을 MathVista, HallBench, ChartQA에서 능가합니다.[1]

#### 4.2 작업 도메인별 성과

**수학적 추론** (MathVista, WeMath 평균): 도구 기반 실행과 검증이 중요한 기하/수학 문제에서 **18.1% 향상** 달성[1]

**시각 기반 작업** (HallusionBench, ChartQA): Verifier의 사실적 기반이 시각 환각을 크게 감소시켜 **12.2% 향상** 달성[1]

#### 4.3 반복적 자기진화

다중 반복 SERC를 통한 성능 진화:[1]

| 반복 | 개선도 |
|-----|--------|
| 기본 모델 | 57.3% |
| 반복 1 | +5.2% (60.5%) |
| 반복 2 | +4.0% (63.6%) |
| 반복 3 | +2.8% (65.5%) |

**누적 향상: 12.2% (8.2%포인트)**로 단조로운 성능 증가를 입증[1]

#### 4.4 프로세스 보상 모델(PRM) 성능

Verifier를 독립형 PRM으로 배포했을 때:[1]

| 모델 | 기본 | Agent0-VL PRM | 향상도 |
|------|------|----------------|--------|
| Qwen2.5-VL-3B | 50.0% | 53.6% | +3.6% |
| Qwen2.5-VL-7B | 58.3% | 62.8% | +4.5% |
| Qwen2.5-VL-32B | 64.4% | 69.1% | +4.7% |

Best-of-8 선택에서 **평균 7.3% 향상**을 달성하며 다양한 모델 규모에 걸쳐 강력한 일반화 능력을 입증합니다.[1]

#### 4.5 절제 연구

| 설정 | 수학(평균) | HallBench | ChartQA | 성능 하락 |
|------|-----------|-----------|---------|----------|
| 완전 Agent0-VL-7B | 59.4% | 72.9% | 87.3% | - |
| 자기수리 제거 | 57.5% | 71.6% | 86.1% | -2.5% |
| 도구 사용 제거 | 53.1% | 67.5% | 86.2% | -6.5% |
| SERC 제거 (SFT만) | 51.8% | 65.8% | 85.4% | -8.7% |

**주요 발견**: SERC 제거 시 **평균 8.7% 하락**으로 양방향 추론-평가 상호작용이 장기 자기진화의 핵심임을 입증[1]

***

### 5. 일반화 성능 향상에 대한 심층 분석

#### 5.1 일반화 성능 향상 메커니즘

Agent0-VL이 일반화 성능을 향상시키는 핵심 메커니즘은 다음과 같습니다:[2][1]

**1) 분포 정렬 메커니즘**: Solver와 Verifier 간의 공동 최적화가 추론과 평가 분포를 동기화합니다. 이는 **분포 외(OOD) 샘플**에 대한 견고성을 증가시킵니다. 기존 텍스트 기반 자기보상은 언어 지름길에 빠져 OOD 샘플에서 실패하는 반면, 도구 기반 검증은 객관적 증거를 기반으로 하여 일반화를 개선합니다.[1]

**2) 크로스 모달 기반**: 도구 호출(Python 실행, 기하학적 계산)은 이미지와 텍스트 양쪽의 증거를 통합합니다. 이는 모달리티별로 일관된 의사결정을 보장하여 모달리티 편향을 감소시킵니다.[2]

**3) 신뢰도 기반 선택적 학습**: 낮은 신뢰도 케이스만 수리를 트리거합니다($\text{conf}_t < c$). 이는 모델이 **높은 불확실성 영역**에서만 추가 학습하도록 유도하여 학습 효율성을 최적화합니다.[1]

**4) 다단계 피드백 신호**: 최종 결과 보상 외에도 단계별 프로세스 보상 $r_t^{\text{proc}}$을 사용하여 중간 추론 단계를 감독합니다. 이는 **스파스 보상 문제**를 완화하고 더 세밀한 학습 신호를 제공합니다.[1]

#### 5.2 작업 도메인별 일반화 특성

**수학적 추론**: MathVista 벤치마크에서 +7.8% 향상을 달성합니다. 도구 기반 계산이 기하학적 논리를 명시적으로 검증하여 **기하학적 추론의 일반화**를 개선합니다. 서로 다른 좌표계나 문제 형식에도 견고한 응답이 가능합니다.[1]

**시각 기반 질의**: HallusionBench에서 +7.9% 향상은 Verifier의 도구 기반 사실 확인이 **시각 환각을 크게 감소**시키기 때문입니다. 모델이 언어 지름길에 의존하지 않고 시각 증거를 명시적으로 검증합니다.[1]

**과학적 분석**: 차트 분석(ChartQA +3.8%)에서 도구 호출을 통해 차트의 좌표값을 직접 추출하여 수치 정확성을 보장합니다.[1]

#### 5.3 다중 반복을 통한 누적 일반화 개선

Agent0-VL은 반복 학습을 통해 **누적적 일반화 향상**을 달성합니다:[1]

- **반복 1**: +5.2% (기본 기법들의 초기 유효성)
- **반복 2**: +4.0% (더 깊은 정제)
- **반복 3**: +2.8% (점진적 개선)

수렴 곡선이 양수를 유지하는 것은 모델이 **수렴에 도달하지 않고** 계속 개선될 여지가 있음을 시사합니다.[1]

#### 5.4 프로세스 보상 모델로서의 일반화

Agent0-VL의 Verifier가 다른 VLM(Qwen2.5-VL-3B, 7B, 32B, InternVL)에 대해 **일관되게 +3.6% ~ +4.7% 향상**을 달성합니다. 이는:[1]

- **모델 크기 독립적**: 작은 3B 모델부터 큰 32B 모델까지 효과적
- **아키텍처 독립적**: 다양한 VLM 아키텍처에 적용 가능
- **작업 독립적**: 기하학, 차트, 시각적 과학 문제 모두에 유효

이는 **강력한 도메인 간 일반화**를 입증합니다.[2][1]

***

### 6. 모델의 한계 및 개선 방향

#### 6.1 현존 한계

**1) 도구 선택의 복잡성**: 현재 Agent0-VL은 도구 선택을 **명시적으로 최적화하지 않습니다**. 모든 검증 단계에서 도구를 호출하면 계산 비용이 증가합니다. 더 효율적인 도구 선택 메커니즘이 필요합니다.[1]

**2) 도메인 특화 도구의 부족**: 현재는 Python 실행, 기하학적 계산 등 기본 도구만 사용합니다. 화학, 생물학, 물리학 등 특화 도메인의 도구가 제한적입니다.[1]

**3) 계산 오버헤드**: 다중 반복 검증과 수리로 인한 계산 비용 증가. 반복 3까지 가면 리소스 소비가 상당합니다.[1]

**4) 신뢰도 임계값의 휴리스틱**: $c = 0.7$의 신뢰도 임계값이 모든 작업에 최적인지 불명확합니다. 작업별/도메인별 적응형 임계값 설정이 필요할 수 있습니다.[1]

#### 6.2 향후 연구 방향

**관련 최신 연구 기반 제안**:[3][4][5][2]

**1) 하이브리드 강화학습 프레임워크**: 최근 Skywork R1V2는 MPO와 GRPO를 결합한 하이브리드 RL을 제안했습니다. Agent0-VL도 다양한 RL 알고리즘을 결합하여 **장점 소실(vanishing advantages) 문제**를 완화할 수 있습니다.[6]

**2) 선택적 샘플 리플레이(SSR)**: VL-Rethinker 연구는 SSR을 통해 고가치 경험을 우선 재입력하여 GRPO의 훈련 안정성을 개선합니다. 이를 Agent0-VL에 적용하면 더 안정적인 자기진화가 가능합니다.[7]

**3) 강제 사고(Forced Rethinking)**: VL-Rethinker는 사고 트리거 추가로 깊은 사고 패턴을 유도합니다. Agent0-VL의 Verifier가 복잡한 문제에서 더 깊은 검증을 수행하도록 설계할 수 있습니다.[7]

**4) 스케일 능력 강화**: VISTA-Gym은 7개 작업의 13개 데이터셋을 통합하여 도구 통합 시각 추론을 규모 있게 훈련합니다. Agent0-VL은 더 큰 규모의 다양한 작업 데이터셋으로 훈련하여 일반화를 더욱 향상시킬 수 있습니다.[8]

**5) 도메인 특화 도구 통합**: 최근 AIDE(Agentic Improvement through Domain Experts) 프레임워크는 도메인 전문가 도구를 활용하여 VLM을 자동으로 개선합니다. Agent0-VL은 도메인별 전문가 도구를 통합하여 특정 분야에서의 성능을 극대화할 수 있습니다.[9]

**6) 적응형 도구 선택**: 강화학습을 통해 언제 어떤 도구를 호출할지 학습하는 메커니즘을 추가합니다. 이는 계산 오버헤드를 줄이면서 성능을 유지할 수 있습니다.[8]

**7) 크로스모달 일반화**: EvoVLA(자기진화 VLA 모델)는 시뮬레이션-실제 환경 전이에서 10.2%포인트 향상을 달성합니다. Agent0-VL도 크로스모달 작업(텍스트-이미지-액션)에서의 일반화를 개선할 수 있습니다.[10]

***

### 7. 연구의 영향 및 앞으로의 고려 사항

#### 7.1 학술 영향

Agent0-VL은 **자기진화 비전-언어 모델 연구의 새로운 패러다임**을 제시합니다:[2][1]

**패러다임 전환**:
- 기존: 외부 보상/인간 주석에 의존한 감독 학습
- 신규: 도구 기반 자기검증을 통한 자율 진화

이는 2025년의 주요 자기진화 VLM 연구들(VisPlay, Vision-SR1, EvoVLA, LACY, VL-Rethinker)에 큰 영향을 미쳤습니다.[11][12][3][7]

**방법론적 기여**:
- **도구 검증의 신뢰성**: 평가 환각 문제의 실질적 해결책 제시
- **분포 정렬**: Solver와 Verifier의 공동 최적화가 자기진화의 핵심
- **SERC 프레임워크**: 신뢰도 기반 선택적 학습 메커니즘의 도입

#### 7.2 실무 적용 전망

**단기 적용 (1-2년)**:
- **교육용 도구**: 기하학, 물리학 문제 풀이에 대한 설명 가능한 AI 튜터
- **과학 분석**: 실험 데이터 해석, 차트 분석
- **품질 검사**: 자동 검증 시스템에서 PRM으로 활용

**중기 적용 (2-5년)**:
- **로봇 공학**: 시각-언어-행동 통합 에이전트의 기초
- **지능형 문서 분석**: 도형, 표, 텍스트가 혼합된 복잡 문서 처리
- **멀티모달 의료 진단**: 이미지, 텍스트, 수치 데이터의 통합 분석

**장기 전망 (5년 이상)**:
- **일반 비전-언어 에이전트**: 외부 감독 없이 지속적으로 개선되는 AI 시스템
- **과학 발견**: 자기진화 에이전트가 새로운 물리/화학 법칙 발견 참여

#### 7.3 개발자/연구자 고려 사항

**1) 도메인별 맞춤화**:
- 특정 도메인(의료, 법률, 금융)의 특화 도구 개발 필요
- 도메인별 평가 메트릭 설정 필수

**2) 계산 효율성**:
- 다중 도구 호출의 비용 증가 관리
- 적응형 계산 예산 설정 (VISTA-Gym의 접근법 참고)[8]

**3) 안전성 및 설명 가능성**:
- 자기생성 보상의 편향 위험 모니터링
- 도구 검증의 결과가 항상 신뢰할 수 있는가?
- 자기수리 메커니즘의 오류 전파 방지

**4) 데이터 구성**:
- SFT 단계에서 고품질 도구-통합 궤적이 중요
- 다양한 난이도의 자동 생성 데이터셋 구축 필요

**5) 하이퍼파라미터 튜닝**:
- 신뢰도 임계값 $c$, 수리 페널티 $\gamma_{\text{repair}}$, KL 계수 등이 모두 성능에 영향
- 각 작업/도메인에 맞는 하이퍼파라미터 탐색 필수

#### 7.4 관련 연구와의 연계

**시너지 기회**:

1. **VISTA-Gym과의 통합**: VISTA-Gym의 다양한 시각 도구 라이브러리를 Agent0-VL에 통합하여 더욱 강력한 도구 생태계 구축[8]

2. **VL-Rethinker의 SSR 적용**: 선택적 샘플 리플레이를 Agent0-VL의 GRPO 훈련에 적용하여 안정성 향상[7]

3. **SEAgent의 자율 학습 패러다임**: 컴퓨터 사용 에이전트의 자율 진화 원리를 시각-언어 도메인에 확대[3]

4. **VisPlay의 역할 분리**: 질문자-응답자 역할 분리가 Agent0-VL의 Solver-Verifier 구조에 영감을 줌[13]

***

### 8. 종합 결론

**Agent0-VL**은 외부 감독 없이 도구 기반 자기 검증을 통해 비전-언어 모델의 **자율적 진화를 실현하는 획기적 프레임워크**입니다. 

**핵심 혁신**:
- ✓ 도구 통합을 검증 단계까지 확대하여 평가 신뢰성 혁신
- ✓ Solver-Verifier의 통합 구조로 추론과 평가 분포의 완전한 정렬
- ✓ 신뢰도 기반 선택적 학습으로 자기진화의 효율성 극대화
- ✓ 기하학, 차트, 과학 분석 등 다양한 도메인에서 평균 12.5% 성능 향상 달성

**일반화 성능 개선의 실제 메커니즘**:
- 분포 정렬을 통한 OOD 견고성 증가
- 크로스 모달 기반으로 모달리티 편향 감소
- 다단계 피드백 신호로 스파스 보상 문제 해결

**향후 방향**:
2025년의 관련 연구들(VISTA-Gym, VL-Rethinker, SEAgent, AIDE 등)과 결합하여 더욱 강력한 자기진화 에이전트 개발이 가능하며, 특히 도메인 특화 도구 통합, 적응형 도구 선택, 하이브리드 RL 알고리즘 적용이 주요 개선 방향입니다.

***

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4a760bfc-3d8a-4427-a3a8-78116020afde/2511.19900v2.pdf)
[2](https://www.semanticscholar.org/paper/ed71e484c8d368302e21c09e614d78a13e467ad6)
[3](https://arxiv.org/abs/2508.04700)
[4](https://www.semanticscholar.org/paper/b0d506ad403eae8286017402a74cea8687dce0a0)
[5](https://arxiv.org/abs/2506.23468)
[6](https://aiflower.tistory.com/210)
[7](https://arxiv.org/pdf/2504.08837.pdf)
[8](https://arxiv.org/abs/2511.19773)
[9](http://arxiv.org/pdf/2502.09051.pdf)
[10](https://www.semanticscholar.org/paper/108f12ee8575a97beb8c1bd5c3e5059d7452ece9)
[11](https://arxiv.org/abs/2508.19652)
[12](https://arxiv.org/abs/2511.02239)
[13](https://www.semanticscholar.org/paper/092fee5a7a9a55a2b60d68c6129923a3fd03d99a)
[14](https://ieeexplore.ieee.org/document/11094676/)
[15](https://arxiv.org/abs/2509.25787)
[16](http://arxiv.org/pdf/2501.02189.pdf)
[17](https://arxiv.org/pdf/2308.12966.pdf)
[18](https://arxiv.org/html/2503.04250v1)
[19](https://arxiv.org/html/2411.00828v1)
[20](https://arxiv.org/abs/2403.09027)
[21](https://arxiv.org/html/2502.13130v1)
[22](https://arxiv.org/html/2502.07949v1)
[23](https://huggingface.co/papers/2511.19900)
[24](https://papers.cool/arxiv/2511.19900)
[25](https://huggingface.co/learn/cookbook/fine_tuning_vlm_grpo_trl)
[26](https://arxiv.org/html/2511.19900v1)
[27](https://arxiv.org/abs/2511.19900)
[28](https://www.emergentmind.com/topics/agent0-vl)
[29](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl)
[30](https://github.com/aiming-lab/Agent0)
