# Deep Fragment Embeddings for Bidirectional Image–Sentence Mapping

**핵심 주장 및 주요 기여**  
이 논문은 이미지와 자연어 문장을 공동의 잠재 공간에 임베딩하여 양방향 검색(이미지→문장, 문장→이미지)을 수행할 때, 전체 이미지·문장 단위뿐 아니라 세부 단위(fragment)에서 상호 정렬을 학습함으로써 성능을 크게 향상시킨다.  
1. 이미지 내부의 객체 탐지(fragment)와 문장의 의존구문 관계(fragment)를 각각 임베딩하고,  
2. 전통적 글로벌 랭킹 손실(Global Ranking Objective) 외에 fragment 간 정렬 손실(Fragment Alignment Objective)을 도입해 상호 조절학습을 수행하며,  
3. 이를 통해 단순 전역 표현만 사용하던 기존 방법 대비 Recall@K 기준에서 최대 약 2배 가까운 성능 향상을 보였다.

***

## 1. 해결 과제  
- **양방향 검색 한계**: 기존 방법은 이미지 전체 또는 문장 전체를 고정 크기 벡터로만 임베딩하여 세부 정보(객체, 속성, 관계)를 놓치는 경우가 많음.  
- **확장성 문제**: 커널 CCA 등 비확장성 기법이 많아 큰 규모 데이터에 적용하기 어려움.  

## 2. 제안 방법  
### 2.1 모델 구조  
- 이미지 프래그먼트: RCNN으로 상위 19개 박스와 전경 이미지를 검출한 뒤, 각 박스 내부 픽셀을 CNN(4096차원)→ 선형 사상 $$W_m$$으로 임베딩.  
- 문장 프래그먼트: Stanford CoreNLP 의존구문 트리의 모든 관계 $$(R, w_1, w_2)$$를  

$$
    s = f\bigl(W_R [\,W_e w_1;\,W_e w_2\,] + b_R\bigr),\quad
    f(x)=\max(0,x)
  $$  
  
  형태로 임베딩($$W_e$$는 사전 학습된 단어 임베딩).  
- 최종 이미지–문장 유사도: 모든 프래그먼트 페어의 양수 내적 값을 평균한 뒤 thresholding  

$$
    S_{kl} = \frac{1}{|g_k|(|g_l|+n)} \sum_{i\in g_k}\sum_{j\in g_l} \max(0,\,v_i^\top s_j),
  $$  
  
  여기서 $$n$$은 문장 길이 편향 제거용 스무딩 상수.

### 2.2 학습 손실  
1. **Fragment Alignment Objective**  
   - MIL(multiple-instance learning) 기반으로, 문장 프래그먼트 $$j$$에 대해 해당 이미지 내 객체 중 최소 하나를 양성으로, 타 이미지 객체는 모두 음성으로 학습:  

$$
       C_F(\theta) = \min_{\{y_{ij}\}}
       \sum_{i,j} \kappa_{ij} \max\bigl(0,\,1 - y_{ij} v_i^\top s_j\bigr)
     $$
     
  $$\sum_{i\in p_j}\frac{y_{ij}+1}{2}\ge1,\;y_{ij}\in\{\pm1\}$$

2. **Global Ranking Objective**  
   - 올바른 페어 $$(k,k)$$의 유사도가 다른 모든 페어 $$(k,l)$$, $$(l,k)$$보다 마진 $$\Delta$$만큼 크도록 강제:

$$
       C_G(\theta)=\sum_k\sum_{l\neq k}\Bigl[\max(0,\,S_{kl}-S_{kk}+\Delta)+\max(0,\,S_{lk}-S_{kk}+\Delta)\Bigr].
     $$

3. **정규화**: $$C(\theta)=C_F(\theta)+\beta\,C_G(\theta)+\alpha\|\theta\|_2^2$$.

### 2.3 최적화  
- 초기 10 에폭: fragment alignment $$C_0$$만, CNN 고정  
- 이후 MIL objective $$C_F$$ + global $$C_G$$ + CNN 미세조정  
- SGD(모멘텀 0.9), 배치 크기 100, 총 15 에폭

***

## 3. 성능 향상 및 한계  
| 데이터셋 | 이미지→문장 R@1 | 문장→이미지 R@1 |
|-----------|-----------------|-----------------|
| Pascal1K  | 39.0% → 23.6%[†] | 23.6% → 39.0%[†] |
| Flickr8K  | 12.6% → 9.7%    | 9.7% → 12.6%    |
| Flickr30K | 14.2% → 10.2%   | 10.2% → 14.2%   |

† MIL 및 CNN 미세조정 포함.  
- **프래그먼트 정렬의 효과**: 글로벌 목표만 사용 시 대비 R@1에서 최대 2배 향상.  
- **객체 검출 중요성**: 전체 이미지 표현만 사용할 때 성능 큰 하락.  
- **의존구문 기반 문장 단위**: 단어 수준(BOW)·바이그램 대비 일관된 이득.  
- **제한점**:  
  - 수량 표현(“세 명”), 공간 관계, 복합 구문(“black and white dog”) 처리 미흡  
  - RCNN NMS 오류로 인한 중복/누락 객체  
  - 프래그먼트 간 순서·위치 정보 무시

***

## 4. 일반화 성능 향상 가능성  
- **MIL 손실의 확장**: 더 정교한 집합 학습(formulation)으로 희소한 속성이나 멀티토큰 개념(예: “each other”) 포착 가능  
- **공간 정보 통합**: 박스 좌표·관계 그래프 이용 시 물체 간 위치적 제약 학습  
- **수량·관계 학습**: 집계 레이어나 수치 비교 모듈 추가로 “세 개” 등 처리  
- **언어 구조 심화**: 의존구문 넘어 구·절 단위 또는 순서 정보 반영

***

## 5. 향후 연구 방향 및 고려 사항  
- **하위 구조화된 프래그먼트**: 명사구·전치사구 단위로 분해하여 복합 표현 강화  
- **다중 속성·관계 모델링**: 그래프 신경망(GNN)으로 프래그먼트 간 상호작용 학습  
- **제로샷 일반화**: 학습에 없던 객체·속성 조합 예측 성능 평가 및 메타러닝 기법 적용  
- **효율화·확장성**: 대규모 웹 이미지·문장 코퍼스에 적용 가능한 경량화 아키텍처  
- **다중 모달 후처리**: 생성 모델(Captioning, Visual QA)에 임베딩 재활용 연구

이 논문은 이미지–문장 상호 검색 분야에서 세부 단위 정렬 학습의 중요성을 입증했으며, 이후 멀티모달 이해 및 생성, 제로샷 학습, 관계 추론 연구에 큰 영향을 줄 것으로 기대된다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d6af3d8d-32ad-4cb9-9efe-b15d3c0f31d3/1406.5679v1.pdf
