# LLAVA-1.5 : Improved Baselines with Visual Instruction Tuning

## 1. 핵심 주장과 주요 기여  
이 논문은 **LLaVA** 프레임워크를 기반으로 하는 대규모 다중모달 모델(LMM)의 설계 선택을 체계적으로 비교·분석하여, 다음과 같은 핵심 기여를 제시한다:[1]
- **MLP 기반 비전-언어 커넥터**: 기존의 선형 투영 대신 2-layer MLP를 도입하여 표현력을 강화.  
- **학술용 VQA 데이터 활용**: VQA-v2, OKVQA, A-OKVQA, OCRVQA, TextCaps 등 학술 과제지향 데이터 추가로 학습의 다중작업 균형 달성.  
- **응답 형식 프롬프트**: “Answer using a single word or phrase” 등 명시적 포맷 지시어로 단답형/장문형 답변 균형 조절.  
- **고해상도 입력 확장**: 336px→448px 그리드 분할-인코딩 방식으로 임의 해상도 지원(LLaVA-1.5-HD).  
- **데이터 효율성**: 단 1.2M 공개 데이터로 11개 벤치마크에서 SOTA 달성, 8×A100 한 노드에서 하루 만에 완전 학습.[1]

## 2. 문제 정의  
기존 LMM들은  
1) **자연 대화형 추론**: LLaVA는 대화형 비주얼 QA에 강하지만,  
2) **단답형 VQA**: InstructBLIP은 단답형 VQA에 강하나 자연 대화 능력 저하  
라는 **멀티태스크 학습 불균형** 문제를 보였다.[1]

## 3. 제안하는 방법  
### 3.1 비전-언어 커넥터  
- 기존: 선형 투영 $$f_{\rm lin}(V)=W V + b$$  
- 제안: 2-layer MLP  

$$
    f_{\rm MLP}(V) = W_2 \,\sigma\bigl(W_1 V + b_1\bigr) + b_2
  $$  
  
  이를 통해 시각 피처의 비선형 정합력 강화.[1]

### 3.2 응답 형식 프롬프트  
- VQA 질문 말미에 “Answer the question using a single word or phrase.” 추가  
- 명시적으로 단답/장문 지시어 제공으로 모델이 출력 형식 조절 가능.  

### 3.3 데이터 구성  
- LLaVA 원본 데이터(158K) + ShareGPT 텍스트 대화(40K)  
- VQA-v2(83K), GQA(72K), OKVQA(9K), A-OKVQA(66K), OCRVQA(80K), TextCaps(22K)  
- RefCOCO/Visual Genome 등 영역별 QA 포함, 총 665K 대화 샘플.[1]

### 3.4 고해상도 입력 처리 (LLaVA-1.5-HD)  
1. 이미지를 $$224^2$$ 크기 패치로 분할  
2. 각 패치 개별 인코딩 후 다시 병합  
3. 전역 문맥 확보를 위해 다운샘플된 전체 이미지 피처 추가 연결  
4. 가변 해상도 지원, 추가 positional interpolation 불필요.[1]

## 4. 모델 구조  
```text
Input Image → CLIP-ViT-L-336px → flatten → MLP connector → LLaMA-based LLM → Output
```
- 고해상도 버전: 이미지 분할 → 병합 → 전역 문맥 피처 concat → LLM.[1]

## 5. 성능 향상  
| 벤치마크    | LLaVA-7B | LLaVA-1.5-7B | 향상폭                |
|-----------|----------|--------------|---------------------|
| MME       | 809.6    | 1510.7       | +701.1             |
| GQA       | –        | 62.0         | –                  |
| MM-Vet    | 25.5     | 31.1         | +5.6               |
| 평균      | –        | SOTA (12 task) | 대다수 지표 5–100pt↑ |

- **데이터 효율성**: 전체 데이터의 50%만 사용해도 성능 98% 이상 유지.[1]
- **고해상도 입력**: 448px 입력 시 OCR 등 세부 인지 과제에서 추가 성능↑.[1]

## 6. 한계  
- **학습 속도**: 고해상도 입력 시 사전 인코딩 비용 증가, iteration 당 시간 두 배.  
- **다중 이미지 미지원**: 단일 이미지 컨텍스트만 처리.  
- **특정 도메인 문제 해결력**: 여전히 제한적, 고급 사고력/전문 지식 응용 과제에서 미흡.  
- **잠재적 환각**: 여전히 허위 정보 생성 위험, 특히 세부 묘사 시 발생 가능.[1]

## 7. 일반화 성능 향상 관점  
- **명시적 응답 포맷 지시**가 과제별 출력 요구사항(단답·장문·JSON·무답변)으로의 적응력 증가.  
- **학술용 VQA 데이터** 추가로 단답형 과제 일반화 강화.  
- **ShareGPT 텍스트 대화**로 영어 외 다국어 지시어에도 적응, 중국어 MMBench-CN에서 +7.3%↑.[1]
- **데이터 절반 샘플링** 실험으로 과적합 억제 및 일반화 유지 가능성 확인.  

## 8. 미래 연구에의 영향 및 고려 사항  
- **단순 구조의 힘**: 복잡한 비전 샘플러 대신 MLP 커넥터와 응답 프롬프트 만으로 SOTA 달성.  
- **데이터 효율적 설계**: 과적합 없는 압축 샘플링 전략, “less-is-more” 접근 연구 필요.  
- **고해상도 확장성**: 패치 분할-병합 방식 일반화, 멀티이미지·비디오 지원으로 확대 가능성.  
- **환각 억제**: 고해상도 입력→환각 감소 관찰, 멀티그리드 문맥 통합 연구로 신뢰성 향상 모색.  
- **다국어·다도메인**: 영어 기반 다국어 적응력 한계 극복, 전문 도메인(의료·법률) 튜닝 데이터 확보 중요.  

앞으로, **단순·효율·일반화**를 핵심으로 하는 LMM 설계 패러다임이 자리잡을 전망이며, 데이터·구조·학습 기법의 최적 균형을 찾는 연구가 요구된다.  

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d62f5799-2c8b-4a80-bc31-51de5f876ea6/2310.03744v2.pdf
