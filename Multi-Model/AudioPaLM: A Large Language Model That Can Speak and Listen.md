# AudioPaLM: A Large Language Model That Can Speak and Listen

### 1. 핵심 주장 및 주요 기여 요약

**AudioPaLM**은 Google에서 개발한 통합 음성-텍스트 대규모 언어 모델(LLM)로, 텍스트 기반 모델(PaLM-2)과 음성 기반 모델(AudioLM)을 융합한 멀티모달 아키텍처입니다. 이 모델의 핵심 주장은 **통합된 음성-텍스트 어휘를 통해 단일 모델로 다양한 음성 및 텍스트 작업을 동시에 수행**할 수 있다는 것입니다.[1]

주요 기여는 다음과 같습니다:[1]

- 텍스트 기반 사전학습의 지식을 활용하여 음성 처리 능력을 향상시킨 통합 음성-텍스트 LLM 개발
- 자동 음성 인식(ASR), 자동 음성 번역(AST), 음성 간 번역(S2ST)을 포함한 다양한 작업에서 최첨단 성능 달성
- 음성 화자 전이(voice transfer)를 지원하는 S2ST 시스템으로 기존 방식을 능가하는 음성 품질 및 음성 보존 달성
- 훈련 중 보이지 않은 언어 쌍에 대한 영점학습(zero-shot) 음성-텍스트 번역 능력 시연

***

### 2. 문제 정의 및 해결 방법

#### 해결하고자 하는 문제

기존의 음성 처리 시스템은 다음과 같은 한계를 가지고 있었습니다:[1]

1. **모듈식 한계**: 음성 인식, 번역, 생성 등이 각각 별도의 모델로 구성되어 모듈 간 정보 손실 발생
2. **캐스케이드 방식의 한계**: ASR → MT → TTS 파이프라인에서 오류 누적 및 부자연스러운 결과 생성
3. **병렬 언어 정보 손실**: 기존 음성 인식 시스템은 화자 신원, 감정 등 비언어적 정보 손상
4. **언어별 모델 필요성**: 각 언어 쌍마다 별도의 모델 학습 필요

#### 제안하는 해결 방법

**통합 어휘 기반 멀티모달 접근**:[1]

AudioPaLM은 **텍스트 토큰과 음성 토큰을 통합한 단일 어휘**를 구성하여, 이를 Transformer 디코더에 입력함으로써 문제를 해결합니다. 이는 다음 수식으로 표현됩니다:[1]

$$\text{Vocabulary} = \text{TextTokens} \cup \text{AudioTokens}$$

여기서 텍스트 토큰은 SentencePiece로 처리되고, 음악 토큰은 w2v-BERT, USM-v1, 또는 USM-v2 인코더에서 추출됩니다.[1]

**임베딩 행렬 확장**:[1]

기존 텍스트 기반 모델의 임베딩 행렬 $E$를 확장하는 방식으로 구현됩니다:

$$E_{\text{expanded}} = \begin{pmatrix} E_{\text{text}} \\ E_{\text{audio}} \end{pmatrix}$$

여기서 $E_{\text{text}}$는 기존 PaLM-2에서 유지되고, $E_{\text{audio}}$는 새로이 초기화되어 학습됩니다.[1]

**과제 태깅 시스템**:[1]

모델이 수행할 작업을 명시하기 위해 다음과 같은 간단한 텍스트 태그를 사용합니다:

- ASR(자동 음성 인식): `[ASR French]`
- AST(자동 음성 번역): `[AST English French]`
- S2ST(음성 간 번역): `[S2ST English French]`
- 결합 과제: `[ASR AST S2ST English French]`

이러한 과제 정의는 모델이 다중 작업 학습을 통해 각 작업 간의 시너지를 활용하게 합니다.[1]

***

### 3. 모델 구조 및 아키텍처

#### 전체 구조 개요

AudioPaLM의 아키텍처는 **디코더 전용(decoder-only) Transformer** 기반으로 설계되었습니다. Figure 1에 명시된 바와 같이, 이는 다음 단계로 구성됩니다:[1]

1. **음성 토큰화(Audio Tokenization)**
2. **텍스트-음성 임베딩 확장**
3. **Transformer 디코더 처리**
4. **음성 재합성(Audio Resynthesis)**

#### 음성 토큰화 메커니즘

AudioPaLM은 세 가지 음성 토큰화 방식을 실험하였습니다:[1]

**w2v-BERT 토크나이저**:[1]
- w2v-BERT 다국어 모델에서 임베딩 추출
- k-means 클러스터링을 통해 1024개의 의미적 토큰(semantic tokens) 생성
- 처리 속도: 25Hz (시간 해상도 0.04초)

**USM-v1 (Universal Speech Model)**:[1]
- 2B 파라미터 다국어 음성 인코더 사용
- 중간층에서 임베딩 추출
- 동일한 1024 토큰 어휘 크기

**USM-v2 (개선된 USM)**:[1]
- 보조 ASR 손실함수로 미세조정
- 다국어 성능 우수
- 실험 결과 가장 우수한 성능 달성

음성 토큰 추출의 수학적 표현:[1]

$$s_t = \text{quantize}(\text{AudioEncoder}(x_t))$$

여기서 $x_t$는 음성 신호, $\text{AudioEncoder}$는 사전학습된 음성 인코더이고, quantize는 k-means를 통한 양자화 함수입니다.[1]

#### 핵심 모듈: 임베딩 행렬 확장

원래 PaLM-2의 임베딩 행렬을 수정하는 방식을 사용합니다:[1]

$$E' = \begin{pmatrix} E_{\text{text}} \in \mathbb{R}^{t \times m} \\ E_{\text{audio}} \in \mathbb{R}^{a \times m} \end{pmatrix} \in \mathbb{R}^{(t+a) \times m}$$

여기서:
- $t$: 텍스트 토큰 수 (~32,000)
- $a$: 음성 토큰 수 (1,024)
- $m$: 임베딩 차원 (기본값: 모델 크기에 따라 변함)

이 설계의 장점은 **최소한의 아키텍처 수정**으로 멀티모달 기능을 추가할 수 있다는 것입니다.[1]

#### 연쇄 작업(Combined Tasks) 처리

복잡한 작업을 단계적으로 분해하여 모델이 중간 결과에 주목하도록 유도합니다:[1]

예를 들어, S2ST를 위해:

$$\text{Input} \to \text{[ASR English]} \to \text{English text} \to \text{[AST French]} \to \text{French text} \to \text{[TTS French]} \to \text{French audio}$$

각 단계에서 모델은 이전 모든 토큰에 접근 가능하여, 입력 음성과 중간 처리 결과를 모두 활용합니다.[1]

#### 음성 재합성

생성된 음성 토큰을 원본 음성으로 변환하는 두 가지 방식:[1]

**AudioLM 방식 (자기회귀)**:[1]
- Stage 2: 의미적 토큰(semantic tokens)을 저수준 SoundStream 토큰으로 변환
- Stage 3: 높은 비트레이트의 음성 신호 재구성

**SoundStorm 방식 (비자기회귀)**:[1]
- 병렬 처리로 2배 빠른 속도
- 더 높은 일관성과 음성 품질

***

### 4. 성능 향상 및 실험 결과

#### 주요 성능 지표

**표 2. 최상위 성능 결과**:[1]

| 모델 | CoVoST2 AST BLEU↑ | CVSS S2ST ASR-BLEU↑ | VoxPopuli ASR WER↓ |
|------|------------------|---------------------|-------------------|
| Whisper Large-v2 1.5B | 29.1 | − | 13.6 |
| mSLAM-CTC 2B | 25.2 | − | 9.1 |
| MAESTRO 600M | 25.2 | − | 8.1 |
| **AudioPaLM 8B AST** | **35.4** | − | 11.1 |
| **AudioPaLM 8B S2ST** | 36.2 | 32.5 | 16.0 |
| **AudioPaLM-2 8B AST** | **37.8** | − | 9.8 |

AudioPaLM-2는 CoVoST2 AST에서 37.8 BLEU를 달성하여 기존 최고 성능(Whisper: 29.1)을 **30% 이상 초과**합니다.[1]

#### 영점학습(Zero-shot) 성능

**표 3. FLEURS 데이터셋의 영점학습 성능**:[1]

AudioPaLM-2는 AST 데이터를 보지 못한 26개 언어(ASR-observed)에서 **20.7 BLEU**를 달성하여, Whisper(19.6 BLEU)를 능가합니다. 이는 다음을 의미합니다:[1]

- 언어 쌍 $(x, y)$에 대해 AST 데이터 없이도 번역 가능
- 텍스트 모델의 번역 능력이 음성 도메인으로 전이됨
- 개선도: AST-observed 언어에서 **28% 향상**, ASR-observed에서 **107% 향상**

#### 음성 품질 평가

**표 4. 객관적 및 주관적 음성 품질 평가**:[1]

| 지표 | CVSS-T | Translatotron 2 | AudioPaLM |
|-----|--------|-----------------|-----------|
| 객관적 MOS | 3.41 | 3.36 | **3.65** |
| 주관적 MOS | 3.88 | 3.96 | **4.44** |
| 음성 유사도(객관) | 0.24 | 0.18 | **0.40** |
| 음성 유사도(주관) | 3.70 | 3.51 | **4.00** |
| 음향 일관성 | 0.54 | 0.44 | **0.81** |

AudioPaLM은 Translatotron 2보다 모든 메트릭에서 우수하며, 심지어 지상 진실(ground truth) CVSS-T 데이터보다 좋은 성능을 보입니다.[1]

#### 주요 개선 요인 분석

**실험 5.4.2: 사전학습 체크포인트의 영향**:[1]

| 초기 상태 | CoVoST2 AST BLEU↑ | CoVoST2 ASR WER↓ |
|---------|------------------|------------------|
| PaLM 1B from scratch | 6.5 | 66.0 |
| PaLM 8B from scratch | 6.9 | 63.3 |
| **PaLM 8B finetuned** | **18.4** | **40.2** |

사전학습된 텍스트 모델 초기화가 처음부터 학습하는 것보다 **2.7배 우수**한 성능을 제공합니다.[1]

**실험 5.4.3: 토큰화 방식의 영향**:[1]

| 토크나이저 | CoVoST2 AST BLEU↑ | CoVoST2 ASR WER↓ |
|----------|------------------|------------------|
| w2v-BERT | 15.2 | 50.1 |
| USM-v1 | 18.5 | 40.2 |
| **USM-v2** | **26.9** | **22.3** |

USM-v2는 w2v-BERT 대비 **77% BLEU 향상**을 달성합니다.[1]

**실험 5.4.4: 연쇄 작업의 영향**:[1]

| 설정 | CoVoST2 AST BLEU↑ | 향상도 |
|-----|------------------|-------|
| 직접 작업 | 18.5 (USM-v1) / 26.9 (USM-v2) | − |
| **연쇄 작업** | **22.1 (USM-v1) / 30.5 (USM-v2)** | **+3.6 / +3.6** |

연쇄 작업 방식(ASR → AST → TTS)이 직접 변환보다 일관되게 더 나은 성능을 제공합니다.[1]

**실험 5.4.6: 데이터 스케일의 영향**:[1]

CoVoST2 (0.9k 시간) → 공개 데이터셋 (13.5k 시간) → 공개 + YouTube (20.6k 시간) → 공개 + YouTube + WMT/TED (40.8k 시간) → 공개 + PaLM MT TTS (57.1k 시간)로 진행함에 따라:

- CoVoST2 AST: 30.5 BLEU → 35.4 BLEU (향상도: +16%)
- VoxPopuli ASR: 25.3 WER → 11.1 WER (향상도: 56% 감소)

**실험 5.4.9: 모델 크기의 영향**:[1]

| 모델 크기 | VoxPopuli ASR WER↓ | CoVoST2 ASR WER↓ | CVSS AST BLEU↑ |
|---------|-------------------|------------------|-----------------|
| 128M | 15.9 | 30.2 | 16.6 |
| 1B | 11.9 | 21.5 | 30.4 |
| **8B** | **9.7** | **17.4** | **37.2** |

128M에서 8B로 확장 시 **42% WER 감소** 및 **123% BLEU 향상**을 달성합니다.[1]

***

### 5. 모델의 일반화 성능 향상 가능성 (중점 분석)

#### 5.1 텍스트 사전학습의 일반화 전이

AudioPaLM의 가장 혁신적인 발견은 **텍스트 모델의 언어적 지식이 음성 도메인으로 자동 전이**된다는 점입니다.[1]

**증거 1: 영점학습 번역**:[1]
- AST 데이터를 보지 못한 26개 언어에서 20.7 BLEU 달성
- 이는 PaLM-2의 기본 번역 능력이 음성 입력에서도 작동함을 의미
- AudioPaLM-2 (PaLM-2 기반)가 AudioPaLM (PaLM 기반)보다 28% 더 우수한 성능 (AST-observed: 28.6 vs 22.4 BLEU)

$$\text{일반화 능력} = f(\text{TextLLM}_{\text{translation}} \to \text{Audio}_{\text{input}})$$

**증거 2: 모델 크기에 따른 스케일링 법칙**:[1]

모델이 클수록 일반화 성능이 향상됩니다:

$$\text{Performance} \propto \text{ModelSize}^{\alpha}$$

여기서 $\alpha \approx 0.3$ (실험에서 128M→8B로 3배 증가 시 성능 향상 배수: WER 약 1.64배, BLEU 약 2.23배).[1]

#### 5.2 다중 작업 학습의 시너지

**상호작용 효과 분석**:[1]

| 학습 설정 | CoVoST2 AST BLEU | 효과 |
|---------|-----------------|-----|
| AST only | 16.0 | 기본 | 
| AST + ASR | 18.5 | +15.6% |
| AST + ASR + 연쇄과제 | 22.1 | **+38.1%** |
| + S2ST 작업 추가 | 27.8 | +76% (전체) |

다중 작업 학습이 **단일 작업 학습 대비 76% 성능 향상**을 제공합니다. 이는 다음 효과에서 비롯됩니다:[1]

1. **청각-의미 매핑 강화**: ASR 과제가 음성 토큰을 텍스트 의미와 연결
2. **문맥 이해 확대**: 동일 데이터셋의 여러 과제가 숨겨진 구조 발견
3. **중간 표현 안정화**: 연쇄 과제가 각 단계별 최적화

#### 5.3 데이터 효율성과 일반화

**약한 감시 하에서의 일반화**:[1]

YouTube ASR 데이터셋 추가로 인한 성능 변화:
- 공개 데이터만: 33.1 BLEU
- + YouTube (자동 전사): 34.8 BLEU (**+5.1%**)

이는 모델이 자동 생성된 약한 라벨도 활용하여 일반화할 수 있음을 보여줍니다. WMT/TED TTS로 합성된 음성 데이터 추가 시:
- 35.4 BLEU (**+6.9%**)

$$\text{일반화력} = \propto \text{DataQuantity} + \text{DataDiversity} + \text{WeakSupervision}$$

#### 5.4 특정 언어에 대한 일반화 한계와 극복

**고자원 vs 저자원 언어 성능**:[1]

부록 D의 상세 결과에서:
- 고자원 언어 (프랑스어, 독일어, 스페인어): 44.8, 43.4, 44.2 BLEU
- 저자원 언어 (웨일스어, 타밀어, 몽골어): 13.7, 9.0, 7.6 BLEU

**해결 방안**:[1]
1. 과제 태그에 언어명 명시 (일반적 태그 vs 구체적 언어명)
2. 다국어 음성 인코더(USM) 사용
3. 다양한 언어 조합의 데이터셋 혼합

#### 5.5 신경망 토큰화의 한계와 일반화 영향

**임계점 발견** (실험 5.4.3):[1]

토큰화 방식이 일반화에 결정적 영향:

```math
\text{ASR WER} = \begin{cases} 50.1\% & \text{(w2v-BERT)} \\ 40.2\% & \text{(USM-v1)} \\ 22.3\% & \text{(USM-v2)} \end{cases}
```

**원인 분석**:[1]
- w2v-BERT: 정규화로 인한 화자 정보 손실 (다국어 설정에서 특히 악영향)
- USM-v1: 개선된 다국어 표현 제공
- USM-v2: 보조 ASR 손실로 의미적 구조 강화

저질의 토큰화는 아무리 좋은 모델이라도 **상한선(ceiling) 제약**이 되어, 일반화 성능을 근본적으로 제한합니다.[1]

#### 5.6 사전학습 전이의 한계

**발견: 전체 모델 미세조정 필수**:[1]

논문의 제한사항에서 명시:
> "Unlike Flamingo-like approaches which freeze most weights, we empirically found it necessary to finetune the whole model."

이는 다음을 의미합니다:[1]
- 음성 임베딩 추가만으로는 충분하지 않음
- 기존 텍스트 가중치도 재학습 필요
- **원래 모델 성능 보존 불가**: 사전학습된 텍스트 능력이 부분적으로 손상될 수 있음

$$\text{Full fine-tuning} \Rightarrow \text{Higher performance but Risk of catastrophic forgetting}$$

***

### 6. 한계(Limitations)

#### 6.1 토큰화 의존성

논문에서 명시된 주요 한계:[1]

> "The fact that our model can natively produce audio is a consequence of using tokenized audio. This introduces a strong dependency on the quality of the audio tokenizer."

**구체적 문제**:[1]
- 음성 토큰화 품질이 최종 음성 품질 상한선 결정
- 토큰화 손실(quantization loss)로 인한 정보 손실
- 서로 다른 토큰화 방식 간 호환성 부족

실험 결과에서 토큰화 변경만으로 **WER이 50.1%에서 22.3%로 55% 개선**될 정도로 영향이 큽니다.[1]

#### 6.2 모델 전체 미세조정의 필요성

**일반화 성능과의 트레이드오프**:[1]
- 텍스트만 처리하는 기존 PaLM-2 능력이 부분적으로 손상될 가능성
- Flamingo 등 다른 멀티모달 모델들과 달리 선택적 동결(selective freezing) 불가
- 계산 비용 증가

#### 6.3 벤치마크의 한계

**음성 작업 평가의 미성숙**:[1]

논문의 개방형 질문 섹션에서:
> "Compared to text, the richness of benchmarks for generative text/audio tasks is less developed."

**구체적 문제**:[1]
- 음성 번역, 합성 평가 메트릭 부족
- 주관적 평가의 일관성 문제
- 예리한 오디오 거짓(deepfake) 탐지 벤치마크 없음

***

### 7. 향후 연구에 미칠 영향 및 최신 동향 (2024-2025)

#### 7.1 음성 언어 모델 시대의 개막

최신 조사에 따르면, 음성 LLM 개발이 급진전하고 있습니다:[2]

**최신 모델들의 등장**:[2]
- **Roadmap 논문 (2024.10)**: 5단계 음성 LLM 발전 로드맵 제시 (기본 ASR → 초인간적 음성 이해)[2]
- **Recent Advances Survey (2025.02)**: 음성 언어 모델 최신 동향 종합[3]
- **FunAudioLLM (2024.07)**: SenseVoice와 CosyVoice로 음성 이해와 생성 통합[4]
- **ESPnet-SpeechLM (2025.02)**: 음성 언어 모델 개발 표준 도구 제시[5]

AudioPaLM이 개척한 **텍스트-음성 통합 패러다임**이 업계 표준으로 자리잡았습니다.[2]

#### 7.2 멀티모달 LLM의 일반화 능력 향상

**GPT-4o의 등장 (2024.05)**:[6]
- 음성, 이미지, 텍스트를 네이티브로 처리
- AudioPaLM보다 더욱 정교한 모달리티 간 상호작용 구현
- 음성 감정 인식, 음향 환경 분석 등 확장 기능

**기술 진화의 가속화**:[7]
> "음성, 제스처, 시선 등 다양한 입력 방식을 통합하여 훨씬 자연스럽고 직관적인 상호작용 경험 제공"

#### 7.3 영점 학습 능력의 개선

AudioPaLM의 영점 번역 능력이 후속 연구들의 기준점이 되었습니다:[3][2]

**개선 방향**:[3]
- 파이프라인 대비 end-to-end 직접 변환 성능 향상
- 저자원 언어에 대한 영점 성능 개선
- 도메인 외 데이터에 대한 강건성 강화

#### 7.4 음성 토큰화의 표준화

AudioPaLM이 드러낸 **"토큰화 품질 = 성능 상한선"** 문제가 새로운 연구 방향을 제시했습니다:[1]

**후속 연구 요구사항**:[1]
1. 토큰화 품질 측정 방법론 개발
2. 손실 없는 또는 저손실 음성 토큰화 기법
3. 다중 해상도 토큰화 (계층적 접근)

$$\text{음성품질} = \min(\text{모델성능}, \text{토큰화품질})$$

#### 7.5 합성 데이터 활용의 확대

AudioPaLM이 시연한 **TTS 기반 합성 데이터 생성** 방식이 표준화되었습니다:[1]

**최신 응용**:[5][4]
- PaLM MT TTS: 기존 모델로 합성 데이터 생성하여 새 모델 학습
- Qwen-Audio: 30+ 과제 지원하기 위해 광범위한 합성 데이터 구성[8]
- Audio Dialogue 모델: 대화식 음성 상호작용을 위한 합성 대화 데이터[9]

#### 7.6 음성 깊은 위조(Deepfake) 및 검증 문제

최신 우려사항:[10]
- 고품질 음성 합성 기술의 악용 가능성
- TraceableSpeech: 음성 합성 워터마킹 기술 개발
- "프레임 단위 각인" 기술로 편집 공격 방어

**연구 필요성**:[10]
> "음성이 생성되는 단계에서부터 의도적으로 식별 가능한 흔적을 남겨, 출처를 역추적하는 선제적 추적(Proactive Traceability) 기술"

#### 7.7 언어별 성능 격차 해소

**현재 상황**:[1]
- 고자원 언어: 44.8 BLEU (프랑스어)
- 저자원 언어: 7.6 BLEU (몽골어)
- **격차: 약 5.9배**

**해결 방향**:[3][2]
1. 다국어 사전학습 강화
2. 저자원 언어 데이터 수집 및 증강
3. 매개변수 효율적 미세조정(LoRA 등)
4. 다양한 어족 간 전이 학습

#### 7.8 일반화 능력 평가 벤치마크

**새로운 평가 도구 개발**:[11][9]
- **AudioBench (2024.11)**: 8개 작업, 26개 데이터셋으로 음성 LLM 평가[11]
- **Audio Dialogue Benchmark (2024.12)**: 개방형 음성 대화 이해도 평가[9]
- **SAGI Benchmark (2024.10)**: 음성 이해 일반화 능력 측정[2]

이들은 AudioPaLM의 한계(벤치마크 부족)를 직접 해결하고 있습니다.[1]

***

### 8. 향후 연구 시 고려할 점

#### 8.1 토큰화 방법론의 우선순위 재정립

**추천 사항**:
1. 손실 최소화 토큰화 개발 (예: 확률적 양자화 대신 결정론적 방식)
2. 음성 특성별 계층적 토큰화 (의미 ↔ 음향 정보 분리)
3. 토큰화-모델 공동 설계 (end-to-end 최적화)

#### 8.2 선택적 미세조정 메커니즘

**기술 방향**:
- Adapter 기반 접근: 텍스트 가중치 동결 후 소규모 어댑터 추가
- LoRA (Low-Rank Adaptation): 기본 모델은 유지하고 저랭크 업데이트
- 모듈 동결 전략: 특정 레이어만 재학습

#### 8.3 다국어 사전학습 강화

**전략**:
1. 비균형 데이터 처리: 저자원 언어 과샘플링
2. 언어족(language family) 기반 클러스터링 학습
3. 자가-감시 학습(self-supervised) 확대: 더 많은 비라벨 다국어 음성 활용

#### 8.4 도메인 적응 연구

**미해결 과제**:
- 소음이 많은 환경에서의 성능 유지
- 의료, 법률 등 특수 도메인 용어 처리
- 악센트, 방언 등 음성 변동성 처리

#### 8.5 효율성과 일반화의 균형

**트레이드오프 분석 필요**:
- 모델 크기 vs 일반화 성능
- 학습 데이터량 vs 추론 비용
- 정확도 vs 실시간 처리 능력

#### 8.6 윤리 및 안전성

**필수 고려사항**:[10]
1. 음성 딥페이크 탐지 및 검증 메커니즘
2. 개인정보 보호: 화자 신원 정보 활용 제한
3. 편향성 평가: 악센트, 성별, 나이별 성능 분석
4. 투명성: 모델의 의사결정 과정 해석

#### 8.7 평가 메트릭 표준화

**요구 사항**:
- 음성 번역: BLEU 외 다양한 메트릭 (CIDEr, METEOR 등)
- 음성 품질: 주관적 평가의 일관성 향상
- 일반화 평가: 보이지 않은 도메인/언어에 대한 표준 테스트 셋

***

### 결론

**AudioPaLM**은 음성과 텍스트를 통합하는 멀티모달 LLM의 획기적 사례입니다. 텍스트 기반 대규모 사전학습이 음성 도메인에 전이될 수 있음을 최초로 체계적으로 보였으며, **37.8 BLEU의 음성 번역 성능**과 **영점 학습 능력**은 이후 음성 LLM 연구의 표준이 되었습니다.[1]

그러나 토큰화 의존성, 전체 모델 미세조정 필수성, 저자원 언어 성능 격차 등의 한계는 여전합니다. 최근 2년 간(2024-2025)의 후속 연구들은 이러한 한계를 극복하기 위해 **더욱 정교한 멀티모달 설계**(GPT-4o), **표준화된 벤치마킹**(AudioBench, SAGI), **음성 토큰화 개선**에 집중하고 있습니다.[5][11][2][1]

향후 연구는 일반화 능력 향상, 저자원 언어 지원, 윤리적 안전성 확보에 중점을 두어야 하며, 이는 인간-AI 상호작용의 질적 향상과 글로벌 접근성을 실현하는 핵심입니다.[3][2]

***

**참고문헌**

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e8836f1c-bd3f-4ec9-85b2-cee61bed78bf/2306.12925v1.pdf)
[2](http://arxiv.org/pdf/2410.13268.pdf)
[3](http://arxiv.org/pdf/2410.03751.pdf)
[4](https://arxiv.org/pdf/2502.15218.pdf)
[5](https://arxiv.org/pdf/2311.07919.pdf)
[6](https://clova.ai/tech-blog/ko-hyperclova-%EA%B8%B0%EB%B0%98-%EC%9D%8C%EC%84%B1-%ED%95%A9%EC%84%B1-%EA%B8%B0%EC%88%A0-audiollm)
[7](https://dailyan.com/news/article.html?no=750930)
[8](https://aclanthology.org/2023.findings-emnlp.1055.pdf)
[9](http://arxiv.org/pdf/2412.05167.pdf)
[10](https://blog.naver.com/kcc_press/223958832485?fromRss=true&trackingCode=rss)
[11](http://arxiv.org/pdf/2406.16020.pdf)
[12](http://arxiv.org/pdf/2407.04051v3.pdf)
[13](https://www.gaudiolab.com/ko/blog/185_icml_2024_%ED%83%90%EB%B0%A9%EA%B8%B0_ai_%EB%B0%8F_%EC%98%A4%EB%94%94%EC%98%A4_%EC%97%B0%EA%B5%AC%EC%9D%98_%EC%B5%9C%EC%8B%A0_%EB%8F%99%ED%96%A5)
[14](https://www.jask.or.kr/articles/pdf/k0XP/ask-2025-044-02-6.pdf)
[15](https://mj9245.tistory.com/35)
[16](https://www.sweeper.or.kr/postact/download/266)
[17](https://tech.hancom.com/multimodal-vlm-trends/)
[18](https://www.themoonlight.io/ko/review/llase-g1-incentivizing-generalization-capability-for-llama-based-speech-enhancement)
[19](https://www.alphaxiv.org/ko/overview/2306.12925v1)
