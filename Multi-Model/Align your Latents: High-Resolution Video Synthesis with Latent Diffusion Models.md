# Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models

### 1. 핵심 주장 및 주요 기여 요약

본 논문의 핵심 주장은 **사전 학습된 이미지 생성 모델을 효율적으로 비디오 생성 모델로 변환할 수 있다**는 것입니다. 논문의 제목 "Align your Latents"는 이 핵심 아이디어를 직관적으로 표현하는데, 잠재 공간에서 비디오 프레임들을 시간적으로 정렬(align)함으로써 시각적 일관성을 달성한다는 의미입니다.[1]

**주요 기여:**

1. **시간적 계층 삽입을 통한 효율적 비디오 생성**: 고정된 공간 계층(spatial layers)을 유지하면서 학습 가능한 시간적 계층(temporal layers)만 추가하여, 대규모 이미지 데이터셋의 사전 학습 이점을 활용[1]

2. **시간적 일관성 확보**: 오토인코더 디코더의 시간적 미세조정을 통해 프레임 간 깜빡임 아티팩트를 제거[1]

3. **고해상도 장기 비디오 생성**: 512×1024 해상도의 실제 운전 장면 비디오를 여러 분 길이로 생성하며, 최대 1280×2048 해상도까지 확장 가능[1]

4. **개인화된 텍스트-비디오 생성 최초 시연**: 학습된 시간적 계층이 다양한 미세조정된 이미지 모델에 일반화될 수 있음을 입증[1]

***

### 2. 문제 정의, 제안 방법, 성능 및 한계

#### 2.1 해결하고자 하는 문제

비디오 생성은 이미지 생성보다 근본적으로 더 복잡한 문제입니다. 주요 도전 과제는:[1]

- **계산량 증가**: 시간 차원의 추가로 메모리 및 계산 요구사항이 기하급수적으로 증가
- **시간적 일관성 유지**: 연속 프레임 간의 자연스러운 움직임과 공간적 안정성 보장
- **데이터 부족**: 대규모 공개 비디오 데이터셋의 제한
- **장기 비디오 생성**: 확장 프레임 시퀀스에서의 장기 시간 의존성 모델링
- **고해상도 출력**: 고해상도 비디오 합성의 계산 복잡성

#### 2.2 제안 방법

##### 2.2.1 기본 아키텍처

본 논문은 Latent Diffusion Model (LDM) 패러다임을 비디오 도메인으로 확장합니다. 기본 공식은:[1]

오토인코더를 통한 이미지-잠재 공간 변환:

$$\hat{x} = D(E(x)) \approx x$$

여기서 \(E\)는 인코더, \(D\)는 디코더, \(\hat{x}\)는 재구성된 이미지입니다.[1]

잠재 공간에서의 확산 모델 학습:

$$\arg\min_{\theta} \mathbb{E}_{x \sim p_{\text{data}}, \tau \sim p_{\tau}, \epsilon \sim \mathcal{N}(0,I)} \left\| y - f_{\theta}(z_{\tau}^+; c, \tau) \right\|_2^2$$

여기서 \(z_{\tau}^+ = \alpha_{\tau}z + \sigma_{\tau}\epsilon\)는 확산 처리된 인코딩이고, \(c\)는 조건정보(텍스트 프롬프트 등), \(y\)는 목표 벡터입니다.[1]

##### 2.2.2 시간적 계층 구조

비디오 생성을 위해 논문은 공간 계층과 시간 계층을 교대로 배치합니다:[1]

공간 계층에서는 B×T개의 독립적 이미지로 처리:
$$z \in \mathbb{R}^{(B-T) \times C \times H \times W}$$

시간 계층에서는 비디오 형식으로 재구성:
$$z' \leftarrow \text{rearrange}(z, (bt)chw \rightarrow bcthw)$$

각 시간 계층 후 학습 가능한 혼합 매개변수 \(\alpha \in [0,1]\)을 사용하여 공간 출력과 시간 출력을 결합:

$$z_{\text{out}} = \alpha z + (1-\alpha) z'$$

이러한 설계는 추론 시 \(\alpha=1\)로 설정하여 원래의 이미지 생성 능력을 보존할 수 있습니다.[1]

##### 2.2.3 주요 학습 목표

**핵심 학습 목표** (공간 계층 고정, 시간 계층만 학습):

$$\arg\min_{\phi} \mathbb{E}_{x \sim p_{\text{data}}, \tau \sim p_{\tau}, \epsilon \sim \mathcal{N}(0,1)} \left\| y - f_{\theta,\phi}(z_{\tau}^+; c, \tau) \right\|_2^2 \quad (2)$$

여기서 \(\theta\)는 고정된 공간 계층 매개변수, \(\phi\)는 학습 가능한 시간 계층 매개변수입니다.[1]

#### 2.3 모델 아키텍처의 핵심 구성요소

##### 2.3.1 시간적 오토인코더 미세조정

이미지 데이터로만 학습된 오토인코더는 비디오 시퀀스 인코딩 시 시간적 깜빡임을 유발합니다. 이를 해결하기 위해:[1]

- 디코더에 추가 시간 계층 삽입
- 3D 컨볼루션 기반 시간적 판별기를 사용한 훈련
- 인코더는 프레임별 처리 유지 (잠재 공간 재사용성 보장)[1]

##### 2.3.2 장기 비디오 생성을 위한 예측 모델

짧은 시퀀스 생성 모델의 한계를 극복하기 위해 예측 모델을 도입합니다. 이진 마스크 \(m_s\)를 사용하여 조건 프레임을 지정:[1]

$$\mathbb{E}_{x \sim p_{\text{data}}, m_s \sim p_s, \tau \sim p_{\tau}, \epsilon \sim \mathcal{N}(0,1)} \left\| y - f_{\theta,\phi}(z_{\tau}^+; c_s, c, \tau) \right\|_2^2 \quad (3)$$

여기서 \(c_s = (m_s \odot z, m_s)\)는 마스크된 조건 프레임입니다.[1]

추론 시 분류기-자유 확산 가이드(classifier-free diffusion guidance)를 적용하여 안정성을 강화:[1]

$$f_{\theta,\phi}(z_{\tau}^+; c_s) = f_{\theta,\phi}(z_{\tau}^+) + s \cdot (f_{\theta,\phi}(z_{\tau}^+; c_s) - f_{\theta,\phi}(z_{\tau}^+))$$

여기서 \(s \geq 1\)은 가이드 스케일입니다.[1]

##### 2.3.3 고프레임율을 위한 시간 보간

키 프레임 생성 후 보간 모델을 사용하여 고프레임율 달성:

- T → 4T 및 4T → 16T 체계로 동시 학습
- 마스크 조건화 메커니즘 재사용
- 동일한 아키텍처로 효율성 확보[1]

##### 2.3.4 시간 정렬 초해상화 모델

픽셀-공간 DM 또는 잠재-공간 DM 업샘플러를 시간 정렬:[1]

$$\mathbb{E}_{x \sim p_{\text{data}}, (\tau, \tilde{\tau}) \sim p_{\tau}, \epsilon \sim \mathcal{N}(0,1)} \left\| y - g_{\theta,\phi}(x_{\tau}^+; c_{\tau\eta}, T_{\gamma}, \tau) \right\|_2^2 \quad (5)$$

여기서 \(c_{\tau\eta} = \alpha_{\tilde{\tau}}x + \sigma_{\tilde{\tau}}\epsilon\)는 노이즈 첨가 저해상도 이미지입니다.[1]

패치 기반 훈련으로 계산 효율성을 보장하고 합성곱 방식의 추론으로 메모리 절감[1]

***

### 3. 성능 향상 및 평가

#### 3.1 운전 장면 비디오 합성

Real Driving Scene (RDS) 데이터셋에서의 성능:[1]

| 방법 | FVD | FID |
|------|-----|-----|
| LVG (기존 SOTA) | 478 | 53.5 |
| Video LDM (우리) | 389 | 31.6 |
| Video LDM (조건부) | 356 | 51.9 |

- **FVD (Fréchet Video Distance)** 개선: 약 26% 향상[1]
- **FID (Fréchet Inception Distance)** 개선: 약 41% 향상[1]

**사용자 연구 결과:**[1]

| 비교 | 선호도 A (%) | 선호도 B (%) | 동등 (%) |
|------|-------------|-------------|---------|
| 우리(조건) vs 우리(무조건) | 49.33 | 42.67 | 8.0 |
| 우리(무조건) vs LVG | 54.02 | 40.23 | 5.74 |
| 우리(조건) vs LVG | 62.03 | 31.65 | 6.33 |

#### 3.2 초해상화 모델의 시간 정렬 효과

| 방법 | FVD | FID |
|------|-----|-----|
| 이미지 업샘플러 (시간 미정렬) | 165.98 | 19.71 |
| 비디오 업샘플러 (시간 정렬) | 45.39 | 19.85 |

시간 정렬이 FVD를 **73% 개선**하면서 FID는 유지:[1]

#### 3.3 오토인코더 미세조정의 중요성

| 디코더 상태 | FVD | FID |
|------------|-----|-----|
| 이미지만 학습 | 390.88 | 7.61 |
| 시간 미세조정 | 32.94 | 9.17 |

시간 미세조정이 FVD를 **약 92% 개선**:[1]

#### 3.4 텍스트-비디오 생성

UCF-101 및 MSR-VTT 벤치마크에서의 성능:[1]

**UCF-101:**

| 방법 | IS (상향↑) | FVD (하향↓) |
|------|-----------|-----------|
| CogVideo (영문) | 25.27 | 701.59 |
| Make-A-Video | 33.00 | 367.23 |
| Video LDM (우리) | 33.45 | 550.61 |

**MSR-VTT:**

| 방법 | CLIP-SIM (상향↑) |
|------|-----------------|
| Make-A-Video | 0.3049 |
| Video LDM (우리) | 0.2929 |

#### 3.5 아블레이션 연구

RDS 데이터셋에서의 아블레이션:[1]

| 방법 | FVD | FID |
|------|-----|-----|
| 픽셀-공간 기준선 | 639.56 | 59.70 |
| 엔드-투-엔드 LDM | 1155.10 | 71.26 |
| 어텐션만 사용 | 704.41 | 50.01 |
| 우리 (3D Conv) | 534.17 | 48.26 |
| 우리 + 문맥 가이드 | 508.82 | 54.16 |

3D 컨볼루션 기반 시간 계층이 어텐션 전용 모델보다 우수:[1]

***

### 4. 모델의 일반화 성능 향상

이 논문이 특별히 강조하는 부분 중 하나가 **일반화 성능**입니다.

#### 4.1 아키텍처적 일반화

**분리된 공간-시간 구조의 이점:**[1]

논문의 핵심 설계 결정은 공간 계층(θ)을 고정하고 시간 계층(φ)만 학습하는 것입니다. 이는:

1. **크로스 모델 일반화**: 학습된 시간 계층을 다양한 미세조정된 이미지 모델에 전이 가능[1]

2. **개인화된 비디오 생성**: DreamBooth를 사용하여 특정 대상으로 미세조정한 Stable Diffusion 체크포인트에 시간 계층을 삽입하면, 개인화된 비디오 생성 가능[1]

#### 4.2 DreamBooth 전이 실험

DreamBooth를 사용한 개인화 텍스트-이미지 모델에 학습된 시간 계층을 직접 삽입한 결과:[1]

- 신원 일관성 유지: 개인화 이미지의 특성을 보존하며 동시에 자연스러운 움직임 생성
- 추가 훈련 불필요: 시간 계층 전이만으로 작동
- 질적 평가: 목표 객체의 신원이 제대로 유지되면서 표현력 있는 비디오 생성[1]

#### 4.3 데이터 효율성

**이미지 데이터 활용의 이점:**[1]

- 대규모 이미지 데이터로 사전 학습된 공간 계층 활용
- 시간 계층은 상대적으로 작은 비디오 데이터셋(WebVid-10M: 52K 시간)으로도 효과적 학습
- 비디오 데이터 부족 문제를 이미지 사전 학습으로 완화

#### 4.4 시간적 정렬의 일반화

**"Convolutional in Time" 특성:**[1]

상대적 사인 위치 인코딩과 스칼라 혼합 매개변수를 사용함으로써:

- 학습 시퀀스(4초)보다 긴 비디오 생성 가능
- 공간적으로도 확장 가능 ("Convolutional in Space")
- 최대 30초 길이의 고해상도 비디오 생성 실증[1]

***

### 5. 한계 및 제약

#### 5.1 모델 성능의 한계

1. **장기 비디오 생성의 품질 저하**: 예측 모델을 통한 반복 적용 시 누적 오류 발생, 특히 "convolutional-in-time" 생성에서 취약[1]

2. **텍스트-비디오 성능 중간 수준**: Make-A-Video와 비교 시 UCF-101에서 FVD 점수가 다소 낮음(550.61 vs 367.23)[1]

3. **이미지 품질 저하**: Stable Diffusion을 WebVid-10M 데이터(320×512)에 미세조정하면서 원본 SD의 이미지 품질 손상[1]

#### 5.2 데이터 및 윤리 제약

1. **데이터 소스 제한**: 
   - 실제 운전 데이터는 내부 데이터셋
   - WebVid-10M은 인터넷 스톡 푸티지 소스로 스타일 제한[1]
   - 상용화 부적합성[1]

2. **윤리적 우려**:
   - 생성된 비디오가 실제 콘텐츠와 구별 불가능해질 가능성
   - 딥페이크 등 악의적 사용 가능성
   - 대규모 인터넷 데이터 학습에 따른 바이어스 문제[1]

#### 5.3 기술적 한계

1. **계산 요구사항**:
   - 고해상도 생성을 위해 여전히 상당한 GPU 리소스 필요
   - 추론 시간이 상대적으로 길 수 있음[1]

2. **메모리 제약**:
   - 패치 기반 학습에도 불구하고 장시간 비디오에는 메모리 효율성 제한[1]

3. **평가 메트릭의 신뢰성**:
   - FVD(Fréchet Video Distance)의 신뢰성 문제 인정
   - 인간 평가에 대한 의존도 증가[1]

***

### 6. 앞으로의 연구에 미치는 영향 및 고려사항

#### 6.1 학술 영향

**본 논문의 기여가 이후 연구에 미친 영향:**[2]

1. **아키텍처 패러다임 전환**: "temporal layer insertion" 방식이 후속 연구의 표준이 됨[3]
   - Make-A-Video, Imagen Video 등이 유사한 구조 채택
   - Stable Diffusion 기반 비디오 모델의 표준 개발 방식

2. **효율성 중심 연구 촉진**: LDM 기반 비디오 생성이 확산 모델의 주류 방식으로 정착[4]

3. **전이 학습의 새로운 가능성**: 서로 다른 이미지 모델 체크포인트 간 시간 계층 공유 가능성 제시[1]

#### 6.2 최신 연구 동향 (2023-2025)

**최근 관련 연구들:**[5][6][2][4]

1. **시공간 일관성 강화 (2024-2025)**:
   - **Spatiotemporal Consistency Survey (2025)**: 공간 및 시간 일관성을 동시에 고려하는 새로운 평가 프레임워크 제시[4]
   - **Motion-Guided Latent Diffusion (2024)**: 움직임 정보를 명시적으로 활용한 시간 일관성 강화[7]
   - **Video-3DGS (2025)**: 3D Gaussian Splatting과 결합하여 시간 일관성 개선[8]

2. **효율성 개선 (2024)**:
   - **Accelerating Video Diffusion via Distribution Matching (2024)**: 확산 단계 감소로 추론 속도 향상[9]
   - **Cascaded 모델 구조**: 본 논문의 업샘플러 개념 확장[6]

3. **아키텍처 혁신**:
   - **Video Diffusion Transformer (VDT, 2023)**: 트랜스포머 기반 시공간 모델링[10]
   - **Lumiere (2024)**: Space-Time U-Net으로 전체 시간 길이를 한 번에 생성[3]
   - **W.A.L.T (2024)**: 인과 인코더와 윈도우 어텐션으로 효율성 개선[11]

4. **일반화 및 제어성 향상**:
   - **BIVDiff (2024)**: 이미지-비디오 모델 브릿징으로 유연한 작업 일반화[12]
   - **Video Diffusion Alignment via Reward Gradients (2024)**: 보상 기반 미세조정[13]

5. **평가 및 이해 (2024-2025)**:
   - **Video Diffusion Models Learn the Structure of the Dynamic World (2024)**: 비디오 확산 모델이 시간 역학을 어떻게 학습하는지 분석[14]
   - 새로운 평가 메트릭 개발 (TCC, TMC 등)[4]

#### 6.3 앞으로 연구할 때의 고려사항

1. **시공간 일관성의 균형**:
   - 공간 품질과 시간 연속성의 트레이드오프 관리
   - 장기 비디오 생성에서의 누적 오류 제어 방안[4]

2. **계산 효율성 추구**:
   - 추론 시간 단축을 위한 증류(distillation) 및 가속 샘플링 기법[9]
   - 메모리 효율적인 아키텍처 설계

3. **데이터 다양성 확보**:
   - 고품질 상용 비디오 데이터셋 구축
   - 다양한 도메인 비디오로의 일반화 강화

4. **제어성 및 편집성**:
   - 세밀한 동작/콘텐츠 제어 메커니즘 개발
   - 비디오 편집, 스타일 전이 등 응용 확장

5. **평가 메트릭 개선**:
   - FVD의 신뢰성 문제 해결
   - 시공간 일관성을 통합한 평가 프레임워크 개발[4]

6. **윤리 및 안전성**:
   - 생성 비디오의 투명성 제시 방법 연구
   - 악의적 사용 방지를 위한 검증 기술 개발

7. **모달리티 간 전이**:
   - 오디오-비디오 동기화
   - 3D 일관성 있는 비디오 생성[15]
   - 텍스트, 이미지, 비디오, 오디오의 통합 생성

8. **확장성 및 실시간 생성**:
   - 초고해상도(4K, 8K) 실시간 생성
   - 장시간(분 단위 이상) 안정적 생성

***

### 결론

"Align your Latents"는 잠재 확산 모델 기반 고해상도 비디오 생성의 이정표가 되는 논문입니다. **사전 학습된 이미지 모델을 효율적으로 비디오 생성기로 변환**하는 방식은 이후 비디오 생성 연구의 표준이 되었고, **개인화된 비디오 생성의 첫 시연**은 콘텐츠 창작의 새로운 가능성을 열었습니다.[1]

다만 장기 비디오 생성의 안정성, 텍스트-비디오 성능의 추가 개선, 그리고 계산 효율성 측면에서는 여전히 개선 여지가 있습니다. 최근 2024-2025년의 연구들은 이러한 한계들을 **시공간 일관성 강화, 효율성 개선, 새로운 아키텍처 도입, 엄밀한 평가 메트릭 개발** 등을 통해 극복하려는 노력을 보여주고 있습니다.

향후 연구자들은 **점진적 효율성 개선, 다양한 제어 메커니즘, 윤리적 고려, 그리고 멀티모달 통합**을 중점적으로 검토해야 할 것입니다.

***

### 참고 자료

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/d6aaecab-88bb-4a95-a73f-90349f681e18/2304.08818v2.pdf)
[2](http://arxiv.org/pdf/2405.03150.pdf)
[3](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)
[4](https://arxiv.org/html/2502.17863v1)
[5](https://arxiv.org/abs/2405.03150)
[6](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Upscale-A-Video_Temporal-Consistent_Diffusion_Model_for_Real-World_Video_Super-Resolution_CVPR_2024_paper.pdf)
[7](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06048.pdf)
[8](https://openreview.net/forum?id=s1zfBJysbI)
[9](https://arxiv.org/html/2412.05899)
[10](http://arxiv.org/pdf/2305.13311.pdf)
[11](https://eccv.ecva.net/virtual/2024/poster/2132)
[12](http://arxiv.org/pdf/2312.02813.pdf)
[13](http://arxiv.org/pdf/2407.08737.pdf)
[14](https://openreview.net/forum?id=SIZhZrU41O)
[15](https://arxiv.org/html/2412.01821)
[16](https://arxiv.org/pdf/2205.09853.pdf)
[17](https://arxiv.org/html/2306.11173)
[18](https://blogs.torus.ai/untitled/)
[19](https://arxiv.org/abs/2304.08818)
