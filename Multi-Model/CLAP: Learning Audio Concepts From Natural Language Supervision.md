# CLAP: Learning Audio Concepts From Natural Language Supervision

## 1. 핵심 주장과 주요 기여  
**CLAP(Contrastive Language–Audio Pretraining)**은 자연어 캡션과 오디오를 대조 학습(contrastive learning)으로 연결하여, 텍스트-오디오 표현을 공통 임베딩 공간에 매핑함으로써 **사전 정의된 클래스 레이블 없이도 유연한 제로샷(zero-shot) 분류**를 가능하게 한 모델이다.  
- 128,010개의 오디오–텍스트 쌍으로 학습  
- 16개 다운스트림 과제에서 제로샷 성능 SoTA 달성  
- 5개 과제에서 지도 학습(supervised) SoTA 달성  

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계  

### 2.1 해결하고자 하는 문제  
- 기존 오디오 분석 모델은 한정된 클래스 레이블로 학습되어 **새로운 클래스 예측에 유연성이 부족**  
- Self-supervised 학습은 언어 의미를 반영하지 못하고, 지도 학습은 여전히 사전 정의된 레이블에 의존  

### 2.2 제안 방법  
- 오디오와 자연어 캡션 쌍 $$\{X^{a}, X^{t}\}$$를 입력으로, 각각 오디오 인코더 $$f_a$$와 텍스트 인코더 $$f_t$$를 통해 임베딩 생성  
- 선형 프로젝션 $$L_a, L_t$$를 적용해 양측 임베딩을 차원 $$d$$의 **공통 멀티모달 공간**으로 투사  
- 배치 내 모든 올바른 쌍과 잘못된 쌍을 포함한 유사도 행렬  

$$
    C = \tau \,(E_t E_a^\top),\quad E_a = L_a(f_a(X^a)),\; E_t=L_t(f_t(X^t))
  $$  

- 대조 손실(대칭 크로스엔트로피)  

$$
    \mathcal{L} = \tfrac12\big(\ell_{\text{text}}(C) + \ell_{\text{audio}}(C)\big)
  $$  

### 2.3 모델 구조  
- **오디오 인코더**: CNN14 (2048-dim 임베딩, 80.8M 파라미터)  
- **텍스트 인코더**: BERT base uncased (768-dim, 110M 파라미터)  
- **공통 투사층**: 1024-dim 선형 프로젝션  
- **학습**: 40 epochs, 배치 크기 최대 768, 온도 파라미터 $$\tau$$ 학습, Adam 옵티마이저  

### 2.4 성능 향상  
- **제로샷**: ESC50 82.6% (사람 성능 81%), FSD50K mAP 0.3024, US8K 73.24% 등 16개 과제 대다수에서 기존 연구를 능가  
- **지도 학습**: GTZAN Music vs Speech 100%, Mridangam Stroke 97.94% 등 5개 과제 SoTA  
- 프롬프트(“this is a sound of [class]”) 적용 시 ESC50에서 5%p 성능 개선  

### 2.5 한계  
- **음성·언어 과제**(감정 인식, 키워드 검출)에서 제한적 성능  
- AudioSet 전체(170만 쌍 추가) 활용 시 성능 저하 관찰 → **캡션의 품질**이 결정적  
- 대규모 배치 크기(≥768)에서 성능 비일관  

## 3. 일반화 성능 향상 가능성  
- **자연어 캡션 다양성** 확대: 인간 음성 설명이 풍부한 캡션 데이터 확보 시 음성 과제 일반화 개선 가능  
- **인코더·프롬프트 설계**: 더 정교한 프롬프트 템플릿 및 텍스트 인코더 동결/해제 전략으로 멀티도메인 적응력 강화  
- **노이즈-로버스트 학습**: 잡음이 섞인, 비정형 환경 오디오-텍스트 쌍 활용하여 견고성 제고  

## 4. 향후 연구 방향 및 고려 사항  
- **데이터 수집**: 대규모 고품질 오디오 캡션 코퍼스 구축—특히 대화, 감정, 키워드 설명 포함  
- **모델 확장**: Vision–Language 모델(CLIP)에서 사용된 대규모 웹 스케일 데이터 적용 방법론 차용  
- **프롬프트 학습**: 프롬프트 튜닝(prompt tuning) 및 자동화된 템플릿 검색으로 제로샷 분류 성능 최적화  
- **다중 모달 통합**: 오디오·비디오·텍스트 동시 학습으로 복합적 멀티모달 이해 능력 강화  

CLAP은 오디오 분야에서 자연어 감독을 활용한 **제로샷 오디오 이해**의 가능성을 제시하며, 향후 **오디오 파운데이션 모델** 연구의 중요한 전기를 마련할 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/aaa66a5e-f7f0-4a0e-a229-2b14184868e3/2206.04769v1.pdf
