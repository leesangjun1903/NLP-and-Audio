# Visual Instruction Tuning

## 1. 핵심 주장 및 주요 기여
Visual Instruction Tuning 논문은 **언어-이미지 멀티모달 모델**에 “지시문(instruction)”을 자연어로 부여하여 모델이 다양한 시각·언어 태스크를 인간 의도에 맞춰 수행하도록 학습시키는 첫 시도이다.  
주요 기여는 다음과 같다:  
- **멀티모달 지시문 데이터 생성**: GPT-4를 활용해 이미지-텍스트 쌍을 질문·응답 형식의 지시문 데이터로 자동 변환  
- **LLaVA 모델 제안**: CLIP 비전 인코더와 Vicuna LLM을 연결해, 경량 프로젝션 레이어로 시각 토큰을 언어 영역에 매핑하는 단순·효율적 구조  
- **LLaVA-Bench**: COCO 및 In-the-Wild 환경에서 모델의 지시문 수행 능력을 정량 평가할 수 있는 벤치마크 구축  
- **State-of-the-Art 달성**: ScienceQA 멀티모달 과학문제해결 벤치마크에서 GPT-4와 앙상블하여 최고 92.53% 정확도 기록  

***

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계

### 문제 정의
기존 멀티모달 모델은 개별 태스크별로 학습되거나, 단순 이미지 캡셔닝·분류용으로만 언어가 사용되어, 사용자의 구체적 지시를 동적으로 수행하기 어려웠다.  
이를 해결하기 위해 “다목적 시각 어시스턴트”로서 자연어 지시문을 명시적으로 이해·수행할 수 있는 **일관된 인터페이스**가 필요하다.

### 제안 방법
1. **멀티모달 지시문 데이터 생성**  
   - GPT-4에 이미지 캡션 및 객체 바운딩박스 정보를 입력해 ‘대화형 질문-응답’, ‘상세 설명’, ‘복합 추론’ 3종류의 지시문-응답 샘플 158K개 생성  
2. **모델 학습**  
   - Stage 1: **특징 정렬(Feature Alignment)**  
     - CLIP으로 추출한 시각 특징 $$Z_v$$를 투영층 $$W$$만 업데이트하며 LLM 임베딩 공간에 일치시킴  
     - $$H_v = W \cdot Z_v $$  
   - Stage 2: **엔드투엔드 파인튜닝**  
     - $$W$$와 Vicuna 파라미터 $$\phi$$를 모두 업데이트하며, 지시문 시퀀스를 입력으로 어시스턴트 응답을 생성하도록 학습  
     - 최대우도 목표:  

$$
       p(X_a \mid X_v, X_\mathrm{instr}) = \prod_{i=1}^L p_\theta(x_i \mid X_v, X_\mathrm{instr},<i, X_a,<i)
       $$  

3. **아키텍처**  
   - CLIP ViT-L/14 비전 인코더(고정)  
   - 시각 특징을 언어 임베딩 차원으로 선형 투영  
   - Vicuna 13B LLM 디코더(파인튜닝)  

### 성능 향상
- **LLaVA-Bench (COCO)**: GPT-4 준거 대비 85.1% 상대점수  
- **LLaVA-Bench (In-the-Wild)**: OpenFlamingo 대비 +48%, BLIP-2 대비 +29%  
- **ScienceQA**: 기본 LLaVA 90.92% → GPT-4 심판 앙상블 92.53% (종전 SoTA 91.68%)  

### 한계
- **고해상도·세부 정보 인식 한계**: 작은 객체나 다국어 텍스트 인식 실패 사례 존재  
- **‘패치 백’ 현상**: 이미지 전체를 패치 단위로 분해해 의미적 통합에 취약  
- **비교적 작은 파인튜닝 데이터**: 158K 샘플 규모로, 더 큰 다양성·품질 데이터 필요  
- **Hallucination**: 근거 없는 정보 생성 위험  

***

## 3. 모델의 일반화 성능 향상 가능성
- **지시문 다양성**: 대화, 상세 설명, 복합 추론 데이터를 결합해 학습함으로써, **추론 능력과 대화 능력 상호 보완**  
- **단순 투영 구조**: 경량 투영층만 변경해도 LLM 지식 활용 극대화 가능 → 새로운 비전 인코더 교체·확장 용이  
- **앙상블 전략**: GPT-4 판정자 활용 시, 이미지 불필요 문제에서도 전이 효과 → 다양한 환경에서 일반화 가능성 증대  

***

## 4. 향후 연구에 미치는 영향 및 고려 사항
- **표준 벤치 확장**: LLaVA-Bench 기반 추가 태스크(비디오, 3D)로 확장 연구  
- **고해상도 객체 인식 개선**: Q-former, 교차 어텐션 등 고급 투영 기법 도입  
- **데이터 편향 해소**: 다양한 소스·문화권 이미지·언어 데이터 수집으로 공정성 강화  
- **안정성·안전성**: 할루시네이션·악용 방지 필터링 기법 연구 지속  
- **융합 모델 앙상블**: 대형 비전·언어 모델 간 협업·판정자 프레임워크로 신뢰도 상승  

앞으로 Visual Instruction Tuning은 **멀티모달 어시스턴트 연구의 표준 패러다임**으로 자리잡으며, 다양한 비전·언어 융합 응용 분야와 안전·공정성 연구에 중대한 기여를 할 것으로 기대된다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/288a02af-1e65-4005-98e1-643d05e2d585/2304.08485v2.pdf
