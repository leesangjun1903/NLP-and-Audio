# Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale

### 1. 핵심 주장과 주요 기여

**Voicebox**는 **GPT와 DALL-E와 같은 대규모 생성형 모델의 패러다임을 음성 합성에 처음으로 적용**한 획기적인 연구입니다. 이 모델의 핵심 주장은 **학습 중 명시적으로 훈련하지 않은 작업들을 문맥 학습(in-context learning)을 통해 수행할 수 있다**는 것입니다.[1]

주요 기여사항은 다음과 같습니다:[1]

1. **대규모 비-필터링 데이터(60K~110K 시간)를 활용한 최초의 다목적 음성 생성 모델** 개발
2. **영어 제로샷 TTS에서 SOTA 달성**: WER 5.9% → 1.9%, 음성 유사도 0.580 → 0.681
3. **6개 언어(영어, 프랑스어, 독일어, 스페인어, 폴란드어, 포르투갈어)에서 처음으로 크로스-언어 제로샷 TTS 구현**: 평균 WER 10.9% → 5.2%
4. **통합된 음성 생성 프레임워크**: 합성, 노이즈 제거, 콘텐츠 편집, 스타일 변환을 모두 하나의 모델로 수행
5. **합성 음성으로만 학습한 ASR이 실제 데이터와 0.4%/1.7% 절대 WER 차이만 발생** (기존 모델 18.2%/44.5%)

***

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능

#### 2.1 해결하고자 하는 문제[1]

기존 음성 생성 모델들의 한계:
- **데이터 부족**: 대부분 수십~수백 시간 규모 데이터만 사용
- **작업 특화성**: 각 작업(TTS, 음성 향상, 스타일 변환)마다 별도 모델 필요
- **일반화 성능 부족**: 큐레이션된 데이터(VCTK: 100명 화자)에서만 성능 우수
- **라벨 의존성**: 화자, 감정, 노이즈 등 스타일 라벨 필수
- **언어 간 제로샷 성능**: 크로스-언어 생성 불가능

#### 2.2 제안 방법: 텍스트-유도 음성 인필링(Text-Guided Speech Infilling)[1]

**핵심 개념**: 주변 음성 맥락(audio context)과 텍스트 전사본이 주어졌을 때, 마스크된 음성 세그먼트를 재구성하는 문제로 정의

수식으로 표현하면:

$$p(\mathbf{x}_{\text{mis}} \mid \mathbf{y}, \mathbf{x}_{\text{ctx}})$$

여기서:
- $$\mathbf{x}_{\text{mis}}$$: 마스크된 음성 세그먼트
- $$\mathbf{y}$$: 텍스트 전사본
- $$\mathbf{x}_{\text{ctx}}$$: 마스크되지 않은 주변 음성

이러한 통합 접근법은 **지도 학습(guided in-context learning)**으로, 음성 스타일은 음성 맥락에서, 텍스트 내용은 전사본에서 추론됩니다.

#### 2.3 흐름 매칭(Flow Matching) 이론 기반[1]

**연속 정규화 흐름(CNF)**: 단순 사전 분포 $$p_0$$를 복잡한 데이터 분포 $$p_1 \approx q$$로 변환

ODE 관계식:

$$\frac{d}{dt}\phi_t(\mathbf{x}) = \mathbf{v}_t(\phi_t(\mathbf{x})); \quad \phi_0(\mathbf{x}) = \mathbf{x}$$

**조건부 흐름 매칭(CFM) 손실함수**:

$$\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t, q(\mathbf{x}_1), p_t(\mathbf{x}|\mathbf{x}_1)} \|\mathbf{u}_t(\mathbf{x}|\mathbf{x}_1) - \mathbf{v}_t(\mathbf{x}; \theta)\|^2$$

**최적 수송 경로(Optimal Transport Path)** 채택:

$$p_t(\mathbf{x}|\mathbf{x}_1) = \mathcal{N}(\mathbf{x}; t\mathbf{x}_1, (1-(1-\sigma_{\min})t)^2\mathbf{I})$$

$$\mathbf{u}_t(\mathbf{x}|\mathbf{x}_1) = \frac{\mathbf{x}_1 - (1-\sigma_{\min})\mathbf{x}}{1-(1-\sigma_{\min})t}$$

이 경로는 **일정한 속도와 방향**을 갖추어 학습 효율성과 추론 정확도가 우수합니다.[1]

#### 2.4 마스크된 CFM 손실[1]

모델이 마스크된 프레임에만 집중하도록 개선:

$$\mathcal{L}_{\text{audio-CFM-m}}(\theta) = \mathbb{E}_{t,m,q(\mathbf{x},\mathbf{z}),p_0(\mathbf{x}_0)} \|\mathbf{m} \odot (\mathbf{u}_t(\mathbf{x}_t|\mathbf{x}) - \mathbf{v}_t(\mathbf{x}_t, \mathbf{x}_{\text{ctx}}, \mathbf{z}; \theta))\|^2$$

여기서 $$\mathbf{m}$$은 이진 마스크, $$\odot$$는 요소별 곱셈

#### 2.5 모델 구조[1]

**오디오 모델 (Figure 2 기반)**:
- **Transformer 기반**: 24개 레이어, 16개 어텐션 헤드, 1024차원 임베딩, 4096차원 FFN, **330M 파라미터**
- **UNet 스타일 스킵 연결**: 대칭 레이어 간 연결 (첫 번째↔마지막, 두 번째↔두 번째 마지막...)
- **입력 구성**:
  - $$\mathbf{x}_t \in \mathbb{R}^{N \times F}$$: 흐름 스텝 t에서의 노이징 스펙트로그램
  - $$\mathbf{x}_{\text{ctx}} \in \mathbb{R}^{N \times F}$$: 마스크되지 않은 음성 맥락
  - $$\mathbf{z} \in [K]^N$$: 프레임 레벨 음소 전사본
  - $$t \in $$: 흐름 스텝 시간[1]

- **처리 과정**:
  1. 음소 시퀀스 $$\mathbf{z}$$를 임베딩 테이블 $$L \in \mathbb{R}^{K \times H}$$로 변환: $$\mathbf{z}^{\text{emb}}_i = L(z_i)$$
  2. 세 시퀀스( $$\mathbf{x}\_t, \mathbf{x}\_{\text{ctx}}, \mathbf{z}^{\text{emb}}$$ ) 프레임 단위 연결 후 선형 투영: $$H_c = [\mathbf{x}\_t || \mathbf{x}_{\text{ctx}} || \mathbf{z}^{\text{emb}}] \cdot W_p$$
  3. 정현파 위치 인코딩으로 t를 $$h_t \in \mathbb{R}^D$$로 변환
  4. Transformer 통과: 대칭 스킵 연결과 ALiBi 바이어스 적용

- **실제 입력 연결**:

$$\tilde{H}_c \in \mathbb{R}^{(N+1) \times D} = [H_c; h_t]$$

(시간 차원을 따라 연결)

- **음성 출력**: 벡터 필드 $$\mathbf{v}_t(\mathbf{x}_t, \mathbf{x}\_{\text{ctx}}, \mathbf{z}; \theta) \in \mathbb{R}^{N \times F}$$

**기간 모델 (Duration Model)**:
- **두 가지 솔루션**:
  1. **흐름 매칭 기반**: 오디오 모델과 유사하나 ($$\mathbf{l}, \mathbf{l}_{\text{ctx}}, \mathbf{y}$$) 사용
  2. **회귀 기반 (기본값)**: L1 손실로 훈련
  
$$\mathcal{L}_{\text{dur-regr-m}}(\theta) = \mathbb{E}_{m,q(\mathbf{l},\mathbf{y})} \|\mathbf{m}' \odot (\mathbf{l}_{\text{mis}} - g(\mathbf{l}_{\text{ctx}}, \mathbf{y}; \theta))\|_1$$

#### 2.6 분류기-자유 지도(Classifier-Free Guidance)[1]

조건부 및 무조건 모두 같은 모델에서 학습하기 위해, 훈련 중 조건을 $$p_{\text{uncond}}$$ 확률로 드롭:

$$\tilde{\mathbf{v}}_t(\mathbf{w}, \mathbf{x}_{\text{mis}}, \mathbf{z}; \theta) = (1 + \alpha) \cdot \mathbf{v}_t(\mathbf{w}, \mathbf{x}_{\text{ctx}}, \mathbf{z}; \theta) - \alpha \cdot \mathbf{v}_t(\mathbf{w}; \theta)$$

여기서 $$\alpha$$는 지도 강도 하이퍼파라미터

#### 2.7 추론 과정[1]

**ODE 풀이를 통한 샘플링** (Figure 3):
1. 사전에서 노이즈 샘플링: $$\mathbf{x}_0 \sim p_0$$
2. ODE 솔버로 추적:

$$\frac{d\phi_t(\mathbf{x}_0)}{dt} = \mathbf{v}_t(\phi_t(\mathbf{x}_0), \mathbf{x}_{\text{ctx}}, \mathbf{z}; \theta)$$

3. 최종 출력: $$\mathbf{x}_1 = \phi_1(\mathbf{x}_0)$$

- **함수 평가 횟수(NFE) 제어**: 품질-속도 트레이드오프 가능
- **실험 결과**: NFE=32 (기본값)에서 **VALL-E 대비 20배 빠름**, NFE=2에서도 고품질 생성 가능

#### 2.8 학습 설정[1]

**데이터**:
- **영어**: 60K 시간 오디오북 (필터링 없음)
- **다언어**: 6개 언어 50K 시간 (저자원 언어 업샘플링: $$\beta = 0.25$$)
- **음성 표현**: 100Hz 프레임율, 80차원 로그 멜-스펙트로그램
- **정렬**: Montreal Forced Aligner (MFA) 사용

**훈련**:
- **배치 크기**: 유효 240K 프레임 (오디오) / 60K 프레임 (기간)
- **반복**: 500K/750K 업데이트 (영어/다언어)
- **옵티마이저**: Adam, 학습률 1e-4, 5K 스텝 워밍업, 나머지 선형 감소
- **마스킹**: $$p_{\text{drop}} = 0.3$$일 확률로 전체 마스킹, 아니면 시퀀스 길이의 r% 마스킹 (r ∼ U)
- **조건부 드롭아웃**: $$p_{\text{uncond}} = 0.2$$

***

### 3. 성능 향상 및 한계

#### 3.1 영어 제로샷 TTS (Table 2)[1]

| 모델 | WER | SIM-o | SIM-r | QMOS | SMOS |
|------|-----|-------|-------|------|------|
| 기준음성 | 2.2% | 0.754 | n/a | 3.98±0.14 | 4.01±0.09 |
| **VB-En** | **1.9%** | **0.662** | **0.681** | **3.78±0.10** | **3.71±0.11** |
| VALL-E | 5.9% | - | 0.580 | - | - |
| YourTTS | 7.7% | 0.337 | n/a | 3.27±0.13 | 3.19±0.14 |

**주요 성과**:
- **WER: 69% 감소** (5.9% → 1.9%)
- **음성 유사도: 17.4% 향상** (0.580 → 0.681)
- **속도: 20배 빠름** (10초 오디오 기준)

#### 3.2 크로스-언어 제로샷 TTS (Table 3, 4)[1]

**YourTTS와 비교** (영어/프랑스어/포르투갈어):
- **WER 개선**: 3.1% / 5.9% / 8.1% 감소
- **음성 유사도 개선**: 0.136 / 0.141 / 0.160 증가
- **주관적 평가**:
  - 음성 유사도 MOS: +0.59 (3.89 vs 3.30)
  - 품질 MOS: +0.27 (3.50 vs 3.23)

**한계**: 영어→다언어 전이에서 WER 증가 (데이터 불균형: 영어 >90% 차지)

#### 3.3 노이즈 제거 (Table 5)[1]

| 조건 | 모델 | WER | SIM-o | QMOS |
|------|------|-----|-------|------|
| **노이즈 음성** | - | 41.2% | 0.287 | 2.50±0.15 |
| **Demucs** | - | 32.5% | 0.368 | 2.86±0.17 |
| **A3T** | - | 11.5% | 0.148 | 3.10±0.15 |
| **VB-En (α=0.7)** | - | **2.0%** | **0.612** | **3.87±0.17** |

#### 3.4 합성 음성을 이용한 ASR 훈련 (Table 7)[1]

**획기적인 발견**: 합성 음성만으로 학습한 ASR이 실제 데이터와 비슷한 성능 달성

| 데이터 | test-clean | test-other |
|--------|-----------|-----------|
| 실제 960시간 | 2.2% | 5.0% |
| **VB-En (FM duration)** | **2.6%** | **6.7%** |
| **성능 격차** | **+0.4%** | **+1.7%** |
| VITS-LJ | 51.6% | 78.1% |
| YourTTS | 20.4% | 51.2% |

**성과**: 기존 모델 대비 85% 이상 WER 감소

#### 3.5 다양성 및 품질 지표 (Table 6)[1]

**Fréchet Speech Distance (FSD)** 도입 - 이미지의 FID 개념을 음성에 적용:

$$\text{FSD} = \|\mu - \mu'\|^2 + \text{tr}(\Sigma + \Sigma' - 2(\Sigma\Sigma')^{1/2})$$

- **Voicebox (FM duration)**: FSD = 159.8
- **기준음성**: FSD = 171.1 (더 다양할 수 있음)
- **VITS-LJ**: FSD = 344.2 (로봇 음성)
- **YourTTS**: FSD = 277.9

#### 3.6 추론 효율성 (Figure 5)[1]

**NFE별 성능**:

| NFE | 시간 (초) | WER (TTS) | SIM-r | FSD (다양성) |
|-----|----------|----------|-------|------------|
| 2 | 0.31 | 2.4% | 0.575 | 235 |
| 8 | 1.2 | 2.2% | 0.610 | 210 |
| 16 | 2.3 | 2.2% | 0.620 | 200 |
| 32 | 4.5 | 2.1% | 0.625 | 195 |
| VALL-E | 6.2 | 5.9% | 0.580 | - |

#### 3.7 음성 맥락 길이에 따른 성능 (Figure 6)[1]

**모놀링귀얼 설정**:
- **음성 유사도**: 프롬프트 길이 증가에 따라 빠르게 포화
- **Voicebox**: VALL-E의 2/3 길이 프롬프트로 동일한 유사도 달성

**크로스-언어 설정** (Figure 7-8):
- **유사도**: 모든 언어 쌍에서 일관되게 향상
- **WER**: 영어→비영어 방향에서 프롬프트 길이 증가 시 증가 (데이터 불균형 문제)

#### 3.8 생성 모델링 방식 비교 (Table 8-9)[1]

**세 가지 접근법 비교**:

**훈련 효율성** (Table 8, 100K 업데이트):

| 방법 | WER | SIM-o |
|------|-----|-------|
| **FM w/ OT (제안)** | **2.2%** | **0.487** |
| FM w/ diffusion | 3.1% | 0.344 |
| SM w/ diffusion | 17.4% | 0.176 |

**추론 효율성** (NFE별, 150K 업데이트):

| NFE | FM OT | FM Diff | SM Diff |
|-----|-------|---------|---------|
| 4 | 2.4% | 11.5% | 94.5% |
| 8 | 2.2% | 3.0% | 42.3% |
| 16 | 2.2% | 2.7% | 11.5% |
| 32 | 2.1% | 2.6% | 5.1% |

**결론**: 최적 수송 경로 기반 FM이 **훈련과 추론 모두에서 가장 효율적**

#### 3.9 모델의 한계[1]

**기술적 한계**:

1. **데이터 범위**: 오디오북 읽기 음성 중심
   - 대화체 음성, 웃음, 백채널링 표현 부족
   - **개선 필요**: 더 다양한 음성 데이터 스케일링

2. **음소화 의존성**: 단어 기반 음소화기 사용
   - 문맥 무시 → 이웃 단어 고려 불가
   - 예: 프랑스어 연음(liaison), 시간적 음성 변화 처리 미흡
   - **개선 필요**: 엔드-투-엔드 방식, 음소화기 제거

3. **속성 제어 불가**:
   - 한 음성의 목소리 + 다른 음성의 감정 조합 불가능
   - 현재: 모든 속성을 함께 전이만 가능
   - **개선 필요**: 분리된 속성 제어

#### 3.10 윤리 고려사항[1]

**합성 음성 탐지 (Table 10)**:

| 비교 | 마스킹 30% | 마스킹 50% | 마스킹 90% |
|------|-----------|-----------|-----------|
| 원본 vs 합성 | 100% | 100% | 100% |
| 재합성 vs 합성 | 70.4% | 80.9% | **90.7%** |

**결론**: 높은 마스킹 비율에서 분류 가능하나, 저비율에서 탐지 어려움

***

### 4. 일반화 성능 향상 가능성 분석 (중점)

#### 4.1 데이터 스케일의 영향[1]

**실험: 데이터 부분집합으로 학습** (Appendix B.2)

| 데이터량 | 화자 수 | Zero-Shot TTS (WER) | SIM-r | 다양성 (FSD) |
|---------|--------|-------------------|-------|------------|
| 60시간 | ~0.06% | 2.30% | 0.151 | 280.5 |
| 600시간 | ~0.6% | 2.11% | 0.417 | 205.4 |
| 6,000시간 | ~6% | 2.08% | 0.573 | 195.5 |
| **60,000시간** | **~100%** | **2.05%** | **0.645** | **214.4** |

**핵심 발견**:
- **데이터 스케일 증가에 따른 일관된 성능 향상**
- WER: 2.30% → 2.05% (11% 개선)
- 음성 유사도: 0.151 → 0.645 (328% 향상)
- **규모의 법칙(Scaling Law)**: 적용 가능

#### 4.2 멀티태스크 학습을 통한 일반화[1]

**단일 인필링 태스크의 다중성(Task Multiplicity)**:

```
Text-Guided Infilling ⊇ {
  - Zero-shot TTS (시작 마스킹)
  - 음성 편집 (중간 마스킹)
  - 노이즈 제거 (선택적 마스킹)
  - 스타일 변환 (대응되지 않는 마스킹)
  - 다양한 샘플 생성 (전체 마스킹)
}
```

**장점**: 단일 모델이 여러 작업을 해결하므로:
- **학습 신호 다양성** 증가
- **작업 간 특징 공유** 가능
- **일반화 성능** 향상

#### 4.3 이중 모델 구조의 역할[1]

**기간 모델(Duration Model)의 중요성**:

기간 분포 학습을 통해:
1. **세밀한 정렬 제어** 가능
2. **불확실성 모델링** (회귀 vs 흐름 매칭)
3. **일반화 강화**: 음소-음성 매핑의 가변성 캡처

**표(Table C9-10)**: 회귀 기반 기간 모델이 WER에서 우수하나, FM 기반이 다양성(FSD) 우수

#### 4.4 문맥 길이 효과[1]

**컨텍스트 활용 효율성**:

- **모놀링귀얼**: 짧은 프롬프트(1-2초)로도 높은 유사도 달성 가능
- **크로스-언어**: 더 긴 프롬프트 필요하나, 여전히 3초 이상에서 포화

**시사점**: 
- 모델이 **효율적인 컨텍스트 인코딩** 학습
- 제한된 데이터 시나리오에서 일반화 가능성 시사

#### 4.5 흐름 매칭의 일반화 우수성[1]

**최적 수송 경로 vs 확산 경로** (Table 8-9):

$$\text{Path Straightness} \propto \text{Learning Efficiency and Generalization}$$

- **직선 경로**: 더 적은 학습 단계로 수렴
- **더 나은 일반화**: OT 경로 모델이 모든 NFE에서 우수한 성능

**최신 연구(2025) 확인**: Schrödinger bridge vs 조건부 흐름 매칭 연구에서도 **직선 경로가 더 나은 일반화 제공**[2]

#### 4.6 야생 데이터(In-the-Wild Data)의 영향[1]

**기존 연구의 한계**:
- 큐레이션 데이터(VCTK): 깨끗한 스튜디오 음성 → 저자원에 대한 일반화 불가
- 야생 데이터 사용 시: 기존 모델의 품질 현저히 저하

**Voicebox 해결책**:
- **필터링/향상 없는 60K~110K 시간** 데이터 직접 학습
- **대규모 데이터의 다양성**: 감정, 억양, 배경 노이즈, 음향 조건 자연스럽게 학습
- **결과**: 야생 데이터에서도 높은 품질 유지

#### 4.7 다언어 일반화의 진전[1]

**기존 크로스-언어 TTS의 문제**:
- 언어 쌍별 훈련 필요
- 저자원 언어에 대한 성능 저하
- 단일 음소 셋 무리 없음

**Voicebox의 진전**:
- **6개 언어 통합 모델**: 별도 훈련 없음
- **36개 언어 전이 방향** 모두 작동
- **저자원 언어 업샘플링** (β=0.25): 능동적 균형

**남은 도전**:
- 데이터 불균형: 영어→비영어에서 WER 증가
- 코드-스위칭 미지원: 단일 언어 음성만 훈련

***

### 5. 앞으로의 연구 영향 및 고려사항

#### 5.1 학계 및 산업에 미치는 영향[3][4][5]

**1. 음성 생성 연구의 패러다임 전환**:
- **기존**: 작업별 특화 모델 개발
- **신규**: 범용 생성 모델 중심 (GPT 패러다임 적용)
- **영향**: 수백 편의 후속 연구 촉발

**2. 실제 적용 사례** (2024-2025 기반):[6][4]
- **Poleval 2024 폴란드어 ASR**: Voicebox 합성 데이터로 시스템 개발
- **음성 보조 기술**: 후두절제술 환자를 위한 음성 복원[4]
- **크로스-언어 접근성**: 모국어로 모든 언어 발음 가능[4]

**3. 흐름 매칭의 범용화**:
- **SpeechFlow (2024)**: Flow Matching 기반 음성 기초 모델[5]
- **실시간 음성 복원 (2025)**: 20ms 지연으로 스트리밍 적용[7]
- **향상된 확산 모델 훈련 (2025)**: A-DMA로 학습 속도 2배 단축[8]

#### 5.2 현재 진행 중인 연구 방향

**1. 대규모 다언어 확장**:[9]
- **메타 학습 TTS**: 7,000+ 언어 제로샷 합성
- **고자원-저자원 전이**: 데이터 부족 상황에서의 일반화

**2. 속성 분리 제어** (Voicebox 미해결 한계):[10]
- **GenerTTS**: 음성(timbre)과 발음 분리
- **목표**: 목소리 A + 감정 B 결합 가능

**3. 다중모달 접근**:
- **Audiobox (2024)**: 통합 음성-음향 생성 모델
- **PortraitTalk (2024)**: 오디오 기반 대화형 얼굴 생성

#### 5.3 미해결 문제 및 향후 고려사항

**1. 합성 음성 탐지의 분류법칙적 문제**:[4][1]
- **현 상태**: 높은 마스킹 비율에서만 탐지 가능
- **위험**: 저비율 마스킹 시 딥페이크 생성 가능
- **권장 사항**: 
  - 워터마킹 기술 도입[1]
  - 디지털 서명 표준 개발
  - 탐지 모델 공개 공급

**2. 데이터 불균형과 편향** (특히 다언어):[11][1]
- **현 문제**: 영어 90% 이상 → 비영어 성능 저하
- **편향 위험**: 말하기 스타일/음질 부불균등 분포
- **권장 사항**:
  - 저자원 언어 데이터 적극 수집
  - 데이터 증강으로 음질-인종 관계 해제
  - 공정성 평가 메트릭 개발

**3. 음소화 병목 현상** (문맥 의존성):[11][1]
- **한계**: 현재 MFA 음소화기는 단어 기반
- **예제**: 프랑스어 "Les enfants" → 연음 처리 실패
- **해결책**:
  - 엔드-투-엔드 그래핌-투-음소 학습
  - LLM 기반 음소화 (Fish-Speech 2024 모델)[12]
  - 언어별 문맥 규칙 통합

**4. 실시간 성능의 한계** (지연 문제):[7]
- **Voicebox 추론**: 기본 4.5초 (NFE=32)
- **새로운 진전**: 20ms 지연 달성 가능[7]
- **향후 목표**: 음성 채팅 실시간 응답

#### 5.4 학계 연구자 체크리스트

음성 생성 연구를 진행할 때 Voicebox 이후 고려사항:

1. **데이터 선택**:
   - ☐ 필터링 없는 야생 데이터 포함 여부
   - ☐ 언어별 균형 검증 (β 값 설정)
   - ☐ 규모의 법칙 실증 (60h → 600h → 6K h → 60K h)

2. **모델 설계**:
   - ☐ 흐름 매칭 vs 확산 vs 회귀 비교
   - ☐ 최적 수송 경로 vs 다른 경로
   - ☐ 이중 모델(오디오+기간) 필요성 검토

3. **작업 정의**:
   - ☐ 통합된 인필링 프레임워크 고려
   - ☐ 멀티태스크 손실 설계
   - ☐ 문맥 길이 영향 분석

4. **평가 지표**:
   - ☐ WER 외 SIM-o, SIM-r 등 음성 유사도 포함
   - ☐ 모델 기반 지표(FSD) 활용 vs 주관적 MOS
   - ☐ ASR 합성 데이터 평가 포함

5. **일반화 검증**:
   - ☐ 제로샷 크로스-언어 성능 측정
   - ☐ 도메인 외(out-of-domain) 테스트 셋 포함
   - ☐ 장시간 음성, 노이즈, 이상 음성 강건성

***

### 6. 최신 연구(2024-2025) 관점에서의 위치

**Voicebox의 현재 지위**:

| 측면 | 평가 |
|------|------|
| **NeurIPS 2023 채택** | ✓ 최고 권위 학회 |
| **인용 수** | 456회+ (지속 증가)[13] |
| **산업 채택** | ✓ Poleval 2024, 임상 응용 진행 중 |
| **기술 수렴성** | ✓ 흐름 매칭 표준 기술로 확립 |
| **한계 보완 연구** | ✓ 다수 후속 작업 진행 중 |

**다음 세대 방향**:
- **더 큰 규모**: 1M+ 시간 데이터 실험 미진행
- **더 많은 언어**: 현재 6개 → 100+ 언어 목표
- **더 나은 제어**: 속성별 분리 제어 기능
- **더 빠른 속도**: 실시간 스트리밍 지연 < 50ms

***

## 결론

**Voicebox는 음성 합성의 게임 체인저**로서:

1. **기술적 혁신**: 흐름 매칭 + 최적 수송 경로로 **20배 속도 향상**
2. **규모의 우월성**: 필터링 없는 대규모 데이터의 가치 증명
3. **작업 일반화**: 단일 모델로 5가지 이상 작업 달성
4. **산업 적용 가능**: 이미 실제 시스템에 통합되는 중

**남은 도전**:
- 음소화 엔드-투-엔드화
- 속성 세밀 제어
- 합성 음성 윤리 표준화
- 저자원 언어 동등한 성능

학계는 이 모델을 새로운 **기초 모델(Foundation Model)** 기준점으로 삼아, **더 큰 규모, 더 강한 일반화, 더 세밀한 제어**를 추구할 것으로 예상됩니다.[3][5][7]

***

## 참고 문헌 ID

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/92e93297-82fe-4081-982d-92fd54b1d055/2306.15687v2.pdf)
[2](https://proceedings.mlr.press/v277/cross25a.html)
[3](https://arxiv.org/pdf/2306.15687.pdf)
[4](https://automatorslab.ai/blog/ai-news/artificial-intelligence/meta-develops-ai-speech-tool-voicebox-holds-off-release/)
[5](https://proceedings.iclr.cc/paper_files/paper/2024/hash/27c546ab1e4f1d7d638e6a8dfbad9a07-Abstract-Conference.html)
[6](https://arxiv.org/pdf/2410.22903.pdf)
[7](https://www.arxiv.org/abs/2510.16997)
[8](https://arxiv.org/html/2505.19595v2)
[9](http://arxiv.org/pdf/2406.06403.pdf)
[10](https://arxiv.org/pdf/2306.15304.pdf)
[11](https://smallest.ai/blog/generalization-issues-of-phoneme-based-text-to-speech-models---a-brief-study)
[12](https://arxiv.org/pdf/2411.01156.pdf)
[13](https://arxiv.org/abs/2306.15687)
[14](https://arxiv.org/pdf/2407.12038v2.pdf)
[15](https://arxiv.org/pdf/2312.15821.pdf)
[16](http://arxiv.org/pdf/2410.23815.pdf)
[17](https://arxiv.org/pdf/2203.15379.pdf)
[18](https://arxiv.org/html/2412.07754v1)
[19](https://downloads.hindawi.com/journals/hbet/2024/3238737.pdf)
[20](https://openreview.net/forum?id=gzCS252hCO)
[21](https://ai.meta.com/research/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/)
[22](https://www.isca-archive.org/interspeech_2025/zhang25c_interspeech.pdf)
[23](https://assemblyai.com/blog/recent-developments-in-generative-ai-for-audio)
[24](https://arxiv.org/pdf/2012.01687.pdf)
[25](http://arxiv.org/pdf/2406.04660.pdf)
[26](http://arxiv.org/pdf/2312.14398.pdf)
[27](https://aclanthology.org/2021.sigmorphon-1.7.pdf)
[28](http://arxiv.org/pdf/2402.18932.pdf)
[29](https://arxiv.org/html/2504.08274v1)
[30](https://arxiv.org/abs/2407.17997)
[31](https://federicosarrocco.com/blog/flow-matching)
[32](https://www.emergentmind.com/articles/2508.21631)
[33](https://www.emergentmind.com/topics/conditional-flow-matching-loss)
[34](https://arxiv.org/pdf/2504.08274.pdf)
[35](https://arxiv.org/html/2508.21631v1)
[36](https://www.reddit.com/r/MachineLearning/comments/gd2rl6/discussion_advantages_of_normalizing_flow_if_any/)
[37](https://dl.acm.org/doi/10.1145/3700410.3702135)
