# ActBERT: Learning Global-Local Video-Text Representations

**핵심 주장 및 주요 기여**  
ActBERT는 대규모 무라벨 비디오-텍스트 데이터를 활용한 자기지도 학습(self-supervised learning) 모델로, 전역 행동(global action)과 국부 객체(local region)가 서로 보완적으로 작용하도록 설계된 **TaNgled Transformer(TNT)** 블록을 도입했다. 이를 통해 비디오 클립과 자막 텍스트 간의 세밀한 관계를 학습하며, 다양한 비디오-언어 태스크에서 최첨단 성능을 기록한다.

***

## 1. 문제 제기  
기존 비디오-텍스트 사전학습 모델(VideoBERT 등)은  
- 행동(action) 단위의 전역 시퀀스 정보를 무시하거나  
- 객체(region) 단위의 국부 정보를 희생하면서 학습한다.  

이에 따라 **행동 의도**와 **객체 상호작용**을 동시에 포착하기 어려워, 세밀한 비디오-텍스트 대응이 부족하다.

***

## 2. 제안 방법

### 2.1 입력 구성  
각 학습 샘플은  
- 전역 행동 임베딩 $$a_1, \dots, a_L$$  
- 국부 객체 임베딩 $$r_1, \dots, r_M$$  
- 텍스트 토큰 임베딩 $$w_1, \dots, w_N$$  
를 순차적으로 나열한 뒤, CLS/SEP 토큰으로 구분하여 구성한다.

### 2.2 전역 행동 & 국부 객체 임베딩  
- **행동 임베딩**: 자막에서 추출한 동사(verbs)로 구성한 분류 레이블로 3D-CNN(ResNet-3D)\;softmax 학습 → 평균 풀링 후 벡터화  
- **객체 임베딩**: Faster R-CNN을 통해 추출한 RoI features + 5D 공간 위치 벡터  
- 각 임베딩은 위치(position), 세그먼트(segment), 토큰(token) 임베딩과 합산

### 2.3 TaNgled Transformer (TNT)  
세 가지 입력 모달리티(행동, 객체, 언어)를 위해 각각 별도 Transformer를 두고,  
전역 행동 표현 $$h^l_a$$를 **쿼리**로 하여  

$$
c^w = \mathrm{MultiHead}\bigl(Q=W^q_a h^l_a,\;K=W^k_w h^l_w,\;V=W^v_w h^l_w\bigr)
$$

$$
c^r = \mathrm{MultiHead}\bigl(Q=W^q_a h^l_a,\;K=W^k_r h^l_r,\;V=W^v_r h^l_r\bigr)
$$

를 계산한 뒤, 원래 키·값 쌍과 **스택**하여 상호작용을 강화한다. 이로써 행동→객체, 행동→언어, 객체↔언어의 동시 협력을 구현한다.

### 2.4 사전학습 태스크  
1. **마스크 언어 모델링** (MLM): 전역 행동·국부 객체 단서를 활용해 마스크된 단어 예측  
2. **마스크 행동 분류**: 행동 임베딩을 마스킹하고, 나머지 모달리티로 행동 레이블 예측  
3. **마스크 객체 분류**: 지역 객체를 마스킹하고, Faster R-CNN 분포를 타깃으로 KL-발산 최소화  
4. **교차 모달 매칭**: CLS 토큰 출력을 통해 비디오-텍스트 쌍의 연관성 이진 분류  

학습 입력으로 HowTo100M(1억 클립) 사용. 텍스트 트랜스포머는 BERT(12-layer, 768D) 초기화, 나머 시각 트랜스포머는 무작위 초기화.

***

## 3. 성능 및 한계

### 성능 향상  
- **텍스트-비디오 검색**: YouCook2 R@1 9.6→ 26%↑, MSR-VTT R@1 8.6→ +1.1%p  
- **비디오 캡셔닝**: METEOR 11.94→ 13.30 (+1.36)  
- **행동 분할**: COIN 프레임 정확도 34.30→ 56.95% (+22.65)  
- **단계 위치 지정**: CrossTask 평균 리콜 33.6→ 41.4% (+7.8)  
- **비디오QA**: MSR-VTT 다중선택 83.4→ 85.7%  

특히 사전학습만으로도 **비지도 설정(non-finetuned)** 에서 기존 TVJE 대비 우위 달성.

### 한계  
- HowTo100M의 **노이즈**(ASR 자막 불일치)  
- **계산 비용**: 3개의 Transformer 병렬 학습  
- **모달 불균형**: 객체·행동 간 정보량 차이로 미묘한 학습 불안정

***

## 4. 일반화 성능 강조  
ActBERT는 전역 행동 임베딩을 통해 긴 시퀀스 의도를 포착하고, 국부 객체 임베딩으로 세밀한 장면 분석을 동시에 수행함으로써, 사전학습된 표현을 어느 비디오-언어 과제에도 **견고하게 전이**한다. 텍스트-비디오 매칭·검색, 캡셔닝, 질의응답 등 다양한 도메인에서 미세한 성능 향상이 관찰되어, 특히 **비지도 추론 환경**에서 일반화 능력이 뛰어나다.

***

## 5. 향후 연구 방향 및 고려 사항  
- **효율화**: TNT의 모달리티 병렬 블록을 가벼운 구조(저랭크 어텐션)로 대체  
- **노이즈 완화**: ASR 오류에 강인한 텍스트 전처리·정제 기법 적용  
- **추가 모달리티**: 오디오 및 센서 데이터를 포함한 다중모달 확장  
- **세부 행동 인식**: 복합 행동(composite action) 시퀀스 분해 및 계층적 학습  
- **실시간 응용**: 온디바이스 추론을 위한 모델 경량화 및 양자화 연구  

이러한 고려를 바탕으로 ActBERT는 더 넓은 비디오-언어 응용 분야에 걸쳐 확장될 수 있다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b32a52dd-0674-448f-b5a5-dba9a17e483b/2011.07231v1.pdf)
