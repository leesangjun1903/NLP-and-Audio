# Align before Fuse: Vision and Language Representation Learning with Momentum Distillation

# 핵심 요약  
**ALBEF(Align Before Fuse)**는 이미지와 텍스트 표현을 결합하기 전에 대조 학습으로 정렬(align)한 뒤 멀티모달 인코더로 융합(fuse)함으로써 비전-언어 표현 학습의 효율과 일반화 성능을 크게 향상시킨다.[1]

# 1. 해결하고자 하는 문제  
기존 비전-언어 사전학습(VLP) 방식은  
- 사전 학습된 물체 검출기(Object Detector) 의존으로 주석(annotation) 및 계산 비용이 높고,  
- 이미지 피처와 단어 임베딩이 다른 공간에 존재하여 융합 효율이 낮으며,  
- 웹에서 수집된 이미지-텍스트 쌍의 노이즈로 인해 일반화 성능이 저하되는 한계가 있다.[1]

# 2. 제안 방법  
## 2.1 모델 구조  
- **이미지 인코더**: ViT-B16 기반 Transformer (12-layer)  
- **텍스트 인코더**: BERT-base 상위 6-layer  
- **멀티모달 인코더**: BERT-base 하위 6-layer, 각 레이어마다 Cross-Attention 융합  

## 2.2 사전학습 목적함수  
1) **이미지-텍스트 대조 손실(ITC)**  

$$
   p_{i2t}^{(m)} = \frac{\exp(s(i, t_m)/\tau)}{\sum_{n}\exp(s(i, t_n)/\tau)},\quad
   L_{\mathrm{ITC}} = -\frac{1}{2}\bigl[\log p_{i2t} + \log p_{t2i}\bigr]
   $$  
   
여기서 $$s(i,t)=g_v(v_{\mathrm{cls}})\cdot g_w(w_{\mathrm{cls}})$$이며, $$\tau$$는 학습 가능한 온도 파라미터다.[1]

2) **마스크 언어 모델링(MLM)**  
   멀티모달 인코더의 출력으로 마스킹된 단어 예측, 교차엔트로피 손실  

$$
   L_{\mathrm{MLM}} = -\sum_{k} y_k\log p_k(I,T)
   $$

3) **이미지-텍스트 매칭(ITM)**  
   양·음성 쌍 분류를 위한 CLS 토큰에 Softmax 적용  

$$
   L_{\mathrm{ITM}} = -y\log p_{\mathrm{match}}(I,T)
   $$

4) **모멘텀 증류(Momentum Distillation, MoD)**  
   지수이동평균(EMA)으로 업데이트되는 모멘텀 모델을 교사로 삼아, 대조 및 MLM 예측을 KL-divergence로 정규화  

$$
   L_{\mathrm{MoD}} = L + \lambda\,D_{\mathrm{KL}}(q_{\text{mom}}\|p)
   $$  
   
이를 통해 노이즈 라벨에 덜 민감한 **뷰(view) 다양성** 기반 일반화가 가능해진다.[1]

## 2.3 상호 정보 관점  
모델은 서로 다른 **뷰**(예: 모달리티 분리, 토큰 마스킹, 모멘텀 샘플) 간의 상호정보(mutual information)를 최대화하도록 학습되며, 이는 InfoNCE 하한을 극대화하는 것과 동등하다.

# 3. 성능 향상  
- **이미지-텍스트 검색**: 4M 데이터 학습 기준 CLIP·ALIGN 대비 소규모 데이터로 우월한 성능 달성  
- **VQA / NLVR2 / SNLI-VE**: 이전 최첨단 대비 절대 2–4% 포인트 향상  
- **추론 속도**: 객체 검출기 불필요, 저해상도 입력으로 10배 이상 빠른 속도  
- **시각적 근거(Grad-CAM)**: 물체·속성·관계 수준의 정밀한 시각적 근거 제공  

# 4. 한계 및 고려사항  
- **웹 데이터 노이즈**: 민감 정보·부적절 콘텐츠 포함 가능  
- **데이터 편향**: 어노테이션 불균형으로 잠재적 편향  
- **사회적 영향**: 정확도 최적화만으로는 부적절 결과 방지 불충분  

# 5. 일반화 성능 향상 관점  
- **대조 정렬**: 융합 전 표현 정렬로 멀티모달 인코더의 학습 난이도 감소  
- **모멘텀 증류**: 다중 뷰 전개(data augmentation)로 표현의 **뷰 불변성(view-invariance)** 학습  
- **상호정보 최대화**: 다양한 뷰 상호정보를 극대화하여 노이즈 대응 및 일반화 강화  

# 6. 향후 연구 방향 및 영향  
- **확장 규모**: 더 큰 웹 데이터(수십억 이미지-텍스트) 적용 가능성  
- **데이터 정제**: 웹 데이터 오염 제거·윤리적 필터링  
- **프라이버시**: 개인 정보 보호와 안전한 배포 고려  
- **모달 확대**: 오디오·비디오 등 멀티모달 확장  
- **이론적 분석**: 상호정보 관점 추가 정교화 및 새로운 뷰 생성 전략 탐색  

ALBEF는 **정렬-융합** 구조와 **모멘텀 증류**로 비전-언어 표현 학습의 새로운 패러다임을 제시하며, 향후 대규모 멀티모달 AI 연구에 중요한 토대를 마련할 것이다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/9d22b341-0dd3-461a-a4ad-9d8f1250d6c8/2107.07651v2.pdf)
