
# MAESTRO: Matched Speech Text Representations through Modality Matching

## 1. 핵심 주장 및 주요 기여

MAESTRO (Matched Speech Text Representations through Modality Matching)는 Google에서 제시한 자기지도학습 방법으로, 음성과 텍스트라는 서로 다른 두 모달리티로부터 통합된 표현을 학습합니다.[1]

**세 가지 핵심 기여:**

1. **모달리티 매칭 알고리즘**: 전사된 음성 데이터의 소량만 활용하여 음성과 텍스트 표현을 효과적으로 통합하며, 명시적 텍스트-음성 변환(TTS) 없이도 모달리티 간 정렬을 달성합니다. 이는 계산 효율성과 정렬된 표현 학습을 동시에 실현합니다.[1]

2. **다국어 표현 학습 개선**: 기계 번역(MT)과 음성 번역(ST) 데이터를 자기지도 사전학습에 활용하여 추가 감독 신호를 통한 다국어 합동 표현을 개선합니다.[1]

3. **최신 성능 달성**: VoxPopuli 다국어 ASR에서 8% 상대 WER 감소, CoVoST-2에서 평균 2.8 BLEU 향상을 달성하며, 3배 더 적은 파라미터로 기존 모델(mSLAM 0.6B)을 초과하는 성능을 보입니다.[1]

***

## 2. 문제 정의, 제안 방법, 모델 구조

### 2.1 문제 정의

기존의 음성-텍스트 합동 학습 방식은 두 가지 주요 한계를 가집니다:[1]

- **다중 작업 학습**: 두 모달리티의 서로 다른 특성으로 인한 간섭 문제와 모델 용량 제약이 성능을 제한합니다.
- **모달리티 변환**: TTS 모델을 통한 텍스트-음성 변환으로 인한 계산 복잡성과 오버헤드가 발생합니다.

MAESTRO는 명시적 모달리티 변환 없이 잠재 표현 공간에서 암묵적으로 정렬하여 두 접근 방식의 장점을 결합합니다.[1]

### 2.2 모델 아키텍처

MAESTRO의 프레임워크는 다음의 핵심 구성 요소로 구성됩니다:[1]

- **음성 인코더**: Conformer XL 아키텍처의 하위 6층으로, 대조 손실을 통해 국부적 음향/음운 정보를 학습
- **텍스트 임베딩 추출기**: 3개의 합성곱 층과 6층 Transformer로 어휘적 정보를 캡처
- **정렬 메커니즘**: 리샘플러와 지속 기간 모델을 통해 텍스트 임베딩을 음성 임베딩 길이로 조정
- **정제기(Refiner)**: 2층 8헤드 자기 주의 블록과 경량 합성곱으로 정렬된 표현을 정제
- **공유 인코더**: 상위 18층 Conformer 블록으로 음성과 텍스트의 합동 표현을 학습[1]

### 2.3 제안하는 방법 (핵심 수식)

#### A. 초기 임베딩 학습

음성 임베딩은 대조 손실을 통해 학습되고, 텍스트 임베딩은 어휘적 정보를 캡처합니다.

#### B. 정렬된 임베딩 학습

**쌍을 이루는 음성-텍스트 데이터의 경우:**

RNN-T 디코더를 사용하여 정렬을 생성하고, 리샘플러와 정제기를 통해 텍스트 임베딩을 처리합니다:[1]

$$\mathbf{e}_t^{\text{aligned}} = \text{Refiner}(\text{Resample}(\mathbf{e}_t, \text{Align}_{\text{RNN-T}}(\mathbf{e}_s, t)))$$

**모달리티 매칭 손실:**

$$\mathcal{L}_{\text{MM}} = \text{MSE}(\mathbf{e}_s, \mathbf{e}_t^{\text{aligned}}) \quad \cdots (1)$$

여기서 $$\mathbf{e}_s$$는 음성 인코더 출력이고, $$\mathbf{e}_t^{\text{aligned}}$$는 정렬된 텍스트 임베딩입니다.[1]

**쌍을 이루지 않은 텍스트의 경우:**

지속 기간 모델은 예측된 지속 기간을 사용하여 텍스트 임베딩을 리샘플합니다:[1]

$$d(t_i) = \text{DurationModel}(\mathbf{e}_t)$$

#### C. 정렬된 마스크 언어 모델(LA-MLM)

$$\mathcal{L}_{\text{LA-MLM}} = \text{RNN-T}(\text{Mask}(\mathbf{e}_t^{\text{aligned}}), t) \quad \cdots (2)$$

여기서 마스킹은 시간 및 주파수 영역에서 SpecAugment 스타일로 적용됩니다.[1]

#### D. 지속 기간 예측 모델

$$d_i = \text{DurationPredictor}(\mathbf{e}_t[i])$$

지속 기간은 리샘플 과정에서 텍스트 프레임을 음성의 대응 프레임 수만큼 복제하는 데 사용됩니다.[1]

***

## 3. 성능 향상 및 주요 결과

### 3.1 단일 언어 다중 도메인 ASR (SpeechStew)

MAESTRO는 TTS4Pretrain 2.0 대비 최대 6% 상대 WER 감소, SLAM 대비 최대 12% 상대 WER 감소를 달성했습니다. 특히 Librispeech 테스트에서 1.5 WER로 기존 방법들을 능가합니다.[1]

### 3.2 다국어 ASR (VoxPopuli)

| 모델 | 파라미터 | 쌍을 이루는 시간 | 텍스트 | 평균 WER |
|------|----------|------------------|-------|---------| 
| W2v-BERT | 0.6B | - | 429k | 8.8 |
| mSLAM | 0.6B | 2.4k | 429k+mC4 | 9.2 |
| **MAESTRO** | **0.6B** | **1.3k** | **429k+VP-T** | **8.2** |

MAESTRO는 W2v-BERT 대비 **8% 상대 WER 감소**를 달성하며, 특히 슬로베니아어(10시간)와 같은 소수 언어에서도 의미 있는 개선을 보입니다.[1]

### 3.3 다국어 음성 번역 (CoVoST-2)

| 모델 | 파라미터 | 데이터 | 평균 BLEU |
|------|----------|--------|----------|
| mSLAM (0.6B) | 0.6B | 429k+mC4 | 22.4 |
| **MAESTRO** | **0.6B** | **429k+mC4+MT+ST** | **25.2** |

MAESTRO는 mSLAM (0.6B) 대비 **2.8 BLEU 절대 향상**을 달성하며 새로운 SOTA를 기록합니다. 특히 파라미터는 30% 적으면서도 더 높은 성능을 보입니다.[1]

***

## 4. 일반화 성능 향상 (일반화 능력 중심)

### 4.1 일반화 능력의 핵심 원인

**모달리티 간 정렬을 통한 표현 정제:**

MAESTRO의 모달리티 매칭 메커니즘은 다음 세 가지를 통해 일반화 성능을 향상시킵니다:[1]

1. **공동 표현 공간**: 음성과 텍스트가 동일한 잠재 공간에서 정렬되어 모달리티 특정 잡음을 감소
2. **화자/운율 불변성**: TTS 합성 대신 재샘플링을 통해 음성 고유 특성(화자, 운율) 제거
3. **음운 수준 정렬**: 두 모달리티를 음운 수준에서 연결하여 언어학적 구조 학습[1]

### 4.2 다국어 일반화의 강점

**언어별 개선율이 데이터량에 반비례합니다:**

- 슬로베니아어(10시간): 약 15% 상대 개선
- 크로아티아어(20시간): 약 12% 상대 개선
- 핀란드어(30시간): 약 10% 상대 개선
- 영어(150시간): 약 5% 상대 개선[1]

**핵심 발견**: 음성 데이터가 적을수록 음성-텍스트 정렬의 이점이 크며, 다국어 표현이 단일 언어 모델보다 일반화 성능이 우수합니다.[1]

### 4.3 저자원 환경에서의 일반화

MAESTRO의 주요 강점:[1]

- **저자원 상황에서의 성능**: 10시간 음성 데이터로도 의미 있는 표현 학습 가능
- **크로스링구얼 전이**: 다국어 사전학습으로 고리소(low-resource) 언어의 성능 향상
- **제로샷/퓨샷 일반화**: 음성-텍스트 정렬의 보편성으로 미보이 언어/도메인 적응 가능[1]

***

## 5. 모델의 한계

### 5.1 기술적 한계

1. **지속 기간 모델의 정확성**: 쌍을 이루지 않은 텍스트의 지속 기간 예측 부정확 가능성과 저자원 언어에서의 성능 저하 위험[1]

2. **정렬 메커니즘의 강인성**: 입력 길이 변동에 민감하며, 복잡한 음성 변이(노이즈, 억양)에 대한 견고성이 미검증됨[1]

3. **계산 오버헤드**: 리샘플링, 정제, 지속 기간 예측 등의 추가 연산이 추론 레이턴시에 영향[1]

### 5.2 데이터 관련 한계

1. **쌍을 이루는 데이터 의존성**: RNN-T 정렬 학습을 위해 초기 쌍을 이루는 음성-텍스트 데이터가 필수적[1]

2. **텍스트 데이터 품질**: 비구어 텍스트 품질이 모델 성능에 직접 영향 (VP-T vs mC4 성능 차이 확인)[1]

3. **다국어 토큰화**: 101개 언어를 커버하기 위해 4k SentencePiece 토큰 사용으로 특정 언어의 토큰화 품질 저하 가능[1]

### 5.3 평가 관련 한계

1. **제한된 벤치마크**: VoxPopuli, CoVoST-2, SpeechStew 중심 평가로 음성 이해(키워드 스팟팅, 의도 분류) 등 다른 과제에서의 성능 미검증[1]

2. **특수 도메인**: 일반 음성에 최적화되어 감정 음성, 방언 등 특수 도메인 성능이 미평가[1]

3. **노이즈 견고성**: 깨끗한 음성 중심 평가로 배경 잡음이나 음성 왜곡에 대한 견고성 미검증[1]

***

## 6. 앞으로의 연구에 미치는 영향 및 고려사항

### 6.1 음성-텍스트 합동 학습의 새로운 패러다임

MAESTRO는 세 가지 주요 발견을 제시합니다:[1]

1. **명시적 변환 없는 모달리티 정렬의 가능성**: TTS 모델이 불필요하여 계산 효율성이 향상되고, 자체 감독 지속 기간 예측이 더 효과적입니다.

2. **음운 수준 정렬의 유효성**: 화자/운율 불변 표현 학습이 가능하며, 다국어/저자원 환경에서 강한 일반화 능력을 보입니다.

3. **확장 가능한 다국어 사전학습**: 101개 언어를 동시에 학습하며, 데이터량과 성능의 상관관계를 예측할 수 있습니다.[1]

### 6.2 최신 연구 방향 (2024-2025)

#### A. 통합 음성 기초 모델

**UniWav (2025)**: 표현 학습과 생성 작업을 통합하는 첫 프레임워크로, MAESTRO와 같은 음성-텍스트 정렬 원리를 확대하여 TTS와 ASR을 단일 모델로 동시 학습합니다.[2]

#### B. 다국어 음성 기술의 확장

**XEUS (2024)**: 4057개 언어를 지원하여 기존 언어 커버리지의 4배를 달성하며, MAESTRO의 다국어 정렬 원리를 더욱 확장합니다. 1백만 시간 이상의 다국어 음성 데이터를 활용하고, 진정한 극저자원 언어(1-10시간) 지원이 새로운 도전입니다.[3][4]

#### C. 크로스모달 정렬 기술의 발전

**Modality Translation Learning (2024)**: 모달리티 간 번역 기반 정렬로 MAESTRO 이후의 정렬 메커니즘을 고도화하며, 감정 음성 등 특수 도메인 적응에 활용됩니다.[5]

**VQ-CTAP (2024)**: 크로스모달 미세입도 수준(프레임 수준) 표현 학습을 제공하여 음성 생성 작업의 성능을 개선하고, TTS, 음성 변환, ASR 작업을 통합합니다.[6]

#### D. 음성 토큰화의 진화

**SpeechTokenizer (2024)**: 의미론적/음향적 토큰을 통합하여 음성 언어 모델 구축의 기반을 제공하며, MAESTRO의 다층 표현 학습과 상호보완됩니다.[7]

**음성 대형 언어 모델(Speech LLMs)**: 음성과 텍스트를 이산 토큰으로 통합하여 MAESTRO의 연속 표현을 이산 토큰으로 확장합니다.[8]

### 6.3 향후 연구 시 고려사항

#### 1. 모달리티 정렬의 최적화
- 더 정교한 지속 기간 예측 방법(주의 기반)
- 다양한 음성 변이에 강인한 정렬 메커니즘
- 동적 정렬 가중치 학습

#### 2. 저자원 환경에서의 성능
- 진정한 극저자원 언어(<5시간) 성능 평가
- 제로샷 다언어 전이 학습 방법 개발
- 협동 학습(collaborative learning) 활용

#### 3. 특수 도메인 적응
- 감정 음성 인식에 대한 최적화
- 노이즈 음성의 견고성 향상
- 방언/비표준 발음에 대한 일반화

#### 4. 효율성 개선
- 지속 기간 모델 경량화
- 추론 시간 단축(리샘플링 최적화)
- 메모리 효율적 구현

#### 5. 강건성 개선
- 배경 잡음, 음성 왜곡에 대한 테스트
- 코드 스위칭 음성 처리
- 생성된 음성(합성 음성)에 대한 성능 검증

#### 6. 학제적 확장 가능성
- 비전-음성-텍스트 통합 학습으로 확대
- 음성 대형 언어 모델에서의 맥락 내 학습(in-context learning) 적용

***

## 7. 결론 및 종합 평가

### MAESTRO의 핵심 혁신

MAESTRO는 음성-텍스트 표현 학습에서 **세 가지 핵심 혁신**을 달성합니다:[1]

1. **계산 효율성과 성능의 균형**: TTS 모델 없이도 모달리티 간 정렬을 달성하며, 3배 더 적은 파라미터로 기존 모델을 초과하는 성능을 보입니다.

2. **언어 전반에 걸친 일반화**: 음성 데이터가 적은 언어에서 특히 강한 성능을 보이며, 다국어 사전학습의 보편적 이점을 입증합니다.

3. **명시적 변환 없는 정렬**: 지속 기간 예측 기반 자가지도 정렬로 음향-음운-언어 정보를 통합적으로 처리합니다.

### 앞으로의 전망

MAESTRO 이후의 연구 동향은 다음과 같이 진행되고 있습니다:

- **2024**: 음성 토큰화 및 언어 모델 결합(SpeechTokenizer, Speech LLMs)
- **2024-2025**: 통합 기초 모델 개발(UniWav, 다국어 확장 XEUS)
- **향후**: 멀티모달 정렬, 특수 도메인 적응, 극저자원 환경 지원

MAESTRO의 접근 방식은 앞으로의 음성-언어 처리 연구에 있어 **모달리티 간 정렬의 새로운 패러다임**을 제시하며, 이는 기초 모델 시대의 효율적이고 확장 가능한 멀티모달 학습의 중요한 사례가 될 것으로 예상됩니다.[4][3][5][2][7][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3695a5ce-675b-4baf-b4e6-ae520630e6da/2204.03409v2.pdf)
[2](https://openreview.net/forum?id=yj9lLwMjnE)
[3](https://arxiv.org/abs/2407.00837)
[4](https://aclanthology.org/2024.emnlp-main.570/)
[5](https://www.isca-archive.org/interspeech_2024/liu24d_interspeech.html)
[6](https://ieeexplore.ieee.org/document/10976417/)
[7](https://openreview.net/forum?id=AF9Q8Vip84)
[8](https://github.com/ga642381/speech-trident)
[9](https://arxiv.org/abs/2505.17067)
[10](https://www.nature.com/articles/s41597-024-04142-x)
[11](https://ieeexplore.ieee.org/document/10381805/)
[12](https://arxiv.org/abs/2404.02702)
[13](https://ieeexplore.ieee.org/document/10982640/)
[14](https://ieeexplore.ieee.org/document/10625827/)
[15](https://arxiv.org/abs/2410.13179)
[16](https://arxiv.org/pdf/2110.07205.pdf)
[17](http://arxiv.org/pdf/2402.18932.pdf)
[18](https://www.ijfmr.com/papers/2024/3/17163.pdf)
[19](https://aclanthology.org/2023.findings-acl.566.pdf)
[20](https://aclanthology.org/2023.findings-emnlp.438.pdf)
[21](http://arxiv.org/pdf/2407.05421.pdf)
[22](https://arxiv.org/pdf/2209.15329.pdf)
[23](https://arxiv.org/html/2503.23108v1)
[24](https://www.amazon.science/publications/multi-stage-multi-modal-pre-training-for-automatic-speech-recognition)
[25](https://arxiv.org/abs/2510.22961)
[26](https://www.isca-archive.org/interspeech_2025/tran25_interspeech.pdf)
[27](https://aclanthology.org/2024.lrec-main.1045.pdf)
[28](https://proceedings.iclr.cc/paper_files/paper/2025/file/a11ac2705602ba9de957cda3befcba36-Paper-Conference.pdf)
[29](https://arxiv.org/abs/2302.10315)
