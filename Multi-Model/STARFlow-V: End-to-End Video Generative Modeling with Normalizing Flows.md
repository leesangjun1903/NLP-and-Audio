
# STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows

## 1. 핵심 주장 및 주요 기여 요약

**STARFlow-V**는 Apple 연구팀이 발표한 혁신적인 비디오 생성 모델로, **정규화 흐름(Normalizing Flow)**을 이용한 **엔드-투-엔드 비디오 생성**의 새로운 패러다임을 제시합니다. 이 논문의 핵심 주장은 다음과 같습니다:[1]

**주요 기여:**
- **엔드-투-엔드 가능도 기반 학습**: 기존 확산 모델의 다단계 노이즈 제거 방식과 달리, 정확한 최대 가능도 추정(MLE)을 통한 직접 학습
- **인과적 비디오 생성**: 스트리밍 및 상호작용형 애플리케이션을 위한 엄격한 인과성 보장
- **오류 축적 완화**: 글로벌-로컬 아키텍처를 통해 자기회귀 생성 시 발생하는 누적 오류 문제 해결
- **다중 작업 호환성**: 동일한 모델 백본으로 텍스트-비디오(T2V), 이미지-비디오(I2V), 비디오-비디오(V2V) 생성 지원

## 2. 해결하고자 하는 문제

### 2.1 비디오 생성 분야의 근본적 도전과제

기존 확산 기반 비디오 생성 모델들은 다음과 같은 문제점을 갖고 있습니다:[1]

**(1) 자기회귀 생성 시 오류 축적**: 자기회귀 확산 모델은 학습 중 실제 프레임에 조건화되지만, 추론 중에는 자신의 불완전한 예측값에 조건화되어 훈련-테스트 불일치(exposure bias)로 인한 시간이 지남에 따른 품질 저하가 발생합니다.[1]

**(2) 비인과적 생성 구조**: 표준 확산 모델은 모든 프레임을 병렬로 노이즈 제거하는 비인과적 방식을 사용하여, 스트리밍이나 실시간 대화형 애플리케이션에 부적합합니다.[1]

**(3) 계산 효율성**: 비디오의 높은 시공간 복잡도로 인해 샘플링 과정이 매우 느리며, 특히 장시간 비디오 생성 시 실시간 성능을 달성하기 어렵습니다.[1]

**(4) 비엔드-투-엔드 학습**: 확산 모델은 다양한 노이즈 수준에서 개별적으로 학습되어야 하며, 이는 훈련 비용을 크게 증가시킵니다.[1]

### 2.2 정규화 흐름의 역사적 한계

정규화 흐름은 이론적으로 강력하지만 실제로는 제한된 주목을 받았습니다:[1]
- 아키텍처 경직성으로 인한 확장성 문제
- 훈련 불안정성
- 고해상도 이미지 생성에서의 성능 부족

## 3. 제안하는 방법 및 수식

### 3.1 정규화 흐름의 기본 원리

정규화 흐름은 변수 변환 공식(change-of-variables formula)을 통해 정확한 가능도를 계산합니다:[1]

$$L_{NF}(\theta) = \mathbb{E}_x \left[ \log p_0(f_\theta(x)) + \log|\det(J_{f_\theta}(x)))| \right]$$

여기서:
- $\(f_\theta: \mathbb{R}^D \rightarrow \mathbb{R}^D\)$ 는 학습 가능한 역변환
- $\(J_{f_\theta}(x)\)$는 야코비안 행렬식
- $\(p_0\)$는 단순한 사전분포(예: 표준 정규분포)

### 3.2 자기회귀 정규화 흐름 블록

STARFlow의 기본 구성 요소는 다음 아핀 변환입니다:[1]

$$z = \frac{x - \mu_\theta(x \odot m)}{\sigma_\theta(x \odot m)}, \quad \sigma_\theta(\cdot) > 0$$

여기서:
- $\(\odot\)$는 아다마르 곱(Hadamard product)
- $\(m\)$은 자기회귀 마스크(자기-배제 인과 마스크)
- $\(\mu_\theta, \sigma_\theta\)$는 조건부 평균과 표준편차 함수

### 3.3 STARFlow-V의 글로벌-로컬 아키텍처

STARFlow-V의 핵심 혁신은 비디오 생성에 맞춤화된 글로벌-로컬 분해입니다:[1]

$$p_\theta(x) = p_0(z) |\det J_{f_D}(u)| |\det J_{f_S}(x)|$$

이를 **프레임 단위 자기회귀 인수분해**로 재표현하면:[1]

```math
p_\theta(x) = \prod_{n=1}^{N} p_D(u_n | u_{ < n}) |\det J_{f_S}(x_n)|
```

여기서:
- $\(f_D\)$: 깊은 블록(Deep block) - 글로벌 시간적 맥락 캡처, 프레임 간 인과 의존성 모델링
- $\(f_S\)$: 얕은 블록(Shallow block) - 각 프레임 내에서만 작동, 국소적 세부 정보 처리
- $\(u_n = f_S(x_n)\)$: 프레임 $\(n\)$의 국소 잠재 변수
- $\(u_{ < n}\)$: 이전 프레임들의 국소 잠재 변수

### 3.4 유니버셜 근사 특성

글로벌-로컬 구조가 유니버셜 근사성을 보존하는 이유:[1]

얕은 스택 $\(f_S\)$는 교대 인과 마스크를 통해 **각 픽셀마다 무한 가우시안 혼합**을 실현합니다. 따라서 $\(f_S\)$를 프레임 내로 제한해도 표현성이 손상되지 않습니다.

### 3.5 흐름-점수 매칭(Flow-Score Matching)

노이즈 증강 학습에서 생성된 약간 노이즈가 있는 샘플을 정제하기 위해 제안된 방법입니다:[1]

$$\mathcal{L}_{denoise}(\phi) = \mathbb{E}_{x, \epsilon} \left\| s_\phi(\tilde{x}) - \sigma \nabla_{\tilde{x}} \log p_\theta(\tilde{x}) \right\|_2^2$$

여기서:
- $\(\tilde{x} = x + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)\)$
- $\(s_\phi\)$: 학습 가능한 경량 노이즈 제거기(8계층 Transformer)
- 목표: 흐름의 점수 $\(\sigma \nabla_{\tilde{x}} \log p_\theta(\tilde{x})\)$를 근사

**핵심 장점**:
1. 신경망의 매끄러운 귀납적 편향이 고주파 잡음 제거
2. $\(s_\phi\)$에 **원-프레임 룩어헤드**로 엄격한 인과성 유지

추론 시 업데이트:[1]

$$\hat{x} = \tilde{x} + \sigma s_\phi(\tilde{x}) \approx \tilde{x} + \sigma^2 \nabla_{\tilde{x}} \log p_\theta(\tilde{x})$$

## 4. 모델 구조 상세 분석

### 4.1 심-얕 분해(Deep-Shallow Decomposition)

STARFlow-V의 용량 배분 전략:[1]

| 구성 요소 | 역할 | 계산 복잡도 |
|---------|------|-----------|
| 깊은 블록(fD) | 시맨틱 모델링, 시간적 추론 | 높음 (주요 용량) |
| 얕은 블록(fS) | 국소적 재형성, 프레임 내 구조 | 낮음 |

**모델 크기**:
- 3B 모델: fD 폭 3072, 깊이 LD
- 7B 모델: fD 폭 4096 (3B에서 확장)
- fS와 denoiser: 모든 크기에서 동일

### 4.2 자기회귀 블록-와이즈 야코비 반복

표준 자기회귀 디코딩의 $\(15 \times\)$ 속도 향상을 달성하는 방법입니다:[1]

**고정점 시스템 재해석**:

Equation (2.2)의 역변환을 고정점 방정식으로 표현:[1]

$$x = \mu_\theta(x \odot m) + \sigma_\theta(x \odot m) \cdot z$$

**블록-와이즈 야코비 반복**:

$\(B = \{B_1, \ldots, B_J\}\)$로 블록 파티션하면, 각 블록 내에서 병렬 반복:[1]

$$x_i^{(k+1)} = \mu_\theta(x_{ < i}^{(k)}) + \sigma_\theta(x_{ < i}^{(k)}) \cdot z_i$$

수렴 기준: $\(\frac{\|x^{(k+1)} - x^{(k)}\|_2^2}{\|x^{(k+1)}\|_2^2} < \tau\), \(\tau = 0.001\)$

**비디오 인식 초기화**:

새 프레임 \(n+1\)의 초기 추정값을 이전 수렴 프레임에서 초기화: \(x_n^{(0)} = x_n^{(k)}\)

이는 자연 비디오의 강한 글로벌 구조로 인해 수렴을 크게 가속화합니다.

### 4.3 파이프라인 디코딩

병렬 처리 구조:[1]

```
fD 처리 → fS 처리 → 디노이징 (동시 병렬 실행)
```

fD가 일반적으로 가장 느리므로, 엔드-투-엔드 지연은 fD에 의해 주도됩니다.

## 5. 성능 향상 및 기여

### 5.1 정량적 성능 비교

VBench 벤치마크 결과:[1]

| 모델 | 총점 | 품질 | 의미론 | 시간 일관성 | 공간 추론 |
|-----|------|------|--------|-----------|---------|
| WAN 2.1-Causal† | 74.96 | 77.41 | 65.15 | 56.04 | 76.51 |
| NOVA AR† | 75.31 | 77.46 | 66.70 | 56.04 | 79.68 |
| **STARFlow-V†** | **79.70** | **80.76** | **75.43** | **59.73** | **80.61** |

STARFlow-V는 자기회귀 확산 기반 모델들을 **모든 차원에서 능가**합니다.

### 5.2 오류 축적 완화

Figure 3 비교에서 관찰되는 현상:[1]

- **NOVA**: 30초 장기 생성 시 점진적 흐림 및 신원 손실
- **WAN 2.1-Causal**: 강한 색상 왜곡 및 인공물
- **STARFlow-V**: 명확하고 날카로운 프레임, 시간적 일관성 유지

**근본 원인 분석**:

글로벌-로컬 분해의 이점:[1]

1. **유니모달 잠재 공간**: 픽셀 공간의 다중 모달 분포 대신, 잠재 공간 \(u\)에서는 각 단계마다 유니모달 가우시안
2. **역 변환 인버티빌리티**: 샘플링 중 이전에 생성된 **잠재**에만 조건화되므로, 데이터 공간의 오류가 전파되지 않음
3. **정보 손실 방지**: 확산 모델의 노이즈 조건화와 달리, 매핑 \(u \leftrightarrow x\)가 역변환 가능하므로 정보 손실 없음

### 5.3 공정한 비교를 위한 절제 연구

**디노이저 선택 비교** (Figure 5):[1]

| 방법 | PSNR↑ | SSIM↑ |
|-----|-------|-------|
| 노이즈 없음 | 32.22 | 0.8907 |
| 디코더 파인튜닝 | 23.95 | 0.6403 |
| 점수 기반 디노이징 | 22.05 | 0.6490 |
| **흐름-점수 매칭(제안)** | **26.69** | **0.7601** |

흐름-점수 매칭이 기존 방법들을 명확하게 능가합니다.

**블록-와이즈 야코비 초매개변수**:

- 블록 크기 64: 첫 프레임에 최적
- 블록 크기 512: 후속 프레임에 최적 (비디오 인식 초기화 포함)
- 평균 15배 지연 감소

## 6. 모델의 일반화 성능 향상 가능성

### 6.1 현재의 일반화 성과

**확장 길이 생성 능력**:

STARFlow-V는 훈련 길이(5초, 81프레임)를 **6배 초과**한 30초까지 생성할 수 있으며, 기준선 모델들과 달리 **블러 및 구조적 변형 없음**을 달성합니다.[1]

**기계적 메커니즘**:

슬라이딩 윈도우 청크-투-청크 스케줄을 통해:[1]

1. 최근 \(\Delta\) 잠재에서 KV 캐시 재구축
2. 다음 \(N - \Delta\) 잠재까지 자기회귀 계속
3 경계 불일치 완화를 위해 훈련 중 첫 프레임 임의 드롭

### 6.2 일반화 향상을 위한 이론적 토대

**정규화 흐름의 유니버셜 근사성**:

STARFlow-V는 다음을 상속합니다:[1]

Equation (3.2)의 글로벌-로컬 구조에서:
- 얕은 스택 \(f_S\)는 **교대 인과 마스크로 프레임 내 무한 가우시안 혼합** 실현
- 깊은 블록 \(f_D\)는 **프레임 간 글로벌 맥락** 캡처

결합하면 **연속 분포의 유니버셜 근사**를 유지합니다.

### 6.3 예상 일반화 개선 방향

**데이터 품질 및 규모의 영향**:

논문의 제한사항 논의에서:[1]

> "(2) 데이터 품질과 스케일링. 진행은 데이터셋 노이즈와 편향에 의해 제한되며; 현재 큐레이션에서 명확한 스케일링 법칙을 관찰하지 못함"

**개선 가능성**:

1. **도메인별 미세조정**: 전문 데이터(로봇공학, 시뮬레이션, 특정 움직임)에서의 어댑테이션
2. **물리적 제약 학습**: 비물리적 생성 사례 감소를 위한 물리 기반 손실 함수
3. **능동 데이터 선택**: 도전적인 고움직임 시나리오에 중점

### 6.4 세계 모델로서의 가능성

STARFlow-V는 **행동 조건화 기능**을 제공하여 세계 모델로 사용될 수 있습니다:[1]

- 로봇공학의 정책 평가
- 게임/시뮬레이션 환경
- 구체화 AI 애플리케이션

엔드-투-엔드 가능도 추정은 불확실성 정량화를 가능하게 하여, 강화학습 기반 계획에 유리합니다.

## 7. 모델의 한계

### 7.1 공식적 한계 (논문에 명시됨)

**1. 지연 시간** (Latency)[1]

> "가능도 기반 추론에도 불구하고, 추론은 일반 GPU에서 실시간 성능과는 거리가 멈"

- 480p 5초 비디오: 약 30분 (최적화 전)
- 블록-와이즈 야코비로 15배 개선되었으나, 여전히 실시간 미달

**2. 데이터 품질 및 스케일링**[1]

- 현재 큐레이션에서 명확한 스케일링 법칙 미흡
- 데이터셋 노이즈와 편향의 제한적 효과

**3. 비물리적 생성**[1]

Figure 6 실패 사례:
- 문어가 항아리 벽을 통과
- 염소 착지 시 바위가 갑자기 나타남

원인: 현재 모델 규모와 데이터의 부족

### 7.2 암묵적/구조적 한계

**(1) 계산 효율성 대 품질 트레이드오프**:

블록-와이즈 야코비는 수렴 반복을 증가시켜, 블록 크기가 매우 클 때 런타임이 다시 증가합니다.

**(2) 인과성의 엄격한 제약**:

엄격한 인과성 유지를 위해 "원-프레임 룩어헤드" 제한을 두면, 더 나은 디노이징을 위해 미래 정보를 완전히 활용하지 못할 수 있습니다.

**(3) 정규화 흐름의 구조적 제약**:

역변환 가능성 요구로 인해 아키텍처 선택에 제약이 있을 수 있습니다 (확산 모델보다 덜 유연함).

## 8. 최신 관련 연구 탐색 (2020년 이후)

### 8.1 정규화 흐름의 재등장 (2024-2025)

**TARFlow & STARFlow (2024-2025)**[2][3]

- **논문**: "Normalizing Flows are Capable Generative Models" (ICML 2024 Spotlight)
- **혁신**: Transformer 기반 자기회귀 흐름으로 ImageNet 64×64에서 **3.0 BPD 이하** 달성 (최초)
- **STARFlow**: 이를 고해상도 이미지 합성(256×256)으로 확장, 상태 기술 수준 확산 모델과 경쟁

**Goku (2025)**[4]

- 정류 흐름(Rectified Flow) 변형을 활용한 이미지-비디오 결합 생성
- 업계 선도적 성능 달성

### 8.2 비디오 생성의 최신 동향

**흐름 매칭 기반 방법들 (2024-2025)**

1. **Pyramid Flow (2024)**[5]
   - 피라미드 구조 흐름 매칭으로 효율적 비디오 생성
   - 다중 해상도 단계화 접근

2. **WAN 2.1/2.2 (2025)**[6][7]
   - 흐름 매칭 기반, 비인과적 생성
   - 높은 품질이나 스트리밍 제약

3. **FluxFlow (2025)**[8]
   - 시간적 정규화를 통한 비디오 확산 모델 개선
   - U-Net, DiT, 자기회귀 아키텍처 지원

**비디오 편집 발전 (2025)**

- **FiVE 벤치마크**: Pyramid-Edit, Wan-Edit 등 훈련-프리 편집 방법[9]
- **FloVD**: 광학 흐름 기반 카메라-제어 비디오 생성[10]

### 8.3 세계 모델 관점의 진화 (2024-2025)

**장기 지평 생성**[11][12]

- **WorldWeaver**: 깊이 신호를 활용한 장기 비디오 일관성 유지
- **상태-공간 모델 기반 세계 모델**: 트랜스포머 주의의 계산 비용을 회피하고 장기 메모리 확장

**행동 조건화 진전**:

강화학습을 위한 정책 평가 및 계획 능력 개선

### 8.4 인과성 및 스트리밍 생성

**CausVid (2024-2025)**[7][6]

- 자기회귀 확산 모델에서 인과적 구조화 도입
- STARFlow-V보다 먼저 이 문제 다루었으나, 여전히 확산 프레임워크 내에서 작동

**STARFlow-V의 장점**:

- 순전 정규화 흐름으로 **네이티브 인과성** 제공
- 확산 모델의 **비엔드-투-엔드 학습** 문제 해결

## 9. 앞으로의 연구에 미치는 영향

### 9.1 정규화 흐름의 부활

**패러다임 전환 신호**:

STARFlow-V 성공은 다음을 시사합니다:[2][1]

> "정규화 흐름은 비디오 생성의 유망한 연구 방향이며, **세계 모델 구축을 위한 기초**가 될 수 있다"

**학계 영향**:

- 기존의 "확산 모델 중심" 비디오 생성 연구 풍경에 변화
- 정규화 흐름이 "진지한 대안"으로 재평가

### 9.2 엔드-투-엔드 가능도 학습의 중요성

**이론적 진전**:

STARFlow-V는 다음을 입증합니다:

1. **비분산 최대 가능도는 확산의 서로게이트 목적함수보다 우수할 수 있음**
2. **정확한 가능도 계산은 실제로 확장 가능함**

**응용 분야**:

- 불확실성 정량화 (강화학습, 로봇공학)
- 이상 탐지 (비디오 감시)
- 신뢰도 높은 생성 평가

### 9.3 인과성-효율성-품질 트라이아드

STARFlow-V는 세 가지 상충하는 요구사항의 균형을 시연합니다:

| 속성 | 확산 모델 | 이산 자기회귀 | STARFlow-V |
|------|---------|----------|-----------|
| **인과성** | 약함 | 강함 | **강함** |
| **효율성** | 중간 | 낮음 | **중간-높음** (개선 중) |
| **품질** | 높음 | 중간 | **높음** |

### 9.4 잠재 공간 모델링의 재강조

**STARFlow-V의 설계 선택**:

- 고해상도 원본 이미지 대신 **잠재 공간에서 작동**
- 3D 인과 VAE로 시공간 압축

**미래 영향**:

- 토크나이저/VAE 아키텍처의 중요성 재인식
- "더 나은 잠재 공간" 설계에 대한 연구 부흥

### 9.5 멀티태스크 단일 모델 패러다임

**STARFlow-V의 유연성**:

동일한 백본으로 T2V, I2V, V2V 지원하는 것은:[1]

- 역변환 가능성의 자연스러운 귀결
- **조건화 신호만 변경**하면 됨

**영향**: 향후 모델들이 "특정 작업별 모델" 대신 **통합 기반 모델** 개발을 추구할 것으로 예상

## 10. 향후 연구 시 고려할 점

### 10.1 직접적인 후속 연구 방향

**1. 추론 지연 감소**

- **단축 법칙 개발**: 흐름 모델을 위한 증류(distillation) 또는 프루닝
- **하드웨어 최적화**: 구조 최적화 및 컴파일러 기반 가속
- **고정점 방정식 대안**: 더 빠른 수렴을 위한 다른 반복 체계 탐색

**2. 데이터 큐레이션 재방문**

- **능동 데이터 선택**: 도전적인 움직임 및 물리적 시나리오에 중점
- **합성 데이터 생성**: 물리적 제약이 있는 데이터 확대
- **스케일링 법칙 연구**: 정규화 흐름에 특화된 스케일링 동력학 탐색

**3. 물리적 타당성 개선**

- **물리 기반 손실 함수**: 광학 흐름, 깊이 기하학 일관성 제약
- **세계 모델 상호 작용**: 행동 조건화를 통한 물리 이해 향상
- **세밀한 모션 제어**: 정확한 카메라/객체 움직임 모델링

### 10.2 이론적 해석 심화

**1. 정규화 흐름의 표현력**

- Transformer 자기회귀 흐름이 왜 충분히 강력한가에 대한 더 깊은 분석
- 글로벌-로컬 분해의 이론적 보장 및 한계 규명
- 근사 오류 경계(approximation error bounds) 도출

**2. 노이즈 증강의 역할**

- 왜 가우시안 노이즈 증강이 중요한가? (흐름 특화 설명)
- 최적 노이즈 수준 결정 방법
- 흐름-점수 매칭의 수렴 특성 분석

### 10.3 응용 확장

**1. 세계 모델 구축**

- 행동 조건화 완전 구현
- 불확실성 정량화를 통한 계획 알고리즘 개발
- 로봇공학 및 강화학습 실제 배포

**2. 조건화 메커니즘 다양화**

- 스케치/마스크 기반 컨트롤 (현재 I2V/V2V에만 제한)
- 3D 카메라 제어 신호 지원
- 오디오-비디오 동기화

**3. 상호작용형 및 스트리밍 애플리케이션**

- 실시간 지연을 수용하는 아키텍처 최적화
- 스트리밍 편집 (사용자 입력 반응)
- 게임/VR 통합

### 10.4 비교 및 벤치마킹

**1. 공정한 비교 프레임워크**

- 인과적 vs 비인과적 생성: 각각이 최적인 시나리오 정의
- 생성 품질 vs 인과성: 정량적 트레이드오프 곡선
- 확산 vs 흐름: 동등 조건 비교 (모델 크기, 훈련 시간)

**2. 표준 평가 메트릭**

- VBench 외 추가 벤치마크 (시뮬레이션 환경, 특화 도메인)
- 불확실성 정량화 평가 (정규화 흐름의 고유한 강점)

### 10.5 조직적 고려 사항

**개방성 및 재현성**:

- 코드 공개 (이미 GitHub에서 제공) 강조
- 체크포인트 가용성 증대
- 커뮤니티 기여 구조 수립

**더 광범위한 생태계 구축**:

- 다양한 백본 및 토크나이저에서 STARFlow-V의 호환성 탐색
- 다른 양식(오디오, 3D, 멀티모달) 확장

## 11. 결론

STARFlow-V는 **정규화 흐름 기반 비디오 생성의 획기적 성취**이며, 다음을 입증합니다:

1. **정규화 흐름은 비디오 생성에 실제로 경쟁력 있음**
2. **엔드-투-엔드 가능도 학습이 실용적이고 유리할 수 있음**
3. **인과성, 효율성, 품질을 동시에 달성 가능**
4. **세계 모델 및 상호작용형 애플리케이션의 기초 제공**

그러나 **지연 시간, 데이터 품질, 물리적 타당성**에서 여전히 개선이 필요합니다.

향후 연구는 이 방법의 **이론적 기반을 강화**하고, **실제 응용 성숙도를 높이며**, **혼합 확산-흐름 패러다임의 탐색**으로 진행될 것으로 예상됩니다. STARFlow-V의 성공은 **생성 모델 분야의 더 광범위한 패러다임 전환**을 의미할 수 있습니다.

***

## 참고 문헌 요약

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b253b994-f7df-4bb8-9686-17d65ddd88f2/2511.20462v2.pdf)
[2](https://openreview.net/forum?id=3YguS2rxdk)
[3](https://arxiv.org/html/2412.06329v3)
[4](https://arxiv.org/html/2502.04896v1)
[5](http://arxiv.org/pdf/2410.05954.pdf)
[6](https://www.xunhuang.me/blogs/world_model.html)
[7](https://yenchenlin.github.io/blog/2025/01/08/video-generation-models-explosion-2024/)
[8](https://arxiv.org/html/2503.15417)
[9](https://arxiv.org/html/2503.13684v1)
[10](https://openaccess.thecvf.com/content/CVPR2025/papers/Jin_FloVD_Optical_Flow_Meets_Video_Diffusion_Model_for_Enhanced_Camera-Controlled_CVPR_2025_paper.pdf)
[11](https://arxiv.org/html/2508.15720v1)
[12](https://ryanpo.com/ssm_wm/)
[13](https://www.semanticscholar.org/paper/46367bfd8cc4859570d5aa71dba492625ad37b20)
[14](https://link.springer.com/10.1007/s43926-025-00233-2)
[15](https://arxiv.org/abs/2401.10874)
[16](https://www.mdpi.com/2072-4292/17/12/2102)
[17](https://ieeexplore.ieee.org/document/10237279/)
[18](https://link.springer.com/10.1007/s00371-024-03752-1)
[19](https://ieeexplore.ieee.org/document/10655680/)
[20](https://aacrjournals.org/cancerimmunolres/article/13/2_Supplement/B001/751659/Abstract-B001-Effective-generation-of-potent-tumor)
[21](https://aacrjournals.org/cancerres/article/85/8_Supplement_1/6745/759813/Abstract-6745-Generation-and-validation-of-a)
[22](https://aacrjournals.org/cancerres/article/85/8_Supplement_1/3103/756868/Abstract-3103-Perspectives-of-novel-tissue)
[23](https://arxiv.org/pdf/1903.01434.pdf)
[24](https://arxiv.org/html/2305.03989v3)
[25](https://arxiv.org/pdf/2501.13918.pdf)
[26](http://arxiv.org/pdf/2303.08320v3.pdf)
[27](https://arxiv.org/abs/2511.20462)
[28](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06030.pdf)
[29](https://arxiv.org/html/2511.20462v2)
[30](https://arxiv.org/abs/2506.06276)
[31](https://openreview.net/pdf/41b6c84d08c5ccd4938bb0798e893d6167abd660.pdf)
[32](https://github.com/apple/ml-starflow)
[33](https://starflow-v.github.io/paper.pdf)
[34](https://jiataogu.me/papers/gu2025starflow.pdf)
[35](https://www.tavus.io/post/from-random-noise-to-real-images-understanding-diffusion-and-flow-matching)
[36](https://arxiv.org/abs/2506.10097)
[37](https://www.itm-conferences.org/10.1051/itmconf/20257803023)
[38](https://isprs-annals.copernicus.org/articles/X-5-W3-2025/57/2025/)
[39](https://arxiv.org/abs/2509.00448)
[40](https://ieeexplore.ieee.org/document/11236851/)
[41](https://www.semanticscholar.org/paper/6c708659768e470f63d06f791ff8420e7ff0feac)
[42](https://gynecology.orscience.ru/2079-5831/article/view/678992)
[43](https://a916407.fmphost.com/fmi/webd/ASAdb49?script=doi-layout&$SearchString=https://doi.org/10.56315/PSCF9-25Lents)
[44](https://aca.pensoft.net/article/151223/)
[45](https://ijamjournal.org/ijam/publication/index.php/ijam/article/view/559)
[46](https://arxiv.org/pdf/1705.07057.pdf)
[47](https://arxiv.org/abs/1804.00779)
[48](https://arxiv.org/pdf/2112.04643.pdf)
[49](https://arxiv.org/pdf/2310.16624.pdf)
[50](https://arxiv.org/pdf/2111.12506.pdf)
[51](http://arxiv.org/pdf/2302.12024v2.pdf)
[52](https://arxiv.org/pdf/2212.14424.pdf)
[53](https://arxiv.org/pdf/1606.04934.pdf)
[54](https://www.emergentmind.com/topics/transformer-autoregressive-flow-tarflow)
[55](https://icml.cc/virtual/2025/poster/46564)
[56](https://uplatz.com/blog/transformer-based-normalizing-flows-tarflow-a-comprehensive-analysis/)
[57](https://openreview.net/forum?id=2uheUFcFsM)
[58](https://arxiv.org/abs/2412.06329)
[59](https://www.themoonlight.io/en/review/normalizing-flows-are-capable-generative-models)
[60](https://opendrivelab.github.io/challenge2025/WM_Duke.pdf)
