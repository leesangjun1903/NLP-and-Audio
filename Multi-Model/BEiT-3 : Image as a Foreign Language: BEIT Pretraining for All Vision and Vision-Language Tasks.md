# BEiT-3 : Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks

**핵심 주장 및 주요 기여**  
이 논문은 **이미지를 외국어(“Imglish”)로 간주**하여, 이미지·텍스트·이미지-텍스트 쌍을 통일된 마스크-예측(mask-and-predict) 방식으로 전처리·학습하는 BEIT-3 모델을 제안한다. Multiway Transformer 기반의 **단일 백본(shared backbone)**과 단일 사전학습(task)으로 다양한 비전 및 비전-언어 과제에서 기존 최첨단 성능을 일괄적으로 경신한 것이 주요 기여이다.

***

## 1. 해결하고자 하는 문제  
기존 비전-언어 파운데이션 모델들은  
- 멀티태스크(대조학습, 정합학습 등)를 복합적으로 사용하여 **스케일 업에 비효율**  
- 이미지, 텍스트 간 **상호작용 방법이 모델마다 상이**하여 범용성 저하  
- 대규모 배치 크기(32K–65K) 요구로 **전산 비용** 문제  

를 안고 있다. 본 연구는 단일 마스크-예측(task)으로 이미지·텍스트·페어 데이터를 통일 학습해 **모델 단순화·효율화**를 도모한다.

***

## 2. 제안하는 방법  
### 2.1. 통일된 사전학습 과제: Masked Data Modeling  
- 텍스트 → 15% 토큰 마스킹  
- 이미지 패치 → 40% 블록 마스킹  
- 이미지-텍스트 쌍 텍스트 → 50% 토큰 마스킹  
- **목표**: 마스킹된 토큰/비주얼 토큰을 원본 토큰으로 복원  

```math
\mathcal{L} = \mathbb{E}_{X\sim\mathcal{D}}[-\log P_\theta(X_{\text{mask}}\mid X_{\setminus\text{mask}})].
```

### 2.2. 백본 구조: Multiway Transformer  
- **공유 self-attention** 모듈 + **모달리티별 FFN 전문가(experts)**  
- 각 레이어: 비전 전문가(V-FFN), 언어 전문가(L-FFN)  
- 상위 3개 레이어: 비전-언어 전문가(VL-FFN) 추가  
- 입력 모달리티에 따라 전문가 선택 경로(route)  

### 2.3. 모델·데이터 스케일  
- 레이어 40, 히든 사이즈 1408, 총 파라미터 1.9B  
- 사전학습 데이터: 이미지-텍스트 쌍 21M, 이미지 14M, 텍스트 코퍼스 160GB  
- 배치 크기 6144로 **대조학습 대비 1/5 이하**  

***

## 3. 성능 향상  
| 과제           | 기준 모델 대비 개선폭 |
|--------------|-------------------|
| NLVR2        | +5.6%p           |
| VQAv2        | +1.7%p           |
| COCO 캡셔닝   | CIDEr +2.3       |
| COCO 검출    | APbox +0.4       |
| ADE20K 분할  | mIoU +1.4        |
| ImageNet 분류| Top-1 +0.6%p     |

- **이미지-텍스트 retrieval**: 파인튜닝·제로샷 모두 최상위 달성  
- **Dense prediction 과제**(검출·분할)에서도 대규모 파운데이션 모델과 대등하거나 우위  

***

## 4. 한계 및 일반화 성능  
- **고비용 환경**: 1.9B 모델 학습에 대규모 클러스터 필요  
- **데이터 편향**: 공공 코퍼스 중심, 도메인 일반화 시 추가 조정 필요  
- **추론 속도**: 거대 모델로 인한 실시간 응용 제약  

**일반화 성능 강화 가능성**  
- Multiway 전문가 구조로 모달리티별 표현 분리·공유 가능  
- 단일 과제로 모달리티 간 일관된 패턴 학습, 전이학습 능력 우수  
- 중간 파인튜닝(intermediate finetuning)으로 추가 도메인·태스크 확장 용이  

***

## 5. 향후 연구에 미치는 영향 및 고려사항  
- **멀티모달 확장**: 오디오·멀티언어 통합으로 범용 파운데이션 모델 연구 가속  
- **In-context multimodal learning**: LLM 방식으로 다중 모달 예시 내장 학습  
- **효율성 개선**: 전문가 수 감소·지식 증류로 경량 모델 개발  
- **데이터 다양성**: 의료·위성 등 특수 도메인 데이터로 일반화 평가  
- **안정성·공정성**: 대규모 모델의 윤리적·사회적 영향 연구 강화  

이 논문은 **“이미지도 언어처럼”** 사전학습하는 새로운 패러다임을 열어, 범용 멀티모달 파운데이션 모델의 단순화·효율화·성능 향상을 모두 제시한 획기적 연구로 앞으로 다양한 후속 연구의 기초가 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/2d67846f-309c-4ff0-b8e0-911386b57db7/2208.10442v2.pdf)
