# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

### 1. 핵심 주장 및 주요 기여

BLIP(Bootstrapping Language-Image Pre-training)은 비전-언어 사전학습(Vision-Language Pre-training, VLP) 분야의 근본적인 문제를 해결하는 통합 프레임워크입니다. 논문의 핵심 주장은 **기존 VLP 모델들이 이해 기반 작업(예: 이미지-텍스트 검색)과 생성 기반 작업(예: 이미지 캡셔닝)을 동시에 우수하게 수행하지 못하며, 웹에서 수집된 대규모 노이즈 데이터는 모델 학습의 부차최적 신호 원천**이라는 것입니다.[1]

BLIP의 주요 기여는 두 가지입니다:

**(1) Multimodal Mixture of Encoder-Decoder (MED) 아키텍처**: 유니모달 인코더, 이미지-기반 텍스트 인코더, 이미지-기반 텍스트 디코더 세 가지 기능을 수행할 수 있는 유연한 모델 구조를 제안합니다. 이를 통해 이해 기반과 생성 기반 작업을 모두 효과적으로 수행할 수 있습니다.[1]

**(2) Captioning and Filtering (CapFilt) 데이터셋 부트스트래핑 방법**: 웹 데이터의 노이즈 문제를 해결하기 위해 캡셔너가 합성 캡션을 생성하고 필터가 노이즈를 제거하는 이중 메커니즘을 제안합니다.[1]

이러한 접근으로 BLIP은 이미지-텍스트 검색에서 평균 recall@1 대비 +2.7%, 이미지 캡셔닝에서 CIDEr +2.8%, VQA에서 +1.6%의 성능 향상을 달성했습니다.[1]

***

### 2. 해결하고자 하는 문제와 핵심 방법론

#### 2.1 문제 정의

기존 VLP 연구의 두 가지 핵심 제약 사항:[1]

**모델 관점**: 인코더 기반 모델은 텍스트 생성 작업으로의 직접 전이가 어렵고, 인코더-디코더 모델은 이미지-텍스트 검색 작업에서 성능이 떨어집니다.[1]

**데이터 관점**: 기존 SOTA 모델들(CLIP, ALBEF, SimVLM 등)은 웹에서 수집한 이미지-텍스트 쌍을 학습에 사용하지만, 이 데이터는 심각한 노이즈 문제를 가집니다.[1]

#### 2.2 MED 모델 아키텍처

MED는 동일한 매개변수 기반에서 세 가지 모드로 작동합니다:[1]

**(1) Unimodal Encoder**: 이미지와 텍스트를 각각 독립적으로 인코딩하여 각 모달리티의 표현을 생성합니다. 이미지 인코더는 ViT(Vision Transformer)를 사용하고, 텍스트 인코더는 BERT 구조를 기반합니다.

**(2) Image-grounded Text Encoder**: 텍스트 인코더의 각 Transformer 블록에 크로스 어텐션(CA) 계층을 삽입하여 시각 정보를 통합합니다. [Encode] 토큰의 출력 임베딩이 멀티모달 표현으로 사용됩니다.

**(3) Image-grounded Text Decoder**: 이미지-기반 텍스트 인코더에서 양방향 자기 어텐션(SA)을 인과적 자기 어텐션으로 대체하여 텍스트 생성을 가능하게 합니다.

#### 2.3 사전학습 목적 함수

세 가지 목적 함수가 동시에 최적화됩니다:[1]

**Image-Text Contrastive Loss (ITC)**:

$$\mathcal{L}_{ITC} = -\log \frac{\exp(\text{sim}(v, t) / \tau)}{\sum_{j} \exp(\text{sim}(v, t_j) / \tau)}$$

여기서 $v$는 이미지 표현, $t$는 텍스트 표현, $\tau$는 온도 파라미터이며, 모멘텀 인코더를 사용하여 특성 공간을 정렬합니다.

**Image-Text Matching Loss (ITM)**:

$$\mathcal{L}_{ITM} = -[y \log(p) + (1-y) \log(1-p)]$$

여기서 $y$는 이미지-텍스트 쌍이 긍정/부정인지를 나타내고, $p$는 ITM 헤드의 예측 확률입니다. 하드 네거티브 마이닝 전략이 적용됩니다.

**Language Modeling Loss (LM)**:

$$\mathcal{L}_{LM} = -\sum_{t=1}^{T} \log P(x_t | x_1, ..., x_{t-1}, v)$$

텍스트 토큰을 자기회귀적으로 예측하며, 0.1의 라벨 스무싱을 적용합니다.

#### 2.4 CapFilt 데이터 부트스트래핑

CapFilt는 두 가지 구성 요소로 작동합니다:[1]

**캡셔너**: 이미지-기반 텍스트 디코더를 사용하여 웹 이미지에 대한 합성 캡션을 생성합니다. 널클레우스 샘플링(nucleus sampling, p=0.9)을 사용하여 다양성 있는 캡션을 생성합니다.

**필터**: 이미지-기반 텍스트 인코더를 사용하여 원본 웹 텍스트와 합성 텍스트의 노이즈를 제거합니다. ITM 헤드의 예측값을 기반으로 부정 매칭 쌍을 필터링합니다.

최종 데이터셋: $D = \{(I_h, T_h)\} + \{(I_w, T_w')\} + \{(I_w, T_s')\}$

여기서 $I_h$, $T_h$는 인간 주석 데이터, $T_w'$, $T_s'$는 필터링된 텍스트입니다.

#### 2.5 핵심 설계 결정

**매개변수 공유**: 텍스트 인코더와 디코더는 자기 어텐션 계층을 제외한 모든 매개변수를 공유합니다. 이는 인코딩과 디코딩의 근본적 차이인 양방향 vs 인과적 어텐션을 각각 유지하면서도 효율성을 극대화합니다.[1]

**매개변수 디커플링**: CapFilt에서 캡셔너와 필터는 매개변수를 공유하지 않습니다. 이는 확인 편향(confirmation bias)을 방지하고 노이즈 비율을 높게 유지하여 필터링 효과를 극대화합니다.[1]

***

### 3. 모델 구조의 세부 사항

#### 3.1 아키텍처 구성

MED는 단일 사전학습 모델에서 세 가지 역할을 수행합니다:[1]

| 구성 요소 | 특징 | 용도 |
|---------|------|------|
| 이미지 인코더 | ViT-B/16 또는 ViT-L/16 | 이미지 패치 인코딩 |
| 텍스트 인코더 | BERT 기반 + 크로스어텐션 | 이미지-기반 텍스트 표현 |
| 텍스트 디코더 | BERT 기반 + 인과적 SA | 자기회귀 텍스트 생성 |

#### 3.2 연산 효율성

각 이미지-텍스트 쌍당:[1]

- 시각 트랜스포머를 통한 1회 포워드 패스
- 텍스트 트랜스포머를 통한 3회 포워드 패스(ITC, ITM, LM)

공유된 매개변수로 인해 전체 모델 크기를 크게 절감하면서도 다양한 작업 성능을 유지합니다.

***

### 4. 성능 향상 분석

#### 4.1 CapFilt의 효과

표 1 분석 결과:[1]

- 원본 14M 이미지: 기저 성능
- 캡셔너만 적용: ~0.7% 향상
- 필터만 적용: ~0.6% 향상
- 캡셔너+필터: **+2.2% 누적 향상** (상호 보완 효과)
- 129M 이미지 + CapFilt: **+2.3% 추가 향상** (확장성 입증)

**다양성 중요성**: 표 2에서 빔 서치(deterministic) vs 널클레우스 샘플링(stochastic) 비교:[1]

| 생성 방법 | 노이즈 비율 | 성능 개선 |
|---------|-----------|---------|
| 빔 서치 | 19% | 낮음 |
| 널클레우스 샘플링 | 25% | 높음 |

높은 노이즈 비율에도 불구하고 널클레우스 샘플링이 더 나은 성능을 보이는 것은 **다양한 캡션이 새로운 정보를 제공**하기 때문입니다.[1]

#### 4.2 다중 하위 작업에서의 우수성

BLIP은 광범위한 비전-언어 작업에서 SOTA 성능을 달성합니다:[1]

**이미지-텍스트 검색**:
- COCO (Text Retrieval@1): 80.6% → 82.4% (ViT-L 모델)
- Flickr30K 영점이동: 96.7% → 97.4%
- 선행 모델 ALBEF 대비 +2.7% 평균 개선

**이미지 캡셔닝**:
- COCO Karpathy: CIDEr 129.7 → 136.7
- NoCaps: CIDEr 105.1 → 113.2
- 매개변수 공유 설계가 생성 능력 저하 없이 효율성 달성

**시각 질의응답 (VQA)**:
- VQA 2.0 테스트: 77.54% → 78.24% (129M 이미지)
- ALBEF 대비 +1.64% 개선

**자연언어 시각 추론 (NLVR2)**:
- 테스트-P: 82.30% → 83.08%
- 복잡한 멀티모달 추론 능력 입증

***

### 5. 일반화 성능 및 강점

#### 5.1 영점이동 일반화

BLIP의 주목할 만한 특성은 **학습되지 않은 도메인으로의 강력한 전이 능력**입니다:[1]

**비디오-언어 작업 (영점이동)**:
- 텍스트-비디오 검색 (MSRVTT): R@1 43.3% (미세 조정 모델 18.7%보다 +24.6%)
- 비디오 QA (MSRVTT-QA): 19.2% 정확도 (비디오 전용 학습 모델과 경쟁)

이는 **시각 표현의 강건한 일반화 능력**을 시사합니다.

#### 5.2 데이터 확장성

CapFilt는 데이터셋 크기와 모델 크기 모두에 대해 확장성을 보입니다:[1]

**데이터 확장**: 14M → 129M (LAION 포함)
- 기본 성능 향상
- CapFilt 효과의 지속적 유지

**모델 확장**: ViT-B/16 → ViT-L/16
- 캡셔너/필터 고도화로 더 큰 모델의 학습 데이터 품질 향상
- 기저 모델 성능 추가 상승

#### 5.3 하드 네거티브 마이닝의 역할

ITM 목적에서 하드 네거티브 마이닝을 적용하면:[1]
- 배치 내에서 높은 대조 유사도를 가진 부정 쌍을 우선 선택
- 미묘한 이미지-텍스트 부정합을 학습하도록 강제
- 모델의 판별 능력 향상

***

### 6. 한계점 및 제약 사항

#### 6.1 CapFilt 방법의 한계

**반복 부트스트래핑 문제**: 표 13에서 보인 바와 같이, 부트스트래핑된 데이터셋으로 기존 사전학습 모델을 계속 학습하면 성능 개선이 미미합니다. 이는 **새로운 모델을 처음부터 학습해야**하므로 계산 비용이 증가합니다.[1]

**캡셔너-필터 매개변수 의존성**: 캡셔너와 필터가 동일한 사전학습 모델에서 초기화되므로, 초기 모델의 품질이 부트스트래핑 성능을 결정합니다.

**노이즈 비율의 트레이드오프**: 표 2와 4에서 알 수 있듯이:[1]
- 매개변수 공유 시: 노이즈 비율 8% (낮은 필터링)
- 매개변수 미공유 시: 노이즈 비율 25% (높은 필터링)
- 더 높은 필터링이 더 나은 성능을 제공하지만, 관리해야 할 매개변수 증가

#### 6.2 도메인 외 성능의 제한

**NLVR2 작업의 제한적 개선**: 표 8에서 NLVR2는 웹 이미지 추가로 큰 성능 향상을 보이지 않습니다. 이는 **웹 데이터와 다운스트림 도메인 간의 갭**을 시사합니다.[1]

**시간적 정보 무시**: 비디오 작업에서는 프레임을 단순 연결하여 시간적 정보를 무시합니다. 이는 시간적 추론이 필요한 복잡한 비디오 이해 작업에서 성능을 제한합니다.[1]

#### 6.3 계산 비용

**비디오 프레임 샘플링**: 텍스트-비디오 검색에 8프레임, 비디오 QA에 16프레임을 사용하는 것은:[1]
- 계산 비용 대비 정보 손실의 트레이드오프
- 고해상도 비디오 이해에는 부족

***

### 7. 최신 연구에 미치는 영향 (2024-2025)

#### 7.1 아키텍처 진화

BLIP 이후 비전-언어 모델 연구는 다음과 같은 방향으로 발전했습니다:[2]

**CogVLM (2024)**: BLIP의 얕은 정렬 방식을 넘어, 사전학습된 언어 모델의 주의 및 FFN 계층에 학습 가능한 시각 전문가 모듈을 통합하여 **깊은 융합(deep fusion)**을 달성합니다.[2]

**BLIP-2 (2023)**: BLIP의 후속 모델로, 명령어 튜닝을 통해 다양한 비전-언어 작업을 통합적으로 처리하고, 영점이동 성능을 더욱 향상시킵니다.[3]

**LLaVA 시리즈 (2023-2025)**: CLIP 비전 인코더와 LLM을 결합하는 간단한 구조로, 멀티모달 정렬을 개선하며, 2025년 LLaVA-NeXT는 향상된 멀티모달 정렬 성능을 보입니다.[4]

#### 7.2 데이터 품질 개선의 중요성

BLIP의 CapFilt는 **데이터 품질이 모델 성능의 핵심 요소**임을 입증했으며, 최근 연구는 이를 확장합니다:[5]

- **고해상도 학습**: 긴 깨끗한 캡션은 더 높은 해상도(448 이상)로 학습하여 환각 완화
- **다단계 부트스트래핑**: BLIP의 단일 라운드를 넘어 여러 반복 개선 시도
- **문맥 기반 개인화**: 2025년 연구는 동일한 대상을 여러 장면에서 일관되게 식별하도록 학습 데이터 재구성[6]

#### 7.3 일반화 연구의 확대

최근 종합 조사(2025)는 VLM의 일반화를 체계적으로 정리합니다:[7]

**프롬프트 기반 방법**: BLIP의 영점이동 능력을 확장하여 프롬프트 엔지니어링으로 도메인 적응

**매개변수 기반 방법**: LoRA, Adapter 등을 사용한 효율적 전이학습

**특성 기반 방법**: 특성 공간 정렬을 통한 도메인 일반화[7]

#### 7.4 약한 모델에서 강한 모델로의 적응 전이

2025년 연구 TransMiter는 BLIP의 적응 전이 개념을 확장합니다:[8]

- 약한 모델(BLIP-B)의 적응 지식을 강한 모델(BLIP-L)로 전이
- 역전파 없는 경량 어댑터 구조로 효율성 달성
- BLIP 이후 **모델-무관적 적응의 중요성** 입증[8]

#### 7.5 멀티모달 대규모 언어 모델(MLLM)의 등장

GPT-4V, Qwen-VL 등 MLLM은 BLIP을 기반으로 진화했습니다:[4]

- **시각 특성 인코딩**: 더 높은 차원의 시각 표현
- **토큰 압축**: VisionZip 등 기술로 계산 효율성 개선[9]
- **전체 모달리티 통합**: 이미지, 텍스트, 오디오의 동시 처리

***

### 8. 앞으로의 연구 고려 사항

#### 8.1 해결해야 할 과제

**1. 반복 부트스트래핑 최적화**

BLIP은 단일 라운드 부트스트래핑만 수행합니다. 향후 연구는:
- 여러 라운드의 반복 개선 메커니즘 개발
- 수렴 기준 정의 및 최적 반복 횟수 결정
- 계산 효율성과의 균형

**2. 시간적 모달리티 통합**

비디오 작업에서의 현재 프레임 연결 방식은 시간 정보를 무시합니다:
- TimeSformer 등 시간 모델링 메커니즘 통합
- 프레임 간 일관성 학습
- 수정자(modifier) 기반 시간적 주의

**3. 다국어 및 문화 간 일반화**

현재 BLIP은 주로 영어 데이터에 최적화되어 있습니다:[4]
- 다국어 사전학습 데이터 확대 (한국어 포함)
- 문화적 편향 제거
- 저자원 언어 지원

**4. 도메인 특화 최적화**

NLVR2에서의 제한적 개선은 도메인 갭을 시사합니다:
- 도메인별 특화 어댑터
- 메타-학습 기반 빠른 적응
- 도메인 내 부트스트래핑 전략

#### 8.2 기술적 개선 방향

**1. 효율성 강화**

- 시각 토큰 중복성 제거 (VisionZip 등)
- 저정밀도 학습(Mixed Precision) 최적화
- 지식 증류 기반 경량 모델

**2. 환각(Hallucination) 완화**

멀티모달 모델의 고질적 문제:
- 부정 샘플 가중치 조정
- 시각-언어 일관성 제약
- 신뢰도 기반 생성 제어

**3. 해석 가능성 개선**

- 크로스 어텐션 시각화
- 결정 경로 추적
- 오류 분석 자동화

#### 8.3 응용 연구 방향

**1. 실시간 비디오 이해**

현재의 정적 프레임 기반에서:
- 스트리밍 비디오 처리
- 장시간 비디오 이해
- 동적 장면 추론

**2. 개인화된 비전-언어 모델**

- 사용자 기반 선호도 학습
- 개인 데이터 프라이버시 보호
- 연합 학습(Federated Learning) 적용

**3. 로봇 비전과의 통합**

- 실시간 로봇 제어 명령 생성
- 3D 환경 이해
- 인간-로봇 상호작용 언어 생성

***

### 결론

BLIP은 비전-언어 사전학습 연구에서 **모델 구조와 데이터 품질이라는 이중 혁신**을 제시했습니다. MED 아키텍처의 유연한 설계와 CapFilt의 지능적 데이터 처리는 이후 연구의 기준이 되었습니다.[3][1]

특히 BLIP이 주요 성과인 **강력한 영점이동 일반화 능력**과 **데이터 확장성**은 현재 MLLM 시대로 이행하는 핵심 기초가 되었습니다. 다만 도메인 특화 작업에서의 개선 여지, 시간적 정보 통합, 그리고 계산 효율성 간의 트레이드오프는 여전히 해결해야 할 과제입니다.[7][4][1]

2025년 현재, BLIP의 구조적 통찰은 Qwen-VL, LLaVA-NeXT 등 최신 VLM에 계승되었으며, **데이터 품질 개선의 중요성은 더욱 강조**되고 있습니다. 향후 연구자들은 BLIP의 기본 원칙을 유지하면서 멀티모달 AI의 강건성, 효율성, 그리고 해석 가능성을 동시에 추구해야 할 것입니다.[8][4][7]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/fbc73729-303f-4c8c-b837-b19c598fa1ba/2201.12086v2.pdf)
[2](https://arxiv.org/pdf/2311.03079v1.pdf)
[3](http://arxiv.org/pdf/2305.06500.pdf)
[4](https://naakjii.tistory.com/141)
[5](https://junhan.blog/posts/MLLM-Survey)
[6](https://blog.naver.com/kcc_press/224067953839?fromRss=true&trackingCode=rss)
[7](https://arxiv.org/abs/2506.18504)
[8](https://arxiv.org/abs/2508.08604)
[9](https://arxiv.org/abs/2412.04467)
[10](http://arxiv.org/pdf/2501.02189.pdf)
[11](https://aclanthology.org/2023.findings-acl.49.pdf)
[12](https://arxiv.org/pdf/2109.01134.pdf)
[13](http://arxiv.org/pdf/2404.12139.pdf)
[14](http://arxiv.org/pdf/2410.22217.pdf)
[15](https://lunaleee.github.io/posts/blip/)
[16](https://velog.io/@ldj923/VLMVision-Language-Model-%EA%B8%B0%EC%88%A0-%ED%98%84%ED%99%A9-BLIP%EC%97%90%EC%84%9C-LLaVA%EA%B9%8C%EC%A7%80)
[17](https://aclanthology.org/2021.conll-1.13/)
[18](https://mvje.tistory.com/260)
[19](https://brunch.co.kr/@twodreams09/171)
