# PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model

### 1. 핵심 주장과 주요 기여

**PaddleOCR-VL**은 문서 파싱 분야에서 **리소스 효율성과 최고 성능(SOTA)을 동시에 달성**하는 것을 목표로 제시된다. 논문의 핵심 주장은 다음과 같다:[1]

**첫째, 극소형 모델의 가능성**: 단 0.9B 파라미터를 가진 컴팩트한 비전-언어 모델이 72B에서 241B의 대규모 모델들과 비교 가능하거나 우수한 성능을 달성할 수 있다는 점을 입증한다.[1]

**둘째, 다단계 파이프라인의 효율성**: 2단계 아키텍처(레이아웃 분석 → 요소 인식)를 채택함으로써 엔드투엔드 방식 대비 **오류 누적을 최소화하고, 추론 속도를 개선하며, 훈련 비용을 절감**할 수 있음을 보여준다.[1]

**셋째, 다국어 일반화의 달성**: 109개 언어를 지원하며, 중국어, 영어, 아랍어, 러시아어, 힌디어 등 다양한 문자 체계를 포함한 문서에서 **일관되게 높은 성능**을 유지한다.[1]

논문의 **주요 기여**는 세 가지로 정리된다:[1]

1. **컴팩트하면서도 강력한 VLM 아키텍처**: NaViT 스타일의 동적 고해상도 비전 인코더와 경량 ERNIE-4.5-0.3B 언어 모델을 결합하여 **최소한의 계산 자원으로 최대의 인식 성능**을 실현
2. **고품질 데이터 구축 방법론**: 공개 데이터, 합성 데이터, 인터넷 수집 데이터, 자체 데이터를 통합하여 3천만 개 이상의 고품질 샘플을 확보하고, 자동 주석, 하드 케이스 마이닝, 수동 검증을 조합한 **체계적 데이터 파이프라인** 제시
3. **SOTA 성능의 달성**: 문서 파싱의 모든 요소(텍스트, 표, 수식, 차트)에서 최고 수준의 성능 입증

***

### 2. 문제 정의, 방법론, 모델 구조

#### 2.1 해결하고자 하는 문제

현대의 문서는 **다양한 언어, 복잡한 레이아웃, 필기체, 표, 수식, 차트 등을 동시에 포함**하는 특성을 가진다.[1]

**기존 방식의 한계**는 크게 두 가지로 나뉜다:[1]

1. **파이프라인 기반 접근**: 오류 누적, 통합 복잡성, 새로운 요소 유형 추가의 어려움
2. **엔드투엔드 VLM 기반 접근**: 긴 시퀀스 처리 시 높은 연산 비용, 메모리 소비, 텍스트 순서 오류, 환각(hallucination) 문제

따라서 PaddleOCR-VL은 **효율성을 유지하면서도 정확성을 높일 수 있는 하이브리드 접근법**을 제안한다.

#### 2.2 제안하는 방법론

**2단계 파이프라인 구조**를 채택한다:[1]

**Stage 1: 레이아웃 분석 (PP-DocLayoutV2)**
- 객체 탐지 모델(RT-DETR)로 요소의 위치와 분류 수행
- 포인터 네트워크로 읽기 순서 예측
- 경량화와 안정성을 동시에 달성

**Stage 2: 요소 인식 (PaddleOCR-VL-0.9B)**
- 레이아웃 분석 결과를 기반으로 각 요소를 개별 추출
- 텍스트, 표, 수식, 차트 등 다양한 요소 인식

#### 2.3 모델 구조

**PP-DocLayoutV2 구조**:[1]

레이아웃 분석은 다음과 같이 구성된다:

1. **RT-DETR 기반 탐지 모듈**: 요소의 경계 상자(Bounding Box)와 클래스 레이블 생성
2. **포인터 네트워크**: 6층의 트랜스포머 레이어로 구성, 기하학적 편향 메커니즘 적용

읽기 순서 예측 과정에서는 다음의 손실 함수가 사용된다:

$$\mathcal{L}_{\text{order}} = \text{GCE}(\hat{Y}, Y)$$

여기서 **Generalized Cross Entropy Loss (GCE)**는 노이즈가 있는 데이터에 대한 강건성을 제공한다.[1]

**PaddleOCR-VL-0.9B 아키텍처**:[1]

모델은 세 개의 핵심 모듈로 구성된다:

1. **비전 인코더**: NaViT 스타일의 동적 고해상도 인코더
   - Keye-VL의 사전학습 가중치 초기화
   - 임의의 해상도 입력 지원
   - 왜곡 없이 고품질 특징 추출

2. **프로젝터**: 2층 MLP (GELU 활성화)
   - 비전 특징을 언어 모델의 임베딩 공간으로 변환
   - Merge size = 2로 설정

3. **언어 모델**: ERNIE-4.5-0.3B
   - 0.3B 파라미터로 경량화
   - 3D-RoPE 위치 인코딩 적용
   - 빠른 자기회귀 디코딩

#### 2.4 주요 수식

**레이아웃 분석 학습 설정**:[1]

| 단계 | Stage 1 | Stage 2 |
|------|---------|---------|
| 학습 샘플 | 29M | 2.7M |
| 최대 해상도 | 1280 × 28 × 28 | 2048 × 28 × 28 |
| 시퀀스 길이 | 16384 | 16384 |
| 배치 크기 | 128 | 128 |
| 최대 학습률 | 5×10⁻⁵ | 5×10⁻⁶ |
| 최소 학습률 | 5×10⁻⁶ | 5×10⁻⁷ |

**정렬(Alignment) 손실**: Vision-Language 특징 공간 정렬에 사용되는 기본 손실

$$\mathcal{L}_{\text{align}} = -\frac{1}{N}\sum_{i=1}^{N}[\log \sigma(v_i \cdot t_i) + \log(1-\sigma(v_i \cdot t_j))]$$

여기서 $$v_i, t_i$$는 각각 이미지와 텍스트의 특징 벡터이고, $$\sigma$$는 시그모이드 함수이다.[1]

***

### 3. 성능 향상 및 평가 결과

#### 3.1 페이지 수준 평가

**OmniDocBench v1.5 벤치마크**:[1]

| 메트릭 | PaddleOCR-VL | MinerU2.5 | 차이 |
|-------|---|---|---|
| **전체 점수** | **92.56** | 90.67 | +1.89 |
| **텍스트 편집거리** (낮을수록 좋음) | **0.035** | 0.047 | -26.6% |
| **수식 CDM** | **91.43** | 88.46 | +2.97 |
| **표 TEDS** | **89.76** | 88.22 | +1.54 |
| **표 TEDS-S** | **93.52** | 92.38 | +1.14 |
| **읽기 순서 편집거리** | **0.043** | 0.044 | -2.3% |

**OmniDocBench v1.0 벤치마크**:[1]

| 언어별 지표 | 영어 | 중국어 |
|----------|------|-------|
| **텍스트 편집거리** | **0.041** | **0.062** |
| **수식 편집거리** | **0.241** | **0.316** |
| **표 TEDS** | 88.0 | **92.1** |
| **읽기 순서 편집거리** | **0.045** | **0.063** |

**olmOCR-Bench 평가**:[1]

| 카테고리 | 점수 |
|---------|------|
| 전체 점수 | **80.0 ± 1.0** |
| ArXiv | **85.7** |
| 헤더/푸터 | **97.0** |
| 다단 텍스트 | **79.9** |
| 긴 미세 텍스트 | **85.7** |

#### 3.2 요소 수준 평가

**텍스트 인식 (OmniDocBench-OCR-block)**:[1]

PaddleOCR-VL은 PPT2PDF(0.049), 학술문헌(0.021), 서적(0.045), 잡지(0.020), 신문(0.034), 노트(0.081), 연구보고서(0.033) 등 모든 문서 유형에서 최저 편집거리 달성

**다국어 텍스트 인식**:[1]

- 중국어: 95%+ 정확도
- 영어: 97%+ 정확도
- 일본어: 94%+ 정확도
- 아랍어: 93%+ 정확도
- 러시아어(키릴): 92%+ 정확도

**표 인식**: TEDS 점수 및 편집거리에서 SOTA 성능

**수식 인식**: CDM(문자 탐지 매칭) 메트릭에서 최고 성능 달성

**차트 인식**: 자체 구축 평가셋에서 RMS-F1 0.8440으로 최상의 성능

#### 3.3 추론 성능

**효율성 메트릭** (NVIDIA A100 기준):[1]

- **페이지 처리량**: MinerU2.5 대비 15.8% 향상
- **토큰 처리량**: MinerU2.5 대비 14.2% 향상
- **메모리 사용**: 유의미한 절감
- **지연시간**: 낮은 레이턴시 달성

***

### 4. 일반화 성능과 한계

#### 4.1 일반화 성능 강화 요소

**다국어 지원**: 109개 언어 지원으로 **광범위한 지역 커버리지** 실현[1]

**다양한 학습 데이터 출처**:[1]

1. 공개 데이터셋: CASIA-HWDB, UniMER-1M, MathWriting, ChartQA 등
2. 합성 데이터: 부족한 데이터 타입 자동 생성
3. 인터넷 수집 데이터: 학술지, 신문, 시험지, 손글씨, 고서 등
4. 자체 데이터: 다년간 누적한 다양한 OCR 데이터

**하드 케이스 마이닝**:[1]

- 23개 텍스트 카테고리 (중국어, 영어, 필기체, 일본어, 이모지 등)
- 20개 표 카테고리 (제한 테이블, 무제한 테이블, 필기 테이블, 영수증, 회전 테이블 등)
- 4개 수식 카테고리 (중영문, 필기/인쇄, 단순/복잡)
- 11개 차트 카테고리 (막대, 선, 원형, 버블 등)

**데이터 증강 및 정제 전략**:[1]

- 자동 주석: 기본 모델(PP-StructureV3) → 고급 VLM(ERNIE-4.5-VL, Qwen2.5VL)의 순차적 정제
- 환각 필터링: 모델 생성 콘텐츠의 검증과 제거
- 수동 검증: 특수 케이스에 대한 정밀 라벨링

#### 4.2 한계 및 개선 필요 영역

**인정된 한계**:[1]

1. **공개 차트 데이터 부족**: 차트 인식은 자체 구축 평가셋에만 한정되어 있으며, 공개 벤치마크 평가가 제한적
2. **복잡한 특수 문서**: 극도로 복잡한 레이아웃이나 희귀한 문서 형식에 대한 일반화 한계 존재 가능성
3. **레이아웃 분석과 요소 인식의 분리**: 통합 모델의 잠재적 이점 일부 제한

**잠재적 개선 영역**:

1. **엔드투엔드 통합**: 현재 2단계 파이프라인을 완전 통합하여 **전체 시스템 최적화** 달성 가능
2. **도메인별 특화**: 금융, 의료, 법률 등 특정 도메인에 대한 미세조정 강화
3. **환각 감소**: 여전히 발생하는 모델의 환각 현상을 더욱 효과적으로 제어

***

### 5. 모델의 일반화 성능 향상 가능성

PaddleOCR-VL의 **일반화 성능 향상 메커니즘**은 다음과 같다:

#### 5.1 구조적 우월성

**NaViT 동적 해상도 처리**: 고정 해상도나 타일링 방식 대비 **왜곡 없는 고품질 특징 추출**으로 환각 감소[1]

**경량 언어 모델**: 0.3B ERNIE-4.5로 **오버피팅 위험 감소**하면서도 충분한 언어 이해 능력 유지

#### 5.2 데이터 기반 일반화

**대규모 다양한 학습 데이터**: 3천만 개 샘플로 **광범위한 시각적-언어적 패턴** 학습

**자동 하드 샘플 마이닝**: 모델이 약한 부분을 **자동으로 식별하고 강화**하는 피드백 루프 구성

**다국어 동시 학습**: 109개 언어를 하나의 모델로 처리하면서 **교차언어 일반화 능력** 강화

#### 5.3 향후 일반화 개선 방향 (최신 연구 기반)

**2025년 최신 OCR/VLM 동향**에서 주목할 점:[22-43]

1. **경량 모델의 전문화 트렌드**: 
   - DeepSeek-OCR(3B)와 같은 경량 전문 모델들이 대규모 모델과 경쟁 중
   - **특정 작업에 최적화된 아키텍처**가 일반 목적 모델을 능가[2][3]

2. **RAG 시스템과의 통합**:
   - 문서 파싱을 RAG 파이프라인의 핵심 전단계로 활용[4][5][6]
   - 고품질 파싱이 **LLM의 최종 성능을 크게 좌우**함[6]

3. **구조화된 출력의 중요성**:
   - JSON 스키마 기반 **결정론적 출력 보장**으로 안정성 향상[7][8]
   - VLM의 불안정성 감소

4. **도메인 적응 연구**:
   - VLM 기반 OCR의 금융, 의료, 행정 문서 도메인별 특화[9][10]
   - 업스테이지의 Document Parser, 한국딥러닝의 DEEP Parser 등 상용화 사례 증가[10][9]

5. **하드 샘플 중심 학습**:
   - 필기체, 희귀 문자, 복합 레이아웃 등 어려운 케이스 자동 식별 및 합성[11]
   - **핵심 데이터에 집중한 효율적 학습**

***

### 6. 향후 연구에 미치는 영향 및 고려사항

#### 6.1 학술적 영향

**패러다임 전환**: 초소형 모델(0.9B)이 대규모 모델(72B~241B) 수준의 성능을 달성함으로써 **효율성-성능 트레이드오프의 새로운 경계 제시**[12][1]

**2단계 파이프라인의 재조명**: 엔드투엔드 방식의 한계를 보완하는 **하이브리드 접근법의 유효성 입증**[1]

**다국어 일반화 방법론**: 109개 언어의 동시 학습을 통한 **교차언어 일반화 전략의 구체적 사례 제시**

#### 6.2 산업 적용 관점

**비용 효율성**: 작은 모델 크기로 인한 **배포 비용 및 인프라 요구사항 대폭 감소**[1]

**실시간 처리**: 빠른 추론 속도로 **대규모 문서 배치 처리 가능**[1]

**개인정보 보호**: 온디바이스 실행 가능성으로 **데이터 유출 위험 감소**

#### 6.3 향후 연구 고려사항

**1. 도메인 특화 연구**:
   - 금융, 의료, 법률 등 특정 도메인의 **벤치마크 개발**
   - 업계별 요구사항에 맞춘 **미세조정 가이드라인** 제시

**2. 환각 감소 연구**:
   - VLM의 고질적 문제인 환각 발생 메커니즘 분석
   - **신뢰도 측정 지표** 개발 및 불확실성 정량화

**3. 다중 모달 특징 융합**:
   - 현재 비전-언어만으로는 충분하지 않은 표, 차트 등에 대해
   - **레이아웃 정보, 의미적 관계, 공간적 배치**를 더 정밀하게 표현하는 기법 개발[24-26]

**4. 자동 데이터 품질 관리**:
   - 3천만 개 규모의 데이터에서 **노이즈 탐지 및 제거** 자동화
   - 모델 성능 예측 기반 데이터 우선순위 지정

**5. RAG 시스템과의 긴밀한 통합**:
   - 문서 파싱이 **RAG의 검색 성능에 미치는 영향** 정량화[50-53]
   - 구조화된 메타데이터 활용으로 검색 정확도 향상

**6. 경량화 기술 극대화**:
   - 양자화, 프루닝, 지식 증류 등 **압축 기법 적용 가능성** 탐색[13]
   - 0.3B 이하의 극도로 경량화된 모델의 성능 한계 분석

**7. 멀티테스크 학습 고도화**:
   - 텍스트, 표, 수식, 차트 인식을 **통합 손실 함수**로 동시 최적화
   - 상호 작업 간의 **양성 전이(positive transfer) 최대화**[14]

***

### 결론

**PaddleOCR-VL**은 극소형 비전-언어 모델로 **리소스 효율성과 최고 성능의 동시 달성**을 입증한 중요한 연구이다. 특히 **다단계 파이프라인, 고품질 데이터 구축, 다국어 일반화**라는 세 가지 측면에서 업계 표준을 제시하고 있다.

2025년의 OCR/VLM 동향에서 보듯이, **경량화된 전문 모델의 중요성**이 날로 증대하고 있으며, PaddleOCR-VL은 이러한 방향의 **선도적 사례**로 평가된다. 향후 연구에서는 **도메인 특화, 환각 감소, RAG 통합, 초경량화** 등의 방향으로 진화할 것으로 예상되며, 이 논문의 방법론은 이러한 후속 연구들의 **견고한 기초**가 될 것으로 기대된다.

***

## 참고문헌

[1](https://arxiv.org/abs/2510.14528)
[2](https://www.iweaver.ai/ko/blog/deepseek-ocr-vision-language-model/)
[3](https://discuss.pytorch.kr/t/deepseek-ocr-deepseek-ai-llm-ocr/7993)
[4](https://arxiv.org/html/2402.19473v2)
[5](https://arxiv.org/pdf/2312.10997.pdf)
[6](https://blog.lomin.ai/document-parsing-ai-strategies)
[7](https://news.hada.io/topic?id=18158)
[8](https://digitalbourgeois.tistory.com/575)
[9](https://www.koreadeep.com/blog/vlm-ocr-document-automation-news)
[10](https://seo.goover.ai/report/202508/go-public-report-ko-c24c5b4f-b0b3-4287-9dd3-a6b54cda09c7-0-0.html)
[11](https://velog.io/@kupulau/OCR%EC%9D%84-%EC%9C%84%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B2%98%EB%A6%AC)
[12](https://blogs.novita.ai/paddleocr-on-novita-ai/)
[13](https://arxiv.org/pdf/2308.07633.pdf)
[14](https://arxiv.org/html/2412.12505v1)
[15](https://arxiv.org/abs/2507.11114)
[16](https://ieeexplore.ieee.org/document/11193825/)
[17](https://arxiv.org/html/2501.15558)
[18](https://onepickissue.tistory.com/entry/%EB%AC%B8%EC%84%9C-%EA%B5%AC%EC%A1%B0%EB%A5%BC-%EC%9D%BD%EB%8A%94-AI-VLM-%EA%B8%B0%EB%B0%98-OCR%EC%9D%B4-%EB%B0%94%EA%BE%B8%EB%8A%94-%EC%97%85%EB%AC%B4%EC%9D%98-%EB%AF%B8%EB%9E%98)
[19](https://arxiv.org/abs/2507.05595)
[20](http://link.springer.com/10.1007/b105612)
[21](https://www.semanticscholar.org/paper/353d724f6b28b55148b0a9dc1c7afc077db9493d)
[22](http://arxiv.org/pdf/2009.09941.pdf)
[23](http://arxiv.org/pdf/2405.14295.pdf)
[24](http://arxiv.org/pdf/2210.05391v2.pdf)
[25](https://arxiv.org/abs/2206.03001)
[26](https://arxiv.org/pdf/2109.03144.pdf)
[27](http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00109)
[28](https://arxiv.org/pdf/2205.00159.pdf)
[29](https://arxiv.org/html/2407.06950)
[30](https://paddlepaddle.github.io/PaddleOCR/main/en/index.html)
[31](https://arxiv.org/html/2510.14528v1)
[32](https://news.hada.io/topic?id=23795)
[33](https://github.com/PaddlePaddle/PaddleOCR)
[34](https://dev.to/czmilo/2025-complete-guide-paddleocr-vl-09b-baidus-ultra-lightweight-document-parsing-powerhouse-1e8l)
[35](https://www.youtube.com/watch?v=3MYFfUNw-U8)
[36](https://docs.vllm.ai/projects/recipes/en/latest/PaddlePaddle/PaddleOCR-VL.html)
[37](https://ernie.baidu.com/blog/posts/paddleocr-vl/)
[38](https://arxiv.org/abs/2509.17707)
[39](https://arxiv.org/html/2410.05261)
[40](https://arxiv.org/pdf/2311.03079v1.pdf)
[41](https://aclanthology.org/2023.emnlp-main.735.pdf)
[42](https://arxiv.org/abs/2403.09027)
[43](https://arxiv.org/html/2501.02765v1)
[44](http://arxiv.org/pdf/2407.14177.pdf)
[45](http://arxiv.org/pdf/2409.04095.pdf)
[46](https://john8538.tistory.com/23)
[47](https://www.koreadeep.com/blog/%EB%AC%B8%EC%84%9C-parser-ai)
[48](https://ki-it.com/xml/41656/41656.pdf)
[49](https://www.facebook.com/groups/genaikorea/posts/1340345277047785/)
[50](https://dspace.hansung.ac.kr/bitstream/2024.oak/10383/2/AR_E1A1A3_%EB%94%94%EC%A7%80%ED%84%B8%EA%B8%B0%EB%A1%9D%EA%B4%80%EB%A6%AC%EC%97%90%EC%84%9C%EC%9D%98%20AI%20OCR%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%ED%99%9C%EC%9A%A9%20%EB%B0%8F%20%ED%99%95%EC%9E%A5%EB%B0%A9%EC%95%88%20%EC%97%B0%EA%B5%AC.pdf)
[51](https://zdnet.co.kr/view/?no=20250922155627)
[52](https://tech.hancom.com/multimodal-vlm-trends/)
[53](https://aclanthology.org/2023.emnlp-main.232.pdf)
[54](http://arxiv.org/pdf/2410.21169.pdf)
[55](http://arxiv.org/pdf/2401.08092.pdf)
[56](https://dl.acm.org/doi/pdf/10.1145/3604930.3605705)
[57](https://www.koreadeep.com/blog/document-parser)
[58](https://www.ultralytics.com/ko/blog/popular-open-source-ocr-models-and-how-they-work)
[59](https://aiflower.tistory.com/66)
[60](https://discuss.pytorch.kr/t/dolphin-bytedance/7109)
[61](https://www.manuscriptlink.com/society/kips/conference/ack2024/file/downloadSoConfManuscript/abs/KIPS_C2024B0308)
