# Language Models are General-Purpose Interfaces

**핵심 주장 및 주요 기여**  
“Language Models are General-Purpose Interfaces” 논문은 **언어 모델을 범용 작업 레이어로 활용**하여 다양한 모달리티와 작업을 하나의 통합된 프레임워크에서 다루는 **MetaLM**(METALM)을 제안한다.  
- 사전학습된 다중 인코더(언어·비전·다국어 등)가 입력을 **양방향으로 인코딩**하고, 이를 **비(非)인과적(bidirectional) 인코더→일방향(decoder) 언어 모델** 연결부를 통해 접합하여 학습한다.[1]
- **Semi-Causal Language Modeling**이라는 새로운 사전학습 목표를 도입해 인과적·비인과적 모델링 장점을 모두 흡수하며,  

$$ \max \sum_{i=0}^k \sum_{t=e_i}^{s_{i+1}} \log P(x_t\mid x_{ < t}, \{h(x_{e_j:s_j})\}_{j < i}) $$  
  
  의 형태로, 랜덤으로 선택된 span을 bidirectional 인코더로 처리하고 나머지를 autoregressive로 예측한다.[1]
- 결과적으로 **인컨텍스트 학습**, **제로/퓨샷 일반화**, **파인튜닝** 모두에서 특화 모델과 경쟁력 있는 성능을 달성한다.  

***

## 1. 해결하고자 하는 문제  
기존 파운데이션 모델들은 대부분 특정 작업(task)이나 모달리티에 최적화되어 개발되어 왔으며,  
1) **인과적 언어 모델**(GPT 계열)은 제로/퓨샷 학습과 오픈 엔디드 생성에 강하나 파인튜닝 효율이 낮고,  
2) **비인과적 모델**(BERT/T5 등)은 파인튜닝 성능은 뛰어나나 제로/퓨샷 일반화 능력이 제한적이다.  
이를 통합하여 모든 작업과 모달리티를 **자연어 인터페이스** 하나로 다루고자 한다.[1]

***

## 2. 제안 방법

### 2.1 모델 구조  
- **비인과적 인코더**: 언어·비전·다국어 등 입력 모달리티별로 사전학습된 Transformer 인코더  
- **연결자(Connector)**: 각 인코더 출력 차원을 언어 모델 디코더 차원으로 투영  
- **범용 인터페이스(decoder)**: 일방향 Transformer 디코더(GPT-2 아키텍처)  
- 전체 구조: 여러 인코더 출력과 토큰 임베딩을 위치 임베딩과 합산하여 디코더 입력으로 사용.[1]

### 2.2 Semi-Causal 학습 목표  
랜덤으로 선택된 여러 span을 비인과적 인코더로 처리하고, 나머지를 순차적(autoregressive)으로 생성  

$$
\max \sum_{i=0}^{k} \sum_{t=e_i}^{s_{i+1}} \log P(x_t \mid x_{ < t}, \{h(x_{e_j:s_j})\}_{j < i})
$$

- $$h(x_{e_j:s_j})$$: j번째 non-causal span의 인코더 출력  
- 인코더와 디코더를 **공동 학습**하여 양쪽 장점을 통합 획득.[1]

***

## 3. 성능 향상

### 3.1 언어 전용 작업  
- **멀티태스크 파인튜닝**: 34개 데이터셋, 10개 작업군에서 GPT 대비 평균 12.2%p 향상.  
- **단일 작업 파인튜닝**(MNLI): GPT 대비 +3.4%p, BERT/RoBERTa/ELECTRA와 동등 성능.  
- **인스트럭션 튜닝**(제로샷): GPT 대비 평균 +7.6%p 상승.  
- **인컨텍스트 학습**(제로/1/4샷): GPT 대비 전반적 소폭 향상, 특히 zero-shot 평균 60.4%→60.9%.  

### 3.2 비전–언어 작업  
- **제로샷 이미지 캡셔닝**: COCO Karpathy CIDEr 58.3→82.2, NoCaps 42.2→58.7, Flickr30k 31.0→43.3.  
- **제로샷 VQA**: VQAv2 38.6→41.1, OK-VQA 10.5→11.4.  
- **퓨샷 VQA**(1/4샷): VQAv2 38.2→45.3, OK-VQA 12.6→16.0.  
- **파인튜닝 VQA/VL 이해**: VQAv2 test-std 73.4→74.5, NLVR2 out-domain 18.6→21.1, OK-VQA 40.3→46.5.  
- **캡셔닝 파인튜닝**: COCO CIDEr 116.1→126.6.  
- **설명 생성**(E-SNLI-VE): accuracy 79.5→79.9, CIDEr 117.4→119.3.  

***

## 4. 일반화 성능 향상 근거  
1) **인컨텍스트 학습 및 제로샷**: causal LM의 강점 유지  
2) **파인튜닝 성능**: bidirectional encoder의 파인튜닝 효율 획득  
3) **모달 간 전이**: 단일 텍스트-텍스트·이미지-텍스트 통합 학습으로 도메인 간 전이력 개선  
4) **추론 및 대화**: 다중 턴·멀티모달 대화 지원 및 자연어 지시문 활용.  

***

## 5. 연구 영향 및 향후 과제  
- **범용 인터페이스 확장**: 오디오·비디오·센서 데이터 등 추가 모달리티 통합 가능성  
- **다국어 지원**: 다국어 인코더 통합으로 글로벌 범용 AI 실현  
- **파라미터 효율화**: LoRA·IA^3 등 파인튜닝 경량화 기법 적용  
- **컴퓨터 비전 태스크**: 객체 검출·세그멘테이션 등 직접 예측 레이어 확장  
- **지속 학습**: 인터페이스 동적 확장 및 온라인 학습 과제 고려  

**전망**: MetaLM은 범용·확장 가능한 AI 프레임워크의 기틀을 마련하며, 향후 멀티모달·다임적 환경에서 범용 AI 구현을 위한 핵심 토대를 제공할 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/7a8fde2b-a165-48ff-8156-b504726cae26/2206.06336v1.pdf)
