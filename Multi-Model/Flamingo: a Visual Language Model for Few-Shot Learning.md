# Flamingo: a Visual Language Model for Few-Shot Learning

### 1. 핵심 주장과 주요 기여

**Flamingo의 핵심 주장**은 **대규모 사전학습된 비전 인코더와 언어 모델을 효과적으로 결합하면, 소수의 예시만으로 다양한 시각-언어 태스크에 빠르게 적응할 수 있다**는 것입니다.[1]

**주요 기여**는 다음과 같습니다:[1]

- **시각-언어 모델의 아키텍처 혁신**: 세 가지 핵심 기술적 혁신을 제안합니다. (i) 사전학습된 비전 전용 모델과 언어 전용 모델을 연결, (ii) 시각과 텍스트가 임의로 혼합된 시퀀스 처리, (iii) 이미지 또는 비디오의 원활한 입력

- **Perceiver Resampler 구조**: 가변 크기의 고해상도 시각 특성을 고정된 개수의 시각 토큰으로 변환하여 계산 복잡도를 감소시킵니다.[1]

- **Gated Cross-Attention 층**: 새로운 크로스 어텐션 층을 사전학습된 언어 모델 층 사이에 삽입하여 시각 정보를 효과적으로 통합합니다.[1]

- **대규모 평가 및 벤치마크**: 16개의 멀티모달 이미지/비디오 이해 태스크에서 새로운 최신 성능(SOTA)을 달성하며, 6개 태스크에서는 수천 배 많은 태스크 특화 학습 데이터로 미세조정된 모델을 능가합니다.[1]

### 2. 해결하는 문제 및 제안 방법

**문제 정의**

기존 연구의 한계:[1]
- 표준 파이프라인: 대량의 감독 데이터로 사전학습 → 태스크별 미세조정 필요
- 대조적 학습 기반 비전-언어 모델: 유사도 점수만 제공하여 분류 작업만 가능
- 열린 답변 생성 태스크(캡셔닝, 시각 질의응답)에서 성능 부족

**제안 방법**

#### 2.1 모델의 조건부 확률 표현

모델이 텍스트와 시각 입력을 모두 처리하는 기본 수식:[1]

$$P(y_i | y_{ < i}, \mathcal{V}_{ < i}; \theta)$$

여기서 $y_i$는 $i$번째 언어 토큰, $y_{<i}$는 선행 토큰 집합, $\mathcal{V}_{<i}$는 토큰 $i$보다 앞선 이미지/비디오 집합, $\theta$는 Flamingo 모델의 파라미터입니다.[1]

#### 2.2 학습 목표

다중 데이터셋 가중합 손실 함수:[1]

$$\mathcal{L}_{total} = \sum_d w_d \mathbb{E}_{(x,y) \sim \mathcal{D}_d} \left[-\log P(y | x; \theta_d)\right]$$

여기서 $\mathcal{D}_d$는 $d$번째 데이터셋, $w_d$는 데이터셋 가중치입니다.[1]

### 3. 모델 구조

#### 3.1 핵심 아키텍처 컴포넌트

**Flamingo 아키텍처의 세 가지 주요 컴포넌트**:[1]

1. **Vision Encoder (시각 인코더)**
   - 사전학습되고 고정된 NFNet-F6 (Normalizer-Free ResNet) 사용
   - 이미지는 2D 특성 그리드, 비디오는 3D 시공간 특성 그리드로 변환
   - 대조적 목표함수로 사전학습: ALIGN 1.8B 이미지-텍스트 쌍과 LTIP 312M 이미지-텍스트 쌍 사용

2. **Perceiver Resampler**
   - 가변 크기 입력을 고정된 64개의 시각 토큰으로 재샘플링
   - Perceiver와 DETR의 아이디어 적용: 학습된 잠재 쿼리를 사용하여 변동 크기의 시각 특성에 크로스 어텐션
   - 시간 위치 인코딩 추가 (공간 위치 인코딩은 미사용)
   - 공식:

$$X_{out} = \text{MultiHeadAttention}(Q_{latent}, KV_{visual\_features})$$

3. **Gated Cross-Attention Dense 층**
   - 사전학습된 언어 모델 층 사이에 삽입되는 훈련 가능한 새로운 층
   - 시각 특성에서 얻은 키와 값, 언어 입력에서 얻은 쿼리 사용
   - 게이트 메커니즘으로 초기화 시 안정성 보장

**Gated Cross-Attention 공식**:[1]

크로스 어텐션 부분:
$$Y_{ca} = Y + \tanh(\alpha_{xattn}) \times \text{CrossAttn}(Q_Y, KV_X)$$

Feed-Forward 게이팅:
$$Y_{out} = Y_{ca} + \tanh(\alpha_{dense}) \times \text{FFN}(Y_{ca})$$

여기서 $\alpha_{xattn}$과 $\alpha_{dense}$는 0으로 초기화된 학습 가능 스칼라이며, 초기에 모델이 사전학습된 언어 모델과 동일하게 출력하도록 보장합니다.[1]

#### 3.2 다중 이미지/비디오 처리

**이미지별 인과 모델링 (Per-Image Causal Modeling)**[1]
- 각 텍스트 토큰에서 모델은 시퀀스의 바로 앞에 나타난 이미지의 시각 토큰만 어텐션
- 모든 이전 이미지에 대한 의존성은 언어 모델의 자기 어텐션을 통해 유지
- 이 구조는 훈련 중 최대 5개 이미지 사용에도 불구하고, 평가 시 최대 32개 이미지/비디오 쌍 처리 가능

### 4. 학습 데이터 및 훈련 전략

#### 4.1 데이터셋 구성[1]

Flamingo는 세 가지 웹 출처 데이터셋의 혼합으로 훈련됩니다:

1. **M3W (MultiModal MassiveWeb) - 인터리브 이미지-텍스트 데이터셋**
   - 약 4,300만 개의 웹페이지에서 HTML 추출
   - 각 문서에서 256 토큰의 랜덤 서브시퀀스 샘플링
   - 최대 5개 이미지 포함

2. **이미지-텍스트 쌍 데이터셋**
   - ALIGN: 1.8B 이미지-텍스트 쌍
   - LTIP: 312M 개의 장문 이미지-텍스트 쌍 (자체 수집)

3. **비디오-텍스트 쌍 데이터셋 (VTP)**
   - 2,700만 개의 단편 비디오 (~22초 평균)와 문장 설명

#### 4.2 훈련 목표

가중 합 손실로 다중 데이터셋 동시 훈련:
$$\mathcal{L} = \sum_d w_d \mathbb{E}_{(x_d, y_d)} \left[-\log P(y_d | x_d, \theta)\right]$$

**최적화 전략**:[1]
- 모든 데이터셋에 걸쳐 기울기 누적 (라운드 로빈 방식보다 우수)
- 데이터셋별 가중치 조정이 성능의 핵심

### 5. 성능 향상 결과

#### 5.1 벤치마크 성능[1]

Flamingo-80B 모델이 16개 태스크에서 달성한 성능:

| 태스크 | 0-shot | 4-shot | 32-shot | 기존 SOTA |
|--------|--------|--------|---------|-----------|
| OKVQA | 50.6 | 57.4 | 57.8 | 54.4 |
| VQAv2 | 56.3 | 63.1 | 67.6 | 80.2 |
| COCO | 84.3 | 103.2 | 113.8 | 143.3 |
| MSVDQA | 35.6 | 41.7 | 52.3 | 47.9 |
| VATEX | 46.7 | 56.0 | 65.1 | 76.3 |

**핵심 성과**:[1]
- **6개 태스크에서 미세조정 SOTA 초과**: 32-shot 학습으로 수천 배 적은 태스크 특화 데이터 사용
- **9개 태스크에서 few-shot SOTA 달성**: 이전 최고 방법들 능가
- **모델 크기의 확장성**: 3B, 9B, 80B 모델 모두 크기에 따라 성능 향상

#### 5.2 모델 크기별 성능 추세[1]

집계 성능(모든 16개 벤치마크 평균):
- Flamingo-3B: 0-shot 60%, 32-shot 70.2%
- Flamingo-9B: 0-shot 65%, 32-shot 74.5%
- Flamingo-80B: 0-shot 68%, 32-shot 77.4%

**결론**: 더 큰 모델이 일관되게 더 나은 few-shot 학습 성능 보임

### 6. 일반화 성능 향상

#### 6.1 모델 크기의 영향[1]

**확장 법칙 (Scaling Laws)**:
더 큰 언어 모델과 더 많은 시각-언어 훈련 파라미터가 모두 성능 향상에 기여합니다. 특히:
- 언어 모델 크기 증가 (Chinchilla 1.4B → 70B): 누적 성능 32-shot에서 41% 향상
- 비전-텍스트 모듈 크기 증가: 추가 성능 향상

#### 6.2 데이터셋 품질의 중요성[1]

**컨트라스트 모델 사전학습 실험**:

| 데이터셋 | 영상→텍스트 R@1 | 텍스트→영상 R@1 |
|---------|-----------------|-----------------|
| ALIGN만 | 32.2% | 23.7% |
| LTIP만 | 38.6% | 31.1% |
| ALIGN + LTIP (축적) | 42.3% | 31.5% |

**발견**: LTIP가 ALIGN보다 5배 작지만 단독으로 더 나은 성능 → **데이터 품질이 규모보다 중요**[1]

#### 6.3 Few-shot 학습의 특성[1]

**인-컨텍스트 학습의 한계**:
- 32-shot 이후 성능이 정체되는 경향
- 그래디언트 기반 미세조정과 달리 인-컨텍스트 학습은 높은 데이터 레짐에서 추가 이점 제한적

**해결책**:
- RICES (Retrieval In-Context Example Selection): 5000개 지원 예제 중 최고 16개 선택
- ImageNet에서 랜덤 선택 대비 9.2% 향상 (16-shot)

### 7. 한계

#### 7.1 분류 작업에서의 성능 격차[1]

| 모델 | ImageNet Top-1 | Kinetics700 |
|------|---------------|-----------:|
| 미세조정 SOTA | 90.9% | 89.0% |
| Flamingo-80B (0-shot) | 50.6% | 46.4% |
| Flamingo-80B + 16-shot RICES | 76.0% | 63.5% |

**원인**: 대조적 훈련 목표가 텍스트-이미지 검색에 직접 최적화되는 반면, 언어 모델 목표는 그렇지 않음

#### 7.2 언어 모델의 고질적 문제 상속[1]

- **인-컨텍스트 학습의 민감성**: 예제 순서, 형식에 따른 성능 변동
- **시퀀스 길이 한계**: 2048 토큰 이상의 긴 문맥에서 성능 저하
  - VisDial: 32-shot 시 평균 672 문장으로 인해 최대 16-shot으로 제한

#### 7.3 열린 답변 생성의 문제[1]

**환각 (Hallucination)**:
- 모델이 입력 데이터에 근거하지 않은 답변 생성
- 시각적 대화 설정에서 특히 발생 가능
- 해결책: 맥락 있는 예제와 정교한 프롬프트 설계 필요

#### 7.4 편향 및 공정성[1]

COCO 캡셔닝에서 성별, 피부색 기반 편향 평가:
- **성별 편향**: 여성 대 남성 CIDEr 차이 0.03 (통계적 유의성 없음)
- **피부색 편향**: 진한 피부 대 밝은 피부 CIDEr 차이 0.09 (통계적 유의성 없음)

그러나 사전학습 언어 모델과 시각 데이터의 편향 상속 가능성 존재

### 8. 최신 연구 기반 향후 전망

#### 8.1 최신 VLM 발전 방향

**1. Mixture-of-Experts (MoE) 아키텍처**[2]
최신 모델들(Kimi-VL, DeepSeek-VL2, Llama 4)이 MoE 디코더 채택하여:
- 계산 효율성 향상
- 환각 감소
- 광범위한 멀티모달 능력 확대

**2. 크로스 모달 추론 강화**[3]
- GPT-4V, FLAVA 등이 동적 크로스 어텐션으로 비전-언어 특성 심층 융합
- 시각적 문법과 텍스트 개념의 관계 이해 향상

**3. 프롬프트 엔지니어링의 진화**[3]
- PaLI-X: 이미지와 텍스트 템플릿 결합으로 다양한 태스크 통일
- 가중치 조정 대신 입력 프롬프트로 성능 조정

#### 8.2 일반화 성능 개선 전략

**1. 아웃-오브-디스트리뷰션(OOD) 일반화**[4]
- OGEN: 합성 OOD 특성과 적응형 자기 증류로 미세조정 후 성능 개선
- 알려진 클래스 과적합 방지 및 알 수 없는 클래스 성능 향상

**2. 도메인 적응 (Domain Adaptation)**[5]
- VL-FAS: 세밀한 자연어 설명으로 얼굴 영역 중심 주의 조절
- 샘플 레벨 비전-텍스트 최적화로 도메인 의존성 감소

**3. 공간 추론 강화**[6]
- SpatialRGPT: 3D 장면 그래프와 깊이 정보 통합
- 상대 위치, 거리 인식 능력 대폭 향상

#### 8.3 효율성 및 확장성 개선

**1. 파라미터 효율적 미세조정**[7]
- LoRA (Low-Rank Adaptation) 기반 적응
- 미세조정 시간 대폭 단축 (DriveLLaVA 사례)

**2. 경량 어댑터 활용**[3]
- 사전학습 모델에 작은 신경 모듈 삽입
- 일반 지식 보존 + 태스크 특화 적응

**3. 다중 모델 앙상블**[8]
- 여러 VLM의 강점 결합으로 안정적 성능 개선
- 단일 모델의 한계 극복

#### 8.4 새로운 능력 영역

**1. 정밀 오브젝트 인식**[2]
- 객체 감지, 분할, 계수 등 구조화된 출력 생성
- 로컬라이제이션 토큰으로 위치 정보 표현

**2. 체인-오브-쏘트 추론**[9]
- 입력 이미지에서 GPT-4o의 상세 근거 증류
- 강화 학습으로 추론 질 보정

**3. 로봇 제어 및 구체화 AI**[10]
- 비전-언어-행동 흐름 모델 (π0)로 로봇 태스크 학습
- 제로샷 성능 및 언어 명령 추종 능력

### 9. 향후 연구 시 고려할 점

#### 9.1 아키텍처 설계 관점

1. **비전-언어 정렬의 깊이**: 얕은 정렬(CLIP)에서 깊은 정렬(Flamingo의 크로스 어텐션)로 진화하는 추세
2. **대규모 사전학습의 효율성**: 계산 비용 대비 성능 향상의 한계 도달 가능성 고려
3. **다중 모달리티 통합**: 오디오, 3D 정보 등 추가 모달리티 포함 시 아키텍처 확장성

#### 9.2 데이터 선정 및 전처리

1. **품질 대 규모**: LTIP 실험 결과처럼 데이터 품질의 중요성 재평가
2. **데이터 중복 제거**: 훈련-평가 세트 간 누출 방지 필수
3. **다양성 및 편향**: 성별, 인종, 문화적 다양성 보장 및 편향 감소 전략

#### 9.3 평가 방법론

1. **보유 테스트셋 활용**: Flamingo의 DEV/HELD-OUT 분리 방식 적용으로 객관적 평가
2. **새로운 벤치마크 필요**: MMT-Bench, MMMU-Pro 같은 차세대 평가 도구 개발
3. **다양한 능력 평가**: 일반적 벤치마크 + 공간 추론, 시간 이해 등 특화 능력 평가

#### 9.4 실무 배포 고려사항

1. **추론 비용**: 인-컨텍스트 학습의 선형 복잡도 개선 또는 단축 방법 연구
2. **모델 해석성**: 환각 및 편향 문제 해결을 위한 설명 가능성 강화
3. **엣지 디바이스 적응**: 경량 VLM 버전 개발로 실시간 애플리케이션 지원

### 10. 결론

Flamingo는 **사전학습된 비전과 언어 모델의 효과적 결합**을 통해 few-shot 학습 능력을 입증한 선도적 연구입니다. 특히 Perceiver Resampler와 Gated Cross-Attention의 혁신적 설계는 이후 멀티모달 모델 개발의 기초가 되었습니다.[11][1]

최신 연구 동향은 **MoE 아키텍처, OOD 일반화, 공간-시간 추론, 그리고 효율성 개선**에 중점을 두고 있습니다. 향후 연구는 다음 세 가지 핵심 과제에 집중해야 합니다: (1) **일반화 성능 향상** - 도메인 외 샘플과 새로운 클래스에 대한 강건성 개선, (2) **계산 효율성** - 인-컨텍스트 학습의 비용 절감, (3) **모델 신뢰성** - 환각 감소 및 편향 완화를 통한 실무 배포 가능성 확보.[4][5][6][2]

***

**참고**: 본 분석은 DeepMind의 Flamingo 원본 논문(2022년 발표) 및 2024-2025년 최신 비전-언어 모델 연구 기반입니다.[5][6][10][9][8][4][2][3][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8834007a-360b-431d-80ed-df2d9a131c98/2204.14198v2.pdf)
[2](https://huggingface.co/blog/vlms-2025)
[3](https://milvus.io/ai-quick-reference/what-are-the-latest-advances-in-multimodal-fewshot-learning)
[4](https://arxiv.org/abs/2401.15914)
[5](https://ieeexplore.ieee.org/document/10448156/)
[6](https://arxiv.org/abs/2406.01584)
[7](https://dl.acm.org/doi/10.1145/3637528.3671945)
[8](http://arxiv.org/pdf/2311.17091.pdf)
[9](https://arxiv.org/abs/2410.16198)
[10](https://arxiv.org/abs/2410.24164)
[11](https://arxiv.org/abs/2204.14198)
[12](https://ieeexplore.ieee.org/document/11097533/)
[13](https://www.mdpi.com/1424-8220/24/13/4113)
[14](https://ieeexplore.ieee.org/document/10772151/)
[15](https://arxiv.org/abs/2409.09269)
[16](http://arxiv.org/pdf/2406.08394v1.pdf)
[17](http://arxiv.org/pdf/2501.02189.pdf)
[18](https://arxiv.org/pdf/2308.12966.pdf)
[19](https://arxiv.org/pdf/2311.03079v1.pdf)
[20](http://arxiv.org/pdf/2305.11175.pdf)
[21](http://arxiv.org/pdf/2311.12327.pdf)
[22](https://arxiv.org/pdf/2503.12490.pdf)
[23](https://github.com/val-iisc/VL2V-ADiP)
[24](https://www.reddit.com/r/singularity/comments/udwoya/introducing_flamingo_a_generalist_visual_language/)
[25](https://openreview.net/forum?id=7HPmTa_FdY)
[26](https://letter-night.tistory.com/334)
[27](https://www.sciencedirect.com/science/article/abs/pii/S1566253525006955)
[28](https://proceedings.neurips.cc/paper_files/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html)
[29](https://neurips.cc/virtual/2025/poster/115782)
