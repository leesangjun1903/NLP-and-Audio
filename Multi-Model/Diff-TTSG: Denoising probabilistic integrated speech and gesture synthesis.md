
# Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis

## 1. 핵심 주장과 주요 기여

**Diff-TTSG**(Denoising probabilistic integrated speech and gesture synthesis)는 텍스트로부터 자연스러운 음성과 신체 제스처를 동시에 생성하는 **첫 번째 확산 기반 확률 모델**이다. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0ac004bc-0cfb-494f-8c9d-402b494e211f/2306.09417v3.pdf)

### 1.1 핵심 주장

이 논문의 중심 주장은 다음과 같다:

기존의 ISG(Integrated Speech and Gesture Synthesis) 연구는 비확률적 방법을 사용하여 인간 음성과 동작의 변동성을 포착하지 못하고, 과도하게 부드러운 합성(oversmoothing)과 낮은 품질의 제스처를 야기한다. 확산 확률 모델을 이용한 통합 접근법은 이러한 문제를 해결하면서도 작은 규모의 데이터셋(4.5시간)만으로도 효과적으로 학습할 수 있다. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0ac004bc-0cfb-494f-8c9d-402b494e211f/2306.09417v3.pdf)

### 1.2 주요 기여

| 기여 항목 | 설명 |
|---------|------|
| **첫 확산 모델 ISG** | ISG를 위한 첫 번째 확산 기반 확률 모델 제시 |
| **작은 데이터셋 학습** | 대규모 사전 학습 없이 4.5시간 데이터로부터 직접 학습 가능 |
| **단순한 아키텍처** | 다단계 학습 및 네트워크 부분 동결 불필요 |
| **포괄적 평가** | 음성 전용, 제스처 전용, 음성-제스처 적절성의 3중 평가 프레임워크 |
| **성능 개선** | 기존 SOTA 대비 모든 평가 지표에서 유의미한 개선 |

***

## 2. 해결하고자 하는 문제

### 2.1 기술적 문제들

**문제 1: 양식 불일치(Modality Mismatch)**
- 기존 음성 합성(TTS): 읽기 스타일 음성 데이터로 학습
- 제스처 합성(STG): 자발적 음성 데이터로 학습
- 결과: 서로 다른 배우에서 학습한 두 시스템 조합 → 비일관성

**문제 2: 변동성 포착 실패**
- 비확률적 방법(Tacotron 2, Glow-TTS)은 확률 분포 전체를 모델링하지 못함
- 인간의 음성과 제스처는 동일 텍스트에 대해 다양한 실현(realization)이 가능
- 결과: 획일적이고 부자연스러운 합성

**문제 3: 과도한 평활화(Oversmoothing)**
- 결정론적 모델: 모든 가능한 선택지를 평균화 → 저품질 제스처
- T-자세 문제: 관절 회전 공간의 원점으로 회귀하는 팔 뻗은 포즈
- 결과: 부자연스럽고 반복적인 동작

**문제 4: 데이터 효율성**
- 기존 접근법: 24시간 이상의 대규모 음성 데이터셋(LJ Speech)에서 사전 학습 필수
- 자발적 음성-제스처 데이터는 비용이 높음 (마커 기반 모션 캡처 필요)
- 결과: 새로운 도메인/언어 적용 어려움

### 2.2 근본적 동기

인간의 면대면 대화는 **언어적(음성) + 비언어적(제스처) 의사소통의 통합**이다. 두 양식은: [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0ac004bc-0cfb-494f-8c9d-402b494e211f/2306.09417v3.pdf)
- 같은 메시지 표현에서 비롯됨 (공통 기원)
- 수학적으로 유사한 문제: 텍스트 입력 → 연속 벡터 시퀀스 출력
- 통합 모델링이 더 효율적: 중복성 제거, 더 작은 모델 크기, 빠른 합성

***

## 3. 제안하는 방법(수식 포함)

### 3.1 Grad-TTS 기반 아키텍처

Diff-TTSG는 **Grad-TTS**를 기반으로 하며, 이를 제스처 합성을 위해 확장했다. [arxiv](https://arxiv.org/abs/2009.02119)

**Grad-TTS의 3가지 구성요소:**

1. **텍스트 인코더**: 입력 심볼 시퀀스 $s_{1:P}$에 대해 각 심볼의 평균 멜-스펙트럼 예측

$$\hat{\mu}_{1:P} = \text{Enc}(s_{1:P})$$

2. **기간 예측기**: 각 심볼의 로그 기간 예측

$$\ln \hat{d}_{1:P} = \text{DP}(\hat{\mu}_{1:P})$$

3. **U-Net 디코더**: 노이즈가 있는 멜-스펙트럼을 데이터 분포로 변환하는 확산 확률 모델

$$\hat{x}_{1:T} = \text{Dec}(\tilde{y}_{1:T}, \mu_{1:T}, n)$$

### 3.2 확산 확률 모델의 수학적 기초

**순방향 확산 과정(Forward Diffusion):**

$$q(y_t|y_0) = \mathcal{N}(y_t; \sqrt{\alpha_t}y_0, (1-\alpha_t)I)$$

여기서:
- $y_0$: 원본 멜-스펙트럼
- $y_t$: 시간 $t$에서의 노이즈 스펙트럼
- $\alpha_t$: 노이즈 일정(noise schedule)의 누적 곱

**점수 기반 SDE 표현:**

Grad-TTS는 확률 흐름 ODE(Probability Flow ODE)를 사용하여 합성 시 샘플링:

$$\text{d}y = -\frac{1}{2}\nabla_y \log p(y) \text{d}t$$

이는 동일한 목표 분포를 설명하면서 더 나은 수치 특성을 제공한다.

**조건부 분포 학습:**

모델은 조건 $\mu_{1:T}$ 하에서 다음을 최소화하도록 학습됨:

$$\mathcal{L} = \mathbb{E}_{t, y_0} \left[\left\|\hat{x}(y_t, \mu_{1:T}, t) - y_0\right\|_2^2\right]$$

### 3.3 Diff-TTSG의 핵심 수정사항

**모토닉 정렬 탐색(Monotonic Alignment Search):**

텍스트와 음성-제스처의 정렬을 학습하기 위해 동적 계획법 사용:

$$\hat{d}_{1:P} = \arg\min_{d} \sum_p \left\|\mu_{p,\text{upsampled}} - y_{t}\right\|_2^2$$

이는 각 음소의 최적 기간을 결정한다.

### 3.4 이중 경로 아키텍처

Diff-TTSG는 **음성 경로**와 **제스처 경로**를 가진다.

**음성 경로:**
```
텍스트 → 음소화 → 인코더 → 기간 예측 → 업샘플 (μ₁:T)
                                        ↓
                            U-Net (2D Conv) 디코더
                                        ↓
                            멜-스펙트럼 y₁:T → HiFi-GAN 보코더 → 파형
```

**제스처 경로:**
```
μ₁:T → Conformer 사전망 → μ'₁:T
                    ↓
        U-Net (1D Conv) 디코더
                    ↓
            제스처 포즈 g₁:T → 3D 시각화
```

### 3.5 Conformer 사전망(Pre-net)

음성 특성을 제스처 특성으로 매핑:

$$\mu'_{1:T} = \text{Conformer}(\text{Upsampled}(\mu_{1:P}))$$

구성:
- 4개의 Conformer 블록
- 384개의 숨겨진 채널
- 1D 컨볼루션 필터 길이 21

### 3.6 제스처용 1D 컨볼루션의 중요성

멜-스펙트럼(2D)과 달리 포즈 벡터 $g_t$는 공간적 관계가 없음:

$$\hat{g}_{1:T} = \text{Dec}_{\text{1D}}(\tilde{g}_{1:T}, \mu'_{1:T}, n)$$

**왜 1D 필요한가:**
- 멜-스펙트럼: 인접 주파수 빈들이 음성학적 의미 공유 → 2D Conv 효과적
- 포즈 벡터: 개별 특성들 간 간단한 공간 관계 없음 → 1D Conv가 자연스러움
- 결과: T-자세 문제 회피, 자연스러운 팔 움직임

### 3.7 결합 손실 함수

두 경로가 동시에 학습됨:

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{speech}} + \mathcal{L}_{\text{gesture}}$$

$$\mathcal{L}_{\text{speech}} = \mathbb{E}_{t, y_0} \left\|\hat{x}(y_t, \mu_{1:T}, t) - y_0\right\|_2^2$$

$$\mathcal{L}_{\text{gesture}} = \mathbb{E}_{t, g_0} \left\|\hat{g}(g_t, \mu'_{1:T}, t) - g_0\right\|_2^2$$

***

## 4. 모델 구조

### 4.1 전체 시스템 다이어그램

```
입력: 텍스트
        ↓
    [음소화]
        ↓
[텍스트 인코더]  ← 기간 예측기 ← Grad-TTS 경로
        ↓
    [업샘플]
        ↓
    μ₁:T (음성 특성) ─────────┐
        ↓                    │
    [U-Net 2D Conv]      [Conformer]
    (음성 디코더)         사전망
        ↓                    ↓
    멜-스펙트럼        μ'₁:T (제스처 특성)
        ↓                    ↓
    [HiFi-GAN]      [U-Net 1D Conv]
    보코더           (제스처 디코더)
        ↓                    ↓
    음성 파형        제스처 포즈 벡터
        ↓                    ↓
    [오디오 재생]    [3D 시각화]
```

### 4.2 핵심 하이퍼파라미터

| 컴포넌트 | 설정값 |
|---------|--------|
| **텍스트 인코더** | Grad-TTS 기본 설정 |
| **Conformer 블록** | 4개, 384 채널 |
| **음성 U-Net** | 256 채널, 2D Conv (5×5 커널) |
| **제스처 U-Net** | 256 채널, 1D Conv (길이 5) |
| **배치 크기** | 32 |
| **학습률** | 1e-4 |
| **학습 스텝** | 350k |
| **음성 확산 스텝** | 50 (합성 중) |
| **제스처 확산 스텝** | 500 (합성 중) |

### 4.3 데이터 표현

**입력:** 음소 시퀀스 $s_{1:P}$
- P: 심볼 수
- 각 음소 ~2개 심볼

**출력:**
- 음성: $x_{1:T}$ (80차원 멜-스펙트럼 프레임)
- 제스처: $g_{1:T}$ (45차원 포즈 벡터)
- T: 프레임 수 (약 86 Hz, 800ms 발화 = ~69 프레임)

**포즈 벡터 구성:**
- 루트 노드 (엉덩이) 평행이동: 3차원
- 루트 노드 회전: 지수 맵 표현 3차원
- 관절 회전: 13개 관절 × 3차원 = 39차원
- 총: 3 + 3 + 39 = 45차원

***

## 5. 성능 향상 및 평가 결과

### 5.1 실험 설정

**데이터: Trinity Speech-Gesture Dataset II (TSGD2)** [intermusic.kh](https://intermusic.kh.ua/vypusk56/56_probl_17_melnik.pdf)

| 항목 | 값 |
|-----|-----|
| **총 길이** | 6시간 |
| **학습 데이터** | 4.5시간 |
| **테스트/검증** | 1.5시간 |
| **화자** | 1명 (남성, Hiberno English) |
| **오디오 샘플링** | 44 kHz |
| **모션 캡처** | 120 fps 마커 기반 |
| **최종 프레임 레이트** | 86.13 fps |
| **음성 특성** | 80차원 (HiFi-GAN) |
| **제스처 특성** | 45차원 (포즈 벡터) |

### 5.2 비교 시스템

| 시스템 | 설명 |
|--------|------|
| **NAT** | 자연 음성/모션 (보코더 처리) |
| **G-TTS** | Grad-TTS (음성 전용) |
| **T2-ISG** | 이전 SOTA (Tacotron 2 기반, 다단계 학습) |
| **G-TTS+M** | 순차 학습 (음성 먼저, 제스처 나중) |
| **D-TTSG** | 제안 방법 (결합 학습) |

### 5.3 평가 방법론

**음성 전용 평가 (MOS 테스트)**

질문: "합성된 음성이 얼마나 자연스러운가?"
- 척도: 1~5 (완전히 부자연스러움 ~ 완전히 자연스러움)
- 참여자: 30명
- 자극: 15개 (각각 ~10초)
- 총 응답: 450개 (30 × 15)

**음성 전용 결과:**

| 조건 | 점수 | 95% 신뢰 구간 |
|------|-----|--------------|
| NAT | 4.37 | ±0.07 |
| G-TTS | 3.28 | ±0.11 |
| T2-ISG | 2.91 | ±0.12 |
| **D-TTSG** | **3.40** | **±0.11** |

**해석:**
- D-TTSG가 T2-ISG보다 유의미하게 우수 (p < 0.05)
- D-TTSG와 G-TTS 간 차이 없음 (음성만 학습할 때와 동등)
- 결합 학습이 음성 품질을 해치지 않음을 증명

**제스처 전용 평가 (인간미 테스트)**

질문: "제스처 동작이 얼마나 자연스럽고 인간다운가?"
- 척도: 1~5
- 영상만 표시 (음성 없음)
- 참여자: 30명
- 총 응답: 450개

**제스처 전용 결과:**

| 조건 | 점수 | 95% 신뢰 구간 |
|------|-----|--------------|
| NAT | 3.84 | ±0.10 |
| G-TTS+M | 2.96 | ±0.09 |
| T2-ISG | 2.48 | ±0.11 |
| **D-TTSG** | **3.48** | **±0.09** |

**주요 발견:**
- D-TTSG: 자연 제스처의 90.6% 수준 도달
- 결합 학습 >> 순차 학습 (D-TTSG: 3.48 > G-TTS+M: 2.96, 차이 0.52)
- 이전 SOTA 대비 40% 상대 개선 (2.48 → 3.48)

**음성-제스처 적절성 테스트**

방법: 쌍 비교로 시각적 혼동 제어 [arxiv](https://arxiv.org/pdf/2301.06690.pdf)

- **매칭 영상**: 음성과 해당 제스처 쌍
- **불일치 영상**: 같은 음성 + 다른 클립의 제스처 (속도 조정)
- 둘 다 유사한 제스처 품질이지만, 한 영상만 실제로 일치

질문: "어느 캐릭터의 동작이 음성과 더 잘 맞는가? (리듬, 억양, 의미)"
- 척도: -2~+2 (불일치 훨씬 나음 ~ 일치 훨씬 나음)
- 참여자: 60명
- 자극당 영상 쌍: 7개
- 총 응답: 420개 (60 × 7)

**음성-제스처 적절성 결과:**

| 조건 | 점수 | 95% 신뢰 구간 |
|------|-----|--------------|
| NAT | 1.20 | ±0.10 |
| T2-ISG | 0.12 | ±0.10 |
| **D-TTSG** | **0.44** | **±0.10** |

**해석:**
- 양수 점수 = 참여자가 일치 자극을 선호
- D-TTSG는 T2-ISG 대비 **3.7배 향상** (0.12 → 0.44)
- 상대 개선: 267%
- 그러나 자연 행동과의 격차 여전함: 0.44 vs 1.20 (63% 미달)

***

## 6. 성능 향상의 원인 분석

### 6.1 결합 학습의 이점

| 측면 | 이점 |
|-----|-----|
| **공유 정렬** | 음성과 제스처가 같은 기간 예측기 사용 → 일관성 향상 |
| **공통 표현** | 텍스트 특성이 두 양식 모두에서 학습됨 |
| **그래디언트 흐름** | 양방향 신호 전파로 더 나은 표현 학습 |
| **데이터 효율** | 공유 인코더가 더 적은 데이터로도 학습 가능 |

### 6.2 확산 모델의 이점

**확률적 모델링:**

$$p(x_{1:T}, g_{1:T}|s_{1:P}) = \int p(x_{1:T}, g_{1:T}|\mu_{1:T}, \mu'_{1:T}) \, p(\mu_{1:T}, \mu'_{1:T}|s_{1:P}) \, d\mu$$

- 단일 결정론적 예측이 아닌 분포 학습
- 동일 텍스트에 대한 다양한 실현 가능

**과도한 평활화 회피:**
- 결정론적 모델: 여러 가능한 제스처의 평균 → 부자연스러움
- 확산 모델: 스토캐스틱 샘플링 → 다양성 유지

**1D 컨볼루션의 효과:**
- T-자세 문제 제거 (2D Conv의 공간 정규화 없음)
- 포즈 공간의 특성에 맞춤형 구조

***

## 7. 한계

### 7.1 절대적 성능 격차

**음성 품질:**
- D-TTSG: 3.40 vs NAT: 4.37
- 격차: 13% (4.37 - 3.40 = 0.97점)

**제스처 품질:**
- D-TTSG: 3.48 vs NAT: 3.84
- 격차: 9% (3.84 - 3.48 = 0.36점)

**음성-제스처 적절성:**
- D-TTSG: 0.44 vs NAT: 1.20
- 격차: 63% (가장 큼)
- 의미: 생성된 제스처가 음성과 충분히 구체적으로 연결되지 않음

### 7.2 의미적 인식 부족

**원인:**
- 학습 데이터 4.5시간만 사용 (대규모 사전 학습 없음)
- 사전 학습된 단어/문장 임베딩 미사용
- 음성 특성이 고수준 의미 정보 포착 부족

**결과:**
- 제스처가 음성과 단순히 리듬적으로만 일치
- 의미적으로 적절한 제스처 부재 (예: "이것"이라는 단어에 대한 가리키기 제스처)
- 덜 뚜렷하고 구별되는 제스처 생성

### 7.3 균일한 제스처 타이밍

**원인:**
- Grad-TTS의 결정론적 기간 모델링 상속
- 모든 문구가 유사한 기간 구조

**결과:**
- 음성이 텍스트 내용과 무관하게 유사한 시간 구조
- 제스처 다양성 감소
- 자연스러운 운율 변화 불가능

### 7.4 제스처 평균화 문제

**일반적 데이터 기반 문제:**
- 동일 음성에 대한 여러 유효한 제스처 선택지
- 모델이 이들을 평균화하는 경향
- 결과: 덜 뚜렷하고 덜 자연스러운 제스처

**영향:**
- 평가 자극들이 더 유사해 보임
- 제스처 다양성 감소
- 의미적 구분성 부족

### 7.5 시각화 한계 (절대 점수에만 영향)

- **상반신만**: 하반신 동작 없음
- **고정 엉덩이**: 신체 이동 불가능
- **고정 손 포즈**: 손 동작 동역학 불가능
- **손가락 없음**: 세밀한 손가락 애니메이션 불가능

**주의:** 상대 비교에는 영향 없음 (모든 시스템이 동일한 시각화 사용)

***

## 8. 모델의 일반화 성능 향상 가능성

### 8.1 현재 일반화 제한

**화자 수준:**
- 현재: 1명 (남성, Hiberno English)
- 필요: 다중 화자 모델의 경우 200+ 시간 데이터

**언어:**
- 현재: 단일 언어
- 필요: 다국어 시스템의 경우 각 언어당 균형 잡힌 데이터

**도메인:**
- 현재: 카메라를 보며 대화하는 자발적 음성
- 필요: 다양한 컨텍스트 (발표, 뉴스 읽기, 상호작용 대화 등)

**데이터 규모:**
- 현재: 4.5시간 (제한적)
- 향상: 더 큰 데이터셋이 의미적 다양성과 제스처 구분성 증가

**스타일 제어:**
- 현재: 불가능
- 향상: 감정, 말하기 스타일 변수화

### 8.2 제안된 개선 방안

#### 8.2.1 확률적 기간 모델링

**제안:** 결정론적 기간 예측기를 확률적 모델로 대체

$$p(d_{1:P}|s_{1:P}) = \text{Distribution}(d_{1:P} | \text{Encoder}(s_{1:P}))$$

예: 정규분포 또는 혼합 모델

**기대 효과:**
- 동일 텍스트에 대한 다양한 음성-제스처 실현
- 더 자연스러운 운율 변화
- 의미론적 기간 차이 포착 (예: "정말"은 더 긴 음절 기간)

**구현 방법:**
- VAE 기반 기간 모델
- 정규화 플로우 기반 조건부 기간 생성

#### 8.2.2 자감독 텍스트 임베딩

**제안:** 대규모 말뭉치에서 사전 학습된 텍스트 임베딩 활용

**선택지:**
- **BERT/RoBERTa**: 문맥적 단어 표현
- **GPT 스타일**: 자동회귀 언어 모델
- **Wav2Vec 스타일**: 음성에서 학습한 표현 (음성-텍스트 정렬)

**수식:**

$$\text{Enc}_{\text{text}}(s_{1:P}) = \text{PretrainedLM}(s_{1:P})$$

대신

$$\text{Enc}_{\text{text}}(s_{1:P}) = \text{Grad-TTS-Encoder}(s_{1:P})$$

**기대 효과:**
- 더 나은 의미 표현
- 도메인 외 전이 학습 가능
- 제스처 의미 인식 향상

**이점:**
- 기존 모델에 쉬운 플러그-앤-플레이 추가
- 추가 주석 데이터 불필요
- 크로스 랭귀지 가능성

#### 8.2.3 스타일 제어

**제안:** 음성-제스처 스타일을 조건화하는 벡터 추가

$$p(x_{1:T}, g_{1:T}|s_{1:P}, c_{\text{style}})$$

**스타일 차원:**
- 감정 (기쁨, 슬픔, 분노)
- 형식성 (격식있음 vs 친근함)
- 신체 활동 수준
- 말하기 속도

**구현:**
- 스타일 임베딩 $c_{\text{style}} \in \mathbb{R}^{d_s}$
- 조건 $\mu_{1:T}$에 연결: $\mu'\_{\text{conditioned}} = \text{Concat}(\mu_{1:T}, c_{\text{style}})$

**참고:** 선행 연구  활용 [arxiv](https://arxiv.org/abs/2503.14040)

#### 8.2.4 다중 화자 일반화

**접근 1: 화자 임베딩**

$$\mu_{1:T}, \mu'_{1:T} = \text{Enc}(s_{1:P}, e_{\text{speaker}})$$

데이터 요구: 각 화자 50~100 시간

**접근 2: 메타 학습**

소량의 화자별 데이터에서 빠르게 적응

**접근 3: 합성 데이터**

기존 일반 모델에서 데이터 증강

***

## 9. 2020년 이후 관련 최신 연구 비교 분석

### 9.1 음성 합성 (Speech Synthesis) 진화

| 연도 | 방법 | 주요 기여 | 한계 |
|------|-----|---------|------|
| **2021** | **Diff-TTS** [arxiv](https://arxiv.org/html/2505.04996v1) | 첫 확산 기반 TTS | 느린 합성 |
| **2021** | **Grad-TTS** [arxiv](https://arxiv.org/abs/2009.02119) | SDE 기반, ODE 재구성 | 결정론적 기간 |
| **2022** | **DiffGAN-TTS**[미포함] | GAN-DDPM 하이브리드 | 복잡한 학습 |
| **2023** | **Grad-StyleSpeech** [arxiv](https://arxiv.org/abs/2105.06337) | 적응형 합성, 참조 스타일 | 여전히 TTS 전용 |
| **2022** | **Guided-TTS** [dl.acm](https://dl.acm.org/doi/10.1145/3610661.3616554) | 분류기 가이드 | 샘플링 속도 |

### 9.2 제스처 합성 (Gesture Synthesis) 진화

| 연도 | 방법 | 주요 기여 | 한계 |
|------|-----|---------|------|
| **2020** | **DiffWave** 기반 | 음성 조건 제스처 생성 | 단일 스타일 |
| **2023** | **Listen, Denoise, Action!** [arxiv](https://arxiv.org/pdf/2309.03199.pdf) | DiffWave 아키텍처 활용 | 스타일 제어 미흡 |
| **2023** | **GestureDiffuCLIP** [arxiv](https://arxiv.org/pdf/2404.19622.pdf) | CLIP 통합 스타일 제어 | 의미 표현 부족 |
| **2023** | **Discrete Diffusion for Co-Speech Gesture** [dl.acm](https://dl.acm.org/doi/10.1145/3610661.3616556) | VQ-VAE + 이산 확산 | 컴퓨팅 비용 |
| **2023** | **Diffusion-Based Co-Speech Gesture Generation** [dl.acm](https://dl.acm.org/doi/10.1145/3577190.3616117) | CSMP 모듈 (음성-제스처 대조 학습) | 제한된 평가 |

### 9.3 통합 음성-제스처 합성 (ISG) 진화

| 연도 | 방법 | 아키텍처 | 성능 | 한계 |
|------|-----|---------|------|------|
| **2020** | **ISG** (초기) [arxiv](http://arxiv.org/pdf/2405.13336.pdf) | Seq2Seq | 기초 수준 | 저품질 |
| **2021** | **ISG** (Wang et al.) [arxiv](https://arxiv.org/html/2503.17059) | Tacotron 2 + LSTM SOTA | 좋음 | 다단계 학습, 사전 학습 필요 |
| **2023** | **Diff-TTSG** [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0ac004bc-0cfb-494f-8c9d-402b494e211f/2306.09417v3.pdf) | Grad-TTS + Conformer | **우수** | 의미 인식 부족 |
| **2024** | **Match-TTSG** (Flow Matching) [arxiv](https://arxiv.org/pdf/2310.05181.pdf) | 플로우 매칭 | 우수, 더 간단 | 여전히 의미 제한 |
| **2025** | **CoCoGesture** [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S1566253525006852) | 사전 학습 1B 확산 트랜스포머 | 우수 | 큰 데이터 필요 |

### 9.4 의미 인식 제스처 (Semantic Gestures)

**최근 트렌드:**

| 방법 | 해석 | 효과 |
|------|-----|------|
| **RAG-Gesture** (2024) [arxiv](https://arxiv.org/html/2412.06786v3) | 검색 보강 생성 | 의미 제스처 향상 |
| **Semantic Gesticulator** (2024) [arxiv](http://arxiv.org/pdf/2405.09814.pdf) | 의미 대응 학습 | 강한 의미 결합 |
| **GesGPT** (2024) [arxiv](https://arxiv.org/pdf/2303.13013.pdf) | ChatGPT 텍스트 파싱 | 의미 기반 선택 |
| **SIGGesture** (2024) [dl.acm](https://dl.acm.org/doi/10.1145/3680528.3687677) | 시맨틱 인식 확산 | 의미 적절성 향상 |

### 9.5 효율성 개선 (2024-2025)

| 방법 | 기술 | 이점 |
|------|-----|------|
| **MambaTalk** [proceedings.neurips](https://proceedings.neurips.cc/paper_files/paper/2024/file/23c9c94227f937cfb50592a15e7fbb63-Paper-Conference.pdf) | 상태 공간 모델 (Mamba) | 낮은 지연, 빠른 합성 |
| **FreGrad** | 이산 웨이블릿 변환 | 경량 보코더 |
| **Match-TTSG** | 플로우 매칭 | 더 간단한 학습 |
| **CoCoGesture** [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S1566253525006852) | 혼합 전문가 (MoGE) | 화자 적응 효율 |

### 9.6 데이터 크기 트렌드

| 방법 | 학습 데이터 | 특성 |
|------|-----------|------|
| ISG (Wang et al. 2021) | Trinity SG: 2.5시간 | 기초 SOTA |
| Diff-TTSG (2023) | Trinity SG II: 4.5시간 | 향상된 SOTA |
| CoCoGesture (2025) | GES-X: ~1,000시간+ | 제로샷 일반화 |

***

## 10. 앞으로의 연구에 미치는 영향 및 고려사항

### 10.1 학문적 영향

#### 10.1.1 방법론적 기여

**1) 다중 양식 확산 모델의 가능성 입증**

Diff-TTSG는 서로 다른 모달리티를 가진 시퀀스를 동일한 프레임워크에서 모델링할 수 있음을 보여줌:

- 음성: 고차원 연속 신호 (80차원 mel-spec)
- 제스처: 저차원 구조화 신호 (45차원 포즈)

**영향:** 다른 다중 양식 문제 (비디오 생성, 다중 신체 파트 동시 생성) 확대 가능

**2) 작은 데이터셋에서의 확산 모델 효과성**

이전 TTS에서 수십 시간 사전 학습 필수였으나, Diff-TTSG는 4.5시간에서 직접 학습:

$$\mathcal{L}_{\text{total}} < \mathcal{L}_{\text{pretrain}} + \mathcal{L}_{\text{finetune}}$$

(이전 다단계 접근)

**영향:** 저자원 언어, 특수 도메인 음성 합성 가능화

**3) 평가 프레임워크의 정교화**

음성 품질, 제스처 품질, 음성-제스처 적절성을 분리하여 평가:

- 이전: 단순 "좋음/나쁨" 평가
- 현재: 세 가지 독립적 차원

**영향:** 향후 다중 양식 합성 평가의 표준화

#### 10.1.2 새로운 연구 방향 개방

**1) 의미 인식 제스처 생성**

Diff-TTSG의 의미 인식 부족은 다음 과제를 명확히:
- 사전 학습된 언어 모델 통합
- 비디오 클립에서의 의미 학습
- 지식 그래프 기반 의미 강화

**2) 실시간 합성**

500단계의 제스처 확산은 실시간 응용에 어려움:
- 가속화 기법 (DDIM, 비반복 샘플링)
- 상태 공간 모델 대안 (MambaTalk 참고)
- 네트워크 증류(distillation)

**3) 감정/스타일 제어**

결정론적 기간 모델링 극복:
- 벡터 양자화(VQ) 기반 스타일 토큰
- 계층적 확산 (조건-무조건 분리)
- 적응 기간 모델

### 10.2 실제 응용에 미치는 영향

#### 10.2.1 구체화된 대화 에이전트(ECAs)

**현재 한계:**
- 아바타 시각화 기본적 (상반신만)
- 싱크 오류 가능성
- 의미 부족

**개선 기대:**
- 얼굴 표정, 눈 응시 추가
- 전신 제스처
- 상황 인식 제스처

**비용-편익:**
- 이전: 음성 모델 학습 + 제스처 모델 학습 = 2배 투자
- 현재: 통합 학습 = 1.5배 이하 투자
- 미래: 더 작은 시스템, 더 빠른 배포

#### 10.2.2 가상 인플루언서/디지털 휴먼

**활용:**
- 콘텐츠 제작 자동화
- 다국어 아바타
- 개인화된 음성 클론

**필요 개선:**
- 화자별 스타일 학습 (5-10시간/인물)
- 배경 일관성
- 긴 영상 생성 안정성

#### 10.2.3 교육 및 접근성

**음성 장애인을 위한 음성 지원:**
- 텍스트 → 자연스러운 음성 + 의미적 제스처
- 소수 언어/방언 보존

***

## 11. 향후 연구 고려 사항

### 11.1 데이터 요구사항

| 응용 | 권장 데이터 | 사전 학습 |
|------|-----------|---------|
| **단일 화자 고정 도메인** | 4-5시간 | 불필요 (Diff-TTSG처럼) |
| **다중 화자 같은 언어** | 50-100시간 | 선택적 |
| **다국어** | 각 언어 30-50시간 | 권장 |
| **감정/스타일** | +10-20시간 (주석) | 권장 |

### 11.2 기술적 고려사항

**1) 모델 크기-성능 트레이드오프**

```
모델 파라미터 수 ↑
    ↓
기억 용량 ↑, 의미 표현 ↑
    ↓
하지만: 데이터 요구 ↑, 과적합 위험 ↑
```

**권장:** 과소 매개변수화부터 시작, 필요시 확장

**2) 합성 속도 vs 품질**

| 제스처 확산 스텝 | 품질 | 속도 (상대) |
|-----------------|------|-----------|
| 500 (현재) | 우수 | 1.0x |
| 250 | 양호 | 2.0x |
| 100 | 가능 | 5.0x |
| 50 | 저하 | 10.0x |

실시간 응용: 100 스텝이 실용적 타겟

**3) 화자 일반화**

단일 화자에서 다중 화자:
- 데이터: 4.5시간 → 50시간+
- 모델: 독립 모델 → 조건부 모델
- 효율: 증가된 데이터 필요하지만 가능

### 11.3 평가 표준화

**권장 평가 프로토콜 (Diff-TTSG 기반):**

1. **음성 전용** (30명 × 15 자극)
   - MOS 또는 음성 품질 점수

2. **제스처 전용** (30명 × 15 자극)
   - 인간미 점수

3. **적절성** (60명 × 7 쌍)
   - 불일치 제어로 품질 영향 최소화

4. **의미** (선택사항)
   - "제스처가 음성의 의미를 강조하는가?" 질문

5. **객관적 지표**
   - 자동 동기화 지표 (립싱크 에러 유사)
   - 콘텐츠 보존 지표 (특정 의미 제스처 포착 %?)

***

## 12. 결론

Diff-TTSG는 **통합 음성-제스처 합성 분야에 획기적 기여**를 한다:

### 12.1 주요 성과

1. **방법론적 혁신:**
   - 확산 모델을 ISG에 처음 적용
   - 이전 SOTA 대비 모든 지표에서 우수

2. **실용적 개선:**
   - 작은 데이터셋(4.5시간)에서 직접 학습 가능
   - 다단계 복잡한 학습 프로토콜 불필요

3. **평가 표준화:**
   - 체계적인 3중 평가 프레임워크
   - 향후 ISG 연구의 벤치마크

### 12.2 한계 인식

- 의미 인식 부족: 의미적 관련성 낮음
- 기간 다양성 제한: 결정론적 기간 모델
- 절대 성능 격차: 자연 행동 대비 여전히 미달

### 12.3 향후 방향

**근기(1-2년):**
- 사전 학습 텍스트 임베딩 통합
- 확률적 기간 모델 개발

**중기(2-3년):**
- 다중 화자/언어 일반화
- 감정/스타일 제어

**장기(3년+):**
- 완전 신체 모델 (하반신 포함)
- 얼굴 표정 통합
- 상황-인식 제스처 생성

***

## 참고문헌

 Mehta, S., Wang, S., Alexanderson, S., Beskow, J., Székely, É., & Henter, G. E. (2023). Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. *arXiv preprint arXiv:2306.09417v3*, [eess.AS]. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0ac004bc-0cfb-494f-8c9d-402b494e211f/2306.09417v3.pdf)

 Discrete Diffusion for Co-Speech Gesture Synthesis (2023, GENEA Challenge) [dl.acm](https://dl.acm.org/doi/10.1145/3610661.3616556)

 Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation (2023, ACM ICMI) [dl.acm](https://dl.acm.org/doi/10.1145/3577190.3616117)

 Wang, S., Alexanderson, S., Gustafson, J., Beskow, J., Henter, G. E., & Székely, É. (2021). Integrated speech and gesture synthesis. *Proc. ICMI*, 177-185. [arxiv](https://arxiv.org/html/2503.17059)

 Jeong, M., Kim, H., Cheon, S. J., Choi, B., & Kim, N. S. (2021). Diff-TTS: A denoising diffusion model for text-to-speech. *Proc. Interspeech*, 3605-3609. [arxiv](https://arxiv.org/html/2505.04996v1)

 Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., & Kudinov, M. (2021). Grad-TTS: A diffusion probabilistic model for text-to-speech. *Proc. ICML*, 8599-8608. [arxiv](https://arxiv.org/abs/2009.02119)

 Mehta, S., Tu, R., Alexanderson, S., Beskow, J., Székely, É., & Henter, G. E. (2024). Unified speech and gesture synthesis using flow matching. *IEEE ICASSP 2024*. [arxiv](https://arxiv.org/pdf/2310.05181.pdf)

 Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis (2024) [arxiv](http://arxiv.org/pdf/2405.09814.pdf)

 GesGPT: Speech Gesture Synthesis With Text Parsing from ChatGPT (2024) [arxiv](https://arxiv.org/pdf/2303.13013.pdf)

 CoCoGesture: Towards coherent co-speech 3D gesture generation via large-scale generative pre-training (2025) [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S1566253525006852)

 SIGGesture: Generalized Co-Speech Gesture Synthesis via Semantic Integration Guided Diffusion (2024) [dl.acm](https://dl.acm.org/doi/10.1145/3680528.3687677)

 RAG-Gesture: Retrieving Semantics from the Deep: an RAG Solution for Meaningful Gesture Synthesis (2024) [arxiv](https://arxiv.org/html/2412.06786v3)
