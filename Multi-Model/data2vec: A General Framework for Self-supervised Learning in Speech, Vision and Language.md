# data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language

### 1. 핵심 주장과 주요 기여

**data2vec**는 음성 인식, 이미지 분류, 자연어 이해 등 서로 다른 모달리티에 걸쳐 **동일한 자기 지도 학습 목표**를 사용하는 통합 프레임워크입니다. 논문의 근본적인 주장은 기존의 모달리티별 개별 알고리즘이 특정 모달리티에 편향되어 있으며, 인간의 학습이 유사한 처리 과정을 사용한다는 신경 생물학적 이론에 기반하여, 통일된 학습 방식이 더욱 효과적일 수 있다는 것입니다.[1]

**주요 기여:**

- **모달리티 독립적인 학습 목표:** 텍스트, 이미지, 음성 모두에 적용 가능한 단일 학습 방법 제시[1]
- **맥락화된 잠재 표현 학습:** 이산적인 토큰(단어, 음성 단위, 시각적 토큰)을 예측하는 대신, 전체 입력 정보를 포함하는 **연속적 맥락화 표현**을 목표로 설정[1]
- **다층 평균화 타겟 전략:** 네트워크의 상위 K개 층을 평균화하여 더 풍부한 학습 신호 제공[1]
- **자기 증류(Self-Distillation) 접근:** 동일한 모델의 교사 모드와 학생 모드를 활용하여 효율적인 학습 달성[1]

***

### 2. 해결하고자 하는 문제와 제안 방법

#### 2.1 주요 문제점

기존의 자기 지도 학습 방법들은 다음과 같은 문제를 가지고 있습니다:[1]

- **모달리티 특화 설계:** 음성의 경우 이산적인 음성 단위 학습 필요, 시각의 경우 이산적 시각 토큰 또는 픽셀 회귀, NLP의 경우 단어/부분단어 예측 등이 각각 다르게 설계됨
- **고정된 타겟의 제한:** BERT 스타일의 모델은 각 토큰에 대해 고정된 임베딩을 학습하여 모든 문맥에서 동일하게 적용되어야 함
- **지역적 정보만 포함:** 대부분의 마스킹 예측 방법은 지역적 문맥만 포함하는 타겟을 사용

#### 2.2 제안된 방법: 수식 포함

**data2vec의 핵심 학습 과정:**

1. **교사 모드 계수 업데이트 (지수 이동 평균, EMA):**

$$\Delta \leftarrow \tau \Delta + (1 - \tau) \theta$$

여기서 $\Delta$는 교사 모드의 가중치, $\theta$는 학생 모드의 가중치, $\tau$는 모멘텀 계수입니다. $\tau$는 훈련 초기에 $\tau_0$에서 시작하여 선형적으로 $\tau_e$로 증가합니다.[1]

2. **타겟 구성 (다층 평균화):**

$$y_t = \frac{1}{K} \sum_{l=L-K+1}^{L} \hat{a}_t^l$$

여기서 $a_t^l$은 블록 $l$의 시간 단계 $t$에서의 출력이고, $\hat{a}_t^l$은 정규화된 표현입니다. $L$은 총 블록 수이고 $K$는 평균화할 상위 블록 수입니다.[1]

3. **손실 함수 (Smooth L1 손실):**

$$L(y_t, f_t(x)) = \begin{cases} \frac{1}{2\beta}(y_t - f_t(x))^2 & \text{if } |y_t - f_t(x)| \leq \beta \\ |y_t - f_t(x)| - \frac{1}{2\beta} & \text{otherwise} \end{cases}$$

Smooth L1 손실은 이상치(outlier)에 덜 민감하면서도 $\beta$를 통해 제곱 손실과 L1 손실 간의 전환을 제어합니다.[1]

#### 2.3 정규화 전략

타겟 표현의 붕괴(collapse)를 방지하기 위해 모달리티별로 다른 정규화 방식을 적용합니다:[1]

- **음성:** 인스턴스 정규화 (Instance Normalization) - 현재 입력 샘플에 대해 학습 가능한 매개변수 없이 적용
- **NLP와 시각:** 매개변수 없는 층 정규화 (Layer Normalization)

***

### 3. 모델 구조

#### 3.1 기본 아키텍처

data2vec은 표준 Transformer 아키텍처를 사용하며, 모달리티별 특화된 입력 인코더를 적용합니다:[1]

| 모달리티 | 입력 크기 | 인코더 전략 | 시퀀스 생성 |
|---------|---------|-----------|----------|
| 시각 | 224×224 | 16×16 패치 선형 변환 | 196 표현 |
| 음성 | 16kHz 파형 | 7층 1D CNN (50Hz 출력) | 20ms 스트라이드 |
| 언어 | 텍스트 | BPE 부분단어 토큰화 (50K) | 임베딩 벡터 |

#### 3.2 마스킹 전략

모달리티별로 최적화된 마스킹 전략을 적용합니다:[1]

- **시각:** 블록 단위 마스킹 (각 블록 최소 16 패치, 종횡비 무작위) - 60% 마스킹 비율
- **음성:** 스팬 마스킹 (p=0.065 샘플링 확률, 이후 10개 시간 단계 마스킹, ~49% 마스킹)
- **NLP:** BERT 마스킹 (15% 균일 선택 - 80% MASK 토큰 대체, 10% 미변경, 10% 무작위 토큰 대체)

***

### 4. 성능 향상 및 실험 결과

#### 4.1 컴퓨터 비전 (ImageNet-1K)

| 모델 | ViT-B | ViT-L |
|-----|-------|--------|
| **data2vec** | **84.2%** | **86.6%** |
| MAE | 83.6 | 85.9 |
| MaskFeat | 84.0 | 85.7 |
| SimMIM | 83.8 | - |
| MoCo v3 | 83.2 | 84.1 |

data2vec은 단일 모델 설정에서 ViT-B와 ViT-L 모두에서 최고 성능을 달성했습니다.[1]

#### 4.2 음성 처리 (Librispeech)

**기본 모델 (LS-960으로 사전학습):**

| 라벨 데이터 | 10분 | 1시간 | 10시간 | 100시간 | 960시간 |
|-----------|------|-------|---------|---------|---------|
| **data2vec** | 12.3 | 9.1 | 8.1 | 6.8 | 5.5 |
| HuBERT | 15.3 | 11.3 | 9.4 | 8.1 | - |
| wav2vec 2.0 | 15.6 | 11.3 | 9.5 | 8.0 | 6.1 |

특히 저자원 설정(10분)에서 **20% 상대 오류율 개선**을 달성했습니다.[1]

#### 4.3 자연어 처리 (GLUE)

| 작업 | MNLI | QNLI | RTE | MRPC | QQP | STS-B | CoLA | SST | 평균 |
|-----|------|------|-----|------|------|--------|------|-----|------|
| **data2vec** | 83.2 | 90.9 | 67.0 | 90.2 | 89.1 | 87.2 | 62.2 | 91.8 | **82.7** |
| RoBERTa | 84.1 | 90.4 | 69.3 | 89.0 | 89.3 | 88.9 | 56.8 | 92.3 | 82.5 |

RoBERTa 기준선을 평균적으로 상회하면서도 **NLP에서 이산 단위를 예측하지 않는 첫 번째 사전학습 모델**이 되었습니다.[1]

#### 4.4 주요 성능 인상 요인

**다층 평균화 타겟의 효과:**

절제 연구(Ablation Study)에서 다층 평균화가 단일 상위 층 예측보다 모든 모달리티에서 우수함을 입증했습니다.[1]

- 음성: K=1일 때 약 20% 더 높은 오류율 → K=8일 때 최적
- NLP: K=10일 때 GLUE 점수 개선
- 시각: 모든 층 사용 시 약간의 성능 향상

**맥락화된 타겟의 중요성:**

교사 모델의 어텐션 범위를 제한하여 실험한 결과, 더 큰 문맥 크기가 더 나은 하류 성능을 제공했습니다.[1]

***

### 5. 모델의 일반화 성능 향상

#### 5.1 개선 메커니즘

**맥락화된 표현의 이점:**[1]

1. **시퀀스별 적응:** 고정된 목표 대신 각 입력 시퀀스에 특화된 목표를 생성하여 동일한 단위의 서로 다른 문맥적 의미를 학습

2. **개방형 어휘 설정:** NLP에서 이산 토큰의 고정된 집합을 사용하지 않아 새로운 의미 학습 가능

3. **다층 특징 활용:** 신경망이 여러 층에서 서로 다른 유형의 특징을 추출하므로 이들을 모두 활용하여 학습 신호 강화

#### 5.2 저자원 설정에서의 뛰어난 성능

특히 음성 인식의 **극히 제한된 라벨 설정**(10분)에서 data2vec의 강점이 두드러집니다. 이는 맥락화된 표현이 라벨된 데이터가 부족할 때 더욱 강력한 특성 학습을 가능하게 함을 시사합니다.[1]

#### 5.3 모달리티 간 전이 가능성

동일한 학습 목표를 사용함으로써 향후 **다중 모달 학습 시 더 효과적인 표현 전이**가 가능할 것으로 기대됩니다. 논문의 저자들은 이것이 미래의 다중 모달 학습을 더 간단하고 효과적으로 만들 것으로 전망하고 있습니다.[1]

***

### 6. 모델의 한계

#### 6.1 기술적 한계

**모달리티별 특화 요소의 필요성:**[1]

- 입력 인코더: 음성의 경우 CNN 전처리, 이미지의 경우 패치 임베딩, 텍스트의 경우 토큰화 필요
- 마스킹 전략: 각 모달리티의 통계적 특성에 맞는 서로 다른 마스킹 비율 및 전략 필요

이는 "완전히 통합된" 프레임워크라는 원래 목표와 일부 거리가 있습니다.

#### 6.2 계산 복잡성

**교사-학생 아키텍처의 오버헤드:**
- EMA 업데이트는 FP32에서 수치 안정성을 위해 수행되어야 함
- 여러 층의 표현을 저장하고 평균화해야 함

#### 6.3 표현 붕괴 위험

**다음 시나리오에서 붕괴 가능성:**[1]

1. 학습률이 너무 크거나 워밍업 기간이 너무 짧음
2. $\tau$가 너무 낮아 학생 모델이 붕괴되고 이것이 교사로 전파
3. 인접 타겟이 매우 상관관계 있는 모달리티(특히 음성)에서 긴 마스킹 스팬 사용

논문의 저자들은 이를 위해 타겟 정규화와 적절한 하이퍼파라미터 튜닝으로 해결했습니다.[1]

***

### 7. 향후 연구에 미치는 영향

#### 7.1 최근 후속 연구 동향 (2023-2024)

**다중 모달 확장:**

1. **Point2Vec (2023):** data2vec 프레임워크를 3D 포인트 클라우드에 확장하여 형태 분류 및 소수 샷 학습에서 최고 성능 달성[2]

2. **SV-data2vec (2024-2025):** 스켈레톤과 비디오 입력을 모두 활용하여 행동 인식을 위한 비디오 표현 학습 가이드, NTU 60 및 NTU 120에서 기존 최고 성능 초과[3]

3. **AV-data2vec (2023):** 오디오-비주얼 통합 학습으로 음성 인식에서 최고 성능 달성[4]

4. **MedCoSS (2024):** data2vec의 원리를 의료 영상(X-ray, CT, MRI, 병리 이미지) 등 다중 모달 의료 데이터에 적용하여 지속적 학습 접근 방식 제시[5]

**효율성 개선:**

5. **EAT (Efficient Audio Transformer, 2024):** data2vec 2.0의 역블록 다중 마스킹 방법 채택으로 오디오 사전 학습 효율성 대폭 개선, AudioSet에서 48.6% mAP 달성[6]

6. **emotion2vec (2024):** data2vec 특징을 감정 인식에 활용하여 기존 방법 대비 우수한 성능[7]

#### 7.2 이론적 기여

**마스킹 잠재 예측(Masked Latent Prediction)의 일반화:**

최근 종설(survey)에 따르면, data2vec이 시작한 "마스킹 잠재 예측" 패러다임이 이제 비전, 오디오, 시계열, 강화 학습 등 광범위한 분야로 확장되고 있습니다. 이는 data2vec의 핵심 아이디어(이산 타겟 대신 연속 맥락화 표현 예측)의 보편성을 입증합니다.[8]

#### 7.3 자기 지도 학습의 통일화 운동

**멀티모달 학습의 새로운 방향:**

data2vec의 성공은 다음과 같은 인식의 전환을 촉발했습니다:[9]

- 모달리티별 특화 설계보다는 **통일된 학습 목표와 아키텍처**를 먼저 탐색
- 모달리티 간 **지식 전이**의 가능성 재평가
- **연속 자기 지도 학습**(continual SSL) 패러다임 출현

***

### 8. 향후 연구 시 고려할 점

#### 8.1 기술적 개선 방향

1. **완전 통합 인코더 개발:**
   - 모달리티별 특화 입력 인코더의 필요성을 제거하기 위해 Perceiver 같은 **범용 아키텍처**와의 결합 연구

2. **적응형 마스킹 전략:**
   - 고정된 마스킹 비율 대신 모달리티와 데이터 특성에 따른 **동적 마스킹 전략** 개발

3. **효율적인 교사-학생 메커니즘:**
   - EMA 계산의 효율성 개선
   - 경량화된 모달리티별 교사 네트워크 설계

#### 8.2 이론적 심화

1. **맥락화 타겟의 최적성 분석:**
   - 왜 맥락화된 표현이 더 나은 일반화를 제공하는지에 대한 이론적 증거

2. **표현 붕괴 메커니즘:**
   - 다양한 모달리티에서 붕괴 발생 패턴 분석 및 예방 전략 체계화

3. **다중 모달 상충 관리:**
   - MedCoSS 같은 연구에서 드러난 다중 모달 학습의 상충 해결 방법 심화

#### 8.3 응용 확대

1. **도메인별 적응:**
   - 의료, 자율 주행, 로봇 등 특정 도메인에서의 **도메인 특화 사전 학습**

2. **저자원 언어 및 모달리티:**
   - 아시아 언어나 희귀 모달리티에 대한 **일반화 성능 검증**

3. **실시간 모달리티 통합:**
   - 기존의 "개별 모달리티별 학습"에서 **진정한 다중 모달 협력 학습**으로의 전환

#### 8.4 실무적 고려사항

1. **하이퍼파라미터 자동 최적화:**
   - 각 모달리티별 최적 $K$ (평균화 층 수), $\beta$ (손실 함수 매개변수), 마스킹 비율 등의 자동화

2. **계산 효율성 대 성능 트레이드오프:**
   - 모바일/엣지 환경에 배포 가능한 경량 버전 개발

3. **해석성과 투명성:**
   - 맥락화된 표현이 어떤 의미론적 정보를 포함하는지 분석하기 위한 **프로빙 작업** 확대

***

## 요약

**data2vec**는 음성, 시각, 언어 세 모달리티에 적용 가능한 **통합 자기 지도 학습 프레임워크**를 제시함으로써 자기 지도 학습 분야에서 패러다임 전환을 이루었습니다. 이산 타겟 대신 **맥락화된 잠재 표현을 예측**하고 **다층 평균화**를 활용한 아이디어는 이후 다양한 모달리티와 응용 분야로 확장되고 있습니다.[2][3][6][5][7][1]

그러나 모달리티별 입력 인코더와 마스킹 전략의 필요성이 여전히 남아 있으며, 진정한 의미의 멀티모달 학습은 향후 과제입니다. 향후 연구는 **적응형 아키텍처 개발**, **다중 모달 상충 해결**, **저자원 설정에서의 성능 강화** 등에 초점을 맞춰야 합니다. 특히 data2vec이 개척한 "마스킹 잠재 예측" 패러다임의 보편성이 확인되고 있으므로, 이를 더욱 일반화하고 효율화하는 것이 중요한 연구 방향이 될 것입니다.[9][5]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/01ccff33-9f54-4943-8c58-497ad5aa97a5/2202.03555v3.pdf)
[2](http://arxiv.org/pdf/2303.16570.pdf)
[3](https://openaccess.thecvf.com/content/WACV2025/papers/Dozdor_SV-data2vec_Guiding_Video_Representation_Learning_with_Latent_Skeleton_Targets_WACV_2025_paper.pdf)
[4](https://arxiv.org/pdf/2302.06419.pdf)
[5](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_Continual_Self-supervised_Learning_Towards_Universal_Multi-modal_Medical_Data_Representation_Learning_CVPR_2024_paper.html)
[6](https://arxiv.org/pdf/2401.03497.pdf)
[7](https://aclanthology.org/2024.findings-acl.931.pdf)
[8](https://www.emergentmind.com/topics/masked-latent-prediction)
[9](https://arxiv.org/abs/2304.01008)
[10](https://ashpublications.org/blood/article/144/Supplement%201/469/531307/Five-Year-Analysis-of-the-POLARIX-Study-Prolonged)
[11](https://essd.copernicus.org/articles/16/3601/2024/)
[12](https://journal-stiayappimakassar.ac.id/index.php/Concept/article/view/1408)
[13](https://academic.oup.com/rheumatology/article/63/10/2770/7593760)
[14](https://journal.staiypiqbaubau.ac.id/index.php/Al-Tarbiyah/article/view/1191)
[15](https://journals.lww.com/10.4103/ohbl.ohbl_42_24)
[16](http://medrxiv.org/lookup/doi/10.1101/2024.07.22.24310801)
[17](https://aacrjournals.org/cancerres/article/84/9_Supplement/PO3-19-12/744793/Abstract-PO3-19-12-Evaluating-the-impact-of-ePRO)
[18](https://jurnal.ranahresearch.com/index.php/R2J/article/view/774)
[19](https://www.ssrn.com/abstract=4728702)
[20](https://pmc.ncbi.nlm.nih.gov/articles/PMC8671530/)
[21](https://www.medrxiv.org/content/medrxiv/early/2023/06/05/2023.06.01.23290824.full.pdf)
[22](https://arxiv.org/pdf/2202.03555.pdf)
[23](https://pmc.ncbi.nlm.nih.gov/articles/PMC11334056/)
[24](https://arxiv.org/abs/2412.17847)
[25](https://pmc.ncbi.nlm.nih.gov/articles/PMC10912024/)
[26](https://pmc.ncbi.nlm.nih.gov/articles/PMC7959675/)
[27](https://aclanthology.org/2020.emnlp-main.385v1.pdf)
[28](https://github.com/ys-zong/awesome-self-supervised-multimodal-learning)
[29](https://openreview.net/forum?id=Ycmz7qJxUQ)
