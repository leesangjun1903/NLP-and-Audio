# ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

## 1. 논문의 핵심 주장과 주요 기여

**핵심 주장**: ViLBERT는 시각과 언어를 동시에 이해하는 범용적인 사전훈련 모델로서, 기존의 태스크별 학습 방식에서 벗어나 **시각적 접지(visual grounding)를 사전훈련 가능하고 전이 가능한 능력**으로 다루는 패러다임 전환을 제시합니다.[1]

**주요 기여**는 다음과 같습니다:

- **두-스트림 아키텍처**: 시각과 언어 처리를 위한 별도의 스트림을 운영하면서 상호 주의(co-attention) 메커니즘을 통해 상호작용하는 새로운 구조 제안[1]
- **자가지도 사전훈련**: Conceptual Captions 데이터셋(330만 개 이미지-캡션 쌍)에서 마스킹된 다중모달 모델링과 다중모달 정렬 예측이라는 두 가지 프록시 태스크로 사전훈련[1]
- **통합된 전이학습 프레임워크**: 단일 기본 아키텍처로 네 가지 확립된 시각-언어 태스크(VQA, VCR, 참조 표현, 캡션 기반 이미지 검색)에서 state-of-the-art 달성[1]

## 2. 해결하고자 하는 문제와 제안하는 방법

### 해결하려는 문제

기존 시각-언어 태스크 접근법의 핵심 문제는 **시각적 접지(visual grounding) 학습이 태스크 훈련의 일부로만 이루어져** 제한적이고 편향된 데이터에서 일반화가 어렵다는 점입니다. 전통적으로는 별도로 사전훈련된 시각 모델과 언어 모델을 결합하여 접지를 학습했으나, 이는 **근시안적인 접지**를 만들어 일반화 성능이 떨어지는 문제가 있었습니다.[1]

### 제안하는 방법

#### 2.1 모델 아키텍처

ViLBERT는 **두 개의 병렬 BERT 스타일 스트림**으로 구성됩니다:

- **언어 스트림**: BERT_BASE로 초기화 (12개 트랜스포머 레이어, 숨겨진 상태 크기 762, 12개 어텐션 헤드)
- **시각 스트림**: 숨겨진 상태 크기 1024, 8개 어텐션 헤드로 구성

#### 2.2 상호 주의 트랜스포머 (Co-Attentional Transformer)

핵심 기술 혁신인 상호 주의 메커니즘은 다음과 같이 작동합니다:

중간 시각 표현 $$H_V^{(i)}$$와 언어 표현 $$H_W^{(j)}$$가 주어졌을 때, 각 모달리티의 키(K)와 값(V)을 다른 모달리티의 다중 헤드 어텐션 블록에 입력합니다. 이는 다음과 같은 크로스 모달 어텐션을 수행합니다:[1]

- 시각 스트림: 언어로 조건화된 이미지 어텐션
- 언어 스트림: 이미지로 조건화된 언어 어텐션

#### 2.3 이미지 표현

이미지 영역은 Faster R-CNN (ResNet-101 backbone, Visual Genome에서 사전훈련)을 통해 추출되며, **5차원 공간 벡터**로 인코딩됩니다:
- 정규화된 좌상단/우하단 좌표
- 이미지 영역 비율

### 2.4 사전훈련 태스크

#### 마스킹된 다중모달 모델링
- 약 15%의 단어와 이미지 영역을 마스킹
- 마스킹된 이미지 영역의 경우, 정확한 피처 값 회귀 대신 **의미 클래스 분포 예측**
- KL 발산 손실을 사용하여 사전훈련된 탐지 모델의 출력 분포와 일치

수식적으로 표현하면:

$$ \mathcal{L}_{masked} = \mathbb{E}[\text{KL}(p_{detector}(c|v_i) || p_{model}(c|v_i))] $$

#### 다중모달 정렬 예측
- 이미지-텍스트 쌍이 정렬되었는지 예측
- 전체 표현: $$h_{IMG} \odot h_{CLS}$$ (원소별 곱)
- 이진 교차 엔트로피 손실 사용

## 3. 모델 구조 상세 분석

ViLBERT의 혁신적인 두-스트림 구조는 **각 모달리티의 서로 다른 처리 요구사항**을 수용합니다:[1]

- **언어 스트림**: 상당한 맥락 집계가 필요한 단어 시퀀스 처리
- **시각 스트림**: 이미 고수준 피처인 이미지 영역의 제한적 맥락 집계

상호작용은 특정 레이어에서만 발생하며, 텍스트 스트림은 시각적 피처와 상호작용하기 전에 **더 많은 처리**를 거칩니다.

## 4. 성능 향상 및 실험 결과

### 4.1 주요 성능 개선

ViLBERT는 네 가지 확립된 태스크에서 모두 state-of-the-art를 달성했습니다:[1]

- **VQA 2.0**: 70.55% (test-dev)
- **VCR**: Q→A 72.42%, QA→R 74.47%, Q→AR 54.04%
- **RefCOCO+**: val 72.34%, testA 78.52%, testB 62.61%
- **이미지 검색**: R@1 58.20%, R@5 84.90%, R@10 91.52%

### 4.2 아블레이션 연구

**아키텍처 비교**: 단일 스트림 대비 두-스트림 아키텍처에서 **일관된 성능 향상**을 보였습니다.[1]

**사전훈련 효과**: 사전훈련 없는 ViLBERT† 대비 **2-13% 성능 향상**을 달성했습니다.[1]

**깊이 연구**: VQA와 이미지 검색은 더 깊은 모델(6레이어)에서, VCR과 RefCOCO+는 얕은 모델에서 더 좋은 성능을 보였습니다.[1]

### 4.3 데이터셋 크기 영향

Conceptual Captions 데이터셋 크기에 따른 실험에서 **단조 증가하는 성능 향상**을 확인했으며, 이는 더 많은 사전훈련 데이터가 도움이 될 수 있음을 시사합니다.[1]

## 5. 일반화 성능 향상 가능성

### 5.1 제로샷 성능

**제로샷 캡션 기반 이미지 검색**에서 ViLBERT는 파인튜닝 없이도 31.86% R@1을 달성했습니다. 이는 파인튜닝된 모델(58.20%)보다는 낮지만 이전 SOTA(48.60%)와 비교할 때 **의미 있는 성능**을 보여줍니다.[1]

### 5.2 태스크 간 전이성

ViLBERT의 가장 큰 장점은 **단일 기본 아키텍처**로 다양한 시각-언어 태스크에 쉽게 적용할 수 있다는 점입니다. 각 태스크마다 단순히 **분류기 하나만 추가**하면 되어 기존의 복잡한 태스크별 모델 개발 노력을 크게 줄일 수 있습니다.[1]

### 5.3 일반화 메커니즘

- **공간적 인코딩**: 이미지 영역의 위치 정보를 명시적으로 인코딩하여 공간적 이해 향상
- **계층적 상호작용**: 다양한 표현 깊이에서의 크로스 모달 연결로 풍부한 다중모달 표현 학습
- **대규모 약지도 학습**: 웹에서 자동 수집된 330만 개 이미지-캡션 쌍으로 다양한 시각적 내용과 언어적 설명 학습

## 6. 모델의 한계

### 6.1 기술적 한계

- **텍스트 생성 능력 부족**: 양방향 모델의 특성상 기존의 greedy decoder(빔 서치 등) 적용이 어려움[1]
- **긴 시퀀스 처리 한계**: 대화, 체현된 태스크, 비디오 처리에서 발견되는 긴 이미지-텍스트 시퀀스 처리에 대한 미해결 문제[1]
- **노이즈 데이터 영향**: Conceptual Captions의 자동 수집 과정에서 발생한 노이즈와 편집화된 캡션이 학습에 영향[1]

### 6.2 태스크 범위 한계

다음 태스크 군들은 다루지 못했습니다:[1]
- 시각적으로 접지된 대화
- 체현된 질문 답변과 지시 따르기
- 이미지 및 비디오 캡셔닝과 같은 텍스트 생성 태스크

## 7. 미래 연구에 미치는 영향과 고려사항

### 7.1 연구 패러다임 전환

ViLBERT는 시각-언어 연구 분야에서 **"사전훈련-전이학습" 패러다임**을 확립했습니다. 이후 UNITER, VILLA, VinVL 등의 후속 연구들이 이 접근법을 발전시켰습니다.

### 7.2 앞으로의 연구 고려사항

#### 아키텍처 발전
- **통합 트랜스포머**: 더 효율적인 크로스 모달 융합 방법 개발
- **스케일링**: 더 큰 모델과 더 많은 데이터로의 확장 가능성 탐구
- **효율성**: 계산 비용과 메모리 사용량 최적화

#### 사전훈련 개선
- **데이터셋 품질**: 더 고품질의 대규모 다중모달 데이터셋 구축
- **프록시 태스크**: 더 효과적인 자가지도 학습 태스크 설계
- **다중 태스크 학습**: 여러 태스크를 동시에 학습하는 방법론 개발

#### 일반화 능력 확장
- **도메인 적응**: 의료, 로봇공학 등 특정 도메인으로의 적용
- **제로샷/퓨샷 학습**: 적은 데이터로도 새로운 태스크에 빠르게 적응
- **다국어 지원**: 다양한 언어에 대한 시각-언어 이해

### 7.3 실무 적용 고려사항

- **컴퓨팅 자원**: 대규모 모델 훈련과 추론에 필요한 자원 최적화
- **실시간 처리**: 실제 응용에서 요구되는 지연시간 단축
- **윤리적 고려**: 편향성 제거와 공정성 보장 방안 마련

ViLBERT는 시각-언어 AI 분야의 **기초 모델(Foundation Model)** 개념을 선도적으로 제시했으며, 현재까지도 이 분야 연구의 핵심 참조점이 되고 있습니다. 특히 **사전훈련된 다중모달 표현의 전이 가능성**을 입증함으로써, 범용 AI 시스템 개발에 중요한 이정표를 제시했습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/e664cebb-b4c6-4b04-90c0-0865f981ccfd/1908.02265v1.pdf)
