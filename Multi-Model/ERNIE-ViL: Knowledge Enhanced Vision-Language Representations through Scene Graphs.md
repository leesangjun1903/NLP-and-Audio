# ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs

## **핵심 주장과 주요 기여**

**ERNIE-ViL**은 장면 그래프(Scene Graph)로부터 구조화된 지식을 활용하여 비전-언어 간 세밀한 의미 정렬을 학습하는 혁신적인 접근법입니다. 기존 방법들이 무작위 마스킹에 의존하여 일반 단어와 세부 의미 단어를 구분하지 못하는 한계를 극복하고자, **객체(Objects), 속성(Attributes), 관계(Relationships)**라는 세 가지 차원의 세밀한 의미 연결을 통해 비전-언어 표현 학습을 혁신했습니다.[1]

주요 기여는 다음과 같습니다:
- **구조화된 지식 도입**: 비전-언어 사전 훈련에 최초로 장면 그래프 기반 구조화된 지식을 활용[1]
- **Scene Graph Prediction 태스크**: Object Prediction, Attribute Prediction, Relationship Prediction 태스크를 통한 세밀한 의미 정렬 학습[1]
- **SOTA 성능 달성**: 5개 다운스트림 태스크에서 최고 성능을 기록하며, VCR 리더보드에서 절대적으로 3.7% 향상된 1위 달성[1]

## **해결 문제와 제안 방법**

### **문제 정의**
기존 비전-언어 사전 훈련 방법들은 무작위 서브워드 마스킹에 기반하여 일반 단어와 세부 의미를 구별하지 못해, 실제 장면에서 요구되는 세밀한 의미를 제대로 표현하지 못했습니다. 예를 들어, "man standing on boat"와 "man repairing bike"의 차이는 객체, 속성, 관계의 세부 의미에서 나타나는데, 기존 방법들은 이러한 차이를 효과적으로 학습하지 못했습니다.[1]

### **제안 방법과 수식**

**Scene Graph Prediction 태스크**는 다음 세 가지 예측 태스크로 구성됩니다:

**1. Object Prediction**
객체 노드의 30%를 무작위로 선택하여 마스킹하고, 주변 단어와 모든 이미지 영역을 바탕으로 마스킹된 객체 토큰을 복원합니다:[1]

$$ L_{obj}(\theta) = -E_{(w,v)\sim D} \log(P(w_{oi}|w\setminus w_{oi}, v)) $$

**2. Attribute Prediction**  
속성 쌍의 30%를 선택하여 객체는 유지하고 속성 노드만 마스킹합니다:[1]

$$ L_{attr}(\theta) = -E_{(w,v)\sim D} \log(P(w_{ai}|w_{oi}, w\setminus w_{ai}, v)) $$

**3. Relationship Prediction**
관계 삼중체에서 객체는 유지하고 관계 노드만 마스킹하여 예측합니다:[1]

$$ L_{rel}(\theta) = -E_{(w,v)\sim D} \log(P(w_{ri}|w_{oi1}, w_{oi2}, w\setminus w_{ri}, v)) $$

### **모델 구조**

**ERNIE-ViL**은 **Two-stream Cross-modal Transformers** 아키텍처를 채택합니다:[1]

- **텍스트 스트림**: WordPiece 토큰화된 텍스트 시퀀스 처리
- **비전 스트림**: Faster R-CNN으로 추출된 이미지 영역 특징 처리  
- **교차 어텐션**: 두 스트림 간 교차 모달 Transformer 블록을 통한 상호작용

이미지 임베딩은 5차원 위치 벡터 $$ (\frac{x_1}{W}, \frac{y_1}{H}, \frac{x_2}{W}, \frac{y_2}{H}, \frac{(y_2-y_1)(x_2-x_1)}{WH}) $$를 포함하여 공간 정보를 인코딩합니다.[1]

**ERNIE-ViL-base**와 **ERNIE-ViL-large** 두 가지 스케일로 구현되며, 텍스트 스트림의 깊이에서 차이를 보입니다.[1]

## **성능 향상 및 일반화 성능**

### **성능 향상 결과**

**Out-of-domain 데이터셋만 사용**한 경우:
- **VCR Q→AR**: VLBERT-large 대비 6.60% 향상 (58.9% → 65.81%)[1]
- **VQA test-std**: 1.74% 향상 (72.22% → 73.96%)[1]
- **RefCOCO+ grounding**: testA, testB 모두에서 2.40% 향상[1]
- **Flickr30K retrieval**: Image retrieval R@1에서 2.94% 향상[1]

**Out-of-domain + In-domain 데이터셋 사용**한 경우 모든 태스크에서 SOTA 달성.[1]

### **일반화 성능 향상**

**Cloze Test 분석**을 통해 일반화 성능 향상을 검증했습니다:[1]
- **전체 정확도**: SGP 태스크 없는 모델 대비 2.0% 향상 (ACC@1: 49.75% → 51.75%)[1]
- **객체 예측**: 1.20% 향상 (57.14% → 58.34%)[1]
- **속성 예측**: 1.84% 향상 (44.32% → 46.16%)[1]
- **관계 예측**: 3.08% 향상 (47.57% → 50.65%)[1]

특히 **세밀한 의미 정렬이 중요한 태스크**에서 더 큰 개선을 보였습니다. RefCOCO+ grounding 태스크에서 0.69% 향상, Flickr30K image retrieval에서 2.22% 향상을 달성하여, 모델이 시각적 맥락에서 정확한 언어적 세부사항을 추론하는 능력이 크게 향상되었음을 보여줍니다.[1]

**텍스트 초기화 효과**도 중요한 발견입니다. ERNIE 2.0으로 초기화한 경우 BERT 대비 VCR 태스크에서 1.32% 추가 향상을 보여, 상식 추론 능력의 일반화가 개선되었습니다.[1]

## **한계점**

논문에서 명시적으로 제시된 한계점은 제한적이지만, 다음과 같은 잠재적 한계를 파악할 수 있습니다:

1. **장면 그래프 의존성**: Anderson et al.의 Scene Graph Parser에 의존하여, 파싱 품질이 모델 성능에 영향을 미칠 수 있습니다[1]
2. **계산 복잡도**: 추가적인 SGP 태스크로 인한 훈련 시간 증가
3. **데이터셋 범위**: 주로 영어 기반 데이터셋에서 평가되어 다국어 일반화는 미검증

## **향후 연구 영향과 고려사항**

### **연구 영향**

**ERNIE-ViL**은 비전-언어 사전 훈련 분야에 다음과 같은 중요한 영향을 미쳤습니다:

1. **구조화된 지식 통합**: 단순한 텍스트-이미지 매칭을 넘어 구조화된 의미 지식을 활용하는 새로운 패러다임 제시[1]
2. **세밀한 의미 정렬**: 객체-속성-관계 트리플렛 기반의 세부 의미 학습 방법론 확립[1]
3. **성능 기준점 향상**: 여러 다운스트림 태스크에서 새로운 SOTA 기준 설정[1]

### **향후 연구 고려사항**

**논문에서 제시한 향후 방향**:
1. **이미지 장면 그래프 활용**: 텍스트뿐만 아니라 이미지에서 직접 추출한 장면 그래프의 통합[1]
2. **Graph Neural Networks**: 더 많은 구조화된 지식을 통합할 수 있는 GNN 기반 접근법 고려[1]

**추가 고려사항**:
- **효율성 개선**: SGP 태스크의 계산 오버헤드 최적화
- **다국어 확장**: 비영어권 언어에서의 장면 그래프 파싱과 성능 검증
- **실시간 응용**: 실제 응용에서의 추론 속도와 메모리 효율성 개선
- **도메인 적응**: 의료, 로봇공학 등 특정 도메인에서의 세밀한 의미 이해 성능 향상

**ERNIE-ViL**의 성공은 향후 멀티모달 AI 연구에서 **구조화된 지식과 세밀한 의미 정렬**의 중요성을 입증하며, 더욱 정교한 비전-언어 이해 시스템 개발의 토대를 마련했습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b4d4fb2c-0613-4739-87d1-06a79857667d/2006.16934v3.pdf)
