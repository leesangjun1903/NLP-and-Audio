
# Visual-RFT: Visual Reinforcement Fine-Tuning

## 1. 논문 핵심 요약

Visual-RFT(Visual Reinforcement Fine-Tuning)는 **대규모 비전-언어 모델(LVLMs)의 시각적 추론 능력을 강화하기 위해 검증 가능한 보상(verifiable rewards)과 그룹 상대 정책 최적화(GRPO)를 결합한 새로운 패러다임**입니다. 이 논문은 OpenAI o1의 강화 미세 조정(RFT) 개념을 처음으로 시각 인식 작업으로 확장하며, 데이터가 극히 제한된 상황에서 뛰어난 성능 향상을 달성합니다.[1]

**핵심 주장**: 기존의 지도 학습 기반 미세 조정(SFT)과 달리, Visual-RFT는 모델이 다양한 시도를 거쳐 검증 가능한 보상을 통해 "올바른 시도"를 강화하는 방식으로 학습하므로, 제한된 데이터로도 더 깊이 있는 이해와 우수한 일반화 능력을 획득할 수 있습니다.[1]

***

## 2. 해결하고자 하는 핵심 문제

### 2.1 문제 정의

**문제 1: 데이터 부족 환경에서의 성능 저하**

기존 SFT는 대규모 고품질 데이터에 의존합니다. 그러나 실제 응용에서는 특정 도메인의 레이블된 데이터가 매우 제한적입니다. 예를 들어, 희귀 객체 탐지나 세밀한 분류 작업에서 한두 개의 샘플만 제공될 때, SFT는 데이터 부족으로 인해 오히려 성능이 저하됩니다.

**문제 2: 시각 작업에서 강화학습의 미적용**

기존 강화학습 기반 연구는 주로 수학 문제 해결이나 코드 생성 같은 검증이 명확한 작업에 국한되었습니다. 시각 인식 작업(객체 탐지, 이미지 분류, 그라운딩)에서는 보상 함수 설계의 어려움으로 인해 강화학습 적용이 미흡했습니다.

**문제 3: 별도의 보상 모델 학습 필요성**

기존 RLHF 방식은 인간 선호도 데이터로부터 보상 모델을 별도로 학습해야 하므로, 추가적인 주석 작업과 계산 비용이 발생합니다.

***

## 3. 제안하는 방법론 및 수식

### 3.1 강화학습 기본 프레임워크

Visual-RFT는 다음의 목적 함수를 최적화합니다:[1]

$$\max_{\pi_\theta} \mathbb{E}_{o \sim \pi_\theta(q)} [R_{\text{RLVR}}(q, o)]$$

여기서:
- $\pi_\theta$: 현재 정책 모델
- $q$: 입력 질문 또는 프롬프트
- $o$: 모델의 출력(응답)
- $R_{\text{RLVR}}$: 검증 가능한 보상 함수

이는 다음과 같이 분해됩니다:[1]

$$R_{\text{RLVR}} = [R(q, o) - \beta \text{KL}[\pi_\theta(o|q) \| \pi_{\text{ref}}(o|q)]]$$

여기서:
- $R(q, o)$: 작업별 검증 가능한 보상
- $\beta$: KL 발산 제어 계수
- $\pi_{\text{ref}}$: 참조 모델(미세 조정 전의 모델)

***

### 3.2 Group Relative Policy Optimization (GRPO)

GRPO는 별도의 비판자 모델 없이 응답 그룹 간 상대적 품질을 비교합니다. 주어진 질문 $q$에 대해 $G$개의 응답 집합 $\{o_1, o_2, \ldots, o_G\}$을 생성한 후, 각 응답의 상대적 품질을 계산합니다:[1]

$$A_i = \frac{r_i - \text{mean}(\{r_1, \ldots, r_G\})}{\text{std}(\{r_1, \ldots, r_G\})}$$

여기서:
- $A_i$: $i$번째 응답의 상대적 장점
- $r_i$: $i$번째 응답의 보상값
- $\text{mean}$, $\text{std}$: 그룹 내 평균 및 표준편차

***

### 3.3 작업별 검증 가능한 보상 함수 설계

#### 3.3.1 객체 탐지 작업 - IoU 기반 보상

객체 탐지 작업에서 모델은 바운딩 박스와 신뢰도를 출력합니다. 보상 함수는 세 가지 구성 요소로 이루어집니다:[1]

$$R_d = R_{\text{IoU}} + R_{\text{conf}} + R_{\text{format}}$$

**IoU 보상** - 모든 바운딩 박스의 평균 IoU:[1]

$$R_{\text{IoU}} = \frac{\text{iou}_1 + \text{iou}_2 + \cdots + \text{iou}_n}{n}$$

**신뢰도 보상** - 탐지 성공 여부에 따른 조건부 보상:[1]

$$r_c^i = \begin{cases} c_i & \text{if } \text{iou}_i \neq 0 \\ 1 - c_i & \text{if } \text{iou}_i = 0 \end{cases}$$

전체 신뢰도 보상:[1]

$$R_{\text{conf}} = \frac{\sum_{i=1}^n r_c^i}{n}$$

**포맷 보상** - 모델이 예상된 HTML 태그 형식(`<think>`, `<answer>`)을 따르도록 강제

#### 3.3.2 분류 작업 - 정확도 기반 보상

분류 작업의 보상은 더 간단합니다:[1]

$$R_{\text{cls}} = R_{\text{acc}} + R_{\text{format}}$$

여기서 $R_{\text{acc}}$는 이진 값으로, 예측이 정답과 일치하면 1, 아니면 0입니다.

***

### 3.4 모델 구조

Visual-RFT의 전체 파이프라인:[1]

1. **입력**: 이미지와 질문
2. **다중 응답 생성**: LVLM이 추론 과정(`<think>` 태그)과 최종 답변(`<answer>` 태그)을 포함한 여러 응답을 생성
3. **보상 계산**: 작업별 검증 가능한 보상 함수로 각 응답을 평가
4. **정책 최적화**: GRPO를 사용하여 모델 가중치를 업데이트
5. **안정성 제약**: KL 발산을 통해 참조 모델과의 차이 제한

***

## 4. 성능 향상 분석

### 4.1 세밀한 이미지 분류 (Fine-grained Classification)

Visual-RFT는 제한된 데이터에서 뛰어난 일반화 능력을 보입니다:[1]

| 설정 | 기준 | SFT | Visual-RFT | 향상도 |
|------|------|-----|------------|--------|
| 1-shot | 56.0% | 51.7% | 80.3% | **+24.3%** |
| 2-shot | - | 58.8% | 83.5% | **+27.5%** |
| 4-shot | - | 55.6% | 81.9% | **+25.9%** |
| 8-shot | - | 60.3% | 85.1% | **+29.1%** |

특히 주목할 점은 **1-shot 설정에서 SFT가 오히려 -4.3%로 성능이 저하**되는 반면, Visual-RFT는 +24.3%의 큰 향상을 보입니다. 이는 강화학습의 탐색-활용(exploration-exploitation) 메커니즘이 극도로 제한된 데이터에서 더 효과적임을 시사합니다.[1]

### 4.2 Few-shot 객체 탐지 (COCO 데이터셋)

COCO 데이터셋의 8개 카테고리에서 실시한 실험:[1]

| 샷 수 | 모델 크기 | 기준 | SFT | Visual-RFT | 향상도 |
|------|----------|------|-----|------------|--------|
| 2-shot | Qwen2-VL-2B | 19.6 mAP | 21.0 | **41.5** | **+21.9** |
| 4-shot | Qwen2-VL-2B | - | 25.2 | **40.6** | **+21.0** |
| 16-shot | Qwen2-VL-7B | 43.0 | 44.1 | **54.3** | **+11.3** |

데이터가 증가함에 따라 SFT의 성능도 개선되지만, Visual-RFT는 모든 설정에서 지속적인 우월성을 유지합니다.[1]

### 4.3 LVIS 희귀 카테고리 탐지

LVIS 데이터셋의 희귀 카테고리(6개)에서의 10-shot 실험:[1]

| 카테고리 | 기준 | SFT | Visual-RFT | 향상도 |
|----------|------|-----|------------|--------|
| 전체 평균 | 4.0 | 10.0 | **19.4** | **+15.4** |
| kitchen table | 13.4 | 34.1 | **42.2** | **+8.1** |
| 오믈렛 | 4.7 | 4.7 | **20.4** | **+15.7** |

특히 극도로 희귀한 카테고리에서 Visual-RFT의 강점이 두드러집니다.[1]

### 4.4 추론 그라운딩 (LISA 데이터셋)

239개의 훈련 이미지로만 학습하는 상황에서:[1]

| 메트릭 | 모델 크기 | 기준 | SFT | Visual-RFT | 향상도 |
|--------|----------|------|-----|------------|--------|
| mIoU_test | Qwen2-VL-2B | 26.9 | 28.3 | **37.6** | **+10.7** |
| mIoU_test | Qwen2-VL-7B | 40.4 | 39.1 | **43.9** | **+3.5** |

Visual-RFT는 SFT뿐 아니라 GroundedSAM 같은 전문화된 모델도 능가합니다.[1]

### 4.5 오픈 어휘 객체 탐지 (Open Vocabulary Detection)

COCO 기본 65개 카테고리로 훈련하고 15개 새로운 카테고리로 테스트:[1]

| 데이터셋 | 카테고리 | 기준 | SFT | Visual-RFT | 향상도 |
|---------|----------|------|-----|------------|--------|
| COCO | 새로운(15개) | 9.8 mAP | 13.6 | **31.3** | **+21.5** |
| LVIS | 희귀(13개) | 2.7 mAP | 7.6 | **20.7** | **+18.0** |

**핵심 발견**: 기존 또는 SFT 모델이 일부 카테고리(예: egg roll, futon)에서 0 AP를 달성했던 반면, Visual-RFT는 이들을 0에서 20-40 AP로 끌어올렸습니다.[1]

***

## 5. 일반화 성능 향상 메커니즘

### 5.1 이론적 기반

Visual-RFT의 일반화 향상은 다음 세 가지 요인에서 비롯됩니다:

**1. 추론 과정의 명시화**

모델은 최종 답변 전에 `<think>` 태그로 감싸인 추론 과정을 생성하도록 강제됩니다. 이는:[1]
- 모델이 중간 추론 단계를 명시적으로 표현
- 오류 패턴 인식 및 수정 가능성 증대
- 보상 신호가 더 세밀한 추론 패턴을 반영

**2. 탐색-활용의 균형**

GRPO의 그룹 상대 정규화는:[1]
- 그룹 내 모든 응답을 비교하여 우수한 응답 강화
- 예를 들어, 한 응답이 IoU 0.7을 달성하면 그룹 내 상대적으로 우수한 것으로 간주
- 희귀 카테고리에서도 적어도 하나의 올바른 응답을 찾을 확률이 높음

**3. 작업별 맞춤형 보상**

IoU 기반 보상은 탐지의 정확도(정확한 위치)를 직접 최적화하고, 신뢰도 보상은 모델의 불확실성을 올바르게 보정합니다:[1]

$$r_c^i = \begin{cases} c_i & \text{성공적 탐지 시: 높은 신뢰도 보상} \\ 1 - c_i & \text{실패 시: 낮은 신뢰도 보상} \end{cases}$$

이는 모델이 "확신도 높지만 부정확한 예측"을 억제하도록 학습하게 합니다.

### 5.2 분포 외(Out-of-Distribution) 일반화

Monster Girls(MG) 데이터셋(애니메이션 스타일 이미지)의 4-shot 실험:[1]

| 메트릭 | 기준 | SFT | Visual-RFT | 향상도 |
|--------|------|-----|------------|--------|
| 평균 mAP | 20.6 | 26.8 | **61.8** | **+41.2** |

이는 데이터의 도메인이 완전히 다른 경우에도 Visual-RFT가 더 강한 일반화 능력을 가짐을 보여줍니다.[1]

***

## 6. 논문의 한계 및 도전 과제

### 6.1 구체적 한계점

**한계 1: 작업 범위의 제한**

논문에서 다루는 보상 함수는 객체 탐지와 분류로 제한됩니다. 세분화(segmentation), 객체 추적(tracking), 3D 인식 같은 더 복잡한 작업에 대한 보상 함수 설계는 명확하지 않습니다.[1]

**한계 2: 보상 함수 설계의 수작업**

각 새로운 작업마다 수작업으로 검증 가능한 보상 함수를 설계해야 합니다. 이는 확장성 면에서 병목이 될 수 있습니다.

**한계 3: 추론 성능**

다중 응답 생성과 정책 최적화로 인한 추론 시간 증가가 예상되지만, 논문에서는 이를 자세히 논의하지 않습니다.

**한계 4: 대규모 데이터에서의 성능**

논문은 주로 적은 데이터(수십~수백 샘플)에 초점을 맞춥니다. 대규모 데이터셋에서 Visual-RFT가 SFT와 비교하여 어떤 장점을 제공하는지는 불명확합니다.

### 6.2 미해결 연구 문제

1. **보상 함수의 일반화**: 자동으로 또는 최소한의 감독으로 작업별 보상 함수를 생성할 수 있는가?
2. **연쇄적 작업의 처리**: 여러 개의 순차적 시각 작업(예: 객체 탐지 → 분류 → 그라운딩)에서 보상을 어떻게 정의할 것인가?
3. **하이퍼파라미터 민감도**: GRPO와 KL 계수($\beta$)의 최적값이 작업과 데이터 크기에 따라 어떻게 변하는가?

***

## 7. 최신 관련 연구 비교 (2020년 이후)

### 7.1 핵심 연구 트렌드

#### A. Vision R1 시리즈 (2025년)

| 연구명 | 초점 | Vision-RFT와의 관계 |
|--------|------|-------------------|
| **Vision-R1** (3월 2025) | Vision-guided R1-like RL로 사람 주석 제거 | Visual-RFT의 직접 후속, 보상 설계 간소화 |
| **VLM-R1** (4월 2025) | 안정성과 확장성에 초점 | Visual-RFT 방법론의 개선 및 일반화 |
| **Puzzle Curriculum GRPO** (2025) | 지도 없는 RLVR 레시피 | Visual-RFT의 보상 설계 자동화 시도 |

#### B. Verifiable Rewards 관련 연구

**AP-GRPO** (2025): Smooth Numerical Reward Activation으로 정확한 3D 좌표 예측을 위해 Visual-RFT의 IoU 보상을 개선합니다. 절대값 보상을 추가하여 희미한(near-miss) 샘플에 대한 신호를 강화합니다.[2]

$$w(r) = r^\alpha, \quad r \in, \, \alpha \geq 1$$[1]

**Temporal-RLT** (2025): 비디오 이해 작업을 위해 의미적 보상(semantic reward)과 시간적 보상(temporal reward)을 이중 구조로 설계하여 Visual-RFT의 단일 보상 구조를 확장합니다.[3]

#### C. Few-shot Learning의 진화

| 방법 | 핵심 기법 | Vision-RFT 대비 성과 |
|------|----------|------------------|
| **SWAT** (2025) | 검색 증강 학습 + 2단계 미세조정 | +6% 향상, 하지만 더 복잡한 파이프라인 |
| **FMVP** (2025) | 메타-러닝 기반 프롬프트 튜닝 | 일반화 향상, 하지만 계산 비용 증가 |
| **AdvCLIP-LoRA** (2025) | 적대적 LoRA 미세조정 | CLIP 특화, Vision-RFT보다 제한된 범위 |

### 7.2 주요 차별점

**Visual-RFT의 혁신성**:[1]

1. **첫 번째 일반적 RLVR 적용**: 수학/코드에 국한된 RLVR을 다양한 시각 작업으로 확장
2. **간단하고 명확한 보상**: 신경망 기반 보상 모델 대신 규칙 기반 검증 가능한 보상 사용
3. **극소 데이터 성능**: 1-shot에서 SFT 대비 +24.3%, 기존 few-shot 방법 대비 +6-40%

**후속 연구와의 관계**:[2][3][1]

- **Vision-R1**: Visual-RFT의 철학을 계승하면서 보상 설계를 더욱 자동화
- **AP-GRPO**: 공간적 정밀성을 위해 Visual-RFT의 IoU 보상을 정교화
- **Temporal-RLT**: Visual-RFT의 이중 보상 구조 개념을 비디오 도메인으로 확장

***

## 8. 앞으로의 연구 방향 및 고려사항

### 8.1 이론적 고찰

**문제**: Visual-RFT가 왜 효과적인가?

현재 논문은 경험적 결과 중심이며, 이론적 설명은 미흡합니다. 향후 연구는 다음을 다루어야 합니다:[1]

1. **정책 수렴 분석**: GRPO와 검증 가능한 보상 조합이 최적 정책에 수렴하는가?
2. **샘플 복잡도**: SFT 대비 몇 배 더 적은 샘플이 필요한가? 데이터 크기에 따른 수학적 한계는?
3. **보상 설계의 일반성**: 특정 보상 함수 형태(예: IoU)가 다른 작업에도 적용 가능한 조건은?

### 8.2 실질적 개선 방향

**개선 1: 자동 보상 함수 생성**

향후 연구는 수작업 설계 대신 다음을 탐색해야 합니다:

```
작업 정의 → [보상 함수 생성 모듈] → 검증 가능한 보상
```

예를 들어, 메타-러닝을 사용하여 새로운 작업에 대한 보상 함수를 학습할 수 있습니다.

**개선 2: 다중 작업 학습**

현재 Visual-RFT는 단일 작업 미세조정에 초점을 맞춥니다. 다음은 여러 작업에서 공유된 표현을 학습하는 것입니다:

$$\mathcal{L}_{\text{multi}} = \sum_{t=1}^T \lambda_t R_t(q, o)$$

여기서 $T$는 작업 수, $\lambda_t$는 작업별 가중치입니다.

**개선 3: 장기 추론 (Long-horizon Reasoning)**

현재는 단일 응답의 최종 정확도만 고려합니다. 향후 연구는:

- 중간 추론 단계의 품질 평가
- 오류로부터의 자기 수정 능력 강화
- 다단계 의사결정 문제로의 확장

### 8.3 도메인별 특화

**3D 및 공간 추론**

최근 AP-GRPO가 보여주듯, 정밀한 좌표 예측에서는:[2]

$$r = \min(|y_{\text{pred}} - y_{\text{true}}| / \epsilon, 1)$$

같은 연속 보상이 더 효과적일 수 있습니다. 이는 Visual-RFT의 이진 보상 구조의 한계를 보여줍니다.

**비디오 이해**

Temporal-RLT의 예시처럼, 시간축을 따라:[3]

$$r_{\text{video}} = w_s \cdot r_{\text{semantic}} + w_t \cdot r_{\text{temporal}}$$

같은 다중 보상 구조가 필요합니다.

### 8.4 실용적 배포

**도전 1: 계산 효율성**

- 다중 응답 생성($G$=수십 개)으로 인한 추론 비용 증가
- 그룹별 정규화로 인한 메모리 오버헤드
- 해결책: 응답 크기 제한, 배치 처리 최적화

**도전 2: 하이퍼파라미터 튜닝**

각 작업마다 다음을 튜닝해야 합니다:
- GRPO 그룹 크기 $G$
- KL 계수 $\beta$
- 학습률, 최적화 스텝 수
- 데이터 기반 자동 튜닝 메커니즘이 필요

**도전 3: 오프라인 시나리오**

현재 방법은 온라인 학습(새로운 데이터에 대한 즉시 업데이트)을 가정합니다. 보험, 금융 같은 고위험 도메인에서는 오프라인 강화학습 적용이 필요합니다.

***

## 9. 종합 평가 및 임팩트

### 9.1 학술적 기여

Visual-RFT는 **멀티모달 모델 연구의 패러다임 전환**을 나타냅니다:

| 차원 | 기존 연구 | Visual-RFT |
|------|----------|-----------|
| 데이터 효율성 | SFT: 수천~수백만 샘플 | RFT: 수십~수백 샘플 |
| 보상 모델 | 학습 기반(신경망) | 규칙 기반(검증 가능) |
| 작업 범위 | LLM 중심 | LLM + 시각 작업 |
| 데이터 품질 요구도 | 고품질 주석 필수 | 최소한의 이진 레이블 |

### 9.2 산업적 영향

1. **저자원 환경에서의 배포**: 라벨이 극히 제한된 신흥국이나 전문화된 영상 분석 분야에 직접 적용 가능
2. **도메인 적응 비용 감소**: 새로운 카테고리 추가 시 기존 SFT 대비 90% 데이터 감소
3. **실시간 개선**: 사용자 피드백을 직접 강화학습 신호로 변환 가능

### 9.3 한계 및 향후 과제

**기술적 한계**:[1]
- 보상 함수의 수작업 설계
- 대규모 데이터셋에서의 검증 부족
- 추론 속도 증가

**연구적 격차**:[3][2][1]
- 이론적 분석 부족
- 다중 작업 최적화의 미흡
- 장기 추론 능력 확장 필요

***

## 10. 결론

Visual-RFT는 **검증 가능한 보상 기반 강화학습을 시각 도메인에 성공적으로 이식한 획기적 연구**입니다. 극소 데이터 환경에서의 뛰어난 성능(1-shot에서 +24.3% 정확도 향상)과 강한 분포 외 일반화 능력은 실무 적용의 높은 가능성을 시사합니다.

그러나 보상 함수의 수작업 설계, 이론적 토대의 부족, 대규모 데이터에서의 성능 검증이 필요합니다. 향후 연구는 **자동 보상 생성**, **다중 작업 학습**, **장기 추론 확장**, **오프라인 강화학습**에 초점을 맞춰야 하며, Vision-R1, VLM-R1, AP-GRPO, Temporal-RLT 같은 후속 연구들이 이미 이러한 방향으로 나아가고 있습니다.

***

### 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3e072bd3-3cfe-4ecb-8ebe-275f4b1ad0b4/2503.01785v1.pdf)
[2](https://arxiv.org/pdf/2504.02587.pdf)
[3](http://arxiv.org/pdf/2501.02189.pdf)
[4](http://arxiv.org/pdf/2503.18013.pdf)
[5](https://arxiv.org/html/2502.01616v1)
[6](https://arxiv.org/html/2502.07949v1)
[7](https://arxiv.org/html/2504.07615v1)
[8](https://arxiv.org/html/2405.10292v3)
[9](http://arxiv.org/pdf/2310.12921.pdf)
[10](https://nlp.cs.berkeley.edu/pubs/Zhai-Bai-Lin-Pan-Tong-Zhou-Suhr-Xie-LeCun-Ma-Levine_2024_Finetuning_paper.pdf)
[11](https://arxiv.org/html/2601.07695v1)
[12](http://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Few-Shot_Recognition_via_Stage-Wise_Retrieval-Augmented_Finetuning_CVPR_2025_paper.pdf)
[13](https://www.esann.org/sites/default/files/proceedings/2024/ES2024-181.pdf)
[14](https://aiflower.tistory.com/178)
[15](https://www.sciencedirect.com/science/article/abs/pii/S0925231225003601)
[16](https://huggingface.co/blog/vlms-2025)
[17](https://www.emergentmind.com/topics/reinforcement-learning-with-verifiable-rewards-rlvr)
[18](https://hushell.github.io/pmf/)
[19](https://www.sciencedirect.com/science/article/abs/pii/S0968090X25003250)
[20](https://openaccess.thecvf.com/content/ICCV2025/html/Liu_Visual-RFT_Visual_Reinforcement_Fine-Tuning_ICCV_2025_paper.html)
[21](https://arxiv.org/abs/2505.15130)
[22](https://arxiv.org/abs/2310.12921)
[23](https://turingpost.co.kr/p/topic-40-grpo-flow-grpo)
[24](https://openreview.net/forum?id=ep4FPuE1w3)
[25](https://arxiv.org/html/2508.08189v1)
[26](https://arxiv.org/html/2503.01785v1)
[27](https://arxiv.org/html/2506.07280v1)
[28](https://arxiv.org/abs/2411.05273)
[29](https://arxiv.org/html/2506.01908v1)
[30](https://arxiv.org/html/2509.26594v1)
[31](https://arxiv.org/html/2507.17079v1)
[32](https://arxiv.org/html/2503.13817v1)
[33](https://arxiv.org/html/2508.07313v3)
[34](https://arxiv.org/html/2502.04098v1)
[35](https://arxiv.org/html/2510.06783v2)
[36](https://arxiv.org/abs/2512.14944)
