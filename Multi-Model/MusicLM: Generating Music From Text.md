# MusicLM: Generating Music From Text

### 1. 핵심 주장 및 주요 기여

**MusicLM의 핵심 주장**은 자연언어 텍스트 설명으로부터 고충실도 음악을 생성할 수 있다는 것입니다. "차분한 바이올린 멜로디에 왜곡된 기타 리프가 배경"과 같은 텍스트 설명에서 24 kHz로 여러 분 동안 일관성 있게 음악을 생성합니다.[1]

**주요 기여**는 다음과 같습니다:[1]

1. **고품질 장시간 음악 생성**: 텍스트 조건부 신호에 충실하면서 24 kHz에서 24시간 이상 지속되는 고품질 음악을 생성하는 MusicLM 모델 제시

2. **다중 모달 조건화**: 텍스트뿐만 아니라 휘파람, 흥얼림 같은 멜로디도 조건으로 사용 가능하며, 텍스트 프롬프트로 설명된 스타일로 멜로디를 변환할 수 있는 능력

3. **고품질 평가 데이터셋**: 음악 전문가가 수작업으로 큐레이션한 5.5k 음악-텍스트 쌍으로 구성된 MusicCaps 데이터셋 공개

***

### 2. 문제 정의 및 해결 방법

**해결하고자 하는 문제**:[1]

기존 음성 생성 모델들(TTS, MIDI 기반 합성)은 조건 신호와 오디오 출력 간에 시간적 정렬을 가정합니다. 반면 텍스트-음악 생성은 시퀀스 전체에 걸친 고수준 캡션으로부터 부드러운 음악 진행을 생성해야 하며, 이는 다음과 같은 어려움이 있습니다:

- **데이터 부족**: 이미지 도메인과 달리 음악-텍스트 쌍 데이터가 극도로 부족
- **장시간 일관성**: 음악의 장기적 구조와 여러 악기 성분의 합성 필요
- **음악 설명의 어려움**: 이미지보다 음악의 음정, 리듬, 음색을 정확히 언어로 표현하기 어려움

**제안하는 방법**:[1]

MusicLM은 계층적 시퀀스-투-시퀀스 모델링으로 조건부 음악 생성을 구성합니다. 세 가지 독립적으로 사전 학습된 모델을 활용합니다:

#### 2.1 음성 토큰화

**SoundStream (음향 토큰)**:[1]
- 24 kHz 모노 음성에 RVQ (Residual Vector Quantization) 적용
- 12개의 벡터 양자화기, 각각 1024 어휘 크기
- 결과: 6 kbps 비트레이트로 초당 600개 토큰 생성 ($$A$$ 표기)

**w2v-BERT (의미론적 토큰)**:[1]
- 600M 파라미터 w2v-BERT의 7번째 중간 계층에서 추출
- k-means 양자화 (1024 클러스터, 25 Hz 샘플링)
- 결과: 초당 25개의 의미 토큰 ($$S$$ 표기)

**MuLan (조건 임베딩)**:[1]
- 음악-텍스트 공동 임베딩 모델로 128차원 임베딩 생성
- 10초 오디오 윈도우에서 추출 (1초 스트라이드)
- RVQ로 12개 MuLan 토큰으로 이산화 ($$M_A$$ 학습 중, $$M_T$$ 추론 중)

#### 2.2 계층적 생성 모델

MusicLM은 세 단계의 오토레그레시브 모델을 순차적으로 적용합니다:

**단계 1: 의미론적 모델링**[1]
$$p(S_t|S_{<t}, M_A)$$

MuLan 오디오 토큰 $$M_A$$에서 의미 토큰 $$S$$를 예측합니다. 30초 크롭된 시퀀스로 학습.

**단계 2: 음향 모델링 (조회 및 미세)**[1]
$$p(A_t|A_{<t}, S, M_A)$$

의미 토큰과 MuLan 토큰 모두를 조건으로 음향 토큰 $$A$$를 예측합니다.
- **조회 단계**: SoundStream RVQ의 처음 4개 레벨 (10초 크롭)
- **미세 단계**: 나머지 8개 레벨 (3초 크롭)

#### 2.3 모델 아키텍처[1]

각 단계는 동일한 구조의 디코더 전용 Transformer:
- 24개 계층
- 16개 어텐션 헤드
- 1024 임베딩 차원
- 4096 피드포워드 차원
- 0.1 드롭아웃
- 상대 위치 임베딩
- **총 430M 파라미터 (단계당)**

#### 2.4 추론 시 조건 대체[1]

학습 중에는 오디오의 MuLan 임베딩($$M_A$$)을 조건으로 사용하지만, 추론 시에는 텍스트 프롬프트의 MuLan 임베딩($$M_T$$)으로 대체합니다. 이를 통해 **쌍을 이룬 데이터 없이도 학습** 가능합니다.

#### 2.5 온도 샘플링[1]

생성 다양성과 시간적 일관성 간 균형을 위해 단계별 다른 온도 사용:
- 의미 모델링: $$\tau = 1.0$$
- 조회 음향 모델링: $$\tau = 0.95$$
- 미세 음향 모델링: $$\tau = 0.4$$

***

### 3. 성능 평가 및 향상

#### 3.1 평가 메트릭[1]

**음질 평가**:

- **Fréchet Audio Distance (FAD)**: Trill 임베딩(음성 지향)과 VGGish 임베딩(YouTube-8M 이벤트 기반)으로 측정

**텍스트 충실도 평가**:

- **KL Divergence (KLD)**: LEAF 분류기의 다중 라벨 예측 분포 비교로 생성 음악과 참조 음악의 음향 특성 유사성 측정

- **MuLan Cycle Consistency (MCC)**: 텍스트 임베딩과 생성 음악 임베딩 간 코사인 유사도

**인간 평가**:

5점 Likert 척도의 A-vs-B 비교 테스트 (1,200개 등급, 각 모델당 600개 쌍 비교)

#### 3.2 정량적 결과[1]

| 모델 | FAD_Trill ↓ | FAD_VGG ↓ | KLD ↓ | MCC ↑ | 승리 ↑ |
|------|------------|----------|-------|-------|--------|
| Riffusion | 0.76 | 13.4 | 1.19 | 0.34 | 158 |
| Mubert | 0.45 | 9.6 | 1.58 | 0.32 | 97 |
| **MusicLM** | **0.44** | **4.0** | **1.01** | **0.51** | **312** |
| MusicCaps (참조) | - | - | - | - | 472 |

**해석**:[1]
- MusicLM은 FAD_VGG에서 Mubert 대비 79.2% 개선 (13.4 → 4.0)
- KLD에서 36.1% 개선 (1.58 → 1.01)
- MCC에서 59.4% 개선 (0.32 → 0.51)
- 인간 평가에서 Mubert의 212% 더 많은 "승리"

#### 3.3 의미 토큰의 중요성[1]

의미 모델링 단계 제거 시 성능 저하:
- FAD_Trill: 0.44 → 0.42 (미소한 변화)
- KLD: 1.01 → 1.05 (3.96% 악화)
- MCC: 0.51 → 0.49 (3.92% 악화)
- 장기 구조 악화 관찰

의미 토큰은 **텍스트 설명 준수와 장기 일관성** 유지에 중요함을 확인합니다.

#### 3.4 토큰 정보 분석[1]

**고정 의미 토큰, 변수 음향 토큰**:
- 같은 의미·텍스트 토큰으로 여러 번 생성 시 다양한 음성 품질 특성(리버브, 왜곡도) 변화
- 악기는 유사하지만 다른 버전 생성 가능

**모두 변수**:
- 더 높은 다양성: 멜로디와 리듬 특성 변화
- 텍스트와 일관성 유지

***

### 4. 모델의 한계

#### 4.1 기본적 한계[1]

**부정형 미이해**: 모델이 "...없는"과 같은 부정형을 제대로 처리하지 못함

**시간 순서 미표현**: 음악의 시간적 순서(예: "먼저 피아노, 그 다음 기타")를 설명하는 텍스트 이해 부족

**복잡한 설명**: 5개 이상의 악기나 비음악 요소("바람, 사람들 말소리")를 포함한 복잡한 캡션 처리 어려움

#### 4.2 MuLan 의존성 문제[1]

MCC 메트릭 자체가 MuLan에 의존하므로 객관적인 평가에 한계. 논문에서 저자들도 인정하며 "향후 정량적 평가 개선이 필요"하다고 언급

#### 4.3 오버피팅 위험[1]

5.5k MusicCaps 데이터셋의 한정된 규모로 인해:
- 특정 장르나 악기 조합에 편향 가능성
- 학습 데이터에 없는 음악 스타일 생성 성능 저하 가능성

***

### 5. 모델 일반화 성능 강화 가능성

#### 5.1 현재 상태의 일반화 문제[2][3][1]

**데이터 불균형**: MusicCaps는 클래식(13.7%), 전자음악(15.6%), 록음악(10.5%)에 치우쳐 있음

**장르 편향**: 소수 문화권의 음악(전통음악, 아프리카 음악 등)이 3-4% 정도로 심각하게 저표현됨

**사전학습 편향**: w2v-BERT와 SoundStream이 특정 음악 도메인에 사전학습되어 다른 음악 스타일에 부정적 전이 가능

#### 5.2 일반화 개선 방향[4][5][6][2]

**1. 데이터 강화 전략**[5]

- 멀티모달 데이터 통합: 이미지, 비디오 정보를 음악 설명과 함께 활용
- 합성 데이터 생성: LLM을 이용해 기존 음악 메타데이터에서 풍부한 텍스트 설명 자동 생성
- 언어 다양성 확대: 영어뿐 아니라 다양한 언어의 음악 설명 포함

**2. 매개변수 효율적 전이학습(PETL)**[6]

최근 연구는 음악 파운데이션 모델의 PETL 방법 효과를 입증:
- 어댑터 기반 방법
- 프롬프트 기반 방법
- 재매개변수화 방법

이들은 **전체 미세조정의 계산 비용 대폭 감소** 하면서 비슷한 성능 달성

**3. 도메인 특정 특성 활용**[3][2]

최신 연구는 도메인 불변 특성만이 아닌 **도메인 특정 특성도 중요**함을 강조. MusicLM 확장에서:
- 장르별 토큰 임베딩 분리
- 악기 세트별 조건 추가
- 문화권 특정 음악 특성 명시적 모델링

**4. 강화학습 기반 미세조정**[4]

2024년 Meta의 MusicRL 접근:
- 인간 피드백으로부터 보상 모델 학습
- 텍스트 준수와 음악 품질 간 균형 최적화
- **주관적 음악성 개선**

**5. 제로샷 및 퓨샷 적응**[2]

계단식 확산 모델(MeLoDy) 같은 경량 아키텍처:
- 기본 모델의 일반화 유지하면서 실시간 생성
- 추가 데이터 없이 새로운 악기/스타일 적응 가능

#### 5.3 특정 도메인 성능 향상 연구[7]

최신 연구는 저표현 음악 장르의 생성 개선을 다룸:
- 전이학습으로 이란 전통 음악 같은 OOD 장르 적응
- MusicVAE 같은 모델도 조합 창의성 접근으로 개선 가능 입증
- 적응형 미세조정으로 **언더리소스 음악 스타일 생성** 가능

***

### 6. 현재 연구 환경 및 발전 방향

#### 6.1 최신 경쟁 모델들[8][9][10][11][4]

**MusicGen (Meta, 2023)**[11][8]

- 단일 단계 오토레그레시브 Transformer
- 20,000시간 라이선스 음악으로 학습
- MusicLM보다 효율적이면서도 경쟁력 있는 성능
- 오픈소스로 공개되어 확장성 우수

**MeLoDy (2023)**[2]

- LM 가이드 확산 모델
- MusicLM 대비 95.7-99.6% 계산 과정 감소
- 실시간 생성 가능

**MuMu-LLaMA (2024)**[3]

- 멀티모달 음악 이해 및 생성
- AudioLDM 2와 MusicGen 통합
- 멀티모달 입력 기반 생성

**MusiConGen (2024)**[10]

- 리듬과 코드 조건화 추가
- MusicGen 기반 미세조정
- **정밀한 시간적 제어** 실현

#### 6.2 평가 방법론의 진화[12]

최신 조사에서 기존 FAD 기반 평가의 한계 지적:

- **Gaussian 분포 가정 문제**: KAD (Kernel Audio Distance)와 MAD (MAUVE Audio Divergence) 제안
- **작은 샘플 크기에서 높은 신뢰성**: MERT 자감독 임베딩 활용
- **인간 선호도와의 상관성 개선**

#### 6.3 미래 연구 과제[5][4][1]

**가사 생성 통합**: 텍스트 기반 가사 자동 생성 후 음악 합성

**고수준 음악 구조 모델링**: 도입부(Intro), 절(Verse), 후렴(Chorus) 같은 곡 구조 명시적 표현

**고샘플링 레이트 생성**: 현재 24 kHz에서 48 kHz 이상으로 확장

**성악 품질 개선**: 보컬 합성의 자연성 및 발음 정확성 증대

**음악 편집 기능**: 생성 후 특정 구간 수정 및 스타일 변환

**저자원 언어/문화권 음악**: 데이터 부족 환경에서의 생성 능력 확대

***

### 7. 결론

MusicLM은 텍스트-음악 생성 분야의 획기적 진전을 이룬 모델입니다. 계층적 토큰화와 다단계 생성이라는 혁신적 아키텍처로 장시간 일관된 고충실도 음악 생성을 실현했습니다.[1]

하지만 부정형 처리, 시간 순서 이해, 복잡한 설명 처리 등의 한계와 제한된 데이터셋으로 인한 일반화 문제는 여전합니다. 향후 연구는 **강화학습 기반 미세조정**, **매개변수 효율적 전이학습**, **멀티모달 통합**, **평가 메트릭 개선**에 집중할 것으로 예상됩니다.[6][12][4][3][2][1]

시장은 연 25% 성장률로 확대되고 있으며, MusicGen 같은 경쟁 모델들이 다양한 접근을 시도 중입니다. 음악 생성 AI의 미래는 **더 정밀한 제어**, **음악적 구조 이해**, **저자원 도메인 적응**에 달려있을 것입니다.[13][11]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/579ae294-5123-4ece-bf1c-2e0940f22b8b/2301.11325v1.pdf)
[2](https://arxiv.org/pdf/2305.15719.pdf)
[3](https://arxiv.org/html/2412.06660v1)
[4](https://arxiv.org/pdf/2402.04229.pdf)
[5](https://www.studocu.vn/vn/document/university-of-information-technology/khoa-luan-tot-nghiep/ai-enabled-text-to-music-generation-acomprehensive-reviewof-methods-frameworksand-future-directions-25/130135611)
[6](https://arxiv.org/abs/2411.19371)
[7](https://arxiv.org/abs/2306.00281)
[8](https://arxiv.org/pdf/2306.05284.pdf)
[9](http://arxiv.org/pdf/2405.18386.pdf)
[10](https://arxiv.org/html/2407.15060v1)
[11](https://www.pragnakalp.com/generate-music-using-metas-musicgen-on-colab/)
[12](https://arxiv.org/html/2509.00051v1)
[13](https://www.datainsightsmarket.com/reports/ai-music-generation-service-1961592)
[14](https://arxiv.org/pdf/2301.11325.pdf)
[15](https://arxiv.org/html/2409.02845v2)
[16](https://arxiv.org/html/2501.08809v1)
[17](http://arxiv.org/pdf/2501.09972.pdf)
[18](https://musiclm.com)
[19](https://arxiv.org/abs/2301.11325)
[20](https://aclanthology.org/2024.acl-long.437/)
[21](https://arxiv.org/html/2509.23364v1)
[22](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0283103)
[23](https://aclanthology.org/2025.findings-acl.360.pdf)
[24](https://arxiv.org/html/2311.11255v3)
[25](http://arxiv.org/pdf/2410.20478.pdf)
[26](https://arxiv.org/html/2410.02084v1)
[27](http://arxiv.org/pdf/2405.15863.pdf)
[28](http://arxiv.org/pdf/2406.00626.pdf)
[29](https://aclanthology.org/2024.emnlp-main.1.pdf)
[30](https://musicbusinessresearch.wordpress.com/2024/04/29/ai-in-the-music-industry-part-13-text-to-music-generators-music-lm-stable-audio-riffusion-and-musicgen/)
[31](https://www.emergentmind.com/topics/mirex-2025-symbolic-music-generation-challenge)
[32](https://arxiv.org/html/2505.24346v1)
[33](https://musicgen.com)
[34](https://openreview.net/forum?id=bajTv_cvkI)
[35](https://github.com/BlueBash/Musicgen-Text-to-Music)
