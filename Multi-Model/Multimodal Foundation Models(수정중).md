# Multimodal Foundation Models

이 논문에서는 전문 모델에서 범용 어시스턴트(general-purpose assistants)로의 전환에 중점을 두고 비전 및 비전 언어 능력을 보여주는 다중 모드 기반 모델의 분류 및 진화에 대한 포괄적인 조사를 제공합니다.  
연구 환경은 두 가지 클래스로 분류된 다섯 가지 핵심 주제를 포함합니다.  
(i) 잘 정립된 연구 영역에 대한 조사부터 시작합니다.  
시각적 이해를 위한 비전 백본 학습 방법과 텍스트-이미지 생성이라는 두 가지 주제를 포함하여 특정 목적을 위해 사전 훈련된 다중 모드 기반 모델입니다.  
(ii) 그런 다음 우리는 탐구적이고 개방적인 연구 분야의 최근 발전을 소개합니다:  
LLM(대형 언어 모델)에서 영감을 받은 통합 비전 모델, 다중 모드 LLM의 엔드투엔드 교육, LLM과 다중 모드 도구 연결 등 세 가지 주제를 포함하여 범용 어시스턴트 역할을 목표로 하는 다중 모드 기반 모델 입니다.  
이 논문의 대상 독자는 다중 모드 기반 모델의 기본 사항과 최근 발전 사항을 배우고 싶어하는 컴퓨터 비전 및 비전 언어 다중 모드 커뮤니티의 연구원, 대학원생 및 전문가입니다.  

# Introduction
시각은 인간과 많은 생명체가 세상을 인식하고 상호 작용하는 주요 채널 중 하나입니다.  
인공 지능(AI)의 핵심 목표 중 하나는 시각적 신호를 효과적으로 인식하고 생성하여 시각적 세계를 추론하고 상호 작용하는 능력을 모방하는 AI 에이전트를 개발하는 것입니다.  
예를 들어 장면의 개체와 동작을 인식하고 의사소통을 위한 스케치와 그림을 만드는 등이 있습니다.  
시각적 능력을 갖춘 기초 모델을 구축하는 것은 이러한 목표를 달성하기 위해 노력하는 널리 퍼진 연구 분야입니다.

지난 10년 동안 AI 분야는 모델 개발에서 유익한 궤적을 경험했습니다.  
그림 1.1에 설명된 것처럼 이를 네 가지 범주로 나눕니다.  

![](https://wikidocs.net/images/page/236914/Fig_MM_1_01.PNG)

Figure 1.1: 언어 및 비전/다중 양식에 대한 기초 모델 개발 궤적을 보여줍니다.  
네 가지 범주 중 첫 번째 범주는 작업별 모델이고 마지막 세 범주는 기초 모델에 속하며, 언어 및 시각에 대한 기초 모델은 각각 녹색 블록과 파란색 블록으로 그룹화됩니다.  
각 카테고리에 있는 모델의 일부 주요 속성이 강조 표시됩니다.  
언어와 시각 사이의 모델을 비교함으로써 우리는 다중 모드 기반 모델의 전환이 특정 목적을 위한 사전 훈련된 모델에서 통합 모델 및 범용 어시스턴트에 이르기까지 유사한 추세를 따른다는 것을 예측하고 있습니다.  
그러나 멀티모달 GPT-4와 Gemini는 비공개로 유지되기 때문에 그림에서 물음표로 표시된 최상의 레시피를 찾으려면 연구 탐색이 필요합니다.

분류는 언어, 비전, 다중 양식을 포함한 AI의 다양한 분야에서 공유될 수 있습니다.  
우리는 진화 과정을 설명하기 위해 먼저 NLP의 언어 모델을 사용합니다.  
(i) 초기에는 개별 데이터 세트 및 작업에 대해 작업별 모델이 개발되며 일반적으로 처음부터 훈련됩니다.  
(ii) 대규모 사전 훈련을 통해 언어 모델은 BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), DeBERTa (He et al., 2021) and GPT-2 (Radford et al., 2019)와 같은 많은 확립된 언어 이해 및 생성 작업에서 최첨단 성능을 달성합니다.  
이러한 사전 훈련된 모델은 다운스트림 작업 적응의 기초를 제공합니다.  
(iii) GPT-3(Brown et al., 2020)에 예시된 대규모 언어 모델(LLM)은 다양한 언어 이해 및 생성 작업을 하나의 모델로 통합합니다.  
웹 규모 교육 및 통합을 통해 상황 내 학습 및 사고 사슬과 같은 일부 새로운 능력이 나타납니다.  
(iv) 최근 인간-AI 정렬이 발전함에 따라 LLM은 ChatGPT(OpenAI, 2022) 및 GPT-4(OpenAI, 2023a)와 같은 광범위한 언어 작업을 완료하기 위해 인간의 의도를 따르는 범용 어시스턴트 역할을 시작합니다.  
이러한 어시스턴트는 상호작용, 도구 사용 등 흥미로운 능력을 보여주며 범용 AI 에이전트 개발을 위한 기반을 마련합니다.  
기초 모델의 최신 반복은 이전 모델의 주목할만한 특성을 기반으로 구축되는 동시에 추가 능력도 제공한다는 점에 유의하는 것이 중요합니다.

NLP 분야에서 LLM이 거둔 큰 성공에 영감을 받아 컴퓨터 비전 및 비전 언어 커뮤니티의 연구자들이 이런 질문을 하는 것은 자연스러운 일입니다 :  
비전, 비전 언어 및 다중 모드 모델에 대한 ChatGPT/GPT-4의 대응은 무엇입니까?  
비전 사전 훈련과 비전 언어 사전 훈련(VLP / vision-language pre-training)은 BERT의 탄생 이후 점점 더 많은 관심을 끌었으며, 보편적인 전달 가능한 시각 및 시각 언어 표현을 배우거나 매우 그럴듯한 이미지를 생성할 수 있다는 약속과 함께 시각에 대한 주류 학습 패러다임이 되었습니다.  
틀림없이 이러한 것들은 언어 분야의 BERT/GPT-2와 마찬가지로 초기 세대의 다중 모드 기반 모델로 간주될 수 있습니다.

ChatGPT와 같은 언어용 범용 어시스턴트를 구축하기 위한 로드맵은 분명하지만, 연구 커뮤니티에서는 컴퓨터 비전용 범용 시각적 어시스턴트를 구축하기 위한 실행 가능한 솔루션을 탐색하는 것이 점점 더 중요해지고 있습니다.  
전반적으로 범용 에이전트를 구축하는 것은 AI의 오랜 목표였습니다.  
새로운 속성을 갖춘 LLM은 언어 작업을 위한 에이전트를 구축하는 데 드는 비용을 크게 줄였습니다.  
마찬가지로 우리는 텍스트 프롬프트 외에도 사용자가 업로드한 이미지, 사람이 그린 클릭, 스케치 및 마스크와 같은 다양한 시각적 프롬프트로 구성된 지침을 따르는 것과 같은 비전 모델의 새로운 능력을 예상합니다.  
이러한 강력한 제로샷 시각적 작업 구성 능력은 AI 에이전트 구축 비용을 크게 줄일 수 있습니다.

본 논문에서는 다중 모드 기반 모델의 범위를 비전 및 비전 언어 영역으로 제한합니다.  
관련 주제에 대한 최근 survey 논문에는  
(i) 자기 지도 학습(Jaiswal et al., 2020; Jing and Tian, 2020; Ozbulak et al., 2023), SAM(Segment Everything) (Zhang et al., 2023a,c)과 같은 이미지 이해 모델,  
(ii) 이미지 생성 모델(Zhang et al., 2023b; Zhou and Shimada, 2023),  
(iii) VLP(비전 언어 사전 훈련)가 포함됩니다.  
기존 VLP 조사 논문은 사전 훈련 시대 이전의 작업별 VL 문제에 대한 VLP 방법, 이미지 텍스트 작업, 핵심 비전 작업 및/또는 비디오 텍스트 작업을 다룹니다(Zhang et al., 2020; Du et al., 2022; Li et al., 2022c; Ruan and Jin, 2022; Chen et al., 2022a; Gan et al., 2022; Zhang et al., 2023g).  
최근 두 개의 survey 논문에서는 비전 모델과 LLM의 통합을 다루고 있습니다(Awais et al., 2023; Yin et al., 2022).

그 중 Gan et al. (2022)는 2022년 및 그 이전의 시각 및 언어 연구의 최근 발전에 관한 CVPR 튜토리얼 시리즈를 다루는 VLP에 대한 survey입니다.  
이 논문은 2023년 Vision Foundation 모델의 최근 발전에 대한 CVPR 튜토리얼을 요약합니다.  
앞서 언급한 특정 연구 주제에 대한 문헌 검토에 초점을 맞춘 survey 논문과 달리, 본 논문은 대규모 언어 모델 시대에 전문가에서 범용 시각 어시스턴트로 다중 모드 기반 모델의 역할 전환에 대한 우리의 관점을 제시합니다.  
본 survey 논문의 기여는 다음과 같이 요약됩니다.

우리는 시각적 표현 학습 및 이미지 생성을 위해 잘 확립된 모델을 다룰 뿐만 아니라 또한 통합 비전 모델, LLM 교육 및 연결을 포함하여 LLM에서 영감을 받은 지난 6개월 동안의 새로운 주제를 요약하는 최신 다중 모드 기반 모델에 대한 포괄적이고 시의적절한 survey를 제공합니다.

이 논문은 청중에게 다중 모드 기반 모델 개발의 전환을 옹호할 수 있는 관점을 제공할 수 있는 위치에 있습니다.  
특정 비전 문제에 대한 훌륭한 모델링 성공 외에도 우리는 인간의 의도를 따라 광범위한 컴퓨터 비전 작업을 완료할 수 있는 범용 어시스턴트를 구축하는 방향으로 나아가고 있습니다.  
우리는 이러한 고급 주제에 대한 심층적인 토론을 제공하여 범용 시각 어시스턴트 개발의 잠재력을 보여줍니다.

## What are Multimodal Foundation Models?
스탠포드 재단 모델 논문(Bommasani et al., 2021)에서 설명한 바와 같이, AI는 광범위한 다운스트림 작업에 적용할 수 있는 광범위한 데이터에 대해 훈련된 모델 (e.g., BERT, GPT family, CLIP (Radford et al., 2021) and DALL-E (Ramesh et al., 2021a)) 이 등장하면서 패러다임 변화를 겪고 있습니다.  
그들은 이러한 모델을 기초 모델이라고 부르는데, 연구 공동체 전반에 걸친 방법론의 균질화와 새로운 역량의 출현이라는 매우 핵심적이면서도 불완전한 특성을 강조하기 위한 것입니다.  
기술적인 관점에서 볼 때, 기초 모델을 가능하게 하는 것은 전이 학습이고, 이를 강력하게 만드는 것은 규모입니다.

기초 모델의 출현은 BERT에서 ChatGPT에 이르는 NLP 도메인에서 주로 관찰되었습니다.  
이러한 추세는 최근 몇 년 동안 주목을 받아 컴퓨터 비전 및 기타 분야로 확대되었습니다.  
NLP에서는 2018년 말 BERT 도입을 기반 모델 시대의 시작으로 간주합니다.  
BERT의 놀라운 성공은 컴퓨터 비전 커뮤니티에서 자기 지도 학습에 대한 관심을 빠르게 자극하여 SimCLR (Chen et al., 2020a), MoCo (He et al., 2020), BEiT (Bao et al., 2022) 및 MAE (He et al., 2022a)와 같은 모델을 탄생시켰습니다.  
같은 기간 동안 사전 훈련의 성공은 시각 및 언어 복합 분야에 전례 없는 수준의 관심을 불러일으켰습니다.

이 논문에서는 Stanford 논문(Bommasani et al., 2021)에서 논의된 기초 모델의 모든 속성을 상속하지만 비전 및 비전 언어 양식을 처리할 수 있는 능력을 갖춘 모델에 중점을 두는 다중 모드 기초 모델에 중점을 둡니다.  
계속해서 증가하는 문헌 중에서 우리는 능력과 일반성을 기준으로 다중 모드 기반 모델을 그림 1.2에 분류했습니다.  

![](https://wikidocs.net/images/page/236914/Fig_MM_1_02.PNG)

Figure 1.2: 본 논문에서 멀티모달 기반 모델이 해결하고자 하는 세 가지 대표적인 문제인 시각적 이해 작업, 시각적 생성 작업, 언어 이해 및 생성이 포함된 범용 인터페이스에 대한 설명입니다.

각 범주에 대해 이러한 다중 모드 기반 모델에 내재된 기본 능력을 보여주는 모범적인 모델을 제시합니다.  

### 시각적 이해 모델 : 
(그림 1.2에서 주황색으로 강조 표시됨) 강력한 비전 백본을 사전 훈련하는 것은 이미지 수준(예: 이미지 분류, 검색 및 캡션), 지역 수준(예: 감지 및 접지)에서 수준 작업(예: 분할)에 이르는 모든 유형의 컴퓨터 비전 다운스트림 작업의 기초이므로 비전 기반 모델을 구축하려면 일반 시각적 표현을 학습하는 것이 필수적입니다.   
모델 훈련에 사용되는 감독 신호의 유형에 따라 방법을 세 가지 범주로 분류합니다.

#### 라벨 감독 : 
ImageNet 및 ImageNet21K와 같은 데이터 세트는 지도 학습에 널리 사용되었으며 대규모 독점 데이터 세트도 산업 실험실에서 사용됩니다.

#### 언어 감독 : 
언어는 보다 풍부한 감독 형태입니다.  
CLIP 및 ALIGN과 같은 모델은 웹에서 채굴된 수백만 또는 수십억 개의 노이즈가 있는 이미지-텍스트 쌍에 대한 대비 손실을 사용하여 사전 훈련됩니다.  
이러한 모델을 사용하면 제로샷 이미지 분류가 가능하고 기존 컴퓨터 비전(CV) 모델이 개방형 어휘 CV 작업을 수행할 수 있습니다.  
우리는 현장에서 컴퓨터 비전의 개념을 옹호하고, 이를 위한 미래 기반 모델의 개발과 평가를 장려합니다.

#### 이미지 전용 자체 감독 :
이 작업 라인은 대조 학습, 비대비 학습, 마스크된 이미지 모델링에 이르기까지 이미지 자체에서 채굴된 감독 신호로부터 이미지 표현을 학습하는 것을 목표로 합니다.  

#### 다중 모드 융합, 지역 수준 및 픽셀 수준 사전 훈련 : 
이미지 백본을 사전 훈련하는 방법 외에도, 다중 모드 융합(예: CoCa, Flamingo), 개방형 객체 감지(예: GLIP) 및 신속한 분할(예: SAM)과 같은 영역 수준 및 픽셀 수준 이미지 이해를 허용하는 사전 학습 방법에 대해서도 논의합니다.  
이러한 방법은 일반적으로 사전 훈련된 이미지 인코더 또는 사전 훈련된 이미지-텍스트 인코더 쌍을 사용합니다.  

### 비주얼 생성 모델 : 
(그림 1.2에서 녹색으로 강조) 최근 대용량 이미지-텍스트 데이터의 등장으로 기반 이미지 생성 모델이 구축되었다.  
이를 가능하게 하는 기술에는 벡터 양자화 VAE 방법, 확산 기반 모델 및 자동 회귀 모델이 포함됩니다.

#### 텍스트 조건에 따른 시각적 생성 : 
이 연구 영역은 개방형 텍스트 설명/프롬프트를 기반으로 이미지, 비디오 등을 포함한 충실한 시각적 콘텐츠를 생성하는 데 중점을 둡니다.  
텍스트-이미지 생성은 텍스트 프롬프트를 따르기 위해 충실도가 높은 이미지를 합성하는 생성 모델을 개발합니다.  
대표적인 예로는 DALL-E, DALL-E 2, Stable Diffusion, Imagen 및 Parti가 있습니다.  
텍스트-이미지 생성 모델의 성공을 바탕으로 텍스트-비디오 생성 모델은 Imagen Video 및 Make-A-Video와 같은 텍스트 프롬프트를 기반으로 비디오를 생성합니다.

#### 인간이 정렬한 시각적 생성기 : 
이 연구 영역은 인간의 의도를 더 잘 따르도록 사전 훈련된 시각적 생성기를 개선하는 데 중점을 둡니다.  
기본 시각적 생성기에 내재된 다양한 문제를 해결하기 위한 노력이 이루어졌습니다.  
여기에는 공간 제어성 개선, 텍스트 프롬프트 준수 보장, 유연한 텍스트 기반 편집 지원, 시각적 개념 사용자 정의 촉진 등이 포함됩니다.

### 범용 인터페이스 : 
(그림 1.2에서 파란색으로 강조 표시됨) 앞서 언급한 다중 모드 기반 모델은 특정 CV 문제/작업 세트를 다루는 특정 목적을 위해 설계되었습니다.  
최근에는 AI 에이전트의 기반이 되는 범용 모델이 등장하고 있습니다.  
기존 노력은 세 가지 연구 주제에 중점을 두고 있습니다.  
첫 번째 주제는 시각적 이해와 생성을 위한 모델을 통합하는 것을 목표로 합니다.  
이러한 모델은 NLP의 LLM 통합 정신에서 영감을 얻었지만 사전 훈련된 LLM을 모델링에 명시적으로 활용하지는 않습니다.  
대조적으로, 다른 두 주제는 각각 LLM 교육 및 연결을 포함하여 모델링에 LLM을 포함합니다.

#### 이해와 생성을 위한 통합 비전 모델 : 
컴퓨터 비전에서는 특정 목적의 다중 모드 모델의 기능을 결합하여 범용 기반 모델을 구축하려는 여러 시도가 있었습니다.  
이를 위해 다양한 다운스트림 컴퓨터 비전 및 비전 언어(VL) 작업에 통합 모델 아키텍처가 채택되었습니다.  
통일에는 다양한 수준이 있습니다.  
첫째, 모든 폐쇄형 비전 작업을 CLIP, GLIP, OpenSeg 등과 같은 개방형 비전 작업으로 변환하여 비전과 언어를 연결하려는 노력이 널리 퍼져 있습니다.  
둘째, 다양한 세분성 수준에 걸쳐 다양한 VL 이해 작업을 통합하는 것도 가능합니다. UniTAB, Unified-IO, Pix2Seq-v2 등의 I/O 통합 방식과 GPV, GLIP-v2, X-Decoder 등의 기능적 통합 방식 등이 활발히 연구되고 있습니다.  
결국 ChatGPT처럼 모델을 보다 대화형이고 신속하게 만드는 것도 필요하며, 이는 최근 SAM 및 SEEM에서 연구되었습니다.  

#### LLM을 통한 교육 : 
텍스트 프롬프트에서 작업의 지시 및 처리 예를 따라 언어 작업을 처리할 수 있는 LLM의 동작과 유사하게 모델을 다중 모드 작업 해결 방향으로 조정하기 위한 시각적 및 텍스트 인터페이스를 개발하는 것이 바람직합니다.  
LLM의 능력을 다중 모드 설정으로 확장하고 모델을 엔드 투 엔드로 교육함으로써 Flamingo 및 Multimodal GPT-4를 포함한 다중 모드 LLM 또는 대규모 다중 모드 모델이 개발됩니다.  

#### LLM을 사용한 연결 도구 : 
LLM의 도구 사용 능력을 활용하여 대화 인터페이스를 통해 이미지 이해 및 생성을 촉진하기 위해 ChatGPT와 같은 LLM을 다양한 다중 모드 기반 모델과 통합하는 연구가 점점 늘어나고 있습니다.  
이러한 학제간 접근 방식은 NLP와 컴퓨터 비전의 장점을 결합하여 연구자들이 시각적 정보를 처리하고 인간과 컴퓨터 간의 대화를 통해 인간과 유사한 반응을 생성할 수 있는 더욱 강력하고 다재다능한 AI 시스템을 개발할 수 있도록 해줍니다.  
대표작으로는 Visual ChatGPT, MM-REACT 등이 있습니다.

## Definition and Transition from Specialists to General-Purpose Assistants
NLP의 모델 개발 내역과 분류 체계를 기반으로 그림 1.2의 다중 모드 기반 모델을 두 가지 범주로 분류합니다.

특정 목적 사전 훈련된 비전 모델은 시각적 이해 모델(예: CLIP, SimCLR, BEiT, SAM) 및 시각적 생성 모델(예: Stable Diffusion)을 포함한 대부분의 기존 다중 모드 기반 모델을 포괄하며 특정 비전 문제에 대한 강력한 전달 능력을 제공합니다.  

범용 어시스턴트는 인간의 의도를 따라 다양한 컴퓨터 비전 작업을 완료할 수 있는 AI 에이전트를 말합니다.  
범용 어시스턴트의 의미는 두 가지입니다.  
(i) 다양한 문제 유형에 걸쳐 작업을 완료할 수 있는 통합 아키텍처를 갖춘 일반 사용자와  
(ii) 사람을 대체하기보다는 사람의 지시를 쉽게 따를 수 있습니다.  
이를 위해 통합 비전 모델링, LLM 교육 및 연결을 포함한 여러 연구 주제가 적극적으로 탐구되었습니다.

## Who Should Read this Paper?
![](https://wikidocs.net/images/page/236914/Fig_MM_1_03_a.PNG)
![](https://wikidocs.net/images/page/236914/Fig_MM_1_03_b.PNG)

1장에서는 다중모달 기반 모델 연구의 현황을 소개하고, 연구가 전문가에서 범용 어시스턴트로 전환되는 과정에 대한 역사적 관점을 제시합니다.  
2장에서는 강력한 이미지 백본을 학습하는 방법을 중심으로 시각적 데이터를 소비하는 다양한 방법을 소개합니다.  
3장에서는 인간의 의도에 맞는 시각적 데이터를 생성하는 방법을 설명합니다.  
4장에서는 특히 LLM을 사용하지 않는 경우 대화형이고 신속한 인터페이스를 사용하여 통합 비전 모델을 설계하는 방법을 설명합니다.  
5장에서는 이해와 추론을 위해 시각적 입력을 소비하도록 LLM을 엔드투엔드 방식으로 교육하는 방법을 설명합니다.  
6장에서는 새로운 능력을 활성화하기 위해 LLM과 다중 모드 도구를 연결하는 방법을 설명합니다.  
7장에서는 논문을 마무리하고 연구 동향을 논의합니다.  

2장의 시각적 이해와 3장의 시각적 생성을 포함하여 특정 작업에 대한 두 가지 일반적인 다중 모드 기반 모델에 대한 논의로 시작합니다.  
다중 모드 기반 모델의 개념은 원래 작업 이해를 위한 시각적 백본/표현 학습을 기반으로 하기 때문에 먼저 초기 지도 방법에서 최근 언어-이미지 대조 방법으로 진화하는 이미지 백본 학습 방법의 전환에 대한 포괄적인 검토를 제시하고 이미지 표현에 대한 논의를 이미지 수준에서 영역 수준 및 픽셀 수준으로 확장합니다(2장).  
최근에는 비전 생성 기반 모델이 개발되면서 생성 AI가 점점 인기를 얻고 있습니다.  
3장에서는 미리 훈련된 대규모 텍스트-이미지 모델과 커뮤니티가 생성 기반 모델을 활용하여 인간의 의도에 더 잘 부합하도록 만드는 새로운 기술을 개발하는 다양한 방법에 대해 논의합니다.  
LLM이 일상 생활에서 광범위한 언어 작업을 위한 범용 어시스턴트 역할을 한다는 NLP의 최근 발전에 영감을 받아 컴퓨터 비전 커뮤니티는 범용 시각적 어시스턴트를 기대하고 구축하려고 시도해 왔습니다.  
범용 어시스턴트를 구축하는 세 가지 방법을 논의합니다.  
LLM의 정신에서 영감을 받아 4장은 모델링에 LLM을 명시적으로 통합하지 않고 이해 및 생성에 대한 다양한 비전 모델을 통합하는 데 중점을 둡니다.  
이와 대조적으로 5장과 6장에서는 모델링에서 LLM을 명시적으로 강화하여 범용 시각 도우미를 구축하기 위해 LLM을 수용하는 데 중점을 둡니다.  
구체적으로 5장에서는 엔드투엔드 훈련 방법을 설명하고, 6장은 다양한 비전 모델을 LLM에 연결하는 훈련 없는 접근 방식에 중점을 둡니다.  

# Visual Understanding
2.1. Overview
2.2. Supervised Pre-training
2.3. Contrastive Language-Image Pre-training
 2.3.1. Basics of CLIP Training
 2.3.2. CLIP Variants
2.4. Image-Only Self-Supervised Learning
 2.4.1. Contrastive and Non-contrastive Learning
 2.4.2. Masked Image Modeling
2.5. Synergy Among Different Learning Approaches
2.6. Multimodal Fusion, Region-Level and Pixel-Level Pre-training
 2.6.1. From Multimodal Fusion to Multimodal LLM
 2.6.2. Region-Level Pre-training
 2.6.3. Pixel-Level Pre-training
Visual Generation
3.1. Overview
 3.1.1. Human Alignments in Visual Generation
 3.1.2. Text-to-Image Generation
3.2. Spatial Controllable Generation
3.3. Text-based Editing
3.4. Text Prompts Following
3.5. Concept Customization
3.6. Trends: Unified Tuning for Human Alignments
Unified Vision Models
4.1. Overview
4.2. From Closed-Set to Open-Set Models
 4.2.1. Object Detection and Grounding
 4.2.2. Image Segmentation and Referring
4.3. From Task-Specific Models to Generic Models
 4.3.1. I/O Unification
 4.3.2. Functionality Unification
4.4. From Static to Promptable Models
 4.4.1. Multi-modal Prompting
 4.4.2. In-context Prompting
4.5. Summary and Discussion
Large Multimodal Models: Training with LLM
5.1. Background
 5.1.1. Image-to-Text Generative Models
 5.1.2. Case Studies
 5.1.3. OpenAI Multimodal GPT-4 and Research Gaps
5.2. Pre-requisite: Instruction Tuning in Large Language Models
 5.2.1. Instruction Tuning
 5.2.2. Self-Instruct and Open-Source LLMs
5.3. Instruction-Tuned Large Multimodal Models
5.4. Advanced Topics
5.5. How Close We Are To OpenAI Multimodal GPT-4?
Multimodal Agents: Chaining Tools with LLM
6.1. Overview
6.2. Multimodal Agent
6.3. Case Study: MM-REACT
 6.3.1. System Design
 6.3.2. Capabilities
 6.3.3. Extensibility
6.4. Advanced Topics
 6.4.1. Comparison to Training with LLM in Chapter 5
 6.4.2. Improving Multimodal Agents
 6.4.3. Diverse Applications of Multimodal Agents
 6.4.4. Evaluation of Multimodal Agents
 6.4.5. Tool Creation
 6.4.6. Retrieval-Augmented Multimodal Agents
Conclusions and Research Trends
7.1. Summary and Conclusions
7.2. Towards Building General-Purpose AI Agents
