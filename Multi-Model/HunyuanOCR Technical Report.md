# HunyuanOCR Technical Report

### 1. 핵심 주장과 주요 기여

HunyuanOCR은 Tencent의 경량(1B 파라미터) 엔드-투-엔드 OCR 전문 시각-언어 모델(Vision-Language Model, VLM)로, 다음 세 가지 혁신적 돌파구를 제시합니다.[1]

**첫째, 다목적성과 효율성의 통합:** 텍스트 감지(Spotting), 문서 파싱(Parsing), 정보 추출(IE), 시각질문응답(VQA), 다국어 번역 등 다섯 가지 핵심 OCR 기능을 단일 경량 프레임워크에 통합하였습니다. 이는 협소한 "OCR 전문 모델"과 비효율적인 "범용 VLM"의 한계를 극복합니다.[1]

**둘째, 완전 엔드-투-엔드 아키텍처:** 기존 파이프라인 기반 접근 방식과 달리, HunyuanOCR은 순수 엔드-투-엔드 패러다임을 채택하여 사전 처리 모듈(예: 레이아웃 분석)의 의존성을 제거하고, 전통적인 파이프라인에서 발생하는 오류 누적 문제를 근본적으로 해결합니다.[1]

**셋째, 데이터 주도 및 RL 전략:** 고품질 데이터의 중요성을 확인하고, 업계 최초로 OCR 작업에 강화학습(Reinforcement Learning, RL) 전략이 상당한 성능 향상을 제공함을 입증하였습니다.[1]

***

### 2. 해결하는 문제와 제안 방법

#### 2.1 핵심 문제

전통적인 OCR 시스템은 텍스트 감지, 인식, 레이아웃 분석, 수식 인식, 표 인식 등 최소 5개 이상의 독립적 모듈을 순차적으로 연결하는 캐스케이드 구조를 채택하고 있으나, 이는 다음과 같은 근본적 한계를 야기합니다.[1]

- **오류 누적(Error Propagation):** 초기 단계의 부정확성이 후속 처리 단계로 전파되어 최종 출력 품질을 심각하게 저하
- **복잡한 배포와 유지보수:** 여러 독립적 모듈의 조정과 최적화 필요로 인한 높은 개발 비용
- **일반화 능력 부족:** 복잡한 레이아웃과 긴 문자열 처리에 있어 견고성이 제한적

#### 2.2 제안하는 방법론

HunyuanOCR은 세 가지 핵심 모듈로 구성된 효율적이고 컴팩트한 아키텍처를 제시합니다.[1]

**모델 아키텍처:**

$$\text{Output} = \text{LLM}(\text{MLP-Adapter}(\text{ViT}(\text{Image})))$$

여기서:
- **Native Resolution Visual Encoder (Hunyuan-ViT):** SigLIP-v2-400M 사전학습 모델 기반, 0.4B 파라미터 규모로 적응적 패칭 메커니즘을 통해 원본 해상도와 종횡비를 보존하여 긴 문서와 극단적 종횡비의 이미지에 특히 효과적[1]

- **Adaptive MLP Connector:** 시각 인코더의 고해상도 특성 맵에서 생성된 토큰 시퀀스 길이를 공간 차원의 적응적 내용 압축을 통해 감소시키면서, 텍스트 밀집 영역 같은 주요 영역의 의미론적 정보를 보존[1]

- **Lightweight Language Model:** 0.5B 파라미터의 Hunyuan-0.5B LLM으로, XD-RoPE를 통해 텍스트, 높이, 너비, 시간의 4개 독립 부분공간을 분해하여 1D 텍스트 수열, 2D 페이지 레이아웃, 3D 시공간 정보 간 네이티브 정렬을 구현[1]

#### 2.3 강화학습 최적화

HunyuanOCR은 Group Relative Policy Optimization (GRPO) 알고리즘을 채택한 온라인 강화학습을 적용합니다.[1]

GRPO 목적함수:

$$L_{\text{GRPO}}(\theta) = \mathbb{E}\left[q \sim \mathcal{D}, \{o_i\}_i^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)\right] \frac{1}{G} \sum_{i=1}^{G} \left[ \min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i, \text{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right) A_i\right) - \beta D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})\right]$$

여기서 $A_i$는 그룹 보상으로부터 계산된 이점(Advantage)이고, $D_{\text{KL}}$은 KL-발산 정규화 항입니다.[1]

**작업별 보상 설계:**

- **텍스트 감지(Spotting):** 정규화된 편집 거리 기반 보상으로 경계 박스 정확도와 텍스트 인식 정확도를 동시에 개선
- **문서 파싱:** 모델 출력과 그라운드 트루스 간 정규화된 편집 거리 측정
- **VQA:** 의미론적 일치도를 기반으로 한 이진(1 또는 0) 보상
- **번역:** LLM 기반 판사를 통해  범위의 부드러운 보상을 계산 후 로 정규화[2][1]

***

### 3. 모델 구조의 세부 설명

#### 3.1 사전학습 4단계 전략

| 단계 | 목적 | 학습 대상 | 학습률 | 학습 토큰 | 시퀀스 길이 |
|-----|------|----------|-------|---------|-----------|
| Stage-1 | 시각-언어 정렬 | ViT & 어댑터 | 3e⁻⁴ → 3e⁻⁵ | 50B | 8k |
| Stage-2 | 멀티모달 사전학습 | 모든 파라미터 | 2e⁻⁴ → 5e⁻⁵ | 300B | 8k |
| Stage-3 | 장문맥 사전학습 | 모든 파라미터 | 8e⁻⁵ → 5e⁻⁶ | 80B | 32k |
| Stage-4 | 응용 지향 SFT | 모든 파라미터 | 2e⁻⁵ → 1e⁻⁶ | 24B | 32k |

**Stage-1 (시각-언어 정렬):** ViT와 학습 가능한 MLP 어댑터만을 학습하면서 LLM을 동결하여 시각 특성을 텍스트 의미 공간에 정렬. 약 50B 토큰의 일반 이미지 캡션 데이터와 합성 OCR 데이터로 구성되어 텍스트 파싱과 인식 능력 강화[1]

**Stage-2 (멀티모달 사전학습):** 모든 파라미터를 해제하여 엔드-투-엔드 시각-언어 결합 학습 수행. 약 300B 토큰으로 텍스트 파싱, 감지, 번역, VQA 등 다중 작업 합성 데이터를 활용하여 구조화된 내용의 심층 이해 및 인지 추론 능력 향상[1]

**Stage-3 (장문맥 사전학습):** 모델 컨텍스트 윈도우를 32K로 확장하여 긴 문서 파싱 작업 및 장문의 순수 텍스트 데이터 통합. 약 80B 토큰으로 학습[1]

**Stage-4 (응용 지향 SFT):** 인간 주석이 달린 고품질 현실 데이터와 고품질 합성 샘플을 사용한 어닐링 학습. 표준화된 명령 템플릿과 정규화된 출력 형식을 통해 모델의 학습 난이도를 낮추고 후속 강화학습 단계의 보상 모델 설계 용이[1]

#### 3.2 데이터 구성

HunyuanOCR은 약 2억 개의 고품질 이미지-텍스트 쌍으로 구성된 대규모 멀티모달 데이터셋을 구축하였으며, 다음 9개 실제 시나리오와 130개 이상의 언어를 포함합니다:[1]

- 거리 경관, 문서, 광고, 필기체, 스크린샷
- 카드/증명서/송장, 게임 인터페이스, 비디오 프레임, 예술 타이포그래피

**데이터 파이프라인:**

1. **이미지 합성:** SynthDog 프레임워크 확장을 통해 130개 이상 언어의 단락 수준 렌더링, LTR/RTL 양방향 텍스트, 복잡한 필기체 글꼴 지원[1]

2. **이미지 증강:** Warping Synthesis Pipeline을 통해 기하학적 변형(주름, 곡선, 원근 왜곡), 이미징 열화(모션 블러, 가우시안 노이즈, 압축 아티팩트), 조명 변화(전역/국소 조명, 그림자, 반사) 시뮬레이션[1]

3. **QA 쌍 생성:** 경 샘플 검색 → QA 생성 → 일관성 검증의 자동화 파이프라인으로 교차-작업 샘플 재사용을 통해 VQA 데이터 고품질 확보[1]

***

### 4. 성능 향상 분석

#### 4.1 벤치마크별 성능

**텍스트 감지(Spotting) - Multi-Scenes 벤치마크:**

HunyuanOCR은 전체 점수 70.92를 달성하여 BaiduOCR(61.90), Seed-1.6-Vision(59.23), Qwen3-VL-235B(53.62)를 큰 폭으로 상회합니다. 특히 예술 텍스트, 게임 스크린샷, 필기체, 광고 장면에서 우수한 성능을 보입니다.[1]

**문서 파싱 - OmniDocBench:**

- 전체 성능: **94.10** (PaddleOCR-VL 91.93, MinerU2.5 90.67 대비 우수)
- 공식 인식: **94.73** (업계 최고)
- 표 인식: **91.81**
- 정규화 편집 거리: **0.042**[1]

**정보 추출 및 VQA:**

| 데이터셋 | HunyuanOCR | Gemini-2.5-Pro | Qwen3-VL-235B |
|---------|-----------|-----------------|--------------|
| 카드 | **92.29** | 80.59 | 75.59 |
| 영수증 | **92.53** | 80.66 | 78.4 |
| 비디오 자막 | **92.87** | 53.65 | 50.74 |
| OCRBench | 860 | 872 | 920 |[1]

**다국어 문서 파싱 - DocML:**

14개 저자원 언어(독일어, 스페인어, 터키어, 베트남어, 한국어, 말레이어, 포르투갈어, 러시아어, 프랑스어, 인도네시아어, 태국어, 이탈리아어, 일본어)에 걸쳐 SOTA 성능 달성[1]

**텍스트 이미지 번역 - DoTA 벤칙마크:**

- COMET 점수: **73.62** (Qwen3-VL-4B 70.29 대비 우수)
- 다국어 번역 범위: 프랑스어, 독일어, 일본어, 한국어 등 14개 이상 언어 지원[1]

#### 4.2 RL 학습의 효과

강화학습 도입 후 작업별 성능 개선:[1]

- **텍스트 감지:** 예술 텍스트, 스크린샷 범주에서 2점 이상 향상
- **문서 파싱:** OmniDocBench 점수 92.5 → 94.1 (1.6점 향상)
- **정보 추출:** 약 2점 개선
- **OCRBench 평균:** 3.3점 향상
- **텍스트 번역:** 현저한 개선 입증[1]

***

### 5. 모델의 한계

#### 5.1 기술적 한계

**번역 성능 제약:** 상대적으로 작은 언어 모델로 인해 번역 능력이 텍스트 감지, 인식, 문서 파싱 성능과 비교하여 뒤떨어지는 현상. 개발자는 고정밀도 번역이 필요한 경우 멀티모달 파싱 모듈을 Hunyuan-MT-7B 같은 전문 번역 모델과 캐스케이드 연결 가능[1]

**분해능 및 다중 페이지 문제:** 고해상도 및 다중 페이지 문서 처리 개선의 필요성이 인식되고 있으며, 추후 토큰 압축 및 아키텍처 개선을 통한 최적화 계획[1]

**리소스 제한 환경의 배포:** 온디바이스 배포를 위한 추가 최적화 필요[1]

#### 5.2 일반화 성능의 과제

문서 파싱의 경우, Wild-OmniDocBench(현실 촬영 환경: 수동 접기, 구부리기, 조명 변화 포함)에서:
- 전체 성능: 85.21 (디지털 환경 94.10 대비 감소)
- 텍스트 편집 거리: 0.081 (디지털 0.042 대비 증가)[1]

이는 실제 환경의 문서 왜곡 및 열화에 대한 추가 일반화 개선이 필요함을 시사합니다.

***

### 6. 일반화 성능 향상 가능성

#### 6.1 현재의 강점

**다중 시나리오 견고성:** 9개 범주(예술 텍스트, 문서, 게임 스크린샷, 필기체, 광고, 영수증, 스크린, 거리, 비디오)와 130개 이상 언어에 걸친 포괄적 학습으로 상당한 다양성 확보[1]

**합성 데이터의 전략적 활용:** 고품질 합성 데이터와 현실 데이터의 혼합을 통한 모델 견고성 향상으로, 저자원 언어와 복잡한 레이아웃 이해에 우수한 성능 달성[1]

#### 6.2 일반화 개선의 경로

**강화학습의 확대 적용:**

최신 연구들이 RL을 OCR 특화 과제에 효과적으로 적용하는 방법을 제시합니다. Infinity-Parser와 Logics-Parsing 같은 관련 연구는 레이아웃 인식 보상을 통한 OOD(Out-of-Distribution) 성능 개선을 입증하고 있습니다. 이러한 기법을 HunyuanOCR에 통합하면 다양한 문서 유형과 자연 장면에 대한 견고성을 추가로 향상할 수 있을 것으로 예상됩니다.[3][4]

**다중 보상 메커니즘의 정교화:**

복합적 보상 함수(편집 거리, 단락 수 정확도, 읽기 순서 보존 등)를 통해 모델이 여러 측면의 구조적 정확성을 동시에 학습하도록 유도할 수 있습니다. 이는 특히 다중 열 신문이나 포스터 같은 복잡한 레이아웃 이해에 효과적입니다.[4][3]

**원본 해상도 처리의 강화:**

네이티브 ViT의 적응적 패칭 메커니즘을 더욱 정교화하여, 극도로 긴 문서나 고해상도 이미지에서도 모든 세부 정보를 보존하면서 시퀀스 길이를 관리할 수 있습니다. 이는 병목 현상 없이 지속적인 개선을 가능하게 합니다.[1]

**다국어 적응:**

14개 비영어권 언어에 대한 우수한 성능은 향후 더 많은 언어와 스크립트(특히 우르두어, 힌디어, 태국어 같은 복잡한 스크립트)에 대한 학습 데이터 확충으로 일반화를 확대할 수 있음을 시사합니다.[1]

***

### 7. 앞으로의 연구에 미치는 영향

#### 7.1 학술적 영향

**엔드-투-엔드 패러다임의 정당성 입증:**

HunyuanOCR의 성공은 기존의 멀티 스테이지 파이프라인에서 완전 엔드-투-엔드 접근 방식으로의 패러다임 전환 타당성을 강력하게 입증합니다. 이는 문서 이해, 자동 번역 등 다양한 멀티모달 과제에서 유사한 아키텍처 설계를 자극할 것입니다.[1]

**경량 VLM의 가능성:**

1B 파라미터로도 대규모 모델을 능가하는 성능을 달성했다는 것은 데이터 품질과 학습 전략의 중요성을 강조합니다. 이는 향후 엣지 디바이스 배포와 저자원 환경에서의 고성능 모델 개발에 새로운 가능성을 제시합니다.[1]

**강화학습의 OCR 특화 적용:**

업계 최초로 RL이 OCR 작업에 상당한 이득을 제공함을 입증함으로써, 향후 다른 경량 모델들도 이 기법을 채택하도록 자극할 것으로 예상됩니다. 최신 연구들(Logics-Parsing, Infinity-Parser)이 이를 입증합니다.[3][4]

#### 7.2 실무적 영향

**산업 배포의 간소화:**

단일 모델 기반 엔드-투-엔드 처리로 인한 배포 복잡도 감소, 유지보수 비용 절감, 시스템 신뢰도 향상이 실현되어 다양한 비즈니스 시나리오에 즉시 적용 가능합니다.[1]

**다국어 및 다양한 환경 지원:**

130개 이상 언어 지원과 9개 실제 시나리오에 대한 견고성으로 글로벌 스케일의 서비스 구축이 용이해집니다.[1]

***

### 8. 향후 연구 시 고려할 점

#### 8.1 기술적 개선 방향

**동적 해상도 처리 (VisionThink 패러다임):**

최신 연구에서 제시된 VisionThink 같은 기법은 필요에 따라 이미지 해상도를 동적으로 조정하는 방식으로, OCR 작업에서는 고해상도 유지, 일반 VQA는 저해상도로 처리하는 효율성 개선을 보여줍니다. HunyuanOCR에 적용하면 추론 효율을 크게 향상할 수 있습니다.[5]

**도구 통합 추론 (DianJin-OCR-R1):**

다른 전문 OCR 모델의 결과를 참조로 활용하는 "다시 보기" 메커니즘은 환각(hallucination) 문제를 감소시킬 수 있습니다. HunyuanOCR에도 선택적으로 통합 가능합니다.[6]

**도메인 특화 적응:**

CHURRO 같은 역사 문서 인식 모델이나 Baseer 같은 아랍어 특화 모델의 설계 원리를 참고하여, 특정 도메인(의료, 금융, 법률)에 특화된 버전 개발이 효과적입니다.[7][8]

#### 8.2 데이터 및 학습 전략

**다중 보상 메커니즘의 확대:**

Infinity-Parser와 Logics-Parsing에서 제시한 레이아웃 인식 복합 보상(편집 거리, 단락 수 정확도, 읽기 순서 보존)을 HunyuanOCR의 RL 단계에 적용하면 OOD 성능을 크게 향상할 수 있습니다.[4][3]

**선택적 토큰 압축:**

더욱 정교한 토큰 압축 메커니즘을 개발하여 32K 컨텍스트 윈도우를 유지하면서도 추론 속도를 가속화하고 메모리 사용을 감소시킬 수 있습니다.

**세대별 학습 데이터의 지속적 확충:**

새로운 언어, 복잡한 레이아웃, 저품질 이미지 등에 대한 학습 데이터를 체계적으로 확충하여 일반화 성능을 점진적으로 개선할 필요가 있습니다.

#### 8.3 평가 및 벤치마크

**OOD 성능 평가의 강화:**

Wild-OmniDocBench 같이 현실 환경 왜곡을 포함한 벤치마크에 대한 성능 개선과, 추가적인 도메인 외 평가 데이터셋 개발이 필요합니다.[1]

**멀티페이지 및 초고해상도 문서 벤치마크:**

다중 페이지 PDF와 극도로 높은 해상도의 문서에 대한 체계적 평가와 개선이 요구됩니다.[1]

**번역 품질 평가의 고도화:**

현재 COMET 메트릭 외에 의미론적 동등성과 도메인 정확성을 평가하는 추가 메트릭을 도입하여 번역 성능을 더 정밀하게 측정할 필요가 있습니다.[1]

***

### 결론

HunyuanOCR은 경량 엔드-투-엔드 VLM이 어떻게 전통적 파이프라인 시스템과 대규모 범용 모델을 동시에 능가할 수 있는지를 입증하는 매우 중요한 사례 연구입니다. 고품질 데이터, 정교한 멀티 스테이지 사전학습, 작업별 맞춤형 강화학습의 조합은 경량 모델에서도 상업용 수준의 성능을 달성하는 길을 제시합니다.[1]

향후 연구의 방향으로는 강화학습의 다층적 보상 메커니즘 확대, 동적 해상도 처리, 도메인 특화 적응 등이 주요 과제가 될 것으로 예상됩니다. 이러한 개선사항들은 지면 기반 문서 이해를 넘어 비디오 자막 추출, 실시간 번역, 장애인 접근성 지원 등 실생활의 다양한 애플리케이션에 긍정적 영향을 미칠 것입니다.[1]

***

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b023fbb4-6b9d-4c6a-9129-e59543300035/2511.19575v1.pdf)
[2](https://arxiv.org/abs/2501.15558)
[3](https://openreview.net/forum?id=M3GgDDGYec)
[4](https://arxiv.org/html/2509.19760v1)
[5](https://arxiv.org/abs/2507.13348)
[6](https://arxiv.org/abs/2508.13238)
[7](https://arxiv.org/abs/2509.18174)
[8](https://arxiv.org/abs/2509.19768)
[9](https://arxiv.org/abs/2504.16054)
[10](https://arxiv.org/abs/2502.05855)
[11](https://arxiv.org/abs/2503.18484)
[12](https://arxiv.org/abs/2504.13365)
[13](https://arxiv.org/abs/2502.07409)
[14](https://arxiv.org/html/2501.15558)
[15](https://arxiv.org/html/2407.12594v1)
[16](http://arxiv.org/pdf/2305.11175.pdf)
[17](https://arxiv.org/html/2410.05261)
[18](http://arxiv.org/pdf/2405.14295.pdf)
[19](https://arxiv.org/pdf/2312.06109.pdf)
[20](https://arxiv.org/pdf/2206.00311.pdf)
[21](http://arxiv.org/pdf/2108.10904v3.pdf)
[22](https://huggingface.co/blog/vlms-2025)
[23](https://arxiv.org/html/2504.03621v1)
[24](https://arxiv.org/html/2506.18504v1)
[25](https://dev.to/czmilo/2025-complete-guide-how-to-build-end-to-end-ocr-with-hunyuanocr-1k8m)
[26](https://arxiv.org/html/2509.11417v1)
[27](https://arxiv.org/html/2506.03197v3)
[28](https://github.com/Tencent-Hunyuan/HunyuanOCR)
[29](https://aclanthology.org/2025.findings-acl.236.pdf)
[30](https://developer.nvidia.com/blog/approaches-to-pdf-data-extraction-for-information-retrieval/)
[31](https://github.com/alibaba/Logics-Parsing)
