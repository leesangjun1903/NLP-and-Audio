# AudioGen: Textually Guided Audio Generation

### 1. 핵심 주장과 주요 기여

AudioGen은 텍스트 설명에 기반하여 오디오를 생성하는 **자회귀 생성 모델**로서, 이산 오디오 표현(discrete audio representation)을 기반으로 작동합니다. 논문의 핵심 주장은 **자회귀 트랜스포머 모델이 확산(diffusion) 기반 모델과 경쟁할 수 있으며**, 적절한 기법을 통해 우수한 성능을 달성할 수 있다는 것입니다.[1]

주요 기여는 다음과 같습니다:[1]

- 285M과 1B 매개변수 규모의 **최첨단 자회귀 오디오 생성 모델** 제시
- **분류기-없는 지도(Classifier-Free Guidance, CFG)**를 자회귀 설정에 적용하여 텍스트 준수성 향상
- **동적 데이터 혼합 증강**을 통해 복잡한 음원 합성 및 분리 능력 개선
- **다중 스트림 모델링**을 활용한 추론 속도-음질 트레이드오프 탐색
- 조건부 및 무조건부 오디오 연속 생성 능력 확장

***

### 2. 문제 정의, 해결 방법 및 모델 구조

#### 2.1 해결하고자 하는 문제

텍스트-오디오 생성은 이미지 생성과 비교할 때 **근본적으로 다른 어려움**을 포함합니다:[1]

1. **음원 분리의 어려움**: 오디오는 1차원 신호이므로 여러 음원(예: 동시에 말하는 여러 사람)을 구분하기 어려움
2. **실제 녹음 조건**: 배경 잡음, 반향, 실내음향 특성 등
3. **데이터 부족**: 텍스트 주석이 있는 오디오 데이터가 텍스트-이미지 쌍 데이터보다 **수 자릿수 적음**
4. **긴 시퀀스 모델링**: 고충실도 오디오 처리는 매우 긴 시퀀스 필요 (예: 1초 = 500개 토큰)
5. **복잡한 구성**: "개가 짖으면서 누군가 트럼펫을 연주하고 바쁜 거리의 배경음"과 같은 보지 못한 조합 생성

#### 2.2 제안하는 방법

AudioGen은 **2단계 파이프라인**으로 구성됩니다:[1]

**단계 1: 오디오 표현 학습**

신경 오디오 압축 모델을 통해 원본 오디오 $$x \in [-1, 1]^{C_a \times T}$$를 이산 토큰으로 변환합니다. 여기서 $$C_a$$는 채널 수, $$T = d \cdot f_{sr}$$는 샘플 수입니다.[1]

오디오 표현 모델은 다음 구성요소를 포함합니다:
- 인코더 $$E$$: 오디오를 잠재 표현 $$z$$로 변환
- 양자화층 $$Q$$: 벡터 양자화를 통해 압축된 표현 $$z_q$$ 생성
- 디코더 $$G$$: 시간 영역 신호 $$\hat{x}$$ 복원

**훈련 목표 함수:**

시간 영역 손실:
$$\ell_t(x, \hat{x}) = ||x - \hat{x}||_1$$

주파수 영역 손실 (멀티 스케일 STFT):
$$\ell_f(x, \hat{x}) = \frac{1}{|\alpha| \cdot |s|} \sum_{\alpha_i \in \alpha} \sum_{i \in e} \left(||S_i(x) - S_i(\hat{x})||_1 + \alpha_i ||S_i(x) - S_i(\hat{x})||_2\right)$$

여기서 $$S_i$$는 64-빈 멜-스펙트로그램, $$e = \{5, \ldots, 11\}$$는 스케일 집합입니다.[1]

적대적 손실:
$$\ell_g(\hat{x}) = \frac{1}{K} \sum_k \max(0, 1 - D_k(\hat{x}))$$

특징 매칭 손실:
$$\ell_{feat}(x, \hat{x}) = \frac{1}{KL} \sum_{k=1}^K \sum_{l=1}^L ||D_k^l(x) - D_k^l(\hat{x})||_1$$

전체 생성기 손실:
$$L_G = \lambda_t \cdot \ell_t(x, \hat{x}) + \lambda_f \cdot \ell_f(x, \hat{x}) + \lambda_g \cdot \ell_g(\hat{x}) + \lambda_{feat} \cdot \ell_{feat}(x, \hat{x})$$

**단계 2: 오디오 언어 모델링**

텍스트 인코더 $$F$$가 텍스트 $$c$$를 의미 표현 $$u = F(c)$$로 변환하고, 조회 테이블(LUT)이 오디오 토큰 $$\hat{z}_q$$를 연속 공간 $$v = \text{LUT}(\hat{z}_q)$$로 임베딩합니다.[1]

텍스트와 오디오 임베딩을 연결한 시퀀스 $$Z = u_1, \ldots, u_{T_u}, v_1, \ldots, v_{T_v}$$에 대해, 트랜스포머 디코더 언어 모델을 교차 엔트로피 손실로 훈련합니다:

$$L_{LM} = -\sum_{i=1}^N \sum_{j=1}^{T_v} \log p_\theta(v_j^i | u_1^1, \ldots, u_{T_u}^i, v_1^1, \ldots, v_{j-1}^i)$$

#### 2.3 모델 구조

**오디오 인코더-디코더 아키텍처:**[1]

인코더는 다음을 포함합니다:
- 초기 1D 합성곱 (32채널)
- 4개의 합성곱 블록 (각각 잔차 유닛 + 다운샘플링 레이어)
- 2계층 LSTM
- 최종 1D 합성곱 (7×1 커널)
- 스트라이드: (2, 2, 2, 4)

디코더는 인코더를 거울상으로 복제하며, 전치 합성곱 사용

**트랜스포머 디코더 언어 모델:**[1]

- 기본 모델: 숨겨진 크기 768, 24 계층, 16 어텐션 헤드
- 대형 모델: 숨겨진 크기 1280, 36 계층, 20 어텐션 헤드
- 각 어텐션 블록에는 인과(causal) 자기 주의와 교차 주의 포함
- T5 사전 학습 텍스트 인코더 사용 (기본/대형 변형)

#### 2.4 핵심 기술 혁신

**분류기-없는 지도(CFG):**

훈련 중 10% 확률로 텍스트 조건을 무작위로 생략하고, 추론 시 조건부 및 무조건부 확률의 선형 조합을 샘플링합니다:[1]

$$\gamma \log p_\theta(v_j^i | u_1^1, \ldots, u_{T_u}^i, v_1^1, \ldots, v_{j-1}^i) + (1-\gamma) \log p_\theta(v_j^i | v_1^1, \ldots, v_{j-1}^i)$$

여기서 $$\gamma = 3.0$$은 지도 스케일입니다.

**데이터 혼합 증강:**

두 오디오 샘플 $$x_1, x_2$$와 캡션 $$c_1, c_2$$가 주어지면:[1]
- 임의의 시간 오프셋으로 두 샘플 병합
- [-5, 5] dB 범위에서 임의의 신호-대-잡음비(SNR) 선택
- 오디오 샘플 혼합 후 캡션 연결 (예: "개 짖음" + "새 지저귐" → "개 짖음 새 지저귐")

이는 모델이 **내부적으로 음원 분리를 학습**하도록 유도합니다.

**다중 스트림 모델링:**

긴 시퀀스 문제를 완화하기 위해, 길이 $$T_v$$의 시퀀스를 $$k$$개의 병렬 스트림으로 표현합니다. 각 스트림의 길이는 $$T_v/k$$이고 각 코드북 크기는 $$2048/k$$입니다:[1]

- 단일 스트림: 다운샘플링 × 32 (2ms per 토큰)
- 2 스트림: 다운샘플링 × 64, 1024 코드북 × 2
- 4 스트림: 다운샘플링 × 128, 512 코드북 × 4

***

### 3. 성능 향상 및 검증 결과

#### 3.1 실험 설정

**데이터셋:** 10개 데이터셋 활용, 약 4,000시간의 훈련 데이터:[1]
- AudioSet (5,420시간, 태그)
- AudioCaps (145시간, 캡션)
- Clotho v2 (37시간, 캡션)
- BBC 음향 효과, VGG-Sound, FSD50K, Sonniss, 기타

**평가 지표:**

객관적 지표:
- **Fréchet Audio Distance (FAD)**: 음질 측정 (낮을수록 좋음)
- **KL 발산**: 분류기 기반 라벨 분포 비교 (낮을수록 좋음)

주관적 지표 (MUSHRA 프로토콜):
- **전체 품질 (OVL)**: 1-100 척도
- **텍스트 관련성 (REL.)**: 음성-텍스트 매칭 평가

#### 3.2 주요 결과

**표 1: AudioGen vs DiffSound 비교**[1]

| 모델 | 매개변수 | OVL↑ | REL.↑ | FAD↓ | KL↓ |
|------|--------|------|-------|------|-----|
| 참조 | - | 92.08±1.16 | 92.97±0.85 | - | - |
| DiffSound | 400M | 65.68±1.58 | 55.91±1.75 | 7.39 | 2.57 |
| AudioGen-base | 285M | 70.85±1.06 | 63.23±1.65 | 2.84 | 2.14 |
| AudioGen-base (Mix) | 285M | 71.68±1.89 | 66.01±1.79 | 3.13 | 2.09 |
| **AudioGen-large (Mix)** | **1B** | **71.85±1.07** | **68.73±1.61** | **1.82** | **1.69** |

AudioGen-large는 **모든 지표에서 DiffSound를 능가**합니다. 특히 FAD는 **75.4% 향상**, KL은 **34.2% 향상**을 달성했습니다.[1]

#### 3.3 분류기-없는 지도의 효과

지도 스케일 $$\gamma$$에 따른 성능 변화:[1]

- $$\gamma = 1.0$$ (CFG 없음): FAD = 4.5, KL = 2.75
- $$\gamma = 2.0$$: FAD = 3.5, KL = 2.35
- $$\gamma = 3.0$$: FAD = **2.84** (최소), KL = 2.09
- $$\gamma = 4.0$$: FAD = 3.2, KL = **1.85** (최소)

$$\gamma = 3.0$$이 품질과 다양성 사이의 **최적 균형** 제공합니다.

#### 3.4 다중 스트림 트레이드오프

**표 2: 다중 스트림 성능 분석**[1]

| 스트림 수 | DSF | 비트레이트 | SI-SNR | ViSQOL | FAD | KL | 속도향상 |
|----------|-----|---------|--------|--------|-----|-----|---------|
| 1 | ×32 | 5.37 kbps | 5.1 dB | 3.95 | **1.82** | **1.69** | ×1.0 |
| 2 | ×64 | 4.88 kbps | 4.5 dB | 3.94 | 6.89 | 1.86 | ×2.3 |
| 4 | ×128 | 4.39 kbps | 4.2 dB | 3.91 | 10.89 | 2.59 | ×3.6 |

다중 스트림은 **추론 속도 개선**을 제공하지만 **음질 저하**를 초래합니다. 단일 스트림이 최고의 결과를 제공합니다.

#### 3.5 오디오 연속 생성 결과

무조건부 생성 시, 짧은 오디오 프롬프트(0.5-1초)에서 **텍스트가 생성에 주요 영향**을 미칩니다. 반면 긴 프롬프트(4초)에서는 오디오 프롬프트가 주도적 역할을 합니다.[1]

***

### 4. 모델 일반화 성능 향상 가능성

#### 4.1 현재 일반화 능력

**강점:**
- **T5 사전 학습 인코더 사용**: 훈련 데이터에 없는 텍스트 개념에 대한 제로샷 일반화 가능[1]
- **데이터 혼합 증강**: 훈련 데이터에서 존재하지 않는 새로운 음원 조합 생성 능력[1]
- **큰 모델 규모**: 1B 매개변수 모델이 285M 모델보다 **우수한 일반화** 제공

**약점:**
1. **보지 못한 구성의 이해 부족**: "개가 짖다가 조류가 지저귄다"와 같이 **시간적 순서** 관계 이해 제한[1]
2. **음성 생성 제한**: 데이터셋에서 음성 샘플 과다로 필터링했으므로, 모델이 **불명료한 음성** 생성[1]
3. **데이터셋 편향**: YouTube 중심 수집으로 **특정 지리적/인구통계학적 위치 과대 표현**[1]

#### 4.2 일반화 향상 전략

최근 연구 트렌드를 반영한 개선 방향:[2][3][4][5][6][7][8]

**1. 효율성 기반 개선**

**오디오 일관성 모델(AudioLCM)**: AudioLDM의 반복 샘플링 과정을 치환하여 **333배 빠른 추론** 달성하면서도 품질 유지. 확산 모델 대비 **400배 계산 감소**.[3][4]

이를 통해:
- 더 빠른 반복(iteration) 개발 가능
- 적응적 파인튜닝 용이

**2. 멀티태스크 학습**

최근 음성 LLM 연구에서 **다중 작업 학습**이 일반화 능력을 **크게 향상**시킴을 증명:[6]
- 다양한 오디오 세대 작업(음악, 음향 효과, 음성) 동시 훈련
- 공유 표현 학습으로 **도메인 간 이전 학습** 가능

**3. 개선된 토크나이제이션**

최근 이산 음성 토큰 연구 메타분석:[9]

**음성-분리된 토큰(Speaker-Decoupled Tokens)**:
- LSCodec 같은 방법이 **내용과 화자 특성 분리**
- 모델의 의미적 이해 향상

**세만틱 토큰 통합**:
- 기존 음향 토큰(acoustic tokens)과 세만틱 토큰(semantic tokens) 결합
- 더 풍부한 음향 표현 가능

**4. 조건부 생성의 고도화**

**다중 조건 지도** (예: ConditionAudioGen):[10]
- 텍스트 조건만이 아닌 음향 매개변수(음정, 악기 등) 추가 제어
- **세밀한 음향 제어** 통해 생성 정확도 향상

**5. 도메인 적응 기법**

도메인 일반화 연구에서:[11][12]
- **메타 학습 기반 접근**: 다양한 소스 도메인에서 메타 학습하여 **보지 못한 도메인에 직접 일반화**
- **특성 정렬**: 음향 표현 공간에서 도메인 불변 특성 학습

**6. 설명 가능성 기반 개선**

**AudioGenX**(설명 가능 AI 방법):[5]
- 입력 토큰 중요도 강조로 모델 동작 이해 개선
- 약점 자동 식별 및 **선택적 재훈련** 가능

#### 4.3 혼합 방법(Mix Augmentation) 효과

데이터 혼합 증강은 **복잡한 구성 생성에 핵심**입니다:[1]

- **혼합 없음**: FAD = 2.84, KL = 2.14
- **혼합 포함**: FAD = 3.13 (약간 증가), **KL = 2.09** (향상)

혼합이 **조합 가능성** 정확도를 향상시키지만, 음질 측정(FAD)에는 미미한 영향을 미칩니다.

***

### 5. 모델의 한계와 향후 연구 방향

#### 5.1 명시적 한계[1]

1. **긴 시퀀스 모델링**: 음향 토큰이 매우 길어서 (1초 = 500 토큰) 장거리 의존성 학습 어려움
2. **높은 추론 시간**: 자회귀 생성으로 인한 **순차적 샘플링** 필요
3. **음성 생성 제약**: 음성 데이터 필터링으로 **불명료한 음성** 생성 (데이터 증강이나 음성 특성 추가로 완화 가능)
4. **시간적 순서 이해 부족**: 음향 구성의 시간적 관계 파악 한계
5. **데이터셋 편향**: 지리적/인구통계학적 다양성 부족

#### 5.2 향후 고려사항

**1. 기술적 개선 (2024-2025 연구 기반)**

- **더 효율적인 토크나이제이션**: 모델이 더 짧은 시퀀스에서 작동하도록 함[9]
- **구조화된 토큰 시퀀스**: 시간적 순서 정보 명시적 포함
- **음성 특화 모듈**: 음성 품질 향상을 위한 별도 훈련 경로[13]

**2. 데이터 다양성 확대**

- **다국어, 다문화 음향 데이터셋** 구성
- **저자원 음향 환경** (실내/실외, 다양한 배경) 포함
- **도메인별 균형** 조정 (음악, 음성, 환경음 비율)

**3. 평가 방법론 진화**

최신 오디오 생성 평가 추세:
- **CLAP 스코어** (이미지-텍스트 대조 학습 기반) 활용[14][15]
- **세밀한 음향 매개변수 평가** (음정 정확도, 리듬 일관성)
- **시간적 일관성 평가** 지표 개발

**4. 멀티모달 생성으로 확장**

최근 동향 (2024-2025):[16][17]
- **텍스트-비디오-오디오 동시 생성**: 시간적 일관성 있는 멀티모달 생성
- **오디오 인페인팅(inpainting)**: 기존 오디오의 특정 부분 수정

***

### 6. 결론 및 학문적 영향

#### 6.1 AudioGen의 학문적 기여

AudioGen은 **자회귀 모델이 텍스트-오디오 생성에서 경쟁 가능**함을 증명한 중요한 이정표입니다. ICLR 2023 게재 이후 **520회 이상 인용**되며 해당 분야 기준을 설정했습니다.[18][1]

#### 6.2 음성 생성 분야의 미래 방향

1. **효율성-품질 트레이드오프 최적화**: 일관성 모델 기반 접근으로 **300배 이상 속도 향상** 달성 가능[4][3]

2. **멀티태스크 기반 일반화**: 음악, 음성, 환경음 동시 학습으로 **공통 표현** 발전[6]

3. **설명 가능 AI 통합**: 생성 과정의 투명성 향상으로 **신뢰도 증대**[5]

4. **도메인 특화 파인튜닝**: LoRA 같은 저비용 적응 기법으로 **장르/문화별 커스터마이제이션** 가능[7]

#### 6.3 실무 적용 시 고려사항

- **저자원 환경**: AudioLCM과 같은 효율적 모델 선택으로 **제한된 하드웨어에서 배포** 가능
- **데이터 공정성**: 지리적/인구통계학적 편향 제거 필수
- **음성 품질**: 특정 도메인의 음성 생성 필요 시 추가 파인튜닝 필수

***

이 보고서는 AudioGen의 핵심 기여, 기술적 혁신, 성능 향상, 그리고 일반화 개선 방향을 종합적으로 분석했습니다. AudioGen은 텍스트-오디오 생성의 새로운 패러다임을 제시했으며, 후속 연구들이 효율성, 다양성, 그리고 일반화 능력 측면에서 이를 발전시키고 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ecac5ef5-710c-40b6-a0cb-0727162a73cd/2209.15352v2.pdf)
[2](https://www.semanticscholar.org/paper/06ca869b5e1d3904a7bbb1bc2fadfd0e51068ddc)
[3](https://arxiv.org/html/2410.06544v1)
[4](http://arxiv.org/pdf/2406.00356.pdf)
[5](http://arxiv.org/pdf/2502.00459.pdf)
[6](https://www.isca-archive.org/interspeech_2025/xie25b_interspeech.pdf)
[7](https://www.mdpi.com/2076-3417/15/15/8646)
[8](https://arxiv.org/abs/2305.15719)
[9](https://arxiv.org/html/2502.06490v3)
[10](https://arxiv.org/pdf/2308.11940.pdf)
[11](https://openreview.net/pdf/0db4321e9a21c5eac46cb8dbd06ac6e17c392838.pdf)
[12](https://arxiv.org/pdf/2110.10101.pdf)
[13](https://research.samsung.com/blog/High-Fidelity-Text-to-Speech-Via-Discrete-Tokens-Using-Token-Transducer-and-Group-Masked-Language-Model)
[14](http://arxiv.org/pdf/2308.05734.pdf)
[15](https://ai-scholar.tech/en/articles/diffusion-model/audioldm)
[16](http://arxiv.org/pdf/2412.15220.pdf)
[17](https://arxiv.org/html/2407.05551)
[18](https://openreview.net/forum?id=CYK7RfcOzQ4)
[19](http://arxiv.org/abs/2209.15352)
[20](https://arxiv.org/html/2503.16853v1)
[21](https://towardsdatascience.com/breakthroughs-in-speech-recognition-achieved-with-the-use-of-transformers-6aa7c5f8cb02/)
[22](https://openreview.net/pdf?id=CYK7RfcOzQ4)
[23](https://arxiv.org/html/2504.08274v1)
[24](https://felixkreuk.github.io/audiogen/paper.pdf)
[25](https://www.nature.com/articles/s41598-022-12260-y)
[26](https://arxiv.org/abs/2209.15352)
[27](https://ieeexplore.ieee.org/document/10800334/)
[28](https://ieeexplore.ieee.org/document/10448078/)
[29](https://arxiv.org/abs/2502.17119)
[30](https://dl.acm.org/doi/10.1145/3581783.3613854)
[31](https://www.semanticscholar.org/paper/def1104c374eca6df2cb3dcc7bb40aa601431d7b)
[32](https://arxiv.org/pdf/2305.15719.pdf)
[33](https://arxiv.org/html/2409.02845v2)
[34](https://arxiv.org/pdf/2209.03143.pdf)
[35](https://arxiv.org/pdf/2311.00613.pdf)
[36](https://arxiv.org/ftp/arxiv/papers/2301/2301.13267.pdf)
[37](https://arxiv.org/abs/2406.00356)
[38](https://www.isca-archive.org/interspeech_2024/bai24b_interspeech.pdf)
[39](https://trepo.tuni.fi/bitstream/10024/121923/2/GharibShayan.pdf)
[40](https://arxiv.org/abs/2404.00569)
[41](https://mac.kaist.ac.kr/~juhan/gct634/Slides/13.%20audio-level%20music%20generation.pdf)
[42](https://aclanthology.org/2024.findings-naacl.240.pdf)
[43](https://chaksseu.tistory.com/58)
