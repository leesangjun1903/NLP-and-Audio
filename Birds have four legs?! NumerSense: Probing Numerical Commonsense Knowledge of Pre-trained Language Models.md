# Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models

**주요 주장:** 사전 학습된 언어 모델(PTLM)은 텍스트 내 일반적인 상식 지식을 어느 정도 보유하지만, 수치적 상식(예: 동물의 다리 개수)을 정확히 추론하지 못하며, 작은 문장 변형에도 쉽게 무너진다.  

**주요 기여:**  
- 새로운 진단용 마스킹-단어-예측 프로빙 태스크 및 데이터셋 NUMERSENSE(총 13.6K 예제) 제안  
- PTLM(BERT·RoBERTa)의 제로샷 및 원거리 감독(distant supervision) 미세조정(fine-tuning) 성능 평가  
- 인간 성능(96.3% vs 모델 최고 54.06% hit@1) 대비 모델의 현저한 성능 차이 규명  
- 수치적 상식 취약성과 문장 소폭 변형(적대적 예제)에 대한 취약성 분석  
- 향후 PTLM의 수치 상식 학습 및 지식 베이스 구축 연구 촉진  

# 문제, 제안 방법, 모델 구조, 성능 및 한계

## 1. 해결하고자 하는 문제  
사전 학습된 언어 모델이 “A bird usually has MASK legs.” 같은 문장에서 올바른 숫자(“two”)를 예측하는 능력(수치적 상식)을 평가하고, 작은 문장 변형에도 얼마나 견고한지 측정  

## 2. 제안 방법  
### 2.1 태스크 정의  
- 마스킹-단어-예측 프로빙: 모델의 소프트맥스 확률 분포를 이용해 숫자 단어 순위 산정  
- hit@k 정확도(metric): 상위 k개 숫자 단어에 정답이 포함된 비율  

### 2.2 데이터셋 구성  
- **진단용 테스트셋:** 8개 카테고리(일상 사물·생물학·기하·단위 등) 3,145 예제(1,131 코어 + 2,014 적대적 변형)  
- **미세조정용 원거리 감독셋:** GenericsKB에서 수집·검수된 10,492 문장  

### 2.3 수식  
모델의 마스킹-단어-예측 헤드는 소프트맥스 확률을 출력:  

$$
P(w_i \mid \text{context}) = \frac{\exp(e_{w_i}^\top h_\text{[MASK]})}{\sum_{w_j \in V} \exp(e_{w_j}^\top h_\text{[MASK]})}
$$

hit@k는 가장 높은 확률 상위 k개의 숫자 단어에 정답 $$w^*$$가 포함된 경우로 정의  

## 3. 모델 구조  
- **BERT / RoBERTa** (Base & Large): Transformer 인코더 기반, 마스킹-단어-예측(head) 사용  
- **미세조정:** Number 단어 마스킹하여 GenericsKB 문장으로 추가 학습  

## 4. 성능 향상  
| 모델              | Zero-shot hit@1 | Fine-tuned hit@1 |
|-------------------|-----------------|------------------|
| BERT-Large        | 37.63%          | 50.00%           |
| RoBERTa-Large     | 45.85%          | 54.06%           |
| Human (closed)    | 96.30%          | –                |

*Fine-tuning으로 약 12−8%p 향상되나 인간 성능 대비 여전히 크게 낮음.*  

### 적대적 예제에 대한 취약성  
- 평균 약 10%p 추가 성능 하락  
- 문장 소폭 변형(예: “round”)만으로 숫자 예측 순위가 뒤바뀌는 취약성 발견  

## 5. 한계  
- **일반화 부족:** 특정 카테고리 외 예제에 대한 확장성 미검증  
- **편향:** “bird … legs” 템플릿에 과도한 편향 확인  
- **추론 능력 부재:** 문맥 의존적 수치 관계 파악 및 수학적 연산 능력 미비  

# 모델 일반화 성능 향상 가능성

모델의 일반화 성능을 높이려면 다음이 필요하다:  
- **구조적 유도 편향 도입:** 의존구문·의미역(role) 정보를 사전 학습에 통합  
- **수치 토큰 인코딩 개선:** 소프트맥스 기반 예측 한계 보완을 위한 특수 수치 임베딩 또는 회귀 헤드 추가  
- **다양한 원거리 감독:** 지리·물리·문화 등 더 폭넓은 수치 상식 데이터 포함  
- **적대적 학습:** 문장 변형에 대한 견고성 강화를 위한 적대적 예제 사용 미세조정  

# 향후 연구 영향 및 고려사항

### 연구 영향  
- **수치적 상식 베이스 구축:** NUMERSENSE 데이터셋을 활용한 공용 벤치마크 제공  
- **PTLM 개선 방향 제시:** 수치 추론 및 견고성 향상을 위한 연구 촉진  
- **QA 시스템 발전:** How-many 질문 응답 성능 개선  

### 고려사항  
- 다른 언어·문화권 수치 상식 차이 반영  
- 범용 지식 그래프(ConceptNet 등) 연계 통한 지식 보강  
- 수치 범위 추론(분포·범위 예측) 확장  
- 효율적 학습을 위한 경량 모델 설계  

***

**결론적으로**, NUMERSENSE는 사전 학습 언어 모델의 수치적 상식 역량과 취약성을 체계적으로 드러내며, 향후 PTLM의 수치 추론 능력 및 견고성 강화를 위한 필수 연구 방향을 제시한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/73bbf2dd-f208-4760-a8f2-0ed856126909/2005.00683v2.pdf)
