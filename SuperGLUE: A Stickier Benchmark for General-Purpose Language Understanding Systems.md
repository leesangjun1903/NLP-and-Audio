# SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems

**핵심 주장 및 주요 기여**  
SuperGLUE는 기존 GLUE 벤치마크가 최첨단 모델(BERT, XLNet 등)에 의해 인간 성능을 넘어섰기 때문에, 더 어려운 자연어 이해(NLU) 과제를 통해 모델의 **일반화 능력**을 보다 엄격하게 평가할 필요가 있다는 문제 의식에서 출발한다. 주요 기여는 다음과 같다:

1. **어려운 과제 집합** – GLUE의 최상위 난이도 과제(CoLA, RTE)를 유지하고, 기존 모델이 크게 성능을 끌어올리지 못한 6개 과제를 추가로 선정.  
2. **다양한 포맷** – 문장 분류뿐 아니라 질문응답(QA), 코어퍼런스 해결, 단어 의미 판별(WSD) 등을 포함.  
3. **엄밀한 인간 성능 기준** – 모든 과제에 대해 비전문가 군중 작업자 기반의 인간 성능을 추정.  
4. **단일 숫자 점수** – 각 과제 점수를 단순 평균하여 모델 전체 성능을 하나의 숫자로 비교.  
5. **공개 리더보드 및 툴킷** – PyTorch/AllenNLP 기반의 jiant 툴킷 제공 및 대회 사이트(super.gluebenchmark.com) 운영.

***

## 1. 문제 정의 및 목적  
- **한계**: GLUE 벤치마크는 2019년 기준 비전문가 인간 성능(87.1)을 소수 모델이 능가(88.4)하여 더 이상 혁신적 모델 평가에 적절치 않음.  
- **목표**: 일반화·전이·샘플 효율성이 더욱 요구되는, 인간과의 격차가 큰 NLU 과제를 모아 **머신이 해결하기 어려운** 평가 환경 조성.

***

## 2. SuperGLUE 구성 과제  
|과제|형태|예시 입력|평가 지표|훈련 샘플 수|비고|  
|---|---|---|---|---|---|  
|BoolQ|Yes/No QA|지문+질문|정확도|9.4k|Google 문의 기반|  
|CB|텍스트 함의(3-class)|전제+가설|정확도/F1|250|CommitmentBank|  
|COPA|원인·결과 선택|전제+원인/결과 질문+2대안|정확도|400|인과 추론|  
|MultiRC|다중 정답 QA|지문+질문+답안 목록|F1a/EM|5.1k|다중 정답|  
|ReCoRD|클로즈 QA|지문+클로즈 질문|F1/EM|101k|DailyMail|  
|RTE|이진 함의|전제+가설|정확도|2.5k|GLUE RTE 합집합|  
|WiC|단어 의미 판별|문장1+문장2+표시어|정확도|6k|WSD|  
|WSC|코어퍼런스|문장+표시 대명사+지목명사|정확도|554|Winograd|  

***

## 3. 평가 방식 및 점수 집계  
- **과제별 단일 메트릭**: 정확도, F1, EM 등 과제 특성에 맞는 자동화 지표 사용.  
- **전체 점수** $$S$$ 계산:  

$$
S = \frac{1}{8}\sum_{i=1}^{8} s_i
$$  

$$s_i$$: i번째 과제 단일 점수(다중 지표는 평균)  
- **진단 데이터**: 세계지식·언어 현상 진단 및 성별 편향 점검(Winogender) 포함.

***

## 4. 방법론 및 베이스라인  
- **BERT 기반 단일 과제 파인튜닝**: bert-large-cased 모델, 학습률 $$1\times10^{-5}$$, 최대 10 epochs.  
- **STILTs 스타일 중간 과제 전이 학습**:  
  - CB/RTE/BoolQ ← MultiNLI  
  - COPA ← SWAG  
- **결과**: BERT++가 일반 BERT 대비 2–8점 추가 향상.

***

## 5. 성능 및 한계  
|모델|전체 평균(8과제)|최고 인간 성능|차이|  
|---|---|---|---|  
|BERT|≈71.5|≈89.8|−18.3|  
|BERT++|≈72.9|≈89.8|−16.9|  

- 모든 과제에서 **인간 대비 최소 10점** 이상의 차이 존재.  
- WSC, COPA 등 상위 과제에서는 인간 100%에 비해 모델 성능이 현저히 낮음.  
- **데이터 부족 과제**(예: WSC: train 554개)에서 과적합 우려 및 일반화 어려움.

***

## 6. 일반화 성능 향상 관점  
- **샘플 효율적 전이 학습**: 소량 데이터 과제에서의 성능 격차 축소를 위해 중간 과제 전이 강화 필요.  
- **다중 과제·멀티태스크 학습**: jiant 툴킷 기반으로 여러 과제를 동시 학습하여 공유 표현 학습 극대화.  
- **자기지도 학습·데이터 증강**: Commonsense 추론·코어퍼런스에 특화된 대규모 언어 모델 → 더 깊은 추론 능력 확보.

***

## 7. 향후 연구에의 영향 및 고려점  
SuperGLUE는 NLU 모델의 **범용 이해 능력** 및 **저자원 과제 일반화**를 측정하는 새 표준을 마련했다.  
- **영향**: 연구자들은 단일 과제 최적화가 아닌, 다양한 포맷 간 전이와 멀티태스크 모델 설계에 집중할 필요.  
- **고려사항**:  
  1. **데이터 품질 및 라이선스**: 저자원 과제 확장을 위해 공개·재배포 허용 데이터 확보.  
  2. **사회적 편향 검사**: Winogender 확장 등 **포괄적 진단** 필요.  
  3. **메트릭 다양화**: 생성형·추론형 과제에 맞춘 자동화 평가 지표 연구.  

---  
SuperGLUE는 **모델의 한계를 드러내고**, 일반화·추론·편향 검증 연구를 촉진하는 기반을 제공한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/b525e1bb-b50e-46f1-a31e-2a27f366469d/1905.00537v3.pdf)
