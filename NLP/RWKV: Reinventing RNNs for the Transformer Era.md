# RWKV: Reinventing RNNs for the Transformer Era

## 1. 핵심 주장과 주요 기여

**RWKV(Receptance Weighted Key Value)** 는 Transformer의 효율적인 병렬 훈련 방식과 RNN의 효율적인 추론 방식을 결합한 혁신적인 모델 아키텍처입니다.[1]

### 핵심 기여
- **계산 복잡도 개선**: 기존 Transformer의 $$O(T^2d)$$ 시간 복잡도를 $$O(Td)$$로 대폭 축소[1]
- **확장성 검증**: 최대 140억 개 매개변수까지 성공적으로 확장하며, 이는 훈련된 최대 규모의 밀집 RNN[1]
- **성능 동등성**: 동일한 계산량으로 학습된 Transformer 모델들과 비교 가능한 성능 달성[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 기존 문제점

**Transformer의 한계**:
- 시퀀스 길이에 대해 **이차적(quadratic) 복잡도** 증가[1]
- 메모리 및 계산 요구량이 시퀀스 길이 제곱에 비례하여 증가[1]

**RNN의 한계**:
- **기울기 소실 문제(vanishing gradient problem)**[1]
- 시간 차원에서의 **비병렬화**로 인한 확장성 제약[1]

### 제안 방법: WKV 연산자

RWKV의 핵심은 **WKV(Weighted Key Value) 연산자**입니다. 수식으로 표현하면:

$$
wkv_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \odot v_i + e^{u + k_t} \odot v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}}
$$

여기서:
- $$w \in (\mathbb{R}_{\geq 0})^d$$: 채널별 시간 감쇠 벡터[1]
- $$u$$: 현재 토큰에 대한 특별한 가중치 벡터[1]
- $$k_t, v_t$$: 시간 $$t$$에서의 Key와 Value 벡터[1]

이 수식은 **AFT(Attention Free Transformer)**에서 영감을 받았으나, 상대적 위치에 따른 채널별 시간 감쇠를 적용하여 RNN으로 변환 가능하도록 설계하였습니다.[1]

## 3. 모델 구조

### 아키텍처 구성요소

RWKV는 네 가지 핵심 요소로 구성됩니다:[1]

- **R (Receptance)**: 과거 정보의 수신기 역할
- **W (Weight)**: 학습 가능한 위치별 가중 감쇠 벡터  
- **K (Key)**: 전통적인 어텐션의 Key와 유사한 역할
- **V (Value)**: 전통적인 어텐션의 Value와 유사한 역할

### Token Shift 메커니즘

모든 선형 투영 벡터는 현재와 이전 타임스텝 입력의 선형 보간으로 생성됩니다:[1]

$$
r_t = W_r \cdot (\mu_r \odot x_t + (1-\mu_r) \odot x_{t-1})
$$
$$
k_t = W_k \cdot (\mu_k \odot x_t + (1-\mu_k) \odot x_{t-1})
$$
$$
v_t = W_v \cdot (\mu_v \odot x_t + (1-\mu_v) \odot x_{t-1})
$$

### 이중 모드 동작

1. **훈련 시 (Transformer-like)**: 시간 병렬 모드로 전체 시퀀스를 병렬 처리[1]
2. **추론 시 (RNN-like)**: 시간 순차 모드로 상수 시간 복잡도 유지[1]

## 4. 성능 향상 및 일반화 능력

### 확장 법칙 (Scaling Laws)

RWKV는 Transformer와 동일한 로그-로그 선형 스케일링 법칙을 따릅니다. 45개의 다양한 크기 모델을 훈련한 결과:[1]
- **최적점에서 $$r^2 = 0.994$$** 의 우수한 선형 적합도
- **1차수 외삽에서도 $$r^2 = 0.875$$** 의 높은 예측 정확도[1]

### 벤치마크 성능

12개 NLP 태스크에서 동일 계산량의 Transformer 모델들(BLOOM, Pythia, OPT)과 **동등한 성능**을 달성했습니다.[1]

### 긴 맥락 처리 능력

맥락 길이 확장 실험에서 1024 → 2048 → 4096 → 8192 토큰으로 점진적 확장 결과:
- **맥락 길이 증가에 따른 테스트 손실 감소** 확인[1]
- Long Range Arena 벤치마크에서 **S4 모델 다음으로 우수한 성능** (5개 데이터셋에서 2위)[1]

### 일반화 성능 특성

**선형 어텐션의 이점**:
- 학습된 시간 감쇠($$w$$)를 통한 **정보 보존과 전파의 균형**[1]
- 각 레이어에서 서로 다른 시간 감쇠 패턴을 학습하여 **다양한 추상화 레벨의 복잡한 패턴 포착**[1]

**모델 행동 분석**:
- 초기 레이어: 지역적 연산 (텍스트 파싱, 어휘 분석)
- 후기 레이어: 장기 정보 보존 및 전파[1]

## 5. 한계점

### 정보 병목 현상
선형 어텐션으로 인해 **매우 긴 맥락에서 세밀한 정보 회상**에 제약이 있습니다. 이는 정보가 여러 시간 단계를 거쳐 단일 벡터 표현으로 압축되기 때문입니다.[1]

### 프롬프트 엔지니어링 의존성
RNN의 순차적 특성으로 인해 **프롬프트 구성이 더욱 중요**합니다. 실험 결과 프롬프트 순서 변경만으로도 성능이 44.2%에서 74.8%로 대폭 향상되었습니다.[1]

### 수학적 추론 한계
중간 결과가 필요한 복잡한 수학 문제에서 **매우 낮은 정확도**(5.43%)를 보였습니다.[1]

## 6. 미래 연구에 미치는 영향

### 학술적 기여

1. **아키텍처 혁신**: RNN의 효율성과 Transformer의 표현력을 성공적으로 결합한 첫 번째 대규모 실증 연구[1]

2. **확장성 검증**: 수백억 매개변수까지의 RNN 확장 가능성을 최초로 입증[1]

3. **이론적 기반**: 선형 어텐션을 RNN으로 재구성하는 새로운 접근법 제시[1]

### 실용적 응용

**효율적 배포**: 
- 소비자 및 엣지 하드웨어에서의 **LLM 민주화** 가능성[1]
- 추론 비용 절감으로 인한 **접근성 향상**[1]

**다양한 도메인 확장**:
- 자연어 처리 외에도 **생의학 데이터 처리, 기후 모델링** 등 다양한 순차 데이터 처리 태스크에 적용 가능[1]

### 향후 연구 고려사항

1. **표현력 증대**: 시간 감쇠 공식 개선 및 초기 모델 상태 탐색[1]

2. **계산 효율성 향상**: WKV 단계에서 병렬 스캔 적용으로 $$O(B \log(T)d)$$ 복잡도 달성[1]

3. **인코더-디코더 적용**: seq2seq 및 멀티모달 설정에서의 크로스 어텐션 대체[1]

4. **해석가능성 연구**: RWKV의 상태를 활용한 시퀀스 데이터의 해석가능성 및 안전성 연구[1]

5. **내부 상태 확장**: 더 큰 내부 상태 구현을 통한 이전 맥락에 대한 기억력 향상[1]

RWKV는 계산 효율성과 모델 성능 사이의 균형을 달성한 혁신적 아키텍처로서, 향후 효율적인 대규모 언어 모델 연구의 새로운 방향을 제시했습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/76277d58-de61-4d8e-af55-d66a6f430d2b/2305.13048v2.pdf)
