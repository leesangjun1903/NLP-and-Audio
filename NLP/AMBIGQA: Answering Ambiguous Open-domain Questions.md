# AMBIGQA: Answering Ambiguous Open-domain Questions

### 1. 핵심 주장 및 주요 기여

**AMBIGQA** 논문은 개방형 질문 응답(Open-domain QA) 분야에서 근본적인 문제를 제기합니다. 기존의 모든 QA 시스템이 각 질문에 **단일하고 명확한 답변**이 존재한다고 가정하는 반면, 현실에서는 50% 이상의 질문이 **본질적으로 모호성을 내포**하고 있다는 점입니다. 이러한 모호성은 사용자가 답변을 알기 전에 질문을 작성하는 실제 정보 추구 환경(예: 구글 검색 쿼리)에서 빈번하게 발생합니다.[1]

논문의 세 가지 주요 기여는 다음과 같습니다:[1]

1. **새로운 태스크 정의**: 모호한 질문에 대해 가능한 모든 답변을 찾고, 각 답변에 대응하는 **명확화된 질문 재작성**을 함께 제공하는 AMBIGQA 태스크 도입

2. **대규모 데이터셋 구축**: 기존의 NQ-OPEN 벤치마크에서 **14,042개 질문**을 주석 처리한 AMBIGNQ 데이터셋 구성 (평균 2.1개의 서로 다른 답변 보유)

3. **기준 모델 제시**: 약한 지도학습(weakly supervised learning)을 활용하여 여러 답변을 생성할 수 있는 첫 번째 기준 모델 제시

### 2. 해결하는 문제 및 기술적 방법론

#### 2.1 문제의 특성

AMBIGNQ 데이터셋 분석을 통해 밝혀진 모호성의 유형은 다양합니다:[1]

| 모호성 유형 | 비율 | 예시 |
|-----------|------|------|
| 이벤트 참조 모호성 | 39% | "회색의 해부학에서 메레디스와 데렉이 결혼하는 시즌?" (비공식 vs 법적 결혼) |
| 속성 모호성 | 27% | "일곱 가지 대죄 시즌 2의 에피소드 수?" (OVA 포함 여부) |
| 개체 참조 모호성 | 23% | "클레이 매튜스의 경력 사코 수?" (Jr. vs III) |
| 답변 유형 모호성 | 16% | "누가 노래를 부르는가?" (그룹 vs 리드 싱어) |
| 시간 의존성 | 13% | "새 패밀리 가이 시즌은 언제 나오나?" |
| 다중 부질문 | 3% | "quit india 운동 중 영국 PM과 총독은?" |

중요한 점은 이러한 모호성들이 **질문 텍스트만 보고는 인식하기 어렵고, 위키피디아 문서를 검색하고 읽은 후에야 드러난다**는 것입니다.[1]

#### 2.2 수식과 평가 지표

논문은 예측된 질문-답변 쌍 $(x_i, y_i)$와 참조 답변 집합 $(\bar{x}_j, \bar{Y}_j)$를 비교하기 위해 **다층적 평가 메트릭**을 제안합니다:[1]

각 예측 쌍에 대한 정확도 점수 $c_i$는 다음과 같이 계산됩니다:

$$c_i = \max_{1 \leq j \leq n} I[y_i \in \bar{Y}_j] \cdot f(x_i, \bar{x}_j)$$

여기서:
- $I[y_i \in \bar{Y}_j]$: 답변의 정확성 여부 (0 또는 1)
- $f(x_i, \bar{x}_j)$: 질문 간 유사성 함수 (0과 1 사이의 값)

이를 통해 정밀도, 재현율, F1 점수를 계산합니다:

$$\text{Prec}_f = \frac{\sum_i c_i}{m}, \quad \text{Rec}_f = \frac{\sum_i c_i}{n}, \quad F1_f = \frac{2 \times \text{Prec}_f \times \text{Rec}_f}{\text{Prec}_f + \text{Rec}_f}$$

논문은 세 가지 구체적 지표를 제안합니다:[1]

- **F1_ans**: 질문 유사성을 고려하지 않고 답변 정확성만 평가 ($f$는 항상 1)
- **F1_BLEU**: BLEU를 사용하여 질문 문자열 유사성 고려
- **F1_EDIT-F1**: 추가/삭제된 유니그램을 기반으로 의미적 차이만 평가 (새로운 메트릭)

#### 2.3 모델 구조

제안된 기준 모델은 세 가지 주요 컴포넌트로 구성됩니다:[1]

**1) 다중 답변 예측 (SPANSEQGEN)**

기존의 상태 최고(state-of-the-art) 모델인 Karpukhin et al. (2020)을 확장합니다:
- **검색 단계**: BERT 기반 dual encoder로 상위 100개 패시지 검색
- **재순위 단계**: BERT 기반 cross encoder로 재순위 지정
- **생성 단계**: BART 기반 sequence-to-sequence 모델로 여러 답변을 순차적으로 생성 (구분자 [SEP]로 분리)

SPANSEQGEN은 NQ-OPEN에서 사전학습 후 AMBIGNQ에서 미세조정됩니다.

**2) 질문 명확화 (Question Disambiguation) 모델**

BART 기반 생성 모델로, 다음을 입력으로 받습니다:
- 원본 질문 $q$
- 대상 답변 $y_i$
- 다른 가능한 답변들 $y_1, ..., y_{i-1}, y_{i+1}, ..., y_n$
- 검색된 상위 패시지들

각 답변에 대해 최소한으로 편집된 명확화 질문 $x_i$를 생성합니다.

**3) 약한 지도학습을 통한 민주적 공동훈련 (Democratic Co-training)**

Algorithm 1에 명시된 이 방법은 NQ-OPEN의 부분 주석을 활용합니다:[1]

```
입력: 
- D_full: AMBIGNQ의 완전 주석 (여러 답변 목록)
- D_partial: NQ-OPEN의 부분 주석 (단일 답변)

반복 (N번):
  1. 완전 데이터로 C개의 시퀀스-투-시퀀스 모델 φ_i 훈련
  2. 부분 데이터의 각 질문 q_j에 대해:
     - 알려진 답변 y_j를 prefix로 사용하여 예측
     - 여러 모델이 일치하는 추가 답변 찾기 (다수결)
     - 발견된 추가 답변이 있으면 D_full에 다중 답변 케이스 추가
     - 없으면 D_full에 단일 답변 케이스 추가
```

이는 NQ-OPEN의 잠재된 모호성을 발견하여 훈련 데이터를 확장하는 혁신적 방법입니다.

### 3. 성능 향상 및 실험 결과

#### 3.1 주요 성능 지표

개발 셋에서의 최종 성능 (SPANSEQGEN† with Co-training + QD 모델):[1]

| 메트릭 | F1_ans | F1_BLEU | F1_EDIT-F1 |
|--------|--------|---------|-----------|
| 모든 예제 | 42.3 | 14.3 | 8.0 |
| 다중 답변만 | 31.7 | - | - |

테스트 셋에서의 성능:[1]

| 메트릭 | F1_ans | F1_BLEU | F1_EDIT-F1 |
|--------|--------|---------|-----------|
| 모든 예제 | 35.9 | 11.5 | 6.3 |
| 다중 답변만 | 26.0 | - | - |

#### 3.2 약한 지도학습의 효과

공동훈련의 기여도를 명확히 보여주는 비교 결과:[1]

- **SPANSEQGEN 단독**: F1_ans = 33.5 (테스트)
- **SPANSEQGEN 앙상블**: F1_ans = 35.2 (테스트)
- **SPANSEQGEN† with Co-training**: F1_ans = 35.9 (테스트)

이는 NQ-OPEN의 부분 지도가 효과적으로 활용될 수 있음을 입증합니다.

#### 3.3 질문 명확화의 과제

질문 명확화 모델의 성능은 상대적으로 저조합니다:[1]

| 조건 | F1_BLEU | F1_EDIT-F1 |
|------|---------|-----------|
| 전체 태스크 | 14.3 | 8.0 |
| 금 답변 주어진 경우 | 40.1 | 19.2 |

이는 두 가지 주요 문제를 시사합니다:
1. 시퀀스 우도 최대화가 출력의 길이 변동을 제대로 반영하지 못함
2. 훈련 데이터 부족 (NQ-OPEN 약한 지도가 질문 명확화에 도움이 되지 않음)

#### 3.4 에러 분석

50개 샘플 분석 결과:[1]

- **다중 답변 정확 예측**: 2%
- **다중 답변 부분 정확**: 40% (15개 중 11개는 1개 답변만 생성)
- **다중 답변 오류**: 14%
- **단일 답변 정확 예측**: 26%
- **단일 답변 오류 예측**: 12%

모델의 주요 한계는 **답변 재현율 부족**입니다 (정밀도 49.6% vs 재현율 25.3%).

### 4. 모델의 일반화 성능 관련 심층 분석

#### 4.1 일반화 성능의 핵심 과제

논문은 여러 중요한 **일반화 관련 발견**을 제시합니다:[1]

**비훈련 설정에서의 성능 (Zero-shot)**

NQ-OPEN에서만 훈련하고 AMBIGNQ에서 평가한 결과:[1]

| 모델 | 개발 F1_ans | 테스트 F1_ans | 다중 답변 F1_ans |
|------|-----------|-------------|----------------|
| Min et al. (2019b) | 30.8 | 27.5 | 20.4 |
| Asai et al. (2020) | 29.7 | 27.9 | 19.7 |
| Karpukhin et al. (2020) | 35.2 | 30.1 | 23.2 |
| SPANSEQGEN | 36.4 | 30.8 | 20.7 |

SPANSEQGEN은 전체 QA에서는 개선되었지만, 다중 답변 예측에서는 오히려 퇴행합니다. 이는 **시퀀스 길이 편향** 문제를 나타냅니다.

**비-철저한 BLEU 스코어**

F1_BLEU의 낮은 절대값 (테스트에서 11.5)은 현재 모델이 **질문 명확화에서 여전히 큰 어려움**을 겪고 있음을 보여줍니다.[1]

#### 4.2 최신 연구의 일반화 기여도

최근 발표된 연구들이 AMBIGQA 기반 일반화 개선에 어떻게 기여하는지 살펴봅시다:[2][3][4]

**A²Search (2025)**[2]

이 연구는 대규모 언어 모델(LLM)과 강화학습(RL)을 활용하여 AMBIGQA의 일반화 문제에 직접 대응합니다:
- **자동화된 모호성 탐지**: 수동 주석 없이 모호한 질문 자동 식별
- **궤적 샘플링**: 대안 답변을 다중 경로로 수집
- **증거 검증**: 검색된 패시지와 일치 확인
- **성과**: 8개 벤치마크에서 평균 AnsF1@1 48.4% 달성 (ReSearch-32B 46.2% 초과)

**Corpus-Invariant Tuning (2024)**[3][5]

개방형 QA의 일반화 성능을 개선하는 전략:
- **문제 진단**: 리더 모델이 외부 코퍼스의 지식을 과도하게 암기하여 새로운 코퍼스로 전이되지 않음
- **해결책**: 훈련 중 검색된 컨텍스트의 우도를 제어하여 지식 암기 완화
- **효과**: 새로운 코퍼스 버전이나 도메인으로의 전이 성능 향상

**CondAmbigQA (2025)**[4]

조건부 모호한 질문 응답 벤치마크:
- **근본적 통찰**: 명백한 할루시네이션이 실제로는 모호한 쿼리에서 비롯될 수 있음
- **조건 인식 평가**: 명시적 컨텍스트 제약 조건 고려
- **개선폭**: 조건을 고려하지 않을 때 대비 11.75% 정확도 향상, 명시적 조건 제공 시 추가 7.15% 향상

**DeepAmbigQA (2025)**[6]

다중 홉 추론이 필요한 더 복잡한 모호한 질문:
- 3,600개의 질문 데이터셋 (절반은 명시적 이름 모호성 + 다단계 추론)
- GPT-4 같은 최신 모델도 여전히 성능 저하

**ODQA 일반화 연구 (2022)**[7]

개방형 QA의 일반화 카테고리화:
1. **훈련 세트 겹침**: 기본 일반화 요구 없음
2. **합성 일반화**: 새로운 개체 조합에 대한 일반화
3. **신규 개체 일반화**: 훈련 중 미접한 개체 처리

발견 사항: 매개변수 모델(BART, T5)은 신규 개체에 대해 심각하게 저하 (정확도 0-20%) / 검색-기반 모델(DPR)이 더 나은 일반화 특성 보임

#### 4.3 일반화 성능 향상을 위한 체계적 분석

논문 내 중요한 관찰들:[1]

**1) 개발/테스트 분포 불일치**

NQ-OPEN의 구성 방식 때문에 테스트 세트가 더 어려운 질문으로 편향됨:
- 개발 세트: 훈련 세트에서 단일 주석자 샘플
- 테스트 세트: 개발 세트에서 5명의 주석자 샘플

결과: 개발 대비 테스트에서 F1_ans 약 5-8% 성능 저하

**2) 단일 답변 vs 다중 답변 성능 격차**

- 단일 답변 예제 정확도: 약 65% (높음)
- 다중 답변 예제 정확도: 약 32% (낮음)

→ **다중 답변 재현율**이 주요 병목 (25.3%)

**3) 데이터-증강의 효과성**

민주적 공동훈련을 통해 NQ-OPEN의 부분 지도를 활용하면:
- 단독 미세조정: 33.5
- 공동훈련: 35.9 (7.2% 개선)

이는 약한 지도 학습이 효과적일 수 있음을 시사하지만, **질문 명확화에는 직접 도움이 되지 않음**을 보여줍니다.

### 5. 모델의 한계 및 미해결 문제

논문 및 후속 연구가 확인한 주요 한계:[3][4][2][1]

#### 5.1 모델 내재 한계

1. **시퀀스 길이 편향**: 생성 모델이 평균적으로 더 짧은 답변을 생성하는 경향 (3.0 vs 6.7 토큰)

2. **답변 재현율**: 다중 답변 재현율이 25.3%로 매우 낮음

3. **질문 명확화 품질**: F1_EDIT-F1이 6.3으로 매우 낮아, 실제로 구별되는 질문 생성의 어려움

4. **컨텍스트 수신 편향**: 외부 코퍼스의 지식을 과도하게 암기하여 새로운 도메인으로 전이 불가

#### 5.2 데이터 관련 한계

1. **NQ-OPEN과의 불일치**: 약 29.4%의 AMBIGNQ 개발 예제가 원본 NQ-OPEN 답변을 포함하지 않음

2. **데이터 부족**: 질문 명확화 모델이 NQ-OPEN의 약한 지도로부터 이점을 얻지 못함

3. **분포 편향**: 테스트 세트가 의도치 않게 더 어려운 질문들로 과표본화됨

#### 5.3 평가 메트릭 한계

1. **문자열 매칭**: F1_EDIT-F1도 동의어적 표현 변형을 놓칠 수 있음 (예: "작성" vs "만든" 구분 불가)

2. **모호성 탐지 평가 부재**: 모델이 실제로 질문의 모호성을 인식하는지 여부 평가 불가

#### 5.4 사용자 중심 평가 부재

논문 저자들 자신이 제안하는 미래 방향:
- 실제 사용자에게 생성된 답변의 유용성 평가 필요
- 단순히 메트릭 최적화보다는 사용자 만족도 측정

### 6. 향후 연구에 미치는 영향 및 고려사항

#### 6.1 AMBIGQA 이후 연구 트렌드

**패러다임 전환**: 개방형 QA에서 **모호성 인식**이 핵심 요구사항으로 인식됨[4][2][3]

**대규모 언어 모델 활용**: 2023-2025년 연구들이 일관되게 LLM + 검색 시스템 결합 방식 채택[8][2][4]

**약한 지도 학습의 확장**: AMBIGQA의 민주적 공동훈련 개념이 더 복잡한 다중 홉 추론 문제로 확장[6]

#### 6.2 향후 연구 시 고려할 주요 점

**1) 일반화 성능 향상**

- **도메인 간 전이**: AMBIGQA 같은 표준 벤치마크가 새로운 도메인에서 모호성을 얼마나 잘 포착하는지 검증 필요
- **추론 품질**: 시간 의존적이거나 다중 홉 추론이 필요한 모호한 질문 처리
- **컨텍스트 활용 개선**: 현재 과도한 암기 문제 해결을 위한 더 정교한 학습 전략

**2) 모호성의 다차원적 이해**

- AMBIGQA의 6가지 모호성 유형이 서로 독립적이지 않으며, 복합적일 수 있음을 인식
- Tree of Clarifications 같은 트리 구조 접근으로 다차원 모호성 처리[8]

**3) 데이터 품질 및 규모**

- AMBIGNQ의 훈련/개발/테스트 분포 불일치 문제 해결
- 더 큰 규모의 모호성 주석 데이터 수집
- 다중 언어, 다양한 도메인 확장

**4) 평가 메트릭 개선**

- 의미적 동등성을 더 잘 포착하는 메트릭 개발
- 사용자 중심의 정성 평가 체계 구축
- 모호성 탐지 정확도를 명시적으로 평가하는 메트릭

**5) 실제 응용 시스템 개발**

- 사용자 피드백을 통한 동적 모호성 해결 시스템
- 검색 엔진, 챗봇 같은 실제 응용에서의 효과성 검증
- 사용자가 제공한 컨텍스트를 활용한 개인화된 명확화

**6) 추론 효율성**

- 현재 20회 미만의 LLM 호출로 제한 (A²Search의 관찰)
- 더 효율적인 모호성 탐지 알고리즘 개발
- 계산 비용 대비 성능 트레이드오프 분석

**7) 신뢰성 및 검증**

- 모호성 발견의 정확성 보증 메커니즘
- 생성된 명확화 질문의 충실성 검증
- 드물거나 예상 밖의 모호성에 대한 견고성

### 결론

**AMBIGQA**는 개방형 질문 응답 분야의 **역사적 전환점**입니다. 기존의 단순한 "정답 찾기" 패러다임에서 벗어나 **모호성 인식, 다중 답변 생성, 명확화 질문 작성**이라는 삼원적 접근을 제시했습니다.[1]

논문의 기여에도 불구하고, 5년 이상 경과한 현재 지점에서 보면 **질문 명확화(F1_EDIT-F1: 6.3)와 다중 답변 재현율(25.3%)이라는 핵심 병목이 여전히 해결되지 않아** 있습니다.[2][1]

최신 연구들(A²Search, CondAmbigQA, DeepAmbigQA)은 LLM의 맥락 이해 능력과 검색 시스템의 증강을 통해 이를 우회하고 있으며, **모호성의 다차원적 특성과 도메인 간 일반화**가 새로운 초점이 되고 있습니다.[7][3][4][6][2]

향후 연구자들이 반드시 고려해야 할 것은: 모호성 해결의 정확성만큼 **사용자 경험의 품질**과 **실제 응용 환경에서의 신뢰성**입니다.

***

### 참고문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4598fe8c-584f-46de-9e61-0a94cf80eb7a/2004.10645v2.pdf)
[2](https://arxiv.org/abs/2510.07958)
[3](https://arxiv.org/abs/2404.01652)
[4](https://aclanthology.org/2025.emnlp-main.115/)
[5](https://aclanthology.org/2024.findings-naacl.48/)
[6](https://www.themoonlight.io/ko/review/deepambigqa-ambiguous-multi-hop-questions-for-benchmarking-llm-answer-completeness)
[7](https://aclanthology.org/2022.findings-naacl.155.pdf)
[8](https://oneonlee.tistory.com/162)
[9](https://www.aclweb.org/anthology/2020.emnlp-main.466.pdf)
[10](https://pmc.ncbi.nlm.nih.gov/articles/PMC9514517/)
[11](https://pmc.ncbi.nlm.nih.gov/articles/PMC11822271/)
[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC9514516/)
[13](https://pmc.ncbi.nlm.nih.gov/articles/PMC11549044/)
[14](https://pmc.ncbi.nlm.nih.gov/articles/PMC10083634/)
[15](https://pmc.ncbi.nlm.nih.gov/articles/PMC9431833/)
[16](https://pmc.ncbi.nlm.nih.gov/articles/PMC9432449/)
[17](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002889630)
[18](https://arxiv.org/abs/2004.10645)
[19](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART003250967)
