# Unsupervised Commonsense Question Answering with Self-Talk

### 1. 핵심 주장과 주요 기여

이 논문은 **자기 대화(Self-Talk)** 기반의 비지도 학습 프레임워크를 제안하여 상식적 추론에서 언어 모델(LM)의 암묵적 지식을 명시화하는 방법을 제시합니다. 핵심 주장은 다음과 같습니다.[1]

**주요 기여:**
- 외부 지식 베이스에 의존하지 않고도 사전 학습된 언어 모델로부터 지식을 추출하는 새로운 방식 제시[1]
- 브루너의 발견 학습(inquiry-based discovery learning) 원리에 영감을 받아 명확성 질문을 통해 추가 배경 지식을 발견[1]
- 6개의 상식 추론 벤치마크 중 4개에서 영점 설정(zero-shot) 기준선 대비 실질적인 성능 향상 달성[1]

### 2. 해결하는 문제 및 제안 방법

**문제의 정의:**

현재 자연언어 이해 시스템은 두 가지 주요 한계를 가집니다. 첫째, 사전 학습된 언어 모델만을 사용하면 보고 편향(reporting bias)으로 인한 불충분한 커버리지, 정밀도 부족, 그리고 제한된 추론 능력 문제가 발생합니다. 둘째, 외부 지식 베이스(ConceptNet 등)는 각 인스턴스마다 관련성 있는 사실을 구별하기 위한 추가 처리가 필요합니다.[1]

**제안 방법:**

Self-Talk 모델은 다음의 세 단계로 구성됩니다:[1]

1. **명확성 질문 생성**: 문맥과 사전 정의된 질문 접두사(question prefix)를 결합하여 5개의 질문을 생성합니다

$$ Q = \{q_1, q_2, ..., q_5\} \text{ conditioned on context } c \text{ and prefix } p $$

2. **답변 생성**: 각 질문에 대해 대응하는 답변 접두사와 함께 10개의 답변을 생성합니다

$$ A_i = \{a_{i,1}, a_{i,2}, ..., a_{i,10}\} \text{ for each } q_i $$

3. **명확성 통합**: 각 보기에 대한 점수를 다음과 같이 계산합니다

$$ \text{score}(a_i) = \min_{cl \in CL} CE(\text{opt}_i(c, q, cl)) $$
   
   여기서 $$CE $$는 교차 엔트로피 손실이고, 최적의 명확성이 가장 낮은 손실을 산출합니다.[1]

**모델 구조:**

모델은 세 가지 주요 컴포넌트로 구성됩니다:[1]

- **LM 기반선**: 사전 학습된 언어 모델(GPT, GPT-2, XLNet 등)을 사용하여 답변 선택지의 플래시빌리티를 점수화
- **외부 지식 모델**: ConceptNet, Google N-grams, COMeT로부터 추출한 명확성을 활용
- **Self-Talk 모델**: 동일하거나 다른 언어 모델을 사용하여 명확성 질문과 답변을 생성

## 3. 성능 향상 및 일반화 능력

**성능 평가 결과:**

논문은 6개의 상식 추론 벤치마크에서 모델을 평가했습니다:[1]

| 데이터셋 | 개선도 (개발 정확도) | 성능 특성 |
|---------|------------------|---------|
| COPA | +7.25점 (Distil-GPT2) | 상당한 개선 |
| CommonSenseQA | -4.04점 (GPT) | 성능 저하 |
| MC-TACO | +3.53점 (Google N-grams) | 중간 수준 개선 |
| Social IQa | +2.74점 (COMeT) | 중간 수준 개선 |
| PIQA | +4.36점 (Google N-grams) | 우수한 개선 |
| WinoGrande | +0.13점 (GPT) | 미미한 개선 |

**일반화 성능 향상 가능성:**

Self-Talk 방식의 중요한 특징은 문제별로 맞춤형 명확성을 동적으로 생성한다는 점입니다. 이는 정적 지식 베이스의 커버리지 제한을 극복합니다. 그러나 논문의 분석에서는 흥미로운 발견을 제시합니다:[1]

- **유용한 명확성**: 전체 문법적 또는 이해 가능한 명확성 중 약 60%가 인간 평가자에 의해 사실상 정확하다고 판단됨
- **해로운 명확성**: 정확한 답변을 산출한 명확성 중 12%는 인간에 의해 사실상 정확하다고 판단됨

이는 **모델이 명시적으로 정당화 가능하지 않은 이유로 정답을 도출**하기도 한다는 중요한 한계를 드러냅니다.[1]

### 4. 모델의 한계

논문은 다음과 같은 중요한 한계를 지적합니다:[1]

**생성된 명확성의 품질 문제:**
- 관련성 없는 명확성: 전체의 24.7%
- 비정상적이거나 문법 오류: 전체의 19.9%
- 사실상 부정확한 명확성: 상당한 비중
- 단순히 맥락 반복: 전체의 4.1%

**추론 깊이의 제한:**
논문은 다중 홉 추론(multi-hop reasoning)을 시도했으나 혼합된 결과만 얻었으며, 이는 현재 프레임워크가 단일 단계 추론에 최적화되어 있음을 시사합니다.[1]

**지식 통합 문제:**
여러 지식 소스를 결합했을 때 대부분의 경우 성능이 향상되지 않았으며, 오직 MC-TACO에서만 7.9포인트의 개선이 달성되었습니다.[1]

### 5. 앞으로의 연구에 미치는 영향

**최신 연구 기반의 발전 방향:**

Self-Talk 이후의 연구들은 다음과 같은 방향으로 진화하고 있습니다:[2][3][4][5]

**1) 자기 피드백 기반 개선:**
Crystal(2023)은 자기 대화 개념을 확장하여 모델 자신의 피드백으로 보상 신호를 생성하는 **자기 성찰적 추론(introspective reasoning)** 기법을 도입했습니다. 이는 감독 미세조정보다 우수한 성능과 투명성을 제공합니다.[2]

**2) 대규모 언어 모델(LLM) 중심의 접근:**
ZEBRA(2024)와 같은 최신 프레임워크는 검색 기반 증강(retrieval augmentation)과 내부 성찰(introspection)을 결합하여 평균 4.5포인트의 성능 개선을 달성했습니다. 이는 Self-Talk의 기본 원리를 LLM 시대에 적응시킨 것입니다.[5]

**3) 다중 홉 추론의 진전:**
Multi-hop Commonsense Knowledge Injection Framework(2023)는 지식 그래프의 다중 홉 관계를 명확히 하는 합성 QA 데이터셋을 생성하여 대조 학습으로 모델을 훈련하는 방식을 제시했습니다. 이는 Self-Talk에서 시도했으나 성공하지 못한 다중 홉 추론을 개선합니다.[6][7]

**4) 일반화 성능 향상:**
최근 연구(2024)는 LLM의 지식 학습에서 **일반화의 중요성**을 강조합니다. 동일 답변의 다양한 질문 형식에 대한 일반화 능력을 높이기 위해 형식 기반 데이터 증강(formatting-based data augmentation)과 뾰족함 인식 최소화(Sharpness-Aware Minimization)를 제안했습니다.[8]

### 6. 앞으로 연구 시 고려할 점

**1) 사실성과 정당화 일치성:**
Self-Talk의 주요 발견 중 하나는 **모델이 정확한 답변을 도출했어도 해당 추론 과정이 인간의 관점에서 타당하지 않을 수 있다는 점**입니다. 향후 연구는 다음을 고려해야 합니다:[9][1]

- 모델의 추론 과정이 인간의 상식과 일치하는지 평가하는 메커니즘
- 단순 패턴 매칭이 아닌 진정한 상식 추론인지 검증하는 방법

**2) 문맥 기반 지식 기반 개선:**
정적 지식 베이스의 한계를 극복하기 위해 다음이 필요합니다:[9]

- 구조화된 평가 방식 개발
- 문화 적응형 상식 추론 모델
- 인터랙티브 컨텍스트에서의 적용

**3) 노이즈 지식 처리:**
생성된 명확성 중 상당 부분이 노이즈임을 고려할 때:[4]

- 명확성의 신뢰도를 추정하는 메커니즘
- 유용한 지식과 해로운 지식을 구분하는 필터링 기법
- 내부 성찰(introspection) 능력 - "모르는 것이 무엇인지 아는 능력"

**4) 벤치마크와 평가 방식 개선:**
Commonsense Reasoning 분야는 벤치마크 설계 시 다음을 고려해야 합니다:[9]

- 진정한 상식 추론 능력 평가 (표면적 패턴이 아닌)
- 도메인 간 일반화 능력 검증
- 인간-모델 간의 추론 투명성 격차 분석

**5) 프롬프팅 기반 지식 생성 확장:**
Self-Talk는 고정된 템플릿을 사용하는데, 향후 연구는:[9]

- 동적 템플릿 생성
- 다국어 상식 추론 지원
- 대화형 증강(dialogue-based augmentation)

**결론:**

Self-Talk는 사전 학습된 언어 모델에서 명시적으로 상식 지식을 추출하는 혁신적인 접근을 제시했으나, 생성된 지식의 품질, 사실성의 보장, 그리고 깊이 있는 다중 홉 추론 능력에서는 여전히 개선이 필요합니다. 최근 연구들은 이러한 한계를 자기 피드백, 검색 증강, 구조화된 지식 그래프 활용, 그리고 일반화 능력 강화를 통해 극복하고 있으며, 향후 연구는 모델 추론의 투명성, 사실성 검증, 그리고 대규모 언어 모델 시대에 맞는 적응형 접근에 초점을 맞춰야 합니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/de0118b1-23cb-4e41-af55-6b1199c3e687/2004.05483v2.pdf)
[2](https://aclanthology.org/2023.emnlp-main.708.pdf)
[3](https://arxiv.org/pdf/2310.04921.pdf)
[4](https://aclanthology.org/2025.emnlp-main.1203.pdf)
[5](https://arxiv.org/html/2410.05077v1)
[6](https://arxiv.org/abs/2305.05936)
[7](https://www.sciencedirect.com/science/article/abs/pii/S0957417425034219)
[8](https://arxiv.org/html/2503.03705v1)
[9](https://arxiv.org/html/2506.14040v1)
[10](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00659/2369521/tacl_a_00659.pdf)
[11](https://aclanthology.org/2023.emnlp-main.342.pdf)
[12](https://arxiv.org/html/2503.06218v1)
[13](https://arxiv.org/pdf/2302.07926.pdf)
[14](https://www.aclweb.org/anthology/D19-1282.pdf)
[15](https://arxiv.org/pdf/2307.12382v1.pdf)
[16](https://www.themoonlight.io/en/review/are-unified-vision-language-models-necessary-generalization-across-understanding-and-generation)
[17](https://aclanthology.org/2024.findings-emnlp.669.pdf)
[18](https://arxiv.org/pdf/2505.00661.pdf)
[19](https://openreview.net/pdf/62dc3f220769ac040f07c1965f76d11828d87274.pdf)
