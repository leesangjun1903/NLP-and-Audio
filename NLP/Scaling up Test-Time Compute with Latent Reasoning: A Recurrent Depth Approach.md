# Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach

### 1. 핵심 주장과 주요 기여

#### 1.1 핵심 주장

논문의 중심 주장은 **언어 모델이 토큰 생성을 통한 명시적 추론이 아닌, 잠재 공간에서의 암묵적 반복 계산을 통해 테스트 시간 컴퓨팅을 확장할 수 있다**는 것입니다. 이는 기존의 Chain-of-Thought 기반 접근법과 근본적으로 다른 패러다임을 제시합니다.[1]

#### 1.2 주요 기여

**아키텍처 혁신**[1]
- 3.5B 파라미터 모델이 50B 파라미터 모델에 상당하는 계산 부하까지 성능 개선 가능
- 비토큰 기반 스케일링으로 메모리 효율 극대화
- 특화된 훈련 데이터 불필요 (표준 사전학습 데이터로 학습 가능)

**일반화 성능**[1]
- ARC, GSM8K, OpenBookQA 등 추론 벤치마크에서 획기적 개선
- 컨텍스트 윈도우 크기에 관계없이 작동 가능
- 단어로 표현하기 어려운 추론 유형 포착 가능

***

### 2. 문제 정의 및 제안 방법

#### 2.1 해결하는 문제

기존 테스트 시간 스케일링 방법의 한계:

1. **Chain-of-Thought의 비효율성**: 긴 맥락 윈도우 필요, 토큰 생성 비용 높음[1]
2. **매개변수 스케일링의 한계**: 데이터와 계산 요구량 극대화
3. **명시적 추론의 제약**: 공간적 사고, 물리적 직관 등 언어화 불가능한 추론 표현 불가

#### 2.2 제안 방법: 재귀 깊이 아키텍처

**모델 구조**[1]

모델은 세 가지 함수 블록으로 구성됩니다:

$$P: \text{Prelude (입력 임베딩)}, \quad R: \text{Recurrent Block (반복 계산)}, \quad C: \text{Coda (출력 생성)}$$

기본 연산 프로세스:

$$e = P(x)$$

$$s_0 \sim \mathcal{N}(0, \sigma^2 I_{n \cdot h})$$

$$s_i = R(e, s_{i-1}) \quad \text{for} \quad i \in \{1, \ldots, r\}$$

$$p = C(s_r)$$

여기서 $e$는 입력 임베딩, $s_i$는 $i$번째 반복의 잠재 상태, $r$은 재귀 반복 횟수입니다.[1]

**핵심 설계 원리**[1]

1. **입력 주입 (Input Injection)**: 매 반복 단계마다 임베딩 $e$를 주입하여 안정적 수렴 유도
2. **적응형 초기화**: $s_0$를 무작위로 샘플링하여 경로 독립성(Path Independence) 달성
3. **어댑터 기반 통합**: 현재 상태와 입력을 연결하는 어댑터 $A: \mathbb{R}^{2h} \to \mathbb{R}^h$

**정규화 전략 (Sandwich Architecture)**[1]

안정적 재귀 훈련을 위해 다음 형식의 정규화 층 배치:

$$\hat{x}_l = n_2(x_{l-1} + \text{Attn}(n_1(x_{l-1})))$$

$$x_l = n_4(\hat{x}_l + \text{MLP}(n_3(\hat{x}_l)))$$

이 설계는 소규모에서는 차이가 미미하지만, 대규모에서는 필수적입니다.[1]

#### 2.3 훈련 목적 함수

**재귀적 언롤링 (Recurrent Unrolling)**[1]

훈련 목표는 무작위 샘플링된 반복 횟수에 대한 손실 함수의 기댓값:

$$\mathcal{L}(\theta) = \mathbb{E}_{x \in \mathcal{X}} \mathbb{E}_{r \sim \Lambda} \mathcal{L}(m_\theta(x, r), x')$$

**반복 횟수 분포**[1]

로그정규 포아송 분포를 사용하여 반복 횟수 샘플링:

$$\tau \sim \mathcal{N}(\log(\bar{r}) - \frac{1}{2}\sigma^2, \sigma)$$

$$r \sim P(e^\tau) + 1$$

여기서 $\bar{r}$은 목표 평균 반복 횟수(본 연구에서 32), $\sigma = 1/2$입니다.[1]

**잘린 역전파 (Truncated Backpropagation)**[1]

메모리 효율성을 위해 마지막 $k=8$ 반복만 역전파:

- Prelude 블록은 매 단계마다 기울기 업데이트 (입력 $e$ 주입이 매 단계에서 발생하므로)
- 메모리 사용량이 반복 횟수에 관계없이 고정

***

### 3. 모델 구조 상세

#### 3.1 거시적 구조 (Macroscopic Design)

**계층 구성 (l_P, l_R, l_C)**[1]
- **소규모 모델**: (1, 4, 1) 구조, 은닉 크기 $h = 1024$
- **대규모 모델**: (2, 4, 2) 구조, 은닉 크기 $h = 5280$

대규모 모델의 유효 깊이:
- 실제 파라미터: 약 3.5B
- 반복 깊이 $r=32$일 때 유효 깊이: $2 + 4 \times 32 + 2 = 132$ 층[1]

#### 3.2 미시적 구조 (Microscopic Design)

**표준 트랜스포머 블록**[1]
- **자기 주의**: RoPE 임베딩(기저 50000) 포함
- **활성화**: 게이트된 SiLU MLP
- **정규화**: RMSNorm

**초기화 전략**[1]

대규모 훈련을 위한 Takase 외 (2024) 초기화:

$$\sigma_h^2 = \frac{2}{5h}$$

출력 층 분산:
$$\sigma_{\text{out}}^2 = \frac{1}{5h \cdot l}, \quad l = l_P + \bar{r}l_R + l_C$$

본 모델에서 $l = 132$로 설정되어 출력 층이 작은 값으로 초기화됨.[1]

***

### 4. 성능 향상 결과

#### 4.1 표준 벤치마크 성능[1]

| 벤치마크 | 우리 모델 (r=32) | OLMo-7B | OLMo-2-1124-7B | 개선도 |
|---------|-----------------|---------|---------------|--------|
| ARC-E   | 69.91%         | 68.81%  | 82.79%        | +1.1pp |
| ARC-C   | 38.23%         | 40.27%  | 57.42%        | -2.0pp |
| HellaSwag | 65.21%       | 75.52%  | 80.50%        | -10.3pp |
| MMLU    | 31.38%         | 28.39%  | 60.56%        | +3.0pp |

#### 4.2 수학 및 코딩 성능[1]

**GSM8K (수학 추론)**
- 유연한 추출: 38.13% (우리 모델, r=32 + 시스템 프롬프트)
- OLMo-2의 66.79% 대비 상당한 격차
- 그러나 대규모 모델 훈련에도 불구하고 강력한 성능 시연

**HumanEval (코딩)**
- 우리 모델 (r=32): 23.17%
- OLMo-2-1124-7B: 10.36%
- **전문 코드 모델(StarCoder2-7B: 31.70%) 제외 모든 범용 모델 우월**[1]

#### 4.3 테스트 시간 스케일링 효과[1]

반복 횟수 증가에 따른 성능:

| 태스크 | r=1 | r=4 | r=8 | r=16 | r=32 | r=64 |
|------|-----|-----|-----|------|------|------|
| ARC-C | 24.06% | 30.55% | 33.32% | 35.60% | 38.23% | ~38% |
| GSM8K CoT | 0.00% | 4.12% | 8.24% | 18.56% | 38.13% | 40%+ |
| HumanEval | 4.88% | 12.81% | 16.34% | 20.49% | 23.17% | ~24% |

**핵심 통찰**:
- 어려운 태스크(GSM8K)는 더 많은 반복 활용
- 쉬운 태스크(OpenBookQA)는 8-12 반복에서 수렴[1]

***

### 5. 일반화 성능 향상 분석

#### 5.1 재귀 vs 비재귀 비교[1]

**동일한 훈련 설정에서의 비교 (180B 토큰)**

| 모델 | r=1 | r=32 | GSM8K CoT |
|-----|-----|------|-----------|
| 고정 깊이 기준선 | 46.42% (ARC-E) | - | 1.82%/2.20% |
| 우리 모델 초기 | 34.01% (r=1) | 53.62% | 9.02%/10.24% |
| 차이 | -12.4pp | +7.2pp | +7.2pp |

**중요 발견**: r=1에서 고정 깊이 모델보다 성능 저하 → **재귀 특화 구조가 명시적으로 필요함**[1]

#### 5.2 컨텍스트 적응형 수렴[1]

ARC-C 성능과 few-shot 예제 개수의 관계:

$$\text{포화점} \propto \text{컨텍스트 정보량}$$

- 0-shot: ~8-12 반복에서 수렴 (25%→28%)
- 1-shot: ~20 반복에서 수렴 (28%→33%)
- 25-50 shot: 32 반복 활용 (33%→38%)[1]

**해석**: 모델이 더 많은 정보 처리 필요시 자동으로 더 깊은 재귀 활용

#### 5.3 일반화 능력의 메커니즘[1]

**폐쇄형 vs 개방형 QA 성능 비교**

| 모델 | 폐쇄형 | 개방형 | 차이 |
|-----|--------|--------|------|
| 우리 모델 (r=32) | 38.2% | 49.2% | +11.0pp |
| OLMo-2-1124 | 46.2% | 53.4% | +7.2pp |

**해석**: 작은 모델 크기로 인해 사실 기억이 적지만, **컨텍스트 추론 능력이 우수**함을 의미[1]

***

### 6. 잠재 공간의 계산 패턴 (Emergent Behaviors)

#### 6.1 경로 독립성 (Path Independence)[1]

모델이 서로 다른 초기 상태 $s_0$에서 시작해도 유사한 궤적을 따름:

- 최종 상태가 초기 조건에 독립적
- 이는 안정적인 고정점 수렴을 시사

#### 6.2 잠재 공간의 기하학적 패턴[1]

**수렴 (Convergence)**
- 많은 토큰이 고정점으로 수렴
- 특히 자명한 답변이 필요한 경우

**궤도 운동 (Orbits)**
- 수치 추론이 필요한 토큰에서 다차원 궤도 형성
- PCA 상의 여러 방향에서 순환 패턴 관찰[1]

예: 수학 문제의 "3" 토큰
$$\text{Token "3"에서 관찰된 궤도 패턴}$$
- PC1-PC2 평면에서 원형 운동
- PC3-PC4, PC5-PC6에서도 유사한 패턴
- 산술 연산 구현에 유사 (Nanda et al., 2022)[1]

**슬라이더 (Sliders)**
- 특정 방향으로 지속적인 드리프트
- 모델이 반복 수를 "세기" 위한 메커니즘으로 해석[1]

#### 6.3 맥락 의존적 수렴[1]

그림 11 분석에서 관찰:

$$||s_i - s^*|| \text{ (i번째 반복과 수렴점 거리)}$$

- "어려운" 질문 부분: 느린 수렴
- "쉬운" 부분: 빠른 수렴
- 같은 토큰이라도 문맥에 따라 다른 수렴 속도

***

### 7. 모델의 한계

#### 7.1 성능 한계[1]

1. **절대 성능 갭**
   - OLMo-2-1124 대비 여전히 상당한 차이 (예: MMLU 60.56% vs 31.38%)
   - 모델이 단일 훈련 실행 (파이프 라인 최적화 부족)

2. **산술 작업의 제한**
   - 다중 자릿수 덧셈에서 제한적 성능
   - 3-4 피연산자 이상에서 급격한 성능 저하[1]

#### 7.2 해석 가능성 문제[1]

1. **블랙박스 추론**: 잠재 공간 계산이 인간 이해도와 거리 있음
2. **감시 어려움**: Chain-of-Thought 대비 모니터링 및 검증 복잡
3. **투명성 부재**: "생각 과정"이 명시적으로 표현되지 않음

#### 7.3 훈련 복잡성[1]

1. **초기화 민감도**: 특정 초기화 체계 필수 (scale 의존적)
2. **정규화 요구**: 소규모에서 불필요한 Sandwich 정규화가 대규모에서 필수
3. **하이퍼파라미터**: 학습률 등이 매우 민감함 (4×10⁻⁴ 실패, 4×10⁻⁵ 성공)[1]

#### 7.4 계산 비용[1]

- 테스트 시간 반복이 추론 비용 증가
- 간단한 질문에도 많은 반복 가능성
- 적응형 중지가 필수적이나 신뢰성 미검증

***

### 8. 일반화 성능 향상의 핵심 메커니즘

#### 8.1 "생각 중심" 사전학습의 이점[1]

기존 모델: 메모리 저장소 중심 설계
$$\text{파라미터 수} \propto \text{사실 기억 용량}$$

재귀 모델: 계산 능력 중심 설계
$$\text{파라미터 수} \propto \text{추론 심도}$$

결과: 동일 파라미터에서 더 나은 **일반화** (기억이 아닌 문제 해결)

#### 8.2 few-shot에서의 우월성[1]

**개방형 QA (관련 사실 제공)에서 +11.0pp 개선**

- 폐쇄형: 39.2% (OLMo-2) vs 38.2% (우리 모델)
- 개방형: 53.4% (OLMo-2) vs 49.2% (우리 모델)

→ **정보가 주어지면 추론 능력으로 보상** 가능

#### 8.3 작업 난이도에 따른 동적 적응[1]

$$\text{포화 반복 수} = f(\text{작업 복잡도}, \text{컨텍스트 크기})$$

- 단순 회상: r=4~8 충분
- 추론: r=16~32 필요
- 복합 추론: r=64까지 개선

→ **암묵적 작업 난이도 분석**

***

### 9. 최신 연구 동향 및 향후 고려사항

#### 9.1 Test-Time Compute Scaling의 최신 발전[2][3][4]

**패러다임의 변화**: 2024-2025년 연구는 테스트 시간 스케일링을 명시적 추론의 세 번째 축으로 확립[2]

1. **Parallel vs Sequential Scaling**[5]
   - 순차적 스케일링 (CoT 연장): 점감 수익 보임
   - 병렬 스케일링 (다중 시도 + 투표): 더 견고한 개선[5]

2. **효율성 개선**: IBM의 Granite-3.2 사례[6]
   - 8B 모델이 추론 스케일링으로 GPT-4o, Claude-3.5 능가
   - MATH500에서 60% 이상 성능 개선[6]

#### 9.2 잠재 추론의 연구 기회[7]

**미래 아키텍처 통합**: 
1. **MoE와 결합**: "계산 중심" (재귀)과 "메모리 중심" (MoE) 상보성[1]
2. **선형 주의 메커니즘**: 반복 깊이로 모든 필요한 비교 가능[1]
3. **다중 재귀 단계**: 여러 연속적 재귀 블록 적용[1]

#### 9.3 후속 연구 시 고려할 점

**1. 데이터 및 훈련 최적화**[1]
- 더 신중한 데이터 혼합 (본 연구: 단일 실행만 가능)
- 학습률 쿨다운 스케줄 적용
- 반복 횟수 분포 최적화

**2. 추론 메커니즘 분석**[1]
- 토큰 궤적의 더 깊은 해석
- 계산 패턴과 작업 특성의 관계
- 동적 중지 기준 개선

**3. 실제 배포 최적화**
- 적응형 컴퓨팅 메커니즘 검증
- 메모리-계산 트레이드오프 분석
- 정규 하드웨어에서의 효율성[8]

**4. 일반화 능력 강화**
- 산술 성능 개선 메커니즘 연구
- 다국어 및 도메인 특화 일반화
- 안전성 및 편향 완화[1]

***

### 10. 결론 및 임팩트

#### 10.1 논문의 학술적 기여

이 논문은 **테스트 시간 컴퓨팅 스케일링의 새로운 패러다임**을 제시합니다:[1]

1. **매개변수 스케일링**의 대안으로 재귀 깊이 제시
2. **명시적 추론**의 한계를 초월한 잠재 공간 계산
3. 특화되지 않은 사전학습에서의 **자발적 추론 능력 emergence**

#### 10.2 산업적 영향[8]

**기업 AI 배포에 미치는 영향**:
- 소규모 모델의 추론 능력 극대화 → 비용 절감
- 지역 배포 가능성 증대 (파라미터 수 적음)
- 하드웨어 요구사항 완화 (메모리 대역폭 효율 40-50%)[1]

#### 10.3 향후 연구 방향

**근기(Near-term)**:
- MoE와의 하이브리드 아키텍처 연구
- 대규모 훈련 재현 및 최적화
- 다중 언어 및 도메인 검증

**중기(Medium-term)**:
- 신경망 과학의 통찰 활용
- 선형 주의와 재귀 깊이의 결합
- 강화학습과의 통합

**장기(Long-term)**:
- 인간 수준의 적응형 추론 능력
- 다중 모달리티 지원 확장
- 수학적 기초를 통한 수렴성 보장

***

### 참고: 수식 정리

**재귀 블록의 핵심 연산**:
$$s_i = R(e, s_{i-1}), \quad i \in \{1, \ldots, r\}$$

**손실 함수**:

$$\mathcal{L}(\theta) = \mathbb{E}_{x \in \mathcal{X}} \mathbb{E}_{r \sim \Lambda} \mathcal{L}(m_\theta(x, r), x')$$

**반복 횟수 분포**:

$$\tau \sim \mathcal{N}\left(\log(\bar{r}) - \frac{1}{2}\sigma^2, \sigma^2\right), \quad r \sim P(e^\tau) + 1$$

**Sandwich 정규화**:

$$\hat{x}_l = n_2(x_{l-1} + \text{Attn}(n_1(x_{l-1})))$$

$$x_l = n_4(\hat{x}_l + \text{MLP}(n_3(\hat{x}_l)))$$

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/7223e92b-05bd-4f8b-b143-69d1dc3c30e1/2502.05171v2.pdf)
[2](http://arxiv.org/pdf/2501.19306.pdf)
[3](http://arxiv.org/pdf/2411.19477.pdf)
[4](http://arxiv.org/pdf/2403.08540v1.pdf)
[5](https://aclanthology.org/2025.acl-long.232.pdf)
[6](https://research.ibm.com/blog/inference-scaling-reasoning-ai-model)
[7](https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-latent)
[8](https://www.redhat.com/ko/blog/smarter-enterprise-ai-inference-time-scaling)
[9](https://arxiv.org/abs/2502.05171)
[10](https://arxiv.org/pdf/2504.00810v1.pdf)
[11](http://arxiv.org/pdf/2502.12215.pdf)
[12](https://arxiv.org/pdf/2204.02311.pdf)
[13](https://arxiv.org/html/2503.23803v2)
[14](https://arxiv.org/abs/2408.03314)
[15](https://arxiv.org/abs/2504.02495)
[16](https://arxiv.org/abs/2411.19477)
[17](https://ajithp.com/2025/02/14/latent-reasoning-the-next-evolution-in-ai-for-scalable-adaptive-and-efficient-problem-solving/)
[18](https://huggingface.co/papers/2502.05171)
[19](https://www.emergentmind.com/topics/test-time-scaling-law)
