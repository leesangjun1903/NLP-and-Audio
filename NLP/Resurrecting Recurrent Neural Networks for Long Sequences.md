# Resurrecting Recurrent Neural Networks for Long Sequences

## 1. 핵심 주장 및 주요 기여 요약

이 논문의 핵심 주장은 **딥 RNN(순환 신경망)이 최근 주목받는 SSM(State Space Model, 상태공간모델) 계열 모델(S4 등)과 본질적으로 유사한 구조적 특성을 가진다는 점**에 착안, **신중한 설계와 파라미터화, 정규화 기법을 통해 RNN도 SSM 수준의 롱 시퀀스 처리 성능과 효율성을 도달할 수 있다는 것**입니다.  
저자들은 기존 RNN의 성능 한계(특히 장기 의존성 추론 문제)를 극복하기 위해 일련의 개선(선형화, 대각화, 안정적 매개변수화, 정규화)을 체계적으로 분석, **Linear Recurrent Unit(LRU)**라는 새로운 RNN 블록을 제안합니다.  
결론적으로 LRU 기반 RNN은 Long Range Arena(LRA) 벤치마크에서 S4 계열 SSM과 동등한 성능과 효율성을 달성할 수 있음을 실증적으로 보여줍니다.[1]

## 2. 해결하고자 하는 문제, 제안 방법(수식 포함), 모델 구조, 성능 및 한계

### (1) 해결하려는 문제와 동기

- 전통적 RNN은 장기 의존성 추론 시 **그래디언트 소실/폭발 문제**로 학습이 어렵고, 비효율적 순차 처리 특성(직렬성) 때문에 대규모/장시퀀스 문제에서 한계가 있었음.
- 반면, S4 같은 SSM 계열은 **병렬 학습 효율성과 롱시퀀스 처리**에서 혁신을 보여주었으나, 실제 성능 향상이 **복잡한 파라미터화, 초기화, 이론적 기반(HIPPO 등)**에 기인하는지 불분명.
- 저자들은 "본질적으로 SSM이 RNN과 매우 닮아 있다면, RNN도 설계를 개선해 롱시퀀스 문제에서 충분히 강력해질 수 있지 않은가?"라는 질문을 던짐.

### (2) 제안 방법 요지 (LRU: Linear Recurrent Unit)

- **선형화(linearization)**: RNN 순환 블록의 비선형성(예:tanh)을 제거, $$h_{t+1} = Ah_t + Bx_t $$ 처럼 **순수 선형 반복구조**만 사용.  
  - 깊은 네트워크 구성에서 선형 RNN + MLP(또는 GLU) 비선형 블록을 적절히 조합해 **표현력** 손실 없이 오히려 학습 안정성과 병렬성, 장기 기억 능력 향상.
- **대각화(diagonalization)**: 순환 행렬(transition matrix)을 대각 형태로 변환, 결과적으로 각 히든 차원이 독립적으로 순환.  
  - 파라미터/초기화의 분산 제어가 쉬워지고, 반복 연산을 빠르게 병렬화할 수 있음.
- **안정적 지수 매개변수화(stable exponential parameterization)**:  
  - 대각 원소의 크기를 $$ |\lambda| \leq 1 $$로 제어해 그래디언트 폭발/소실을 막고, 장기 의존성 정보를 잘 보존하도록 조절.  
  - $$\Lambda = \exp(-\exp(\nu_{\log}) + i \exp(\theta_{\log})) $$ 형태로 파라미터화.
- **정규화(normalization)**:  
  - 시퀀스가 길수록 forward pass에서 hidden activation의 크기가 발산할 수 있으므로 각 시점마다 적응형 정규화 파라미터(로그 스케일)를 도입해 안정화.
- **초기화**:  
  - 고리(ring) 형태(복소평면에서 단위원 가까이)의 eigenvalue 분포로 고정, 초기에 장기 의존성 보존을 유도.
- **전체 구조**:  
  - [Norm - Linear 재귀 블록(LRU) - GLU - Skip 연결]로 구성된 residual block을 N개 적층
  - 최종 구조는 기존 S4/S5 등과 유사한 높은 딥 네트워크 형태.

### (3) 주요 수식 및 구조적 요약

- **LRU recurrence**:  

$$
  h_{t+1} = \lambda \odot h_t + B u_t
  $$
  
  - $$\lambda $$: 복소 대각 행렬, 각 원소는 $$|\lambda_k| \leq 1 $$ 유지  
  - $$B $$: 입력 투영 행렬, $$u_t $$: 입력

- **정규화 및 지수 매개변수화**:  

$$
  \lambda_k = \exp(-\exp(\nu_{k,log})) \cdot e^{i\exp({\theta_{k, log}})}
  $$
  
  - $$\nu_{k,log} $$, $$\theta_{k,log} $$: 학습가능 파라미터  
  - hidden activation에 log scale normalization 곱함.

### (4) 성능 평가

- **Long Range Arena(LRA) 벤치마크** 각종 롱시퀀스 태스크(텍스트, 리스트 연산, 이미지, 패스파인더 등)에서 S4, S5 등 최신 SSM 계열과 **동등한 성능 달성**  
- **학습 속도, 효율성** 동일 수준(병렬성, GPU utilization, 메모리 효율 모두 우수)
- **추론 속도** 전통적 RNN과 유사하게 긴 시퀀스에서도 뛰어난 효율
- **표현력** Deep linear RNN + MLP 구조가, 이론적으로 Koopman operator(연산자 분해 관점)로 비선형 동역학도 충분히 근사 가능함을 증명.

### (5) 한계점 및 고려사항

- 완전히 비지도 상황, 자동 시퀀스 라벨링 등에서는 여전히 한계 가능성
- RNN 내장적 직렬성/병렬화 잠재력에도 불구, 하드웨어·프레임워크 수준 병렬화(예: TPU, 여러 GPU 모드) 고려 필요
- 극한 긴 시퀀스에서 S4/S5 특유의 해석적 성질(continuous-time ODE와 연결)이 추후 의미 있을 수 있음

## 3. 모델의 일반화 성능 관점에서의 분석

- LRU는 S4와 달리 엄격한 연속적 시스템 이론(ODE 해석, HiPPO 구조)에 의존하지 않으며, 표준 딥러닝 커뮤니티에서 검증된 signal propagation, 적절한 파라미터화, 초기화 전략으로 **일반화 성능**을 확보.
- 실험적으로 복잡한 초기화 없이도, Ring initialization과 정규화(패턴)만 조절한 LRU는 LRA 과제 대부분에서 S4/S5만큼 **루틴하게 일반화**됨.[1]
- Koopman operator 이론적 분석을 통해, 충분히 깊은 linear RNN stack+MLP에서 복잡한 비선형 동역학도 근사 가능한 "표현력의 일반성"도 타당하게 뒷받침.

## 4. 향후 연구 영향과 고려사항

- **향후 연구에 미치는 영향**  
  - 본 논문은 “롱시퀀스 문제 = Transformer or SSM”이라는 기존 대세에 문제를 제기, RNN 구조의 부활과 최적화 여지를 잘 보여줌.  
  - SSM과 RNN계 보완적 융합·해석의 폭이 넓어지고, 롱시퀀스 및 메모리 기반 구조 설계에 더 다양한 선택지가 열림.  
  - 간결한 구조/초기화/정규화로도 SOTA 근접이 가능하다는 시사점 제공.  
  - 실제 deploy 측면에서 추론 비용, 하드웨어 효율이 중요한 환경(예: 임베디드, 실시간 처리)에서 RNN 구조가 다시 유력한 대안이 될 수 있음.
- **향후 연구시 고려점**  
  - 지속적/더 긴 시퀀스, 멀티 모달 시퀀스 등에 대한 확장/검증 필요  
  - SSM/S4 등이 제공하는 continuous-time 해석적 해의 장점과 LRU류의 구조적 단순화·효율성의 조화 가능성  
  - 실제 현업적용시 하드웨어 가속기 프렌들리화, 분산병렬 학습, OOD 일반화 성능 등 추가적인 엔지니어링 필요

***

**참고:**  
본 요약은 논문 전체 내용 및 도식, 부록까지 참고하여 작성되었으며, LRA 실험 상세, 파라미터화 공식, normalization 및 수식적 설명 등은 논문 본문과 부록의 수학적 근거에 근거합니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4c286043-ba9c-4370-9e19-1b12d84766ad/2303.06349v1.pdf)
