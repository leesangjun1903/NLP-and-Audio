# Recipes for building an open-domain chatbot

## 1. 핵심 주장과 주요 기여

이 논문은 **대규모 신경망 모델의 매개변수 크기와 데이터 규모 확대만으로는 고성능 챗봇을 구축하기 어렵다**는 점을 제시하며, 고품질 대화형 AI를 위한 "레시피"를 제시한다. 핵심 기여는 다음과 같다.[1]

**1) 기술 혼합 (Blending Skills)**: 개인적 특성, 감정 이입, 지식이라는 세 가지 대화 기술을 통합하는 것이 중요함을 증명했다. 특히 Blended Skill Talk(BST) 데이터셋을 활용한 미세조정을 통해 작은 모델이 거대 모델의 성능을 능가할 수 있음을 보였다.[1]

**2) 생성 전략의 중요성**: 동일한 복잡도의 모델이라도 디코딩 알고리즘 선택에 따라 성능이 크게 달라짐을 입증했다. 특히 빔 서치(beam search)의 최소 길이 제약이 응답의 "지루함 대 재미있음" 스펙트럼을 효과적으로 제어한다는 점을 발견했다.[1]

**3) 공개 모델과 코드 배포**: 90M, 2.7B, 9.4B 매개변수 모델을 모두 공개하여 재현성과 추가 연구를 가능하게 했다.[1]

***

## 2. 문제 설정, 제안 방법, 모델 구조 및 성능

### 문제 설정

개방형 도메인 챗봇(open-domain chatbot)은 제한된 주제 범위를 벗어나 다양한 주제에서 자연스럽고 매력적인 대화를 나눌 수 있어야 한다. 기존 연구들은 규모 확대의 중요성을 강조했으나, 실제 대화 품질 향상에 필요한 다른 요소들을 간과했다.[1]

### 제안된 방법

#### 2.1 모델 아키텍처

논문에서는 **세 가지 유형의 모델**을 비교 검토한다.[1]

**검색 기반 모델 (Retrieval)**: Poly-encoder 아키텍처를 사용하며, 문맥(context)의 전역 특성을 다중 표현(n codes, n=64)으로 인코딩한다.[1]

$$
\text{점수}(\text{맥락}, \text{응답}) = \text{주의 메커니즘}(\text{응답}, \text{다중 코드})
$$

**생성 기반 모델 (Generative)**: 표준 Seq2Seq Transformer 구조로, 4-32층의 인코더-디코더 구성이다. 9.4B 모델은 4층 인코더, 32층 디코더, 4096차원 임베딩, 32개 어텐션 헤드를 가진다.[1]

**검색-재구성 모델 (Retrieve-and-Refine)**: 대화 검색 또는 지식 검색을 먼저 수행한 후 생성 모델이 이를 조건으로 활용한다.[1]

#### 2.2 훈련 목표 함수

**최대 가능도 추정 (Maximum Likelihood Estimation, MLE)**:[1]

```math
L_{\text{MLE}}^{(i)}(p_\theta, x^{(i)}, y^{(i)}) = -\sum_{t=1}^{|y^{(i)}|} \log p_\theta(y_t^{(i)} | x^{(i)}, y_{ < t}^{(i)})
```

여기서 $$x^{(i)}$$는 문맥, $$y^{(i)}$$는 정답 응답, $$y_t^{(i)}$$는 t번째 토큰이다.

**그릿-앤-레파인 (α-blending)**: 검색된 응답이 무시되는 문제를 해결하기 위해, α% 확률로 정답 응답으로 치환한다.[1]

**우도 부정 손실 (Unlikelihood Loss)**:[1]

$$
L_{\text{UL}}^{(i)}(p_\theta, C_{1:T}, x, y) = -\sum_{t=1}^{|y|} \sum_{y_c \in C_t} \log(1 - p_\theta(y_c | x, y_{ < t}))
$$

$$
L_{\text{ULE}}^{(i)} = L_{\text{MLE}}^{(i)} + \alpha L_{\text{UL}}^{(i)}
$$

여기서 $$C_t$$는 반복되거나 과다 표현된 토큰들의 집합이다. 이 방법은 모델이 인간 분포와 불일치하는 토큰을 덜 생성하도록 유도한다.[1]

#### 2.3 디코딩 전략

**최소 길이 제약**: 응답 길이 제어의 핵심 요소로, 최소 20개 BPE 토큰 제약이 최적임을 발견했다.[1]

**응답 길이 예측**: 인간-인간 대화 데이터를 기반으로 길이를 4개 클래스로 분류하는 분류기를 훈련한다.

**부분수열 차단 (n-gram blocking)**: 생성 중 3-gram 반복을 차단하여 단조로움을 줄인다.[1]

### 성능 향상

인간 평가(ACUTE-Eval)를 통해 논문의 최고 성능 모델들이 기존 최고 수준의 모델인 Meena를 능가함을 입증했다.[1]

| 평가 지표 | 우리 모델 vs Meena | 통계 유의성 |
|---------|-----------------|----------|
| 참여도 (Engagingness) | 75% vs 25% | p < 0.01 |
| 인간다움 (Humanness) | 65% vs 35% | p < 0.01 |

특히 2.7B 매개변수 모델이 9.4B 모델보다 참여도에서 우수한 성능을 보였으며, 이는 복잡도와 성능의 단순 상관관계를 부정한다.[1]

***

## 3. 일반화 성능 향상 가능성

### 현재의 한계

논문에서 명시적으로 지적하는 **일반화 성능의 제약**은 다음과 같다.[1]

**문맥 길이의 제약**: 모델이 128개 BPE 토큰으로 제한되어 장기 대화에서 일관성을 유지하기 어렵다. 이는 사용자가 이전 대화 내용을 언급할 때 모델이 이해하지 못하는 문제로 나타난다.[1]

**편협한 평가 설정**: 14턴의 짧은 다중 기반 대화 평가는 장기 대화에서의 반복, 지루함, 지식 환각 등의 문제를 드러내지 못한다.[1]

**얕은 이해**: 모델이 "하버드(Harvard)"와 "건초(hay)"의 언어유희를 이해하지 못하는 예시에서 보듯이, 표면적 지식은 풍부하나 진정한 개념 이해는 부족하다.[1]

### 제안된 개선 방향

**지식 검색 활용**: Wizard of Wikipedia 작업에서 훈련된 검색-재구성 모델은 충분한 평가 설정에서 더 정확한 정보를 제공할 수 있다. 그러나 현재 평가에서는 개선이 뚜렷하지 않았다.[1]

**우도 부정 손실 (Unlikelihood Training)**: 반복적 표현을 억제하여 장기 대화에서의 일관성을 개선할 수 있으나, 단기 평가에서는 효과가 제한적이다.[1]

**더 세부적인 개인 정보 활용**: 현재의 2문장 페르소나만으로는 장기 일관성을 보장하기 어려우며, 더 풍부한 페르소나 정보가 필요하다.[1]

***

## 4. 최신 연구 기반의 영향 및 고려사항

### 논문의 후속 연구에 미친 영향

**1) 지식 통합 강화**: 이 논문 이후 개방형 챗봇 연구는 **검색 증강 생성(RAG, Retrieval-Augmented Generation)**을 강화하는 방향으로 진화했다. 최신 연구들은 지식 검색이 단순히 정보 정확성뿐 아니라 장기 일관성 유지의 핵심이라는 점을 강조한다.[2][3]

**2) 장기 대화 능력 강화**: 최신 2025년 연구들은 **매우 긴 대화(very long-term conversation)**에서의 일반화 성능을 체계적으로 평가하는 방향으로 발전했다. Transformer의 고정 컨텍스트 길이 문제를 해결하기 위해 Longformer와 같은 장기 컨텍스트 모델이 보편화되었다.[3]

**3) 사용자 지향 개인화**: 2025년 최신 연구(UPC: User-oriented Proactive Chatbot)는 단순한 응답 생성을 넘어 **사용자 배경과 관심사를 능동적으로 탐색하고 회화를 인도하는 능력**을 챗봇에 요구한다. 이는 논문의 "기술 혼합" 개념을 더욱 발전시킨 것으로 볼 수 있다.[4]

**4) 제로샷 및 퓨샷 일반화**: InstructDial 같은 최신 연구는 **59개의 대화 데이터셋과 48개의 작업에 걸쳐 일반화**하는 명령어 기반 미세조정 방식을 제안한다. 이는 블렌디드 스킬 트레이닝의 개념을 대규모로 확장한 것이다.[5]

### 앞으로의 연구 시 고려할 점

**1) 다중 턴 장기 메모리 통합**: 현재 대화 시스템은 **명시적 메모리 메커니즘(explicit memory mechanism)**을 도입해야 한다. 단순한 컨텍스트 윈도우 확대보다는 대화 이력을 요약하고 핵심 정보를 추출하는 기술이 필수적이다.[6]

**2) 평가 메트릭의 개선**: ACUTE-Eval 같은 인간 평가는 짧은 대화에서는 효과적이나, 장기 대화의 **일관성, 기억력, 사실적 정확성**을 종합적으로 측정할 수 있는 자동화된 평가 지표 개발이 필요하다.[7]

**3) 도메인 간 전이 학습**: 이 논문의 한계인 "피상적 이해"를 극복하기 위해, **인진 바이어스(inductive bias)** 기반의 구조적 일반화 방법이 연구되고 있다. 작업 명세(task specification)를 모델 아키텍처에 직접 포함시키는 접근법이 유망하다.[8]

**4) 안전성과 윤리**: 논문에서 제한적으로만 다룬 **독성 언어 완화, 성별 편향 감소, 할루시네이션 방지**에 대한 연구가 심화되어야 한다. 특히 LLM 기반 최신 챗봇은 기존 모델보다 높은 신뢰성이 요구된다.[4]

**5) 프롬프트 기반 모듈화**: ChatGPT 이후 대규모 언어모델의 등장으로, 챗봇 구축 패러다임이 **미세조정에서 프롬프팅 기반 모듈식 설계**로 전환되고 있다. 이는 계산 효율성과 유연성 측면에서 더 우수하다.[9]

***

## 결론

"Recipes for building an open-domain chatbot"은 2020년 당시 **규모 확대 중심의 패러다임을 벗어나 훈련 데이터, 디코딩 전략, 기술 통합이라는 다층적 요소들의 중요성을 최초로 종합적으로 제시한 논문**이다. 특히 작은 모델이 큰 모델을 능가할 수 있다는 발견은 이후 효율적인 대화형 AI 연구의 토대가 되었다. 그러나 현재의 LLM 시대에는 **장기 메모리 통합, 다중 소스 지식 통합, 사용자 중심의 맥락 이해** 등이 새로운 과제로 부상했으며, 이러한 방향으로의 연구 확로의 연구 확장이 계속되고 있다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b01d675d-60c5-430a-9f92-ba309789b4b0/2004.13637v2.pdf)
[2](https://arxiv.org/abs/2308.11995)
[3](https://facerain.me/paper-review-evaluating-very-long-term-conversational-memory-of-llm-agents/)
[4](https://arxiv.org/pdf/2505.12334.pdf)
[5](https://aclanthology.org/2022.emnlp-main.33.pdf)
[6](http://arxiv.org/pdf/2406.05925.pdf)
[7](https://www.confident-ai.com/blog/llm-chatbot-evaluation-explained-top-chatbot-evaluation-metrics-and-testing-techniques)
[8](https://kilthub.cmu.edu/articles/thesis/Towards_Generalization_in_Dialog_through_Inductive_Biases/28582232)
[9](https://arxiv.org/pdf/2305.04533.pdf)
[10](https://arxiv.org/abs/2108.06329)
[11](http://thesai.org/Downloads/Volume9No6/Paper_54-Open_Domain_Neural_Conversational_Agents.pdf)
[12](https://arxiv.org/html/2504.03343v1)
[13](https://arxiv.org/pdf/2303.11708.pdf)
[14](https://www.mdpi.com/2504-2289/5/4/57/pdf)
[15](https://arxiv.org/abs/2010.11853)
[16](https://arxiv.org/html/2505.09807v1)
[17](https://peerj.com/articles/cs-2979/)
[18](https://www.sciencedirect.com/science/article/pii/S1874490725002605)
[19](https://www.sciencedirect.com/science/article/pii/S2949882125001070)
