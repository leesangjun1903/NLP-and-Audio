# Large Language Models as Optimizers

### 1. 핵심 주장과 주요 기여

**"Large Language Models as Optimizers"** 논문은 **자동 미분 없이 최적화가 필요한 현실의 많은 문제들**에서 LLM을 직접적인 최적화기(optimizer)로 활용할 수 있다는 혁신적인 주장을 제시합니다.[1]

**핵심 주장:** 자연어 이해 능력을 갖춘 LLM은 형식적인 수학적 명시(formal specification) 없이도, 자연어로 기술된 최적화 문제를 반복적으로 해결할 수 있습니다. 이는 기울기 정보 없이도 문제 해결이 가능한 "흑박스(black-box) 최적화" 패러다임으로의 전환을 의미합니다.[1]

**주요 기여:**
- **OPRO(Optimization by PROmpting) 프레임워크** 제안: 메타-프롬프트에 이전 솔루션과 그 평가값을 포함시켜 반복적으로 새로운 솔루션을 생성하는 방식
- **수학 최적화 적용**: 선형회귀와 여행판매인(TSP) 문제에서 LLM의 최적화 능력 입증
- **프롬프트 최적화 성과**: GSM8K에서 인간이 설계한 프롬프트 대비 최대 8% 향상, Big-Bench Hard에서 최대 50% 향상 달성[1]

***

### 2. 문제 정의, 방법론, 모델 구조

#### 2.1 해결하고자 하는 문제

많은 실제 최적화 문제들은 **기울기(gradient) 정보를 얻을 수 없습니다**. 특히 프롬프트 최적화의 경우, 이산적(discrete) 프롬프트 공간은 매우 크고 불규칙하여 전통적인 기울기 기반 최적화 알고리즘의 적용이 불가능합니다. 동시에 프롬프트 엔지니어링은 매우 민감해서, 의미상 유사한 프롬프트도 성능이 크게 달라질 수 있습니다.[1]

#### 2.2 제안하는 방법: OPRO 알고리즘

**기본 개념:** LLM을 최적화기로 사용하되, 최적화 과정을 자연어로 기술합니다.

**반복 최적화 절차:**
각 최적화 스텝 \(t\)에서:
1. **메타-프롬프트 구성**: 이전까지의 솔루션-점수 쌍 \(\{(x_1, f(x_1)), (x_2, f(x_2)), \ldots, (x_{t}, f(x_{t}))\}\)
2. **새로운 솔루션 생성**: LLM이 메타-프롬프트를 입력받아 새로운 솔루션 \(x_{t+1}, \ldots, x_{t+k}\) 생성
3. **평가**: 생성된 솔루션들의 목적함수값 계산: \(f(x_{t+i})\)
4. **업데이트**: 평가된 솔루션과 점수를 메타-프롬프트에 추가하여 다음 스텝 준비
5. **종료**: LLM이 더 이상 개선된 솔루션을 찾지 못하거나 최대 스텝에 도달할 때까지 반복[1]

**메타-프롬프트 설계의 핵심 요소:**[1]

- **최적화 문제 설명**: 자연어로 된 목적함수 및 제약조건
- **최적화 궤적(Optimization Trajectory)**: 이전 솔루션들과 점수를 오름차순으로 정렬하여 제시
- **메타-지시사항**: 최적화 목표 설명 및 원하는 출력 형식 지정
- **예시(Exemplars)**: 작업의 특성을 보여주는 입출력 예제

#### 2.3 모델 구조

OPRO는 특정 신경망 구조를 학습하는 방식이 아니라, **기존의 사전학습된 LLM을 활용하는 프롬프팅 기반 접근**입니다.[1]

**주요 구성 요소:**

```
Meta-Prompt
├── 최적화 궤적: 솔루션-점수 쌍
├── 작업 설명: 자연어 기술 + 예시
└── 메타-지시사항: 생성 방식 지정
         ↓
    [LLM Optimizer]
    (생성 온도 τ로 조절)
         ↓
새로운 솔루션 후보들 (일반적으로 8개)
         ↓
    [Evaluator LLM]
         ↓
솔루션-점수 쌍 추가
```

**중요한 설계 선택:**[1]
- **탐색-활용 균형**: 생성 온도를 1.0으로 설정하여 기존 솔루션 주변의 적절한 탐색 수행
- **안정성 향상**: 각 스텝에서 8개의 솔루션을 병렬로 생성하여 분산 감소
- **솔루션 순서**: 점수를 낮은 순서대로 나열하면 LLM이 더 빠르게 수렴 (최근성 편향 효과)[1]

***

### 3. 성능 향상 및 실증적 결과

#### 3.1 수학 최적화 문제에서의 성능

**선형회귀 문제:**[1]
- 1차원 가중치와 절편 최적화에서, LLM(특히 GPT-4)이 흑박스 최적화로 최적해에 도달
- 최적해로부터의 거리에 따라 성능 변화: 근거리(<10)에서는 3.8-7.6단계, 원거리(>10)에서는 35.8-50.4단계 필요

**여행판매인 문제 (TSP):**[1]
- 노드 10개: 모든 모델이 최적해 발견
- 노드 15개: gpt-4는 0% 최적성 갭, 다른 모델들은 1.2-4.4%
- 노드 20개 이상: 성능 급격히 악화 (컨텍스트 윈도우 제한, 최적화 경관의 불규칙성)

**한계**: 대규모 문제에서 OPRO는 전문 솔버나 기울기 기반 방법보다 성능이 낮음[1]

#### 3.2 프롬프트 최적화에서의 성능

**GSM8K 결과:**[1]

| 모델 조합 | 프롬프트 | 정확도 |
|---------|--------|-------|
| 기준 (Kojima et al.) | "Lets think step by step." | 71.8% |
| 기준 (Zhou et al.) | 긴 지시문 | 58.8% |
| **OPRO (PaLM 2-L-IT)** | **"Take a deep breath and work on this problem step-by-step."** | **80.2%** |
| **OPRO (PaLM 2-L)** | **"Break this down."** | **79.9%** |
| OPRO (GPT-3.5) | "A little bit of arithmetic..." | 78.5% |

**Big-Bench Hard (BBH) 결과:**[1]
- 23개 작업 중 19개에서 "Lets think step by step" 대비 5% 이상 향상
- 최대 50% 향상 달성
- 평균적으로 빈 문자열 시작점 대비 20-60% 향상

**전이(Transferability) 성능:**[1]
- GSM8K에서 최적화된 프롬프트를 MultiArith에 적용: 96.8% → 96.8% (유지)
- AQuA에 적용: 54.3% (기준 44.9%보다 향상)

#### 3.3 최적화 과정의 특성

**의미상 유사 프롬프트의 성능 편차:**[1]
- "Lets think step by step." (71.8%)
- "Lets solve the problem together." (60.5%)  
- "Lets work together to solve this problem step by step." (49.4%) ← 두 프롬프트의 조합이 더 낮은 성능!

이는 프롬프트 공간의 **높은 비선형성**을 시사합니다.

**최적화 곡선의 특징:**[1]
- 일반적으로 상향 추세를 보이지만, 계단식(step-like) 점프 관찰
- 초기 단계에서 분산이 크지만, 최적화 진행에 따라 감소 경향

***

### 4. 일반화 성능 향상과 관련된 심층 분석

#### 4.1 일반화 능력 입증

**작은 훈련 집합의 효율성:**[1]
- GSM8K: 7,473개 훈련 샘플 중 3.5% (약 261개)만 사용하여 최적화
- Big-Bench Hard: 250개 샘플 중 20개만 사용
- **발견**: 이렇게 작은 집합으로 최적화된 프롬프트가 전체 테스트 세트에서도 우수한 성능 보임

**교차 작업 전이:**[1]
- GSM8K에서 최적화된 프롬프트가 MultiArith, AQuA 등 다른 수학 추론 작업에 효과적으로 전이됨
- 같은 영역의 작업들 간에 일반화 가능성 입증

#### 4.2 과적합(Overfitting) 분석

**훈련-테스트 갭:**[1]
- 훈련 정확도와 테스트 정확도 사이에 5-20% 갭 관찰
- 그러나 **상대적 순위는 유지**: 훈련 정확도가 높은 프롬프트가 테스트 정확도도 높은 경향

**이유:**
- 각 후보 프롬프트가 비슷한 정도로 과적합
- 따라서 훈련 정확도로 순위를 매겨도 테스트 성능 순위가 유사함[1]

#### 4.3 메타-프롬프트 설계의 영향

**절제 연구(Ablation Studies):**[1]

| 설계 요소 | 효과 |
|---------|------|
| **솔루션 정렬 순서** | 낮음→높음이 최적 (최근성 편향) |
| **정확도 점수 표시** | 있을 때가 없을 때보다 수렴 빠름 |
| **예시 개수** | 3개 기본값이 최적 (10개는 컨텍스트 오염) |
| **생성 온도** | 1.0이 최적 (0.0은 탐색 부족, 2.0은 활용 부족) |
| **스텝당 솔루션 개수** | 8개가 안정성과 효율의 균형 |

**초기 프롬프트의 영향:**[1]
- 강력한 초기 프롬프트("Lets think step by step")에서 시작하면 수렴이 빠름
- 약한 초기값(빈 문자열)에서는 더 많은 단계 필요하지만, 결국 유사한 성능 도달 가능

***

### 5. 주요 한계

#### 5.1 수학 최적화에서의 한계[1]

- **스케일 제한**: LLM의 컨텍스트 윈도우 제한으로 고차원 선형회귀나 큰 TSP 문제 처리 불가
- **복잡한 경관**: 울퉁불퉁한 최적화 경관에서 LLM이 적절한 하강 방향을 제시하지 못함
- **전문 방법과의 비교**: OPRO는 기울기 기반 최적화나 전문 솔버를 능가하도록 설계되지 않음

#### 5.2 프롬프트 최적화에서의 한계

- **과적합**: 훈련 정확도 > 테스트 정확도 (5-20% 갭)
- **모델 특정성**: 서로 다른 LLM이 생성하는 프롬프트 스타일이 크게 다름
- **수렴 속도 변동성**: 초기값과 온도에 따라 수렴 속도 크게 변함

***

### 6. 최신 연구 기반: 미래 연구 방향과 고려사항

#### 6.1 최근 연구 진전

**1) 일반화 능력 향상 연구**[2]

**"Prompting to Prompt (PTP)"** (2025, ICLR 2026 제출):
- **핵심**: 메타-템플릿(meta-template) 학습으로 **작업 간 전이 가능성 증대**
- **방법**: 프롬프트를 "전이 가능한 요소"로 분해하여 구조적 패턴 추출
- **결과**: Arena-hard 같은 도전적 작업에서 기존 방법 대비 10.52% 향상

**2) 모델 규모 제약 극복**[3]

**"Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers"** (ACL 2024):
- **발견**: OPRO가 소규모 LLM(LLaMA-2, Mistral 7B)에서 효과 제한적
- **제안**: 소규모 모델에는 명확하고 구체적인 직접 지시사항을 기본선(baseline)으로 사용
- **함의**: 모델 크기와 계산 비용 균형 고려 필요

**3) 초기화 및 방향 최적화**[4]

**"Dual-Phase Accelerated Prompt Optimization"** (2024):
- **문제**: OPRO는 초기 프롬프트 선택과 효과적 최적화 방향 식별에 의존
- **해결책**: 2단계 프레임워크로 고품질 초기화와 빠른 수렴 달성
- **성과**: 최적화 단계 대폭 감소

**4) 교차 작업 적응 강화**[5]

**"Transfer-Prompting"** (2025):
- **접근**: 소스 작업에서 프롬프트를 먼저 최적화한 후, 타겟 작업으로 전이
- **평가**: 25개 LLM, 9개 데이터셋 대규모 평가
- **결과**: 작업 특정 성능 향상 및 강화된 교차 작업 적응

**5) 다목표 최적화 확장**[6]

**"Autonomous Multi-Objective Optimization Using LLM"** (2024):
- **확장**: OPRO 개념을 다중 목적 최적화(multi-objective optimization) 문제로 확대
- **특징**: LLM이 상충하는 여러 목표를 균형있게 최적화

#### 6.2 앞으로의 연구 시 핵심 고려 사항

**1) 일반화 능력 강화:**
- ✓ 메타-템플릿 학습으로 구조적 패턴 추출
- ✓ 작업 간 전이성을 명시적 목표로 설정
- ✓ 제한된 훈련 데이터에서의 강건성 평가

**2) 계산 효율성:**
- ✓ 소규모 모델의 최적화 능력 개선 필요
- ✓ 초기화 전략과 조기 종료(early stopping) 개발
- ✓ 최적화 단계 수 감소 기법

**3) 모델 간 호환성:**
- ✓ 특정 모델에 최적화된 프롬프트의 일반성 개선
- ✓ 크로스 모델 전이 메커니즘 연구
- ✓ 모델 아키텍처 차이에 대한 강건성

**4) 이론적 이해:**
- ✓ 왜 의미상 유사 프롬프트가 다른 성능을 보이는지 분석
- ✓ LLM의 최적화 능력의 근본적 한계 규명
- ✓ 프롬프트 공간의 기하학적 특성 연구

**5) 실무적 고려:**
- ✓ 검증 세트 활용을 통한 과적합 감소
- ✓ 다양한 초기값으로부터의 강건한 수렴
- ✓ 비용-성능 균형 최적화

#### 6.3 OPRO의 장기적 영향

**학문적 기여:**
- 흑박스 최적화에 자연어 기반 접근의 새로운 가능성 제시
- LLM의 인지적 능력을 최적화 문제 해결로 확장
- 프롬프트 엔지니어링의 자동화 경로 개척

**실용적 영향:**
- 그래디언트 정보 없이도 복잡한 문제 해결 가능
- 프롬프트 최적화의 일관된 개선 입증
- 다양한 LLM과의 호환성으로 광범위한 적용 가능성[1]

**미래 전망:**
- 2024-2025년 연구의 활발한 진행: 메타-템플릿 학습, 교차 작업 전이, 다목표 최적화로 확대
- 점진적 개선 추세: OPRO의 한계를 인식하고 이를 보완하는 방향으로 진화
- 실무 적용 확대: 계산 효율성과 일반화 능력 개선으로 실제 응용 분야 확대 예상

***

### 결론

OPRO는 **자연어를 통한 직관적 최적화**라는 새로운 패러다임을 제시했습니다. 프롬프트 최적화에서의 실증적 성공(GSM8K 8%, BBH 50% 향상)은 이 접근의 유효성을 입증합니다. 특히 **제한된 훈련 데이터로도 일반화 가능**하다는 발견은 실무 적용의 가능성을 높입니다. 다만 수학 최적화에서의 스케일 한계와 모델 특정성 문제는 극복 과제이며, 최근 연구들이 이를 메타-템플릿 학습, 초기화 전략 개선, 교차 작업 전이 강화로 해결하고 있습니다. 앞으로 OPRO 기반 연구는 **더 강한 일반화 능력**, **계산 효율성**, **작은 모델에 대한 확장성**을 중심으로 진화할 것으로 예상됩니다.[4][6][5][2][3][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/433e9d45-b53e-43c3-a8f4-dad2931e77ae/2309.03409v3.pdf)
[2](https://openreview.net/forum?id=SgsZliAE1o)
[3](https://aclanthology.org/2024.findings-acl.100/)
[4](http://arxiv.org/pdf/2406.13443.pdf)
[5](https://arxiv.org/abs/2502.14211)
[6](https://arxiv.org/pdf/2406.08987.pdf)
[7](https://arxiv.org/pdf/2309.03409.pdf)
[8](https://arxiv.org/pdf/2401.00625.pdf)
[9](https://arxiv.org/pdf/2403.07691.pdf)
[10](https://arxiv.org/html/2410.20302v2)
[11](https://arxiv.org/pdf/2205.01068.pdf)
[12](https://arxiv.org/abs/2409.03444)
[13](https://openreview.net/forum?id=Bb4VGOWELI)
[14](https://aclanthology.org/2024.emnlp-main.245.pdf)
[15](https://www.sciencedirect.com/science/article/abs/pii/S0925231224020435)
[16](https://dl.acm.org/doi/full/10.1145/3731599.3767458)
[17](https://aclanthology.org/2022.naacl-main.290.pdf)
[18](https://arxiv.org/abs/2309.03409)
