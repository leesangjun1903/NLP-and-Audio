# AudioLM: a Language Modeling Approach to Audio Generation

### 1. 핵심 주장과 주요 기여

AudioLM은 오디오 신호를 **이산 토큰 시퀀스로 매핑한 후 언어 모델링 작업**으로 오디오 생성 문제를 재정의하는 혁신적인 프레임워크를 제안합니다. 기존 오디오 합성 모델들이 높은 음질 재현과 장기적 구조 일관성 사이의 트레이드오프를 극복하지 못한 반면, AudioLM은 **의미론적 토큰(semantic tokens)과 음향 토큰(acoustic tokens)의 하이브리드 토큰화 기법**을 통해 이 두 목표를 동시에 달성합니다.[1]

**주요 기여:**

- 의미론적 토큰과 음향 토큰을 계층적으로 결합하는 하이브리드 토큰화 프레임워크 제안
- w2v-BERT의 이산화된 활성화와 SoundStream 신경 코덱의 이산 코드가 음성 데이터셋에서 서로 보완적임을 입증
- 텍스트 주석 없이도 음성적·의미적으로 타당한 음성 연속 생성 가능 입증
- 훈련 데이터에 없는 화자의 목소리와 운율을 유지하며 3초 프롬프트로 연속 생성 시연
- 피아노 음악 생성으로 프레임워크의 범용성 입증

***

### 2. 해결 문제 및 방법론

#### A. 핵심 문제

오디오 신호는 **다층적 추상화 수준(음성의 음향 수준에서 의미론적·문법적 수준까지, 음악의 지역적 멜로디부터 전역적 화성과 리듬까지)**을 포함합니다. 기존 오디오 합성 모델들의 한계:[1]

- **음향 중심 모델**: 높은 음질을 달성하지만 구조적 일관성 부족 (예: WaveNet의 "옹알이" 음성 생성)
- **토큰 기반 모델**: 의미적 일관성은 우수하나 음질이 제한적

#### B. 제안 방법 및 수식

**하이브리드 토큰화 체계:**

AudioLM은 세 가지 핵심 구성요소로 구성됩니다:[1]

1. **토크나이저**: 입력 오디오 $$x \in \mathbb{R}^T$$를 이산 토큰 시퀀스 $$h \in \mathcal{H}^T$$로 매핑 ($$T \ll T$$)

2. **언어 모델**: 디코더 전용 Transformer로 최대 우도 최적화
$$\max_{\theta} \sum_{t=1}^{T} \log p_{\theta}(h_t \mid h_{<t})$$

3. **역토크나이저**: 예측 토큰을 오디오 파형으로 복원

**의미론적 토큰 (Semantic Tokens):**

w2v-BERT의 중간 계층에서 추출한 1024차원 특성 벡터를 k-means 클러스터링하여 생성:[1]

$$z_i = \arg\min_{k} \|e_i - \mu_k\|_2$$

여기서 $$e_i$$는 정규화된 w2v-BERT 임베딩, $$\mu_k$$는 클러스터 중심

- 샘플링 레이트: 25 Hz (40ms 간격)
- 시퀀스 길이: $$T_S = T/640$$
- 비트레이트: 250 bps

**음향 토큰 (Acoustic Tokens):**

SoundStream 신경 코덱의 잔차 벡터 양자화기(RVQ)로 생성:[1]

$$Y = [y_1^{(1)}, \ldots, y_{T_A}^{(Q)}] \in \{1, \ldots, N\}^{Q \times T_A}$$

여기서:
- $$Q = 4$$ (거친 양자화기) 또는 $$Q' = 8$$ (미세 양자화기)
- $$N = 1024$$ (코드북 크기)
- 샘플링 레이트: 50 Hz (20ms 간격)
- 비트레이트: 2000 bps (거친) 또는 6000 bps (전체)

**계층적 모델링 (Three-Stage Cascading):**

AudioLM은 조건부 독립 가정 $$p(z_t \mid z_{<t}, y_t) \approx p(z_t \mid z_{<t})$$에 기반한 3단계 구조를 채택합니다:[1]

**1단계 - 의미론적 모델링:**
$$p(z_t \mid z_{<t})$$

**2단계 - 거친 음향 모델링 (조건부):**
$$p(y_t^q \mid z, y_t^{\leq Q}, y_t^{<q})$$
($q \leq Q$, 거친 양자화기 층)

**3단계 - 미세 음향 모델링:**
$$p(y_t^q \mid y_t^{\leq Q}, y_t^{<q})$$
($q > Q$, 미세 양자화기 층)

이는 다음과 같이 표현됩니다:
$$p(z, y^{coarse}, y^{fine}) = p(z) \cdot p(y^{coarse} \mid z) \cdot p(y^{fine} \mid y^{coarse})$$

***

### 3. 모델 구조 및 구성

**Transformer 아키텍처 (모든 단계에서 동일):**[1]

- 레이어: 12개
- 주의 헤드: 16개
- 임베딩 차원: 1024
- Feed-forward 차원: 4096
- Dropout: 0.1
- 위치 임베딩: T5 스타일 상대 위치 임베딩
- 모델 크기: 단계별 0.3B 매개변수

**훈련 설정:**[1]

- 데이터: LibriSpeech unlab-60k (60,000시간 영어 음성)
- 배치 크기: TPUv4 16개, 배치 크기 256
- 훈련 스텝: 1M 스텝
- 입력 길이: 30초(1단계), 10초(2단계), 3초(3단계)
- 온도 샘플링: 0.6(1단계), 0.8(2단계), 0.6(3단계)

**추론 프로세스:**

- **무조건 생성**: 모든 의미론적 토큰을 무조건적으로 샘플링
- **음향 생성**: 실제 의미론적 토큰으로 조건화하여 음향 토큰만 생성
- **연속 생성**: 3초 프롬프트에서 의미론적 토큰 추출 및 음향 토큰 샘플링

***

### 4. 성능 향상 및 실험 결과

#### A. 토큰 특성 비교

표 I의 결과는 의미론적 토큰과 음향 토큰의 상보적 성질을 명확히 보여줍니다:[1]

| 토큰화 | 비트레이트 | 음성 변별성 (ABX, ↓) | 재구성 품질 (ViSQOL, ↑) |
|--------|-----------|-------------------|--------------------------|
| 의미론적 (w2v-BERT) | 250 bps | 6.7 (동일화자), 7.6 (다중화자) | 1.1 |
| 의미론적 (w2v-BERT) | 6000 bps | 5.6, 6.2 | 1.4 |
| 음향 (SoundStream) | 2000 bps | 22.4, 28.7 | 3.3 |
| 음향 (SoundStream) | 6000 bps | 17.8, 26.6 | 3.9 |

**해석**: 음향 토큰은 음질 재현에 우수하나 음성 변별성이 낮고, 의미론적 토큰은 그 반대입니다.

#### B. 언어적 지식 검증

**sWUGGY 및 sBLIMP 메트릭 (표 IV):**[1]

| 모델 | sWUGGY 성공률 (%) | sBLIMP 성공률 (%) |
|-----|-------------------|-------------------|
| GSLM (이전 SOTA) | - | 68.7 |
| CPC-BERT | 80.0 | 59.9 |
| **AudioLM** | **71.5** (all), **83.7** (in-vocab) | **64.7** |

AudioLM은 **비문자 감독 모델 중에서 최고 성능**을 달성하였습니다.

#### C. 음성 연속 생성 평가

**음성 인식 오류율 (표 II):**[1]

| 모델 | CER (%) | WER (%) |
|-----|---------|---------|
| 원본 오디오 | 0.8 | 2.5 |
| SoundStream 재구성 | 0.9 | 2.6 |
| AudioLM (음향 생성) | 3.4 | 6.0 |
| GSLM | 2.9 | 6.6 |

낮은 오류율은 의미론적 토큰이 충분한 언어적 내용을 포착함을 입증합니다.

**화자 분류 정확도 (표 III):**[1]

| 설정 | 정확도 (%) |
|-----|-----------|
| SoundStream 재구성 | 100.0 |
| 음향 생성 (동일 의미론적 토큰) | 3.2 |
| 연속 생성 (프롬프트 포함) | 92.6 |

**의미**: 의미론적 토큰은 화자 정보를 거의 포함하지 않으며, 프롬프트의 음향 토큰으로부터 화자 정체성이 보존됩니다.

#### D. 주관적 평가

**합성 음성 vs 실제 음성 판별 작업:**[1]

- 평가자 성공률: **51.2%** (50% 무작위 할당과 통계적으로 유의하지 않음, p=0.23)
- **해석**: AudioLM이 생성한 음성 연속은 인간 청자에게 실제 음성과 구분하기 어렵습니다.

#### E. 합성 음성 탐지

간단한 CNN 분류기로 탐지 정확도: **98.6%**[1]

이는 높은 음질 생성과 탐지 가능성 사이의 균형을 시연합니다.

#### F. 피아노 음악 생성

40,000시간의 피아노 음악으로 훈련 시:[1]

- AudioLM: 음향만 사용한 모델 대비 **83.3% 선호도**
- 멜로디, 화성, 톤, 리듬의 일관성 유지

***

### 5. 모델의 일반화 성능 향상

#### A. 트레이닝 데이터 다양성의 영향

AudioLM은 **60,000시간의 노이지한 음성(unlab-60k)**로 훈련하여 이전 GSLM의 6,000시간 청정 음성 데이터보다 우수한 성능을 달성합니다. 이는 **더 광범위하고 다양한 데이터에 대한 강건성**을 입증합니다.[1]

#### B. 화자 일반화

**미확인 화자에 대한 화자 정체성 유지:**[1]

- 3초 프롬프트로 92.6%의 화자 분류 정확도
- 음성, 운율, 녹음 조건(반향, 배경 소음 레벨) 보존
- 동일 의미론적 토큰으로 조건화 시 3.2% 정확도 = 음향 토큰이 화자 정보 인코딩

**메커니즘:**
$$p(\text{화자 특성} \mid y^{acoustic}, z^{semantic}) \approx p(\text{화자 특성} \mid y^{acoustic})$$

#### C. 의미론적 토큰의 역할

ABX 분석 및 sWUGGY/sBLIMP 메트릭은 의미론적 토큰이 다음을 캡처함을 보여줍니다:[1]

- **음성 내 화자**: ABX within-speaker 6.7
- **음성 간 화자**: ABX across-speaker 7.6
- **어휘적 지식**: sWUGGY 71.5% 성공률
- **문법적 지식**: sBLIMP 64.7% 성공률

#### D. 계층적 모델링의 효과

3단계 구조가 1-2단계만 사용하는 경우보다 우수한 이유:[1]

$$\text{조건부 독립성}: p(y_t^{fine} \mid y_t^{coarse}, z) = p(y_t^{fine} \mid y_t^{coarse})$$

- 3단계에서 의미론적 토큰 무시 가능 → 시퀀스 길이 감소
- 3초 청크에서 처리 가능 → 대상 시퀀스 길이 무관 스케일링
- 더 많은 RVQ 계층($$Q$$) 사용 가능 → 음질 향상

***

### 6. 모델의 한계

#### A. 기술적 한계

1. **의미론적 토큰의 구어적 편향**: w2v-BERT는 영어 음성에 최적화되어 있어 다국어 및 저자원 언어에서 성능 저하[1]

2. **문법 구조 한계**: 고도의 문법적 복잡성이나 특수 문법에서 오류 (표 II: CER 3.4%, WER 6.0%)

3. **적절 명사 합성 문제**: ASR 평가에서 고유명사 인식 오류가 주요 오류 원인[1]

4. **노이즈 환경에서의 성능**: 배경 잡음이 포함된 합성 오디오의 음질 저하[1]

#### B. 확장성 한계

1. **다중 음성**: 겹치는 음성(overlapping speech) 생성 미검증
2. **다성 음악**: 피아노 음악만 테스트, 다성 음악 생성 능력 미검증
3. **환경 음향**: 음성과 피아노 음악 외 음향 카테고리 미테스트

#### C. 윤리적 한계

**음성 합성 악용 위험:**[1]
- 생체인식 스푸핑 (speaker verification 우회)
- 화자 모방/신원 사기
- 오정보 및 허위 정보 확산

**완화 방안**: 합성 음성 탐지 모델 제공 (98.6% 탐지 정확도)

***

### 7. 앞으로의 연구 영향 및 고려사항

#### A. 영향력

**학술적 영향:**
- AudioLM은 언어 모델링을 통한 **신호 생성의 새로운 패러다임** 제시
- 이후 여러 확장 연구에 영감: LM-VC (음성 변환), Audiobox (통합 오디오 생성), SongEditor (음악 편집)

**실무적 적용:**
- 접근성 향상: 음성 장애인 지원
- 음악 제작: 컴퓨터 보조 음악 작곡
- 대사 연속: 영화/게임 콘텐츠 제작

#### B. 최신 연구 방향 (2023-2025)

**1. 확장 및 개선**

- **LM-VC (2023)**: AudioLM 기반 **제로샷 음성 변환**[2]
  - 마스크 프리픽스 LM 사용
  - 언어 내용 보존 개선
  - 화자 타임브르 분리

- **Audiobox (2023)**: **통합 오디오 생성 프레임워크**[3]
  - 자연 언어 프롬프트 지원
  - 설명 기반 및 샘플 기반 프롬프트
  - 음성/음향 생성의 통일

- **SongEditor (2024)**: **음악 편집 능력 추가**[4]
  - 세그먼트별 및 트랙별 수정
  - 가사, 보컬, 반주 조절
  - 자동회귀 언어 모델 + 확산 모델 조합

**2. 모델 일반화 개선**

- **GZS-TV (2023)**: **분리된 표현 학습**을 통한 미지 화자 적응[5]
  - 화자 임베딩과 음성학적 정보 분리
  - 변분 자동인코더(VAE) 활용
  - 대역 외(out-of-distribution) 데이터에서 성능 향상

- **Typhoon-Audio (2024)**: **저자원 언어 및 지시 따르기 강화**[6]
  - 다국어 오디오 언어 모델
  - 영어와 태국어에서 Gemini-1.5-Pro 대비 성능

**3. 오디오 토큰화 혁신**

- **Autoregressive Speech Enhancement (2025)**: **음향 토큰 정제**[7]
  - 노이즈 억제를 위한 토큰 변환
  - 노출 편향(exposure bias) 감소 방법

- **Make Some Noise (2025)**: **사운드 토큰 활용**[8]
  - LLM에 오디오 추론 및 생성 통합
  - 더 큰 데이터와 개선된 평가 메트릭 필요

**4. 멀티모달 오디오-언어 모델**

- **Audio Flamingo (2024)**: **오디오-언어 모델**[9]
  - 음성 및 비음성 이해
  - 컨텍스트 내 학습(in-context learning)
  - 다중 대화 능력

- **Music Flamingo (2025)**: **음악 이해 특화**[10]
  - 화성, 구조, 음색, 가사, 문화 맥락 분석
  - Chain-of-thought 학습 및 강화학습(GRPO)

#### C. 향후 연구 시 고려 사항

**1. 데이터 다양성 및 품질**
- 다국어, 다방언, 저자원 언어에 대한 훈련 데이터 확대
- 음성 외 오디오 카테고리(환경음향, 다성 음악 등) 포함

**2. 토큰화 혁신**
- 더욱 효율적인 의미론적 토큰화 방법 개발
- 의미론적-음향 토큰 간 더 나은 분리 메커니즘

**3. 생성 제어 강화**
- 자연 언어 설명 기반 세밀한 제어
- 스타일(tone, prosody) 조절 능력

**4. 윤리 및 안전성**
- 합성 오디오 탐지 기술 고도화
- "오디오 워터마킹" 등 방지 기술 개발
- 책임 있는 AI 배포 방안

**5. 효율성 및 확장성**
- 추론 속도 및 메모리 효율성 개선
- 엣지 디바이스에서의 실시간 생성
- 더 긴 오디오 시퀀스 생성 가능성

**6. 평가 메트릭 표준화**
- 오디오 생성의 주관적/객관적 평가 메트릭 통일
- 다양한 오디오 도메인에 대한 벤치마크 구축

***

### 결론

AudioLM은 **언어 모델링을 통한 고음질 오디오 생성**이라는 근본적인 패러다임 변화를 이루었습니다. 의미론적 토큰과 음향 토큰의 하이브리드 체계는 장기 일관성과 음질의 이원적 목표를 동시에 달성하는 우아한 해결책을 제시합니다. 미지 화자의 음성 및 운율 유지, 텍스트 없는 음성 생성, 다양한 오디오 도메인 확장 등의 성과는 후속 연구를 촉발했으며, 현재 오디오 언어 모델 연구의 중심축이 되었습니다. 향후 다국어 일반화, 음성/의미 분리 정교화, 윤리적 안전장치 강화, 그리고 실시간 엣지 배포가 주요 과제입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/a9e26ca9-fd73-46f9-bb15-00ec047a8dec/2209.03143v2.pdf)
[2](https://ieeexplore.ieee.org/document/10229308/)
[3](https://arxiv.org/abs/2312.15821)
[4](https://arxiv.org/abs/2412.13786)
[5](https://arxiv.org/abs/2308.13007)
[6](http://arxiv.org/pdf/2409.10999.pdf)
[7](https://arxiv.org/html/2507.12825v1)
[8](https://arxiv.org/html/2503.22275)
[9](http://arxiv.org/pdf/2402.01831.pdf)
[10](https://www.semanticscholar.org/paper/4b5955f621a652a1cafb6c271968216f70ad34cd)
[11](https://arxiv.org/abs/2412.21037)
[12](https://aclanthology.org/2022.clpsych-1.12)
[13](https://ejournal.pnc.ac.id/index.php/jinita/article/view/2788)
[14](https://ieeexplore.ieee.org/document/10837888/)
[15](https://arxiv.org/abs/2404.01322)
[16](https://ieeexplore.ieee.org/document/8888180/)
[17](https://arxiv.org/pdf/2209.03143.pdf)
[18](https://arxiv.org/pdf/2306.10521.pdf)
[19](https://arxiv.org/pdf/2211.01223.pdf)
[20](http://arxiv.org/pdf/2309.08773.pdf)
[21](https://arxiv.org/html/2411.18953v1)
[22](https://arxiv.org/abs/2209.03143)
[23](https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/)
[24](https://research.samsung.com/blog/Robust-Speaker-Personalisation-Using-Generalized-Low-Rank-Adaptation-for-Automatic-Speech-Recognition)
[25](https://ravinkumar.com/GenAiGuidebook/audio/audio_tokenization.html)
[26](https://www.youtube.com/watch?v=Vucewi_kPEU)
[27](https://www.isca-archive.org/interspeech_2023/wang23c_interspeech.pdf)
[28](https://www.emergentmind.com/topics/audio-tokenizer)
[29](https://dl.acm.org/doi/10.1109/TASLP.2023.3288409)
