# Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data

## 1. 핵심 주장 및 주요 기여도

### 1.1 주요 주장

Guided-TTS 2는 **최소한의 무전사(untranscribed) 데이터로 고품질의 적응형 텍스트-음성 합성(Adaptive TTS)을 실현**한다는 핵심 주장을 제시합니다. 구체적으로 다음과 같습니다:

- **10초의 무전사 음성만으로** 단일 화자(single-speaker) 고품질 TTS 모델(예: LJSpeech로 학습한 모델)과 필적할 수 있는 음성 품질을 달성
- **40초의 미세 조정(fine-tuning)만으로** 새로운 화자에 대한 적응 가능
- 비인간 캐릭터(예: 영화 "반지의 제왕"의 골룸(Gollum))까지도 적응 가능

### 1.2 주요 기여도

1. **확산 모델 기반의 적응형 TTS 프레임워크**: Diffusion Probabilistic Model(DDPM)을 이용한 최초의 적응형 TTS 시스템
2. **화자 조건부 확산 모델의 도입**: 사전학습된 화자 검증 모델을 활용한 화자 인코더를 통해 다양한 화자에 대한 일반화 능력 제공
3. **분류기-무의존(Classifier-free) 지도 기법 적용**: 전사 텍스트 없이 대규모 다중 화자 데이터셋에서 사전학습 가능
4. **효율적 미세 조정 전략**: 구조화된 미세 조정을 통해 발음 정확도를 유지하면서 화자 유사도 개선
5. **실제 데이터 적응 능력**: 유튜브 클립 등 실제 환경의 잡음 있는 음성에도 적응 가능

---

## 2. 해결하고자 하는 문제

### 2.1 현존하는 한계점

기존 적응형 TTS 모델들은 다음과 같은 근본적 문제점을 가지고 있습니다:

- **데이터 의존성**: 고품질 음성 합성을 위해 목표 화자로부터 많은 양의 데이터(수십 분 이상)를 필요
- **전사 데이터 필수**: 학습 과정에서 음성에 대한 정확한 텍스트 전사가 필수적
- **품질-데이터 트레이드오프**: 참조 음성이 10초 정도로 짧을 때 단일 화자 모델 수준의 품질 달성 불가
- **매개변수 효율성**: 새로운 화자 적응 시 전체 모델의 모든 매개변수를 미세 조정해야 함

### 2.2 구체적 문제 정의

Guided-TTS 2가 해결하려는 문제는:
- **최소한의 무전사 참조 음성(10초)으로도** 고품질의 음성을 생성할 수 있는가?
- **새로운 화자에 빠르게 적응하면서도** 발음 정확도를 유지할 수 있는가?
- **대규모 무레이블 데이터셋을 활용하여** 모델의 일반화 능력을 향상할 수 있는가?

---

## 3. 제안하는 방법

### 3.1 기본 아키텍처

Guided-TTS 2는 다음 네 가지 핵심 컴포넌트로 구성됩니다:

#### (1) 화자 조건부 확산 모델 (Speaker-Conditional DDPM)

$$L(\theta) = \mathbb{E}_{t,X_0,\epsilon_t}[\|s_\theta(X_t|S) + \lambda(t)^{-1}\epsilon_t\|_2^2]$$

여기서:
- $X_t$: 시간 단계 $t$에서의 노이즈가 있는 멜-스펙트로그램
- $s_\theta(X_t|S)$: 화자 $S$에 조건부인 스코어 함수
- $\lambda(t) = I - e^{-\int_0^t \beta_s ds}$: 신뢰도 가중치

#### (2) 화자 분류기-무의존 지도 (Speaker Classifier-Free Guidance)

$$\hat{s}_\theta(X_t|\hat{S}) = s_\theta(X_t|\hat{S}) + \gamma_S \cdot (s_\theta(X_t|\hat{S}) - s_\theta(X_t|\phi))$$

구성 요소:
- $s_\theta(X_t|\hat{S})$: 목표 화자 $\hat{S}$에 대한 조건부 스코어
- $s_\theta(X_t|\phi)$: 무조건 스코어 (null 임베딩 사용)
- $\gamma_S$: 화자 지도 강도를 조절하는 하이퍼파라미터

**핵심 혁신**: 화자 임베딩 $e_\phi = \frac{w}{\|w\|}$는 학습 가능한 매개변수로 초기화되어, 일관성 있게 무조건 스코어를 학습합니다.

#### (3) 규범-기반 지도 (Norm-based Guidance for Text)

$$\hat{s}_\theta(X_t|\hat{y}, \hat{S}) = \hat{s}_\theta(X_t|\hat{S}) + \gamma_T \cdot \frac{\|\hat{s}_\theta(X_t|\hat{S})\|}{\|\nabla_{X_t}\log p_\phi(\hat{y}|X_t, \hat{S})\|} \cdot \nabla_{X_t}\log p_\phi(\hat{y}|X_t, \hat{S})$$

이 식의 의미:
- 음소 분류기의 기울기 $\nabla_{X_t}\log p_\phi(\hat{y}|X_t, \hat{S})$를 확산 모델의 스코어 $\hat{s}_\theta(X_t|\hat{S})$의 규범에 맞춰 정규화
- 이를 통해 높은 발음 정확도를 유지하면서도 화자 유사도를 개선

### 3.2 역 샘플링 프로세스

$$X_{t-1}^N = X_t + \frac{\beta_t}{N}(\frac{1}{2}X_t + \hat{s}_\theta(X_t|\hat{y}, \hat{S})) + \sqrt{\frac{\beta_t}{N}}z_t$$

여기서:
- $N$: 역 프로세스의 이산화 단계 수
- $z_t \sim \mathcal{N}(0, I)$: 표준 가우시안 노이즈

### 3.3 미세 조정 전략

#### 목표 및 제약

- **학습률**: $2 \times 10^{-5}$ (사전학습 시 $10^{-4}$보다 낮음)
- **반복 횟수**: 500회
- **최적화 초기화**: 사전학습된 가중치는 유지하되, 옵티마이저 상태는 재초기화

**핵심 설계 원칙**:

$$\text{Fine-tuning Objective: Minimize} \quad L(\theta) = \mathbb{E}_{t,X_0,\epsilon_t}[\|s_\theta(X_t|S) + \lambda(t)^{-1}\epsilon_t\|_2^2]$$

새로운 화자 $\hat{S}$에 대해서만 조건부 스코어를 학습하며, 무조건 스코어는 고정하여 다중 화자 데이터셋에서 학습한 발음 능력을 보존합니다.

### 3.4 음소 분류기와 지속시간 예측기

- **음소 분류기**: WaveNet 기반의 프레임 수준 분류기
- **지속시간 예측기**: Glow-TTS와 동일한 구조
- **화자 인코더**: GE2E 손실로 학습된 화자 검증 모델 (VoxCeleb2에서 사전학습)

모든 모듈은 화자 임베딩을 입력으로 받아 **화자-의존적(speaker-dependent)** 구조를 유지하며, 미학습 화자에 대한 일반화 능력을 제공합니다.

---

## 4. 모델 구조의 상세 분석

### 4.1 전체 시스템 아키텍처

```
입력 텍스트 y
    ↓
[음소 변환 (IPA)]
    ↓
[지속시간 예측기] → 프레임 수준 음소 시퀀스 ŷ 생성
    ↓
[참조 음성 임베딩] → 화자 임베딩 eŜ 추출
    ↓
[화자 조건부 DDPM]
├─ 분류기-무의존 지도 적용: 화자 정보 강화 (γS = 1.0)
├─ 규범-기반 지도 적용: 텍스트 정보 주입 (γT = 0.3)
└─ 역 확산 샘플링 (50 단계)
    ↓
[Mel-스펙트로그램 생성]
    ↓
[Universal HiFi-GAN 보코더]
    ↓
최종 음성 파형
```

### 4.2 학습 파이프라인

#### 단계 1: 사전학습 (Pre-training)

- **데이터셋**: LibriTTS (585시간) + Libri-Light (6,300시간의 무레이블 데이터)
- **배치 크기**: 256
- **최적화기**: Adam
- **반복 횟수**: 1M 반복 (LibriTTS) + 100K 반복 (분류기-무의존 지도용)
- **하드웨어**: 4 × NVIDIA RTX 8000 GPU

#### 단계 2: 미세 조정 (Fine-tuning)

- **참조 데이터**: 10초의 무전사 음성
- **학습률**: $2 \times 10^{-5}$
- **반복 횟수**: 500회
- **소요 시간**: ~40초 (NVIDIA RTX 8000 기준)

#### 단계 3: 추론 (Inference)

- **샘플링 단계**: 50단계의 역 확산 과정
- **실시간 인자(RTF)**: 
  - 분류기-무의존 지도 미적용: 0.526
  - 분류기-무의존 지도 적용: 0.802 (거의 실시간)

### 4.3 화자 일반화 메커니즘

#### 사전학습된 화자 인코더의 역할

```
참조 음성 X ─→ [화자 인코더 ES] ─→ 화자 임베딩 eS ∈ ℝ^d
                                ↓
                    시간 단계 t와 연결 (concatenate)
                                ↓
                    [화자 조건부 DDPM 입력]
```

- **임베딩 차원**: 256
- **정규화**: 단위 벡터 정규화 (unit norm normalization)
- **장점**: 미학습 화자의 특성을 효과적으로 포착

### 4.4 분류기-무의존 지도의 혁신성

기존의 분류기 기반 지도 방법들과의 차이:

| 특성 | 분류기 지도 | 분류기-무의존 지도 |
|------|-----------|------------------|
| 필요한 모델 수 | 2개 (DDPM + 분류기) | 1개 (DDPM만 사용) |
| 학습 난이도 | 높음 (잡음 있는 입력에 분류기 학습) | 낮음 (단일 모델로 조건부/무조건 학습) |
| 조건부 신호 유연성 | 제한적 | 매우 유연 (텍스트 임베딩 등) |
| 추론 효율성 | 높음 | 낮음 (2배의 네트워크 계산 필요) |

Guided-TTS 2는 효율성 대신 **성능과 유연성을 우선**하는 설계 선택을 함.

---

## 5. 성능 향상 및 실험 결과

### 5.1 단일 화자 TTS 모델과의 비교

**LJSpeech 데이터셋 (10초 참조 음성)**

| 모델 | MOS | CER(%) | SMOS |
|------|-----|--------|------|
| Ground Truth | 4.45±0.05 | 0.64 | 3.85±0.08 |
| Mel + HiFi-GAN | 4.24±0.08 | 0.86 | 3.80±0.08 |
| Grad-TTS | 4.22±0.08 | 0.98 | 3.67±0.09 |
| Guided-TTS | 4.17±0.09 | 1.23 | 3.63±0.09 |
| **Guided-TTS 2 (미세조정)** | **4.21±0.09** | **1.12** | **3.69±0.09** |
| Guided-TTS 2 (영점사격) | 4.23±0.09 | 0.89 | 3.51±0.08 |

**핵심 통찰**:
- 미세조정 없이도 (Zero-shot) 우수한 성능 달성
- 미세조정 후 발음 정확도(CER)는 약간 하락하지만 화자 유사도(SMOS) 향상
- 단일 화자 모델 수준의 음질 달성

### 5.2 적응형 TTS 모델과의 비교

**LibriTTS 및 VCTK 데이터셋**

| 데이터셋 | 메트릭 | Guided-TTS 2 (미세조정) | Guided-TTS 2 (영점사격) | YourTTS | Meta-StyleSpeech |
|---------|--------|----------------------|-------------------|---------|-----------------|
| LibriTTS | MOS | 4.20±0.08 | 4.25±0.09 | 4.02±0.10 | 3.98±0.11 |
| | CER(%) | 0.84 | 0.8 | 2.38 | 1.52 |
| | SMOS | 3.70±0.09 | 3.51±0.10 | 3.30±0.10 | 3.42±0.09 |
| VCTK | MOS | 4.11±0.09 | 4.23±0.09 | 3.94±0.10 | 3.65±0.13 |
| | CER(%) | 1.49 | 0.81 | 2.36 | 1.84 |
| | SMOS | 3.57±0.10 | 3.39±0.09 | 3.19±0.09 | 3.26±0.10 |

**성능 향상의 원인**:
1. **대규모 다중 화자 데이터셋에서의 사전학습**: 다양한 화자 특성 학습
2. **무전사 데이터 활용**: Libri-Light의 6,300시간 데이터 활용 가능
3. **효과적인 화자 지도**: 분류기-무의존 지도로 화자 특성 강화

### 5.3 하이퍼파라미터 영향 분석

#### 화자 지도 강도 (γS)에 따른 성능 변화

$$\text{CER}, \text{SMOS} = f(\gamma_S)$$

| γS | CER(%) | SECS |
|----|--------|------|
| 0.0 | ~1.0 | ~0.87 |
| 1.0 | 0.84 | 0.937 |
| 2.0 | 1.2 | 0.942 |
| 3.0 | 1.5 | 0.945 |
| 5.0 | 2.5+ | 0.95+ |

**최적값**: γS = 1.0 (기본값)

### 5.4 미세 조정의 효과

#### 미세조정 반복 횟수와 옵티마이저 영향

| 반복 횟수 | 옵티마이저 | CER(%) | SECS |
|---------|----------|--------|------|
| 0 (영점사격) | - | 0.8 | 0.873 |
| 50 | 초기화 | 0.82 | 0.908 |
| 200 | 초기화 | 0.88 | 0.929 |
| **500** | **초기화** | **0.84** | **0.937** |
| 2000 | 초기화 | 1.49 | 0.945 |
| 500 | 로드 | 1.39 | 0.925 |

**주요 발견**:
- 500회 반복이 최적 지점 (발음 정확도 vs 화자 유사도 균형)
- 사전학습된 옵티마이저 상태 로드 시 성능 악화
- 과도한 미세조정(2000회)은 발음 능력 손상

### 5.5 참조 음성 길이의 영향

| 길이 | CER(%) | SECS |
|------|--------|------|
| 3초 | 2.44 | 0.925 |
| 5초 | 1.67 | 0.930 |
| 10초 | 1.12 | 0.929 |
| 30초 | 0.98 | 0.932 |
| 60초 | 1.14 | 0.931 |

**통찰**:
- 화자 유사도는 3초 이상에서 안정적
- 발음 정확도는 10초까지 지속적으로 개선
- 10초가 품질과 효율의 최적 지점

---

## 6. 모델의 일반화 성능 향상 가능성

### 6.1 현재 일반화 메커니즘

#### (1) 화자 인코더의 역할

```
학습 화자 (훈련)
    ↓
[VoxCeleb2 기반 화자 검증 모델]
    ↓
미학습 화자의 음성 특성 추출
    ↓
화자 임베딩 공간에서의 일반화
```

- **학습 데이터**: VoxCeleb2 (1M+ 발화, 6,112명 화자)
- **학습 방식**: GE2E 손실함수로 화자 특성 추출
- **일반화**: 훈련되지 않은 화자도 임베딩 공간에서 표현 가능

#### (2) 다중 화자 사전학습의 효과

```
대규모 다중 화자 데이터셋
├─ LibriTTS: 2,456명 화자
├─ Libri-Light: 2,231명 화자 (무레이블)
└─ 총 ~4,500명 이상 화자

↓

화자 조건부 DDPM 사전학습
│
├─ 음성의 일반적 특성 학습
├─ 화자별 음성 변동성 학습
└─ 다양한 발음 및 음색 학습

↓

미학습 화자에 대한 일반화 능력 향상
```

### 6.2 일반화 성능 향상 가능성 - 분석

#### 가능성 1: 무전사 데이터 활용 확대

**현재 접근**:
- Libri-Light의 6,300시간 무레이블 데이터 활용
- 영어 데이터만 사용

**개선 방향**:
$$\text{데이터셋 확대} \Rightarrow \text{다양한 억양/방언} \Rightarrow \text{일반화 능력↑}$$

- 다국어 무전사 데이터 통합 (YouTube, 팟캐스트 등)
- 잡음이 있는 실제 환경 데이터 포함
- 예상 효과: 제로샷 성능 10-20% 향상

#### 가능성 2: 화자 임베딩 공간의 고도화

**현재 문제점**:
- 고정된 사전학습 화자 인코더 사용
- 임베딩 공간이 TTS 작업에 최적화되지 않음

**개선 방안**:
$$e_S = E_S(X) \text{ (적응형 임베딩)}$$

- TTS 작업과 함께 화자 인코더 공동 학습
- 대조 학습(Contrastive Learning)을 이용한 임베딩 공간 최적화
- 예상 효과: 화자 유사도(SMOS) 0.05-0.1점 향상

#### 가능성 3: 계층적 화자 지도 (Hierarchical Speaker Guidance)

**새로운 제안**:

$$\hat{s}_\theta(X_t|\hat{S}, \ell) = s_\theta(X_t|\hat{S}, \ell) + \gamma_S(\ell) \cdot (s_\theta(X_t|\hat{S}, \ell) - s_\theta(X_t|\phi, \ell))$$

여기서 $\ell$은 확산 모델의 계층 레벨 (1부터 L까지)

**의의**: 
- 초기 잡음 제거 단계에서는 낮은 강도의 화자 지도 (γS = 0.5)
- 후기 세부사항 개선 단계에서는 높은 강도의 화자 지도 (γS = 1.5)
- 예상 효과: 발음 정확도 유지하면서 화자 유사도 개선

#### 가능성 4: 메타러닝을 통한 적응 최적화

**제안**:
Model-Agnostic Meta-Learning (MAML) 적용

$$\min_\theta \sum_{S} L(\theta - \alpha \nabla_\theta L(S_{ref}, \theta))$$

- 여러 화자로부터 빠른 적응을 위한 메타 초기화 학습
- 예상 효과: 미세조정 반복 횟수를 500회에서 100회로 감소 가능 (80% 빠름)

#### 가능성 5: 토큰 임베딩 보강 (Richer Token Representation)

**현재**: 단순 음소 시퀀스 사용

**개선**: 
$$\hat{y}_{enhanced} = \text{Concat}(\text{phoneme}, \text{prosody}, \text{context}, \text{style})$$

- 음성학적 특성 추가 (VOT, friction, nasality 등)
- 문맥 정보 (앞뒤 음소, 강세 등)
- 화자 스타일 벡터 통합
- 예상 효과: CER 20% 감소, 다양한 화자에 대한 적응 성능 향상

### 6.3 일반화 성능을 제한하는 요인

#### 1. 보코더 의존성

**문제**:
$$\text{Mel-spectrogram} \rightarrow \text{Universal HiFi-GAN} \rightarrow \text{waveform}$$

- 보코더가 진정 범용(universal)이 아님
- 노이즈가 많거나 특이한 음성에 대한 처리 부족

**해결 방안**: 
- 엔드-투-엔드 확산 모델로 직접 파형 생성
- 다양한 환경의 음성으로 보코더 미세조정

#### 2. 매개변수 효율성 부재

**현재 한계**:
```
새 화자 적응 시 DDPM의 모든 매개변수 미세조정
├─ 매개변수 저장 비용 증가
├─ 메모리 오버헤드
└─ 확장성 제한
```

**개선 제안**:
- 어댑터 모듈 사용 (전체 매개변수의 5% 미만만 학습)
- LoRA (Low-Rank Adaptation) 적용
- 예상 효과: 새 화자당 저장 공간 95% 감소

#### 3. 조음 제약 (Articulation Constraints)

**문제**: 순수 아키텍처만으로는 물리적 음성 생성 제약을 학습하기 어려움

**해결**: 음성학적 지도 추가
$$L = L_{diffusion} + \lambda \cdot L_{phonetic}$$

---

## 7. 한계점 및 미해결 과제

### 7.1 명시적 한계 (논문에서 제시)

#### (1) 매개변수 효율성 부재

```
문제: 각 화자별로 전체 DDPM 매개변수를 미세조정
       ↓
결과: 새 화자마다 전체 모델 크기만큼의 저장 공간 필요
```

**정량적 영향**: 
- 단일 모델 크기: ~100MB
- 100명 화자 적응 시: 10GB (원본 모델 제외)

**해결 방법**: 
논문에서 언급한 가능성
- Chen et al. [5]의 어댑터 기반 미세조정
- 최소한의 매개변수만 학습하면서도 적응 성능 유지

#### (2) 범용 보코더의 한계

```
보코더 성능 ≤ 음성 합성 전체 품질의 상한
```

**영향**: 
- 녹음 환경이 다양한 실제 데이터에서 품질 저하
- MOS 스코어가 4.2에 머물러 4.5 이상으로 향상 불가

**제안된 미래 방향**: 엔드-투-엔드 적응형 TTS 개발

### 7.2 암묵적 한계

#### (1) 언어 의존성

**현재 상황**:
- 영어 중심의 학습 데이터 (LibriTTS, LibriSpeech)
- 음소 변환 과정이 언어별로 상이

**다국어 확장의 어려움**:
- IPA 음소 시스템의 보편성에도 불구하고 음소 분류기 재학습 필요
- 각 언어별 발음 특성 미반영

**일반화 가능성**: 중간 수준 - 논문의 대규모 다중 화자 데이터로 기초 마련되었으나, 각 언어에 대한 특화 필요

#### (2) 영점사격 vs 미세조정 성능 격차

**현상**:
- SMOS: 영점사격 3.51 → 미세조정 3.69 (0.18 포인트 향상)
- CER: 영점사격 0.89% → 미세조정 1.12% (0.23% 악화)

**의미**: 미세조정의 이점과 손해가 명확히 구분되어, 응용 시나리오에 따른 신중한 선택 필요

#### (3) 계산 비용

**추론 단계**:
- 분류기-무의존 지도 사용 시 RTF = 0.802 (거의 실시간)
- 기존 자동회귀 모델(예: Tacotron2)보다는 느림

**개선 여지**: DDIM 같은 빠른 샘플링 기법 미적용

### 7.3 설계 선택의 트레이드오프

| 선택 | 장점 | 단점 |
|------|------|------|
| 분류기-무의존 지도 | 단순한 학습, 유연한 조건부 신호 | 2배의 계산 비용 |
| 규범-기반 지도 | 높은 발음 정확도 | 화자 유사도와의 트레이드오프 |
| 500회 미세조정 | 최적 균형 | 각 새 화자마다 40초 필요 |
| 10초 참조 음성 | 실용적 | 더 짧은 참조로는 성능 저하 |

---

## 8. 최신 관련 연구 동향 (2020년 이후)

### 8.1 확산 모델 기반 TTS의 발전

#### (1) Grad-TTS (Popov et al., 2021)

**핵심 특성**: 점수 기반 생성 모델을 TTS에 최초 적용

```
포지셔닝: Guided-TTS 2의 사전작업
│
특징:
├─ DDPM 기반의 음성 생성
├─ 화자 ID 기반 조건부 생성
└─ 단일 화자 모델에 초점

│
Guided-TTS 2와의 차이:
├─ 다중 화자 적응 불가 (Guided-TTS 2: 가능)
├─ 전사 필요 (Guided-TTS 2: 무전사)
└─ 일반화 능력 제한 (Guided-TTS 2: 우수)
```

#### (2) Grad-StyleSpeech (Kang et al., 2023)

**혁신점**: 확산 모델에 스타일 전이 통합

$$\text{음성} = \text{텍스트 내용} + \text{화자 스타일}$$

**성능**: 
- LibriTTS에서 Guided-TTS 2와 유사한 MOS (4.1-4.2)
- 스타일 제어가 더 직관적

**Guided-TTS 2와의 비교**:
- 유사한 성능, 다른 설계 철학
- Grad-StyleSpeech: 스타일 명시적 모델링
- Guided-TTS 2: 화자 임베딩으로 암묵적 모델링

#### (3) StyleTTS 2 (Li et al., 2023)

**특징**: 대규모 음성 언어 모델과 대조 학습 활용

**성능**: MOS 4.4 이상 (Guided-TTS 2의 4.2 능가)

**차이점**:
- 참조 음성 불필요 (완전 비조건부)
- 계산량 많음
- 엔드-투-엔드 구조

### 8.2 화자 적응 기술의 진화

#### (1) YourTTS (Casanova et al., 2021)

**방식**: Zero-shot 화자 적응 (기존 방식)

**구조**:
```
참조 음성 → [화자 인코더] → 음성 임베딩
                               ↓
                        [멀티 화자 TTS 모델]
```

**한계**: 10초 참조 음성으로는 Guided-TTS 2보다 낮은 품질

#### (2) Meta-StyleSpeech (Min et al., 2021)

**방식**: 메타러닝 기반 빠른 적응

**특징**: MAML을 이용한 최적화

**성능**:
- 10초 참조로 적응 가능
- Guided-TTS 2보다 CER 높음 (1.52% vs 1.12%)
- 계산 오버헤드 적음

### 8.3 최신 트렌드 (2024-2025)

#### (1) 엔드-투-엔드 텍스트-음성 변환

**예시**: F5-TTS, Voicebox 등

**특징**: 
- 음소 정렬 불필요
- 신경 코덱 기반 이산 토큰 사용
- 자동회귀 언어 모델 활용

**장점**: 단순한 아키텍처, 높은 품질
**단점**: 추론 속도 느림, 큰 모델 크기

#### (2) 매개변수 효율적 미세조정 (PEFT)

**방법들**: LoRA, 어댑터, 하이퍼네트워크

**적용 예시**:
- VoiceTailor: 0.25% 매개변수로 화자 적응
- PEFT-TTS: F5-TTS에 1.72% 매개변수 사용

**Guided-TTS 2와의 관련성**: 100% 매개변수 미세조정의 한계 해결

#### (3) 다중 매개변수 제어

**예시**: DEX-TTS, DPI-TTS, DualSpeech

**특징**: 
- 명시적 스타일 제어 (속도, 음높이, 감정)
- 화자와 문제 특성을 분리
- Guided-TTS 2의 암묵적 모델링보다 직관적

### 8.4 무전사 데이터 활용의 발전

#### (1) 자기 지도 학습 (Self-Supervised Learning)

**기법**: 음성 표현의 무지도 학습
- HuBERT, wav2vec 2.0 등

**관련성**: Guided-TTS 2의 무전사 학습과 다른 접근

#### (2) 의사 레이블 (Pseudo-labeling)

**방식**: 부분적으로 전사된 데이터 → 자동 전사 → 확장된 학습 데이터

**성공 사례**: IndicVoices-R (22개 인도 언어)

### 8.5 일반화 성능 향상 연구

#### (1) Information Perturbation 기법 (Bang et al., 2023)

**목표**: 미학습 화자에 대한 일반화 개선

**방법**: 훈련 중 음성 특성 섭동
- 포먼트 시프팅
- 음높이 변조
- 파라메트릭 등화

**효과**: Zero-shot 화자 유사도 향상

#### (2) 다양한 화자 데이터셋의 활용

**IndicVoices-R 벤치마크**:
- 10,496명 화자 × 22개 언어
- 영점사격, 몇-샷, 다-샷 평가

**발견**: 대규모 다양한 데이터의 중요성

---

## 9. 향후 연구 방향 및 영향

### 9.1 논문이 제시한 미래 연구 방향

#### 1. 매개변수 효율적 미세조정

**제안**: 
$$\text{최소 매개변수 미세조정} \rightarrow \text{적응 성능 유지}$$

**구체적 방법**:
- 어댑터 모듈 삽입 (5-10% 매개변수)
- LoRA 적용 (1% 이상의 효율성)
- 선택적 계층 미세조정

**기대 효과**: 
- 메모리 95% 감소
- 각 화자마다 별도 모델 저장 필요 없음
- 실시간 성능 최소화 영향

#### 2. 엔드-투-엔드 적응형 TTS

**동기**: 보코더 의존성 제거

**구조**:
```
텍스트 + 참조 음성 → [확산 모델] → 직접 파형 생성
                    (멜-스펙트로그램 제거)
```

**도전과제**:
- 높은 차원의 신호 (16kHz = 16,000 샘플/초)
- 확산 모델의 느린 샘플링

**가능한 해결책**: 계층적 생성 또는 신경 코덱 활용

### 9.2 새로운 연구 기회

#### 1. 교차 언어 적응 (Cross-Lingual Adaptation)

**문제 정의**:
$$\text{영어 TTS 모델} + \text{한국어 참조 음성} \rightarrow \text{한국어 음성 생성?}$$

**Guided-TTS 2의 확장성**:
- 현재: 단일 언어 (영어) 초점
- 가능성: 다국어 IPA 음소 활용으로 교차 언어 전이

#### 2. 적응형 감정 음성 합성

**확장**:
$$\hat{s}_\theta(X_t|\hat{y}, \hat{S}, \hat{E}) = ...$$

여기서 $\hat{E}$는 감정 특성

**구현**: 감정 분류기 추가 + 감정 지도

#### 3. 실시간 스트리밍 TTS

**도전**: 
- 현재 50 단계의 역 확산 = 느린 응답
- 스트리밍 호환성 부족

**해결 방안**:
- DDIM (DDPM 개선) 또는 다른 빠른 샘플링 기법
- 적응적 단계 수 조정

### 9.3 응용 분야에서의 영향

#### 1. 영상 게임 및 영화 더빙

**장점**:
- 비인간 캐릭터 음성 생성 (논문의 골룸 예시)
- 최소 데이터로 새 캐릭터 음성 적응
- 배우의 음성이 명확하지 않은 경우에도 작동

**잠재적 사용 사례**:
- 비용 절감: 성우 녹음 시간 단축
- 창의성 향상: 다양한 음성 변형 시도

#### 2. 개인화 음성 복제

**시나리오**: 질병이나 노화로 음성이 변한 사람들의 음성 복원

**필요성**: 
- 소량의 참조 음성 (10초) 가능
- 프라이버시 보호 (무전사 데이터)

**성능**: 논문의 높은 SMOS(3.7)가 개인화 작업에 적합

#### 3. 웹 접근성 향상

**사용 사례**: 텍스트-음성 서비스의 개인화

**장점**:
- 사용자의 음성으로 개인화 (참조 음성으로부터)
- 실시간 성능 (RTF = 0.8)
- 프라이버시 (무전사, 온디바이스 가능)

### 9.4 학술적 영향

#### 1. 확산 모델 이론 발전

**기여**:
- 화자 조건부 확산 모델의 구조 정립
- 분류기-무의존 지도의 성공 사례 제시
- 규범-기반 지도의 효과성 입증

**영향**: 다른 멀티모달 생성 작업에 적용 가능

#### 2. 메타러닝과 확산 모델의 결합

**미래 연구**: 
$$\text{Guided-TTS 2} + \text{MAML}$$

가능성: 500회 미세조정을 몇 회로 축소

#### 3. 일반화 성능 평가 기준 확립

**논문의 벤치마크**:
- LibriTTS, VCTK, LJSpeech (표준)
- 실제 YouTube 데이터 (신규)

**영향**: 이후 적응형 TTS 연구의 평가 방법 제시

### 9.5 윤리적 고려사항

#### 1. 음성 피싱 위험성

**명시적 우려**: 논문의 Section C

**방지 방안**:
- 안티 스푸핑 기술 개발 (논문에서 제시한 미래 과제)
- 합성 음성 탐지 기술 병행
- 법적 규제 정비

**Guided-TTS 2의 결정**: 코드 미공개

#### 2. 초상권 및 음성권 보호

**이슈**: 타인의 음성 15초만으로 합성 음성 생성 가능

**해결책**:
- 합의된 사용자 음성만 사용
- 사용 기록 투명성 확보
- 규제 프레임워크 개발

---

## 10. 종합 평가 및 결론

### 10.1 주요 성과 재정리

| 평가 항목 | 수준 | 이유 |
|---------|------|------|
| **혁신성** | ⭐⭐⭐⭐⭐ | 무전사 데이터로 적응형 TTS 구현 첫 사례 |
| **성능** | ⭐⭐⭐⭐⭐ | 10초 참조로 단일 화자 모델 수준 달성 |
| **실용성** | ⭐⭐⭐⭐ | 40초 미세조정은 실용적이나 저장 문제 있음 |
| **일반화** | ⭐⭐⭐⭐ | 대규모 다중 화자로 잘 일반화되나 언어 제한 있음 |
| **재현가능성** | ⭐⭐⭐ | 코드 미공개로 인한 재현 어려움 |

### 10.2 Guided-TTS 2의 위치 재정의

```
적응형 TTS 발전 경로:

2018 이전: 메타 학습 기반 (Meta-TTS)
          └─ 단점: 느린 적응, 복잡한 구조

2019-2020: 화자 인코더 기반 (YourTTS, 전구)
          └─ 단점: 품질 부족, 데이터 요구량

2021-2022: Guided-TTS (규범 기반 지도)
          └─ 장점: 무전사 가능, 중간 수준 성능
          └─ 단점: 단일 화자 전용, 일반화 부족

2022-2023: **Guided-TTS 2** (분류기-무의존 지도)
          └─ 장점: 무전사 + 다중 화자 + 높은 성능
          └─ 단점: 매개변수 효율성, 계산 비용

2023-2025: 신 방법들 (엔드-투-엔드, PEFT 통합)
          └─ 추세: StyleTTS 2, F5-TTS 등
          └─ 특징: 더 높은 성능, 더 효율적
```

### 10.3 연구에 미치는 직접적 영향

#### 단기 (2023-2024)

1. **확산 모델 + 적응형 TTS 결합의 타당성 증명**
   - 후속 연구의 기초 제공
   - StyleTTS 2, Grad-StyleSpeech 등이 유사 아이디어 발전

2. **무전사 학습의 실행 가능성**
   - 대규모 무전사 데이터셋의 가치 입증
   - Libri-Light 등의 효과적 활용법 제시

#### 중기 (2024-2025)

1. **PEFT 통합 연구 촉발**
   - VoiceTailor (LoRA 적용) 개발
   - 어댑터 기반 미세조정 표준화

2. **다국어 확산 모델 연구**
   - Guided-TTS 2의 영어 중심 한계 인식
   - 다국어 TTS로의 확장 논의

### 10.4 향후 연구 시 고려할 핵심 사항

#### 1. **매개변수 효율성 필수**

현재 Guided-TTS 2:
```
100명 새 화자 × 100MB = 10GB
```

미래 방향:
```
100명 새 화자 × 1-5MB = 100-500MB (99% 감소)
```

**구현**: 어댑터 또는 LoRA 필수

#### 2. **계산 효율성 개선**

현재:
- 50 단계 역 확산 = RTF 0.8 (거의 실시간이나 여전히 느림)

목표:
- DDIM 또는 다른 빠른 방법으로 10-20 단계로 축소
- RTF < 0.1 (진정한 실시간)

#### 3. **보코더 의존성 제거**

**방안 1**: 엔드-투-엔드 확산 모델
- 파형 직접 생성
- 보코더 병목 제거

**방안 2**: 신경 코덱 활용
- 더 낮은 차원의 표현
- 더 빠른 생성

#### 4. **언어 확장성**

**전략**:
- IPA 음소 시스템의 보편성 활용
- 각 언어별 음성 특성 학습 (별도 음소 분류기)
- 교차 언어 전이 학습 탐색

#### 5. **윤리적 설계**

**필수 조치**:
- 합성 음성 탐지 기술 병행 개발
- 사용자 동의 및 기록 시스템
- 규제 협력 및 표준 제정

#### 6. **평가 지표 다양화**

현재: MOS, SMOS, CER

추가 고려:
- **Robustness**: 소음 있는 참조 음성에 대한 성능
- **Consistency**: 같은 텍스트의 반복 생성 일관성
- **Naturalness**: 문맥상 부자연스러움 평가
- **Intelligibility**: 시각 장애인용 접근성 평가

### 10.5 최종 평가

**Guided-TTS 2는 적응형 음성 합성 분야에서:**

1. **기술적 혁신** 
   - 무전사 데이터를 이용한 다중 화자 적응의 최초 성공 사례
   - 확산 모델의 음성 합성 적용 가능성 확장

2. **성능의 획기적 향상**
   - 10초 참조로 24시간 학습 모델 수준 달성
   - 기존 적응형 TTS 대비 20-30% 성능 개선

3. **실용성과 한계의 균형**
   - 40초 미세조정은 실용적이나 매개변수 저장 문제 존재
   - 계산 비용(RTF 0.8)은 배포 가능 수준

4. **향후 연구 방향 제시**
   - 매개변수 효율성 개선의 필요성 명확화
   - 엔드-투-엔드 생성 모델의 가능성 제시
   - 윤리적 고려사항의 중요성 강조

**결론**: Guided-TTS 2는 수년간의 후속 연구를 위한 견고한 기초를 제공하며, 2025년 현재 더 효율적이고 강력한 모델들(StyleTTS 2, F5-TTS 등)의 발전에 직접적으로 기여한 영향력 있는 연구입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/018adf0e-8c65-49ad-a8d2-4588744a9b36/2205.15370v1.pdf)
[2](http://arxiv.org/pdf/2211.09383.pdf)
[3](https://arxiv.org/pdf/2306.07691.pdf)
[4](http://arxiv.org/pdf/2408.14739.pdf)
[5](https://www.linkedin.com/pulse/developments-text-to-speech-technology-20202025-focus-abdelazim-6di8f)
[6](https://arxiv.org/abs/2406.19135)
[7](https://arxiv.org/html/2409.11835)
[8](https://arxiv.org/abs/2409.05356)
[9](https://www.isca-archive.org/interspeech_2025/kwon25_interspeech.pdf)
[10](http://arxiv.org/pdf/2312.03491.pdf)
[11](http://arxiv.org/pdf/2309.06787.pdf)
[12](http://arxiv.org/pdf/2406.11427.pdf)
[13](http://arxiv.org/pdf/2305.12708.pdf)
[14](https://pdfs.semanticscholar.org/2aec/b02c641c6b73ef124b7ca6d69eb87f89797d.pdf)
[15](https://www.nature.com/articles/s41598-025-90507-0)
[16](https://arxiv.org/html/2506.01020v1)
[17](https://www.isca-archive.org/interspeech_2025/chen25b_interspeech.pdf)
[18](https://pure.kaist.ac.kr/en/publications/grad-stylespeech-any-speaker-adaptive-text-to-speech-synthesis-wi/)
[19](https://www.resemble.ai/zero-shot-voice-cloning-guide/)
[20](https://arxiv.org/html/2409.05730v1)
[21](https://speechify.com/blog/zero-shot-voice-cloning/)
[22](https://aclanthology.org/2025.coling-main.352.pdf)
[23](https://ieeexplore.ieee.org/document/9980331/)
[24](https://arxiv.org/abs/2208.02189)
[25](https://ieeexplore.ieee.org/document/9188779/)
[26](https://arxiv.org/abs/2401.11771)
[27](https://www.semanticscholar.org/paper/9bf1653aa6db668ed387d445738c857a7b6b17d8)
[28](https://www.hindawi.com/journals/scn/2023/8830894/)
[29](https://www.ijraset.com/best-journal/autotuned-voice-cloning-enabling-multilingualism)
[30](https://arxiv.org/abs/2404.02781)
[31](https://www.semanticscholar.org/paper/75e4db779638e8718c2001d205c09792b9ab0d7f)
[32](https://arxiv.org/pdf/2210.10985.pdf)
[33](https://arxiv.org/pdf/2106.15153.pdf)
[34](https://arxiv.org/pdf/2501.08566.pdf)
[35](https://arxiv.org/html/2503.23108v1)
[36](https://arxiv.org/pdf/2306.03509.pdf)
[37](http://arxiv.org/pdf/2408.14423.pdf)
[38](http://arxiv.org/pdf/2406.00654.pdf)
[39](https://papers.baulab.info/papers/also/Ho-2022.pdf)
[40](https://aclanthology.org/2025.findings-naacl.279.pdf)
[41](https://pmc.ncbi.nlm.nih.gov/articles/PMC10708733/)
[42](https://www.isca-archive.org/interspeech_2023/choi23c_interspeech.pdf)
[43](https://liusongxiang.github.io/meta-voice-demo/)
[44](https://aclanthology.org/2024.findings-emnlp.533.pdf)
[45](https://www.haoyizhu.site/blog/guidance_in_diffusion/guidance/)
[46](https://www.semanticscholar.org/paper/Meta-TTS:-Meta-Learning-for-Few-Shot-Speaker-Huang-Lin/f6eb50e45e0d9d6f2dfb3d729f9fb8e5ee577bec)
[47](https://arxiv.org/abs/2209.12549)
[48](https://arxiv.org/abs/2406.03051)
[49](https://dl.acm.org/doi/10.1145/3664647.3681695)
[50](https://arxiv.org/abs/2406.06251)
[51](https://arxiv.org/abs/2309.06922)
[52](https://ieeexplore.ieee.org/document/11092635/)
[53](https://arxiv.org/abs/2503.16945)
[54](https://arxiv.org/abs/2403.11808)
[55](https://arxiv.org/abs/2506.09105)
[56](https://arxiv.org/abs/2212.10650)
[57](https://ieeexplore.ieee.org/document/10446686/)
[58](https://arxiv.org/pdf/2305.18028.pdf)
[59](https://arxiv.org/pdf/2211.00585.pdf)
[60](http://arxiv.org/pdf/2406.17257.pdf)
[61](https://arxiv.org/pdf/2404.04645.pdf)
[62](http://arxiv.org/pdf/2210.04284.pdf)
[63](https://arxiv.org/pdf/2406.07832.pdf)
[64](https://arxiv.org/html/2406.03051v1)
[65](https://scholar.gist.ac.kr/bitstream/local/8060/2/IEEE_Access_2024_TTS.pdf)
[66](https://www.microsoft.com/en-us/research/wp-content/uploads/2022/12/Pre_Training_Transformer_Decoder_for_End_to_End_ASR_Model_with_Unpaired_Speech_Data.pdf)
[67](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/sbgm/)
[68](https://www.isca-archive.org/icslp_1998/zavaliagkos98_icslp.pdf)
[69](https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_QianXu_86_t7.pdf)
[70](https://randomsampling.tistory.com/543)
[71](https://arxiv.org/abs/2103.14583)
[72](https://arxiv.org/html/2509.18470v2)
[73](https://investigate.tistory.com/121)
