# UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data

### 1. 핵심 주장 및 기여
UnitSpeech는 **자기 감독 학습 기반 단위(unit) 표현을 활용하여 비전사 음성 데이터만으로 확산 모델 기반 TTS를 화자 맞춤형으로 미세 조정할 수 있는 방법**을 제안합니다.[1]

**세 가지 핵심 기여:**

첫째, 화자 적응을 위해 **단위 표현을 처음으로 도입**했습니다. 기존의 AdaSpeech 2가 멜-스펙트로그램을 사용한 것과 달리, HuBERT 기반 자기 감독 학습 단위는 음성 내용과 화자 정체성을 더 효과적으로 분리합니다.[1]

둘째, **연결 가능한 단위 인코더**를 제안하여 사전 학습된 TTS 모델에 플러그인처럼 통합할 수 있게 했습니다. 이를 통해 전사 없이 비전사 음성으로만 미세 조정이 가능합니다.[1]

셋째, **분류기-자유 안내(classifier-free guidance) 기법**을 도입하여 발음 정확도를 개선했습니다. 동적 가이던스 강도 조절로 발음 정확도와 화자 유사도의 트레이드오프를 최적화합니다.[1]

### 2. 해결하고자 하는 문제와 제안 방법
#### 2.1 문제 정의

기존 화자 적응 방식의 한계는 명확합니다. 화자 임베딩 기반 방법은 구현이 쉽지만 화자 유사도가 낮고, 미세 조정 기반 방법은 높은 유사도를 달성하나 대량의 전사된 음성 데이터를 필요로 합니다. AdaSpeech 2는 미세 조정 없이 비전사 음성을 활용했지만, 결정적 디코더의 한계로 많은 데이터가 필요하며 TTS 작업에만 한정되었습니다.[1]

#### 2.2 제안 방법의 수학적 기초

**확산 모델의 정방향 프로세스:**

$$dX_t = -\frac{1}{2}X_t\beta_t dt + \sqrt{\beta_t} dW_t, \quad t \in [0, T]$$

여기서 $\beta_t$는 노이즈 스케줄, $W_t$는 위너 프로세스입니다.[1]

**훈련 목표 함수 (Grad-TTS 손실):**

$$L_{grad} = \mathbb{E}_{t,X_0,\epsilon_t}\left[\|(\sqrt{\lambda_t}s_\theta(X_t, t|c_y, e_S) + \epsilon_t\|_2^2\right]$$

여기서 $X_t = \sqrt{1-\lambda_t}X_0 + \sqrt{\lambda_t}\epsilon_t$는 잡음 추가 과정, $s_\theta$는 점수 함수, $c_y$는 정렬된 텍스트 인코더 출력, $e_S$는 화자 임베딩입니다.[1]

**단위 인코더는 동일한 훈련 목표를 사용하되, $c_y$를 $c_u$로 대체합니다:**

$$L_{unit} = L_{grad} + L_{enc}, \quad \text{where } L_{enc} = MSE(c_u, X_0)$$

이는 단위 인코더 출력이 텍스트 인코더 출력과 동일한 표현 공간에 위치하도록 합니다.[1]

**역방향 이산화 프로세스:**

$$X_{t-1}^N = X_t + \frac{\beta_t}{N}\left(\frac{1}{2}X_t + s_\theta(X_t, t|c_y, e_S)\right) + \sqrt{\frac{\beta_t}{N}}z_t$$

**분류기-자유 안내 메커니즘:**

$$\hat{s}(X_t, t|c_c, e_S) = s(X_t, t|c_c, e_S) + \gamma \cdot \alpha_t$$

$$\alpha_t = s(X_t, t|c_c, e_S) - s(X_t, t|c_{mel}, e_S)$$

여기서 $\gamma$는 가이던스 강도를 제어하는 스칼라입니다. TTS에서는 $\gamma=1.0$, VC에서는 $\gamma=1.5$가 최적입니다.[1]

#### 2.3 모델 구조

UnitSpeech의 아키텍처는 세 단계로 구성됩니다:

**단계 1: 자기 감독 단위 추출**
- HuBERT로부터 연속 음성 표현 추출
- K-means 클러스터링으로 음소 수준 이산 단위 생성
- 멜-스펙트로그램 길이로 업샘플링 후 단위 지속 시간에 따라 압축

**단계 2: 단위 인코더 훈련**
- 텍스트 인코더와 동일한 아키텍처
- 단위 시퀀스를 입력으로 하여 음성 내용 인코딩
- 사전 학습된 확산 디코더는 동결
- 200K 반복으로 단위 인코더만 훈련

**단계 3: 화자 적응**
- 목표 화자의 단위-음성 쌍 하나로 디코더 미세 조정
- 단위 인코더 동결 (발음 열화 방지)
- 500 반복으로 수렴 (RTX 8000에서 <1분)
### 3. 성능 향상과 한계
#### 3.1 성능 결과

**TTS 작업 평가 결과:**

UnitSpeech는 Guided-TTS 2와 비교하여 음질(MOS 4.13 vs 4.16)과 화자 유사도(SCOS 0.935 vs 0.937)에서 거의 동등한 성능을 달성하면서도, 훨씬 더 간단한 훈련 절차를 제공합니다. 특히 YourTTS(MOS 3.57)보다 현저히 우수합니다.[1]

**Voice Conversion 성능:**

VC 작업에서는 자연스러움(MOS 4.26)과 화자 유사도(SECS 0.923)에서 모든 기준 모델을 초과합니다. DiffVC(SECS 0.909)와 YourTTS(SECS 0.763)보다 우수한 결과를 보입니다.[1]
#### 3.2 절제 연구의 인사이트

**단위 클러스터 수의 영향:**
- TTS에서는 K 값에 상관없이 일관된 성능 (CER 1.75-1.94%)
- VC에서는 K=200일 때 최적 (CER 3.55%), K=50에서는 12.64%로 급격히 저하
- 더 세분화된 단위가 음운 정보를 더 잘 포착함을 의미합니다.[1]

**미세 조정 반복의 영향:**
- 화자 유사도는 500 반복까지 꾸준히 증가 (0.849→0.935)
- 2000 반복 이상: 과적합으로 발음 정확도 감소
- **500 반복이 효율성과 성능의 최적 지점**[1]

**참조 음성 길이의 중요성:**
- 3초: CER 2.16% (충분하지 않음)
- 5초: CER 1.96% (실용적 기준)
- 30초: CER 1.88% (최적 성능)
- **5초 음성으로도 수용 가능한 성능 달성**[1]

**분류기-자유 안내의 효과:**
- γ=0 (안내 없음): CER 2.83%, SECS 0.941
- γ=1.0 (최적): CER 1.75%, SECS 0.935
- **38% 발음 정확도 개선, 화자 유사도 미세 감소**[1]

#### 3.3 주요 한계

**발음 정확도의 트레이드오프:**
VC 작업에서 CER 3.55%는 기존 방법(BNE-PPG-VC 1.37%)보다 높습니다. 이는 단위 기반 입력이 세밀한 음운 정보를 완전히 포착하지 못하기 때문입니다.[1]

**단위 표현의 제약:**
HuBERT 기반 단위는 주로 음소 정보를 포함하며, 음성 특성(음색, 감정), 방언 특이성, 미세 음운 변이를 불완전하게 모델링합니다.[1]

**데이터 요구 사항:**
최소 5초의 청정 참조 음성이 필요하며, 노이즈가 있는 실제 환경 음성에 대한 성능이 불명확합니다.[1]

**매개변수 효율성:**
전체 확산 디코더를 미세 조정하므로, 최근의 LoRA 기반 방법(VoiceTailor, NanoVoice)보다 매개변수 효율성이 낮습니다.[1]

**평가 제한:**
- LibriTTS(주로 영어)만으로 평가
- YouTube 샘플 제시했지만 정량 평가 부족
- 아동, 노인, 악센트 음성 미평가
- 감정, 음색 제어 기능 미지원

### 4. 일반화 성능 향상 가능성 분석
#### 4.1 현재 일반화 강점

**다양한 작업 지원의 우월성:**
단일 모델로 TTS, VC, 음성-음성 번역 등 다양한 작업을 지원합니다. 이는 학습된 단위 표현이 **작업 불변(task-invariant)** 음성 내용 특성을 효과적으로 포착함을 시사합니다.[1]

**자기 감독 학습의 일반화:**
HuBERT는 2,456명 화자의 LibriTTS에서 훈련되어 다양한 음성 특성에 노출되었습니다. 이는 비학습 화자로의 일반화 가능성을 증대시킵니다.[1]

**효율적인 미세 조정:**
5초 음성으로도 충분한 성능을 달성하면서 과적합을 피합니다. 이는 확산 모델의 **강력한 정규화 특성**을 활용하고 있음을 나타냅니다.[1]

#### 4.2 일반화 개선 필요 영역

**발음 정확도 문제:**
단위 인코더가 제공하는 음운 정보의 불완전성이 발음 정확도 저하의 근본 원인입니다. 개선 방안:

- **다중 레벨 단위 표현:** K-means 외에 계층적 클러스터링으로 음운, 음성 세부, 음소 수준 정보를 다층화
- **음운-음향 매핑 강화:** 텍스트 인코더와 단위 인코더 사이 명시적 음운 정렬 메커니즘 추가
- **멀티 모달 조건화:** 피치, 음성 강도 등 음성 특성을 명시적으로 모델링

**화자 다양성 제한:**
현재 LibriTTS(주로 북미 영어)의 화자만으로 평가되었습니다:

- **다국어 사전 학습:** XLSR-Wav2Vec 등으로 다국어 단위 표현 학습
- **악센트 적응:** 악센트별 미세 조정 경로 추가
- **음성 특성 확장:** 아동, 노인, 병리적 음성 포함

**스타일 제어 미흡:**
단위 표현은 음운 정보에 주로 초점이 맞춰져 있어 스타일 변이 제어가 불가능합니다:

- **스타일 인코더 통합:** AS-Speech 방식으로 음색, 리듬 세부 인코더 추가
- **감정 제어:** ZET-Speech처럼 감정 레이블 기반 제어 메커니즘
- **프로소디 보존:** Stable-TTS처럼 프로소디 언어 모델 활용

**매개변수 효율성 격차:**
VoiceTailor(0.25% 파라미터), NanoVoice(0.02% 파라미터)에 비해 UnitSpeech는 100% 미세 조정을 수행합니다:

- **LoRA 기반 적응:** 병목 모듈에만 LoRA 어댑터 적용
- **어댑터 모듈화:** 낮은 순위 분해를 통한 계층적 미세 조정
- **지식 증류:** 경경량화된 프록시 모델로 지식 압축

#### 4.3 향후 연구 방향

**단기 (1-2년):**
1. 단위 표현 개선: 음운-음향 정보 통합
2. 다국어 지원: 자기 감독 학습 모델 확대
3. 매개변수 효율성: LoRA 기반 적응 도입
4. 평가 확대: 실제 환경, 다양한 화자 포함

**중기 (2-3년):**
1. 스타일 제어: 음색, 감정, 음향 특성 통합
2. 노이즈 견고성: 우리 환경 음성에 대한 강건성
3. 실시간 적응: 온라인 학습 메커니즘
4. 멀티모달 조건화: 텍스트-음성 외 시각 정보 활용

**장기 (3-5년):**
1. 통합 프레임워크: zero-shot와 few-shot의 최적 조합
2. 자동화된 하이퍼파라미터 최적화
3. 계산 리소스 극소화: 엣지 디바이스 배포
4. 윤리적 음성 합성: 목소리 복제 사용 제어

### 5. 최신 연구와의 비교 분석 (2020-2024)
#### 5.1 주요 경쟁 방법들의 특징

**Guided-TTS 2 (2022):**
무조건부 확산 모델을 학습하여 분류기 안내를 가능하게 했습니다. UnitSpeech보다 발음 정확도(CER 0.84%)는 우수하지만, 학습 과정이 더 복잡합니다.[1][2]

**Grad-StyleSpeech (2022):**
계층적 트랜스포머 인코더를 사용하여 화자 스타일을 명시적으로 모델링합니다. 화자 유사도에서 우수(SECS 0.937)하지만 구조가 더 복잡합니다.[3]

**StyleTTS 2 (2023):**
스타일 확산과 적대적 훈련으로 인간 수준의 TTS를 달성했습니다. 전체 음질 측면에서 가장 우수하지만 구조와 훈련이 매우 복잡합니다.[4]

**VoiceTailor (2024):**
LoRA를 사용하여 전체 파라미터의 0.25%만으로 적응을 달성합니다. 매개변수 효율성이 극우수하지만 세밀한 작업 제어가 어렵습니다.[5]

**NanoVoice (2024):**
배치 단위 미세 조정으로 여러 화자를 동시에 적응시켜 4배 빠른 훈련을 달성합니다. 최고 수준의 시간 및 매개변수 효율성을 보입니다.[6]

**AS-Speech (2024):**
세밀한 음색 특성과 리듬을 통합하여 화자 특성을 더욱 정밀하게 포착합니다. UnitSpeech보다 스타일 모델링이 우수합니다.[7]

#### 5.2 UnitSpeech의 경쟁 우위

UnitSpeech의 차별성은 **단위 기반 표현의 효과성과 다양한 작업 지원의 균형**에 있습니다:

1. **비전사 음성의 효율적 활용:** 전사 단계가 완전히 제거되어 데이터 준비 비용 최소화

2. **작업 유연성:** 단위 입력을 사용하면 TTS, VC, S2ST 등 여러 작업을 재훈련 없이 지원

3. **훈련 절차의 간결함:** 무조건부 모델 없이 기존 Grad-TTS 프레임워크 활용

4. **균형잡힌 성능:** 발음 정확도보다는 화자 유사도에 우수하여 인간적 자연스러움 우선

### 6. 결론 및 향후 연구 고려사항
#### 6.1 이론적 기여

UnitSpeech는 **자기 감독 학습 표현이 화자 적응 TTS에서 전사의 역할을 대체할 수 있음**을 최초로 입증했습니다. 이는 음성 처리에서 텍스트 의존성을 감소시키는 중요한 진전을 의미합니다.[1]

#### 6.2 실무적 영향

1. **자원 부족 환경에서의 TTS:** 전사 없이 새 화자에 적응 가능하여 저자원 언어에 적용 가능
2. **실시간 음성 합성:** 5초 음성으로 빠른 적응(500 반복)으로 상호작용형 애플리케이션 지원
3. **다중 언어 환경:** HuBERT의 다국어 특성으로 다국어 적응 가능성

#### 6.3 향후 연구에서 고려할 점

**기술적 측면:**
- 발음 정확도 개선을 위한 음운 정보 강화
- 노이즈 견고성 평가 및 개선
- 실시간 적응을 위한 온라인 학습 메커니즘

**평가 측면:**
- 영어 외 언어에 대한 평가 확대
- 아동, 노인, 병리적 음성 포함
- 실제 환경(회의, 배경음)에서의 성능 검증

**응용 측면:**
- 음성 복제의 윤리적 규제 프레임워크 개발
- 화자 동의 및 인증 메커니즘 통합
- 개인 정보 보호를 고려한 분산 적응

**경제적 측면:**
- 매개변수 효율성 개선으로 배포 비용 절감
- 엣지 디바이스(모바일, IoT) 배포 가능성
- 상용화를 위한 성능-효율 최적화

UnitSpeech는 확산 모델 기반 화자 적응 TTS 분야에서 실질적 진전을 이루었으며, 특히 자기 감독 학습 표현의 활용 가능성을 입증함으로써 향후 다양한 확장 연구의 기반을 제공하고 있습니다.

***

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/d25ae66c-3468-4ae7-aaa8-e9d68d8ece8c/2306.16083v1.pdf)
[2](https://arxiv.org/pdf/2205.15370.pdf)
[3](https://arxiv.org/abs/2211.09383)
[4](https://arxiv.org/pdf/2306.07691.pdf)
[5](https://www.isca-archive.org/interspeech_2024/kim24_interspeech.html)
[6](https://openreview.net/pdf?id=eNl6FGTlmH)
[7](https://ieeexplore.ieee.org/document/10832337/)
[8](https://vjs.ac.vn/index.php/jcc/article/view/18136)
[9](https://arxiv.org/abs/2306.16083)
[10](https://arxiv.org/abs/2303.01849)
[11](https://www.isca-archive.org/interspeech_2023/tran23d_interspeech.html)
[12](https://arxiv.org/abs/2305.13831)
[13](https://ieeexplore.ieee.org/document/10669040/)
[14](https://arxiv.org/abs/2309.02743)
[15](https://www.semanticscholar.org/paper/593009a295f7766edb451a1e1df387d268dabaf8)
[16](http://arxiv.org/pdf/2211.09383.pdf)
[17](http://arxiv.org/pdf/2408.14739.pdf)
[18](https://arxiv.org/pdf/2306.16083.pdf)
[19](https://arxiv.org/abs/2406.19135)
[20](https://arxiv.org/pdf/2409.09311v2.pdf)
[21](https://aclanthology.org/2023.emnlp-main.990.pdf)
[22](https://apxml.com/courses/speech-recognition-synthesis-asr-tts/chapter-4-advanced-text-to-speech-synthesis/voice-cloning-conversion)
[23](https://aclanthology.org/2022.aacl-main.56.pdf)
[24](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/gss/)
[25](https://ieeexplore.ieee.org/document/9956613/)
[26](https://aclanthology.org/2025.findings-naacl.279.pdf)
[27](https://arxiv.org/abs/2309.07598)
[28](https://www.isca-archive.org/interspeech_2020/choi20c_interspeech.pdf)
[29](https://aclanthology.org/2024.acl-short.24.pdf)
[30](https://arxiv.org/pdf/2412.20155.pdf)
[31](https://arxiv.org/html/2507.04817v1)
[32](https://arxiv.org/html/2505.00579v1)
[33](https://web3.arxiv.org/pdf/2211.09383v1)
[34](https://arxiv.org/html/2512.06304v1)
[35](https://arxiv.org/pdf/1906.07414.pdf)
[36](https://arxiv.org/html/2505.14351v1)
[37](https://arxiv.org/pdf/2404.18094.pdf)
[38](https://arxiv.org/html/2412.20155v1)
[39](https://arxiv.org/pdf/2305.18975.pdf)
[40](https://pure.kaist.ac.kr/en/publications/grad-stylespeech-any-speaker-adaptive-text-to-speech-synthesis-wi/)
