# DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion

### 1. 핵심 주장 및 주요 기여

DiffSVC는 **가성 음성 변환(Singing Voice Conversion, SVC) 분야에 확산 확률 모델(Denoising Diffusion Probabilistic Model, DDPM)을 최초로 적용한 시스템**이다. 이 논문의 핵심 주장은 기존의 GAN 기반 또는 시퀀스-투-시퀀스 모델보다 **높은 자연스러움과 음성 유사도를 달성**할 수 있다는 것이다.[1]

주요 기여는 다음과 같다:

1. **확산 모델의 SVC 적용**: 가성 음성 변환 분야에서 처음으로 확산 확률 모델을 도입하여, 이 모델 클래스가 SVC 작업에 효과적으로 적용될 수 있음을 입증했다.[2][1]

2. **우수한 성능 달성**: 주관적 평가(MOS: Mean Opinion Score)에서 자연스러움(Naturalness) 3.97점, 음성 유사도(Voice Similarity) 4.67점을 기록하여 기존 최첨단 시스템들(BLSTM-SVC, Seq2seq-SVC, FastSVC)을 능가했다.[1]

3. **포괄적인 조건부 생성**: 음운 정보(PPG), 기본 주파수(Log-F0), 음성의 크기(Loudness)를 통합적으로 활용한 조건부 생성 방식을 제안했다.[1]

***

### 2. 해결하고자 하는 문제

#### 2.1 기존 SVC 시스템의 한계

기존 SVC 시스템들은 다음과 같은 문제를 안고 있었다:

1. **음질 저하**: GAN 기반 접근법은 우수한 성능을 보이지만, 신경망 음성 합성기(Neural Vocoder) 사용 시 음질이 제한적이었다.[1]

2. **불안정한 훈련**: 생성적 대적 네트워크(GAN)의 고유한 불안정성으로 인해 학습이 어려웠고, 수렴 보장이 약했다.[3][1]

3. **병렬 데이터 의존성**: 기존 접근법 중 일부는 병렬 데이터(source-target 쌍)를 요구하여 데이터 수집 비용이 높았다.[1]

4. **내용 정보 손실**: 가수 신원을 분리하기 위한 적대적 기법들이 음운 정보를 손상시킬 수 있었다.[1]

#### 2.2 DiffSVC의 문제 해결 방향

DiffSVC는 다음 문제들을 해결하고자 했다:

- **고품질 생성**: 노이즈 제거 과정(Denoising Process)을 통해 더 자연스럽고 고품질의 멜 스펙트로그램 생성
- **안정적 훈련**: 확산 모델의 안정적인 학습 과정(ELBO 최적화)
- **포괄적 조건부 정보**: 내용(PPG), 멜로디(F0), 표현력(Loudness)을 동시에 조건으로 사용

***

### 3. 제안하는 방법 (수식 포함)

#### 3.1 확산 모델의 기본 원리

**확산/전방 과정(Diffusion/Forward Process)**:

확산 모델은 데이터를 점진적으로 노이즈로 변환하는 마르코프 연쇄(Markov chain)로 정의된다.[1]

$$q(y_{1:T}|y_0) := \prod_{t=1}^{T} q(y_t|y_{t-1})$$

여기서 각 전이는 조건부 가우시안 분포로 모델링된다:[1]

$$q(y_t|y_{t-1}) := \mathcal{N}(y_t; \sqrt{1-\beta_t}y_{t-1}, \beta_t\mathbf{I})$$

$$\beta_1 < \beta_2 < \cdots < \beta_T$$는 고정된 노이즈 스케줄이다.[1]

**핵심 성질**: 임의의 시간 단계 t에서 직접적으로 노이즈가 추가된 데이터를 샘플링할 수 있다:[1]

$$q(y_t|y_0) = \mathcal{N}(y_t; \sqrt{\bar{\alpha}_t}y_0, \sqrt{1-\bar{\alpha}_t}\mathbf{I})$$

여기서 $\alpha_t := 1-\beta_t$이고 $\bar{\alpha}\_t = \prod_{s=1}^{t}\alpha_s$이다.[1]

**역과정(Reverse Process)**:

역과정은 매개변수화된 마르코프 연쇄로, 노이즈에서 데이터를 복원한다:[1]

$$p_{\theta}(y_{0:T}) := p(y_T)\prod_{t=1}^{T}p_{\theta}(y_{t-1}|y_t)$$

$$p_{\theta}(y_{t-1}|y_t) := \mathcal{N}(y_{t-1}; \mu_{\theta}(y_t, t), \sigma_{\theta}(y_t, t)^2\mathbf{I})$$

#### 3.2 훈련 목적 함수

확산 모델의 훈련은 변분 하한(ELBO, Evidence Lower Bound)을 최대화한다:[1]

$$\mathbb{E}_{q(y_0)}[\log p_{\theta}(y_0)] \geq \mathbb{E}_{q(y_{0:T})}[\log \frac{p_{\theta}(y_{0:T-1}|y_T)p(y_T)}{q(y_{1:T}|y_0)}] := \text{ELBO}$$

**재매개변수화 트릭(Reparameterization Trick)**: 신경망이 추가된 노이즈를 직접 예측하도록 하는 방식을 사용한다:[1]

$$-\text{ELBO} = \mathbb{E}_{y_0,t}[||\epsilon - \epsilon_{\theta}(\sqrt{\bar{\alpha}_t}y_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)||_2^2]$$

여기서 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$이고 $t$는 균일하게 샘플링된 시간 단계이다.[1]

#### 3.3 샘플링 (Langevin 동역학)

훈련된 모델로부터 샘플을 생성할 때, 다음과 같이 역과정을 반복한다:[1]

$$y_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(y_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(y_t, t)\right) + \sigma_t z$$

여기서:
- $\sigma_t^2 = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$
- $z \sim \mathcal{N}(0, \mathbf{I})$ for $t > 1$, $z = 0$ for $t = 1$[1]

#### 3.4 DiffSVC의 조건부 생성

DiffSVC는 다음과 같이 조건부 정보를 통합한다:[1]

1. **음운 사후 확률(Phonetic Posteriorgram, PPG)**: 미리 훈련된 ASR 모델로부터 추출한 음운 정보
2. **기본 주파수(Log-F0)**: 멜로디 정보, 256개 빈으로 양자화
3. **음성의 크기(Loudness)**: 음성 동적 범위, 마찬가지로 256개 빈으로 양자화

**조건자(Conditioner) 계산**:[1]

$$e = \text{PPG}_{\text{prenet}}(x) + \text{MelodyEmbedding}(f_0) + \text{LoudnessEmbedding}(l)$$

여기서 $x$는 PPG, $f_0$는 Log-F0, $l$은 Loudness이다.

***

### 4. 모델 구조

#### 4.1 전체 아키텍처

DiffSVC의 시스템은 다음과 같은 모듈로 구성된다:[1]

1. **PPG 추출기**: Deep-FSMN 기반 ASR 모델
2. **Step 인코더**: 노이즈 레벨을 나타내는 시간 단계 정보를 인코딩
3. **확산 디코더**: 노이즈가 있는 멜 스펙트로그램을 정제하는 신경망
4. **음성 합성기(Vocoder)**: Hifi-GAN을 사용한 최종 파형 생성

#### 4.2 Step 인코더

정수로 표현된 시간 단계 $t$를 128차원 벡터로 변환한다:[1]

$$\text{temb} = [\sin(10^{0 \times 4/63}t), \ldots, \sin(10^{63 \times 4/63}t), \cos(10^{0 \times 4/63}t), \ldots, \cos(10^{63 \times 4/63}t)]$$

이를 두 개의 FC층과 Swish 활성화 함수로 처리한다.[1]

#### 4.3 확산 디코더

**구조**: 양방향 잔차 합성곱 아키텍처 (WaveNet 기반)[1]

- **입력**: 노이즈가 있는 멜 스펙트로그램 $y_t$
- **처리**:
  1. Conv1x1 연산으로 초기 처리
  2. Step 정보를 시간 축 모든 위치에 더함
  3. N개의 잔차 블록 통과 (논문에서는 N=20)
  4. 각 블록마다 조건자 $e$를 Conv1x1으로 처리하여 원소별 덧셈

**잔차 블록 세부 사항**:
- Conv1d 커널 크기 3, dilation rate 1 (멜 스펙트로그램이므로)
- Gated 메커니즘 활용 (WaveNet 스타일)
- Skip 연결을 모든 레이어에서 수집하여 합산
- 최종 두 개의 Conv1x1 층과 ReLU 활성화로 출력 생성[1]

#### 4.4 학습 및 변환 알고리즘

**알고리즘 1: 훈련 절차**[1]

입력: 변환 모델 $\epsilon_{\theta}(\cdot)$, 훈련 집합 $D_{\text{train}} = \{(x, f_0, l, y_0)\}\_{m=1}^M$, $N_{\text{iter}}$ 반복

```
for i = 1, 2, ..., N_iter do
    (x, f_0, l, y_0) ← D_train에서 샘플링
    ε ~ N(0, I)
    t ← {1, ..., T}에서 균일 샘플링
    ∇_θ||ε - ε_θ(√ᾱ_t y_0 + √(1-ᾱ_t) ε, t, x, f_0, l)||²_2에 경사하강법 적용
end for
```

**알고리즘 2: 변환 절차**[1]

입력: 훈련된 변환 모델 $\epsilon_{\theta}(\cdot)$, 테스트 샘플 $(x, f_0, l)$

```
y_T ~ N(0, I) ← 초기 노이즈 샘플링
for t = T, T-1, ..., 1 do
    z ~ N(0, I)
    if t > 1 then
        y_{t-1} = (1/√α_t)(y_t - ((1-α_t)/√(1-ᾱ_t)) ε_θ(y_t, t, x, f_0, l)) + σ_t z
    else
        y_{t-1} = (1/√α_t)(y_t - ((1-α_t)/√(1-ᾱ_t)) ε_θ(y_t, t, x, f_0, l))
    end if
end for
return y_0
```

***

### 5. 성능 향상 및 한계

#### 5.1 성능 향상

**주관적 평가 (MOS, 5점 척도)**:[1]

| 시스템 | 자연스러움(Nat.) | 음성 유사도(Sim.) |
|--------|-----------------|-----------------|
| 원본 녹음 | 4.60±0.09 | - |
| BLSTM-SVC | 2.37±0.15 | 3.67±0.15 |
| Seq2seq-SVC | 2.77±0.17 | 3.57±0.15 |
| FastSVC | 3.83±0.13 | 4.17±0.13 |
| **DiffSVC** | **3.97±0.14** | **4.67±0.10** |

**객관적 평가**:[1]

| 메트릭 | BLSTM-SVC | Seq2seq-SVC | FastSVC | DiffSVC | 설명 |
|-------|-----------|------------|---------|---------|------|
| MCD(↓) | 6.424 | 7.175 | 6.422 | **6.307** | 멜-세프스트럼 왜곡도 (낮을수록 좋음) |
| FPC(↑) | 0.781 | 0.885 | 0.904 | 0.902 | F0 피어슨 상관계수 (높을수록 좋음) |

**성능 향상의 원인**:

1. **안정적인 생성 과정**: ELBO 최적화를 통한 안정적인 훈련
2. **반복적 정제**: 100 단계의 노이즈 제거 과정으로 세밀한 보정
3. **포괄적 조건부 입력**: PPG + F0 + Loudness를 통합하여 음성의 여러 측면을 제어[3][1]

#### 5.2 한계

**논문에서 명시된 한계**:[1]

1. **제한된 훈련 데이터**: 단 14시간의 단일 여성 가수 데이터로만 훈련 (저자는 "저자원 시나리오에서의 훈련 조사"를 향후 연구로 제시)
2. **단일 가수 변환(Any-to-One)**: 다중 목표 가수 변환(Any-to-Many) 미지원
3. **느린 추론 속도**: 100 단계의 반복적 노이즈 제거로 인한 고계산 비용
4. **언어 제약**: 만다린 중국어 음운으로만 훈련된 PPG 모델
5. **멜로디 보존**: F0 메트릭(FPC 0.902)에서 FastSVC(0.904)에 약간 뒤짐[1]

**내재적 문제들**:

1. **PPG의 음성 누출**: PPG 기반 접근법은 원래 가수의 음성 특성이 일부 누출될 수 있음[4][5]
2. **제한된 음성 스타일 모델링**: 성대음, 글리산도, 비브라토 같은 복잡한 가성 표현이 불완전[6][3]
3. **교차 도메인 일반화**: 훈련 데이터와 다른 특성(성별, 나이, 음성 특성)의 가수에 대한 성능 저하 가능성[5][7]

***

### 6. 모델의 일반화 성능 향상 가능성

#### 6.1 현재 DiffSVC의 일반화 한계

DiffSVC가 발표된 2021년 이후, 가성 음성 변환 분야에서 일반화 성능 향상이 중요한 연구 주제가 되었다.[6][5][3]

**주요 문제점**:

1. **저자원 시나리오**: 제한된 훈련 데이터에서의 성능 저하[8][9]
2. **영역 간 변환(Cross-Domain)**: 훈련 데이터의 특성과 다른 영역으로의 일반화 실패[7][8][5]
3. **제로샷 학습**: 미학습 가수에 대한 변환 불가능[10][8][4]

#### 6.2 일반화 성능 향상을 위한 방향성

**2022-2025년 최신 연구에서 제시된 개선 방향**:

1. **잠재 확산 모델(Latent Diffusion Model, LDM) 도입**[11][12][4]
   - 원본 스펙트로그램이 아닌 압축된 잠재 공간에서 확산을 수행
   - 계산 효율성 증가 및 음성 누출 감소
   - LDM-SVC는 분류기 없는 지도(Classifier-free guidance)로 음성 특성 강화[4]

2. **특성 분리(Feature Disentanglement)의 강화**[5][7][10]
   - 음성 내용, 음성 특성, 피치를 명확히 분리
   - 불확실성 모델링 층 정규화(UMLN)로 내용 표현 교란[7][5]
   - 잔차 스타일 적응기(Residual Style Adapter, RSA)로 다양한 음성 표현 포착[13][5][7]

3. **자기감독 학습(Self-Supervised Learning, SSL) 기반 특성 활용**[8][13][10]
   - PPG 대신 ContentVec, Whisper 임베딩 등 사용[3][6]
   - 언어에 제약 없는 범용 음운 특성 추출
   - 다국어 일반화 개선[14][10]

4. **흐름 매칭(Flow Matching) 모델의 도입**[6][3]
   - 확산 모델의 계산 효율성 개선 (1-4 단계로 감소)
   - 일관성 증류(Consistency Distillation)로 추가 가속화[15]

5. **다단계 훈련(Multi-Stage Training) 전략**[6]
   - 초해상도 재구성 → 기법 분리 → 목표 가수 미세조정
   - 점진적 학습을 통한 일반화 성능 향상

#### 6.3 구체적 개선 사례

**HQ-SVC (2025년)**:[9][8]
- 단일 소비자급 GPU와 소규모 데이터셋에서 고품질 제로샷 변환 달성
- 분리된 코덱을 사용한 내용-음성 특성 통합 추출
- 기존 SOTA 방법 대비 음질과 효율성에서 우수

**Everyone-Can-Sing (2025년)**:[13]
- 음성 기반 제로샷 가성 변환으로 확장
- 다양한 표현 요소(감정, 음성 기법) 제어 가능
- 혼합 데이터셋(음성 + 가성)으로 훈련되어 일반화 개선

**REF-VC (2025년)**:[16]
- 잡음 견고한 표현력 있는 음성 변환(음성과 가성 모두)
- SSL 특성의 정보 중복성 제거로 잡음 견고성 향상
- 4 단계 추론으로 계산 속도 개선

**SPA-SVC (2024년)**:[17]
- 자기감독 피치 증강으로 교차 도메인 시나리오에서 성능 향상
- 피치 격차가 큰 경우에도 우수한 성능 유지

#### 6.4 이론적 근거

최근 연구에 따르면 확산 모델의 일반화 성능은 모델 용량과 훈련 데이터 크기 관계에 의존한다:[18]

1. **암기 체계(Memorization Regime)**: 모델 용량 >> 데이터 크기
   - 강한 재현성(Reproducibility)
   - 약한 일반화성(Generalizability)

2. **일반화 체계(Generalization Regime)**: 모델 용량 < 데이터 크기
   - 진정한 데이터 분포의 점수 함수 학습
   - 강한 재현성과 일반화성 동시 달성[18]

따라서 DiffSVC의 일반화를 위해서는:
- **더 큰 규모의 다양한 훈련 데이터** (다양한 가수, 언어, 음성 특성)
- **적절한 모델 정규화** (과적합 방지)
- **특성 분리 메커니즘 강화** (음성 누출 최소화)

***

### 7. 해당 논문이 앞으로의 연구에 미치는 영향

#### 7.1 패러다임 전환

**DiffSVC의 영향**:

1. **생성 모델 선택의 변화**: GAN 중심에서 확산 모델 중심으로 전환[2][3][6]
   - 2021년 논문 발표 이후, 이후 대부분의 SVC 시스템이 확산 모델 기반으로 발전
   - 2025년 SVC 챌린지에서 상위 시스템들 대다수가 확산 모델 또는 흐름 매칭 기반[3][6]

2. **조건부 생성의 표준화**: 멀티모달 조건부 입력 (내용, 피치, 표현) 적용 보편화[13][3][6][1]

3. **안정성과 품질의 우선순위**: 계산 비용보다 품질을 중시하는 경향 (이후 속도 최적화는 별개 문제로 처리)[11][15][3]

#### 7.2 후속 연구들의 확장 방향

**2022-2025년 후속 연구들의 트렌드**:

1. **속도 최적화 (2022-2024)**:[19][15][11]
   - DiffWave (Kong et al., 2020) → FastDiff → LCM-SVC → CoMoSVC
   - 100 단계 → 1-4 단계로 감소, 추론 시간 99% 단축
   - 품질 손실 최소화

2. **일반화 성능 향상 (2023-2025)**:[10][14][8][5][7][13]
   - PPG → ContentVec → SSL 특성으로 진화
   - 단일 가수 → 제로샷 임의 간 변환으로 확장
   - 단일 언어 → 다국어 지원으로 확대

3. **표현력 향상 (2023-2025)**:[13][3][6]
   - 단순 음성 특성 → 음성 기법(비브라토, 글리산도, 음성음), 감정, 발성 스타일 포함[3][6]
   - 특성 분리 메커니즘 정교화

4. **통합 프레임워크 (2024-2025)**:[8][13]
   - SVC → 음성 변환, 가성 합성을 포함하는 통합 모델
   - 단일 모델로 다양한 작업 수행

#### 7.3 기술적 발전에 미친 구체적 영향

**음성 합성 분야 전반으로의 파급**:

1. **텍스트-음성 합성(TTS)**: 확산 모델이 자연스러운 음성 생성의 표준으로 채택[13]

2. **음성 변환(Voice Conversion)**: SVC의 성공이 일반 음성 변환에도 적용되어, VC 분야도 확산 모델 기반으로 전환[20][16]

3. **음성 초해상도(Voice Super-Resolution)**: HQ-SVC의 결과에서 보듯이, 동일한 모델로 SVC와 음성 품질 향상 동시 달성[9][8]

***

### 8. 앞으로의 연구 시 고려할 점

#### 8.1 데이터 측면의 고려사항

**1. 훈련 데이터의 다양성 확보**

기존 DiffSVC는 단일 여성 가수 14시간 데이터로 훈련되어 일반화 한계가 있었다. 향후 연구는:

- **다양한 가수**: 남성, 여성, 아동, 다양한 음성 특성
- **다양한 언어**: 만다린뿐 아니라 영어, 다른 동아시아 언어
- **다양한 장르**: 클래식, 팝, 록, 민요 등
- **다양한 표현 스타일**: 일반 창법, 래핑, 연창 등[14][5][7][10][8]

**2. 저자원 시나리오 대응**

논문의 미해결 문제였던 "저자원 상황에서의 훈련":[1]

- 적은 데이터로도 높은 성능을 유지하는 방법 개발
- 데이터 증강 기법 (스펙트로그램 변환, 피치 시프트 등)
- 메타 학습(Meta-learning) 또는 전이 학습(Transfer learning) 활용
- HQ-SVC의 사례처럼, 소비자급 GPU로도 학습 가능하게[9][8]

#### 8.2 모델 아키텍처 측면

**1. 특성 분리의 정교화**

DiffSVC는 PPG로 내용만 추출했으나, 이는 불완전하다:

- **음운 정보의 개선**: PPG 대신 SSL 기반 특성(ContentVec, WavLM) 사용[8][6][13][3]
- **음성 특성의 정확한 추출**: 단순히 음성 특성만 아니라, 음성 특성의 **미묘한 변화** (동적 특성, 진폭 변화)까지 모델링[5][13]
- **스타일 정보의 독립화**: 음성 기법(비브라토, 글리산도)을 따로 조건으로 사용[6][13][3]

**2. 확산 단계의 적응적 조정**

현재 100 단계는 고정값이나, 향후는:

- **적응적 단계 수**: 입력에 따라 필요한 단계 수를 동적으로 결정
- **조건부 단계 수**: 어려운 경우(큰 음성 차이)는 많은 단계, 쉬운 경우는 적은 단계
- **빠른 추론**: 최근 LCM-SVC, CoMoSVC처럼 1-4 단계로 감소[15][19][11]

#### 8.3 평가 방법론 측면

**1. 평가 메트릭의 확장**

기존 DiffSVC의 평가:
- MOS (자연스러움, 음성 유사도)
- MCD (멜-세프스트럼 왜곡도)
- FPC (F0 피어슨 상관계수)[1]

향후 필요한 평가:
- **음성 기법 평가**: 비브라토, 글리산도 정확도
- **음악성 평가**: 리듬 정확도, 음정 안정성
- **스타일 유지성**: 원본 가수의 고유 특성 보존 정도
- **일반화 성능 평가**: 교차 도메인, 제로샷 시나리오에서의 성능[3][6]

**2. 다층적 평가 프레임워크**

SVC 챌린지(2025)의 사례:
- 음성 신원 변환(Task 1)과 음성 스타일 변환(Task 2) 분리[6][3]
- 다양한 시나리오(도메인 내, 도메인 간)에서의 평가
- 난이도 있는 음성 스타일(음성음, 글리산도, 비브라토) 별도 분석[3][6]

#### 8.4 기술적 개선 방향

**1. 계산 효율성과 품질의 균형**

DiffSVC의 한계: 100 단계 추론으로 인한 느린 속도

해결 방향:
- **흐름 매칭(Flow Matching)**: 확산보다 효율적인 대체[6][3]
- **일관성 증류(Consistency Distillation)**: 사전훈련된 모델에서 더 빠른 모델 유도[11][15]
- **적응적 조기 종료**: 충분히 좋은 결과에 도달하면 조기 중단

**2. 다중 작업 학습 (Multi-Task Learning)**

현재: SVC만 전문
향후:
- **음성 변환 + SVC + 음성 합성**: 단일 모델로 다양한 작업[16][13]
- **음성 스타일 변환**: 음성 기법 변화 포함[3][6]
- **음성 슈퍼 해상도**: 저음질 음성을 고음질로 향상[9][8]

**3. 도메인 적응 기법**

일반화 성능 향상을 위해:
- **메타 학습**: 새로운 도메인에 빠르게 적응[7][5]
- **적대적 훈련**: 도메인 불변 특성 학습
- **가중치 정규화**: 과적합 방지, 일반화 성능 향상

#### 8.5 응용 및 윤리 측면

**1. 실제 응용 고려**

- **실시간 음성 변환**: 지연 시간 최소화 (현재 불가능)
- **엣지 디바이스 지원**: 모바일, 임베디드 시스템에서 실행 가능한 경량 모델
- **다중 가수 변환(Many-to-Many)**: 논문에서 미래 작업으로 제시[1]

**2. 윤리 및 안전성**

- **음성 위조 방지**: 생성된 음성의 진위 판정 기술 병행 개발
- **개인정보 보호**: 음성 데이터 활용에 대한 명확한 동의 및 규제
- **공정성**: 다양한 언어, 성별, 나이의 가수에 대한 공평한 성능 보장

***

### 9. 2020년 이후 관련 최신 연구 동향

#### 9.1 확산 모델 기반 SVC의 진화

**시간별 주요 논문**:

| 연도 | 논문/시스템 | 핵심 기여 | 성능 | 상태 |
|------|-----------|---------|------|------|
| 2021 | **DiffSVC**[1] | 최초 확산 모델 SVC | MOS 3.97 | 시초 |
| 2021 | DiffWave[4] | 오디오 합성용 확산 | SOTA 오디오 | 기반 |
| 2022 | FastDiff[19] | 빠른 조건부 확산 | 100배 속도↑ | 최적화 |
| 2023 | StyleSinger[5] | 제로샷 스타일 전이 | OOD 음성 처리 | 일반화 |
| 2023 | NU-SVC (SVCC)[6] | 흐름 매칭 기반 SVC | 높은 자연스러움 | 개선 |
| 2024 | LDM-SVC[4] | 잠재 확산 + 분류기 지도 | 음성 누출 ↓ | 음질 |
| 2024 | LCM-SVC[11] | 일관성 증류 | 1-4 단계 추론 | 속도 |
| 2024 | CoMoSVC[15] | 일관성 모델 | 빠른 + 고품질 | 통합 |
| 2024 | SPA-SVC[17] | 자기감독 피치 증강 | 교차 도메인↑ | 강건성 |
| 2025 | HQ-SVC[8][9] | 저자원 고품질 SVC | 소규모 데이터 | 실용성 |
| 2025 | Everyone-Can-Sing[13] | 음성 기반 제로샷 | 음성→가성 | 확장성 |
| 2025 | FreeSVC[14] | 다국어 제로샷 | 교차 언어 | 다국어 |
| 2025 | REF-VC[16] | 노이즈 견고성 | 실제 환경 | 견고성 |
| 2025 | SVC Challenge 2025[3][6] | 음성/스타일 변환 | SOTA 비교 | 벤치마크 |

#### 9.2 핵심 기술 진화

**1. 생성 모델 진화**:

```
GAN → 확산 모델 → 흐름 매칭 → 일관성 모델
(불안정)    (안정, 느림)  (빠름)      (극초 고속)
```

**2. 특성 표현 진화**:

```
PPG → ContentVec → SSL 특성(WavLM, Whisper)
(언어특화, 누출)  (범용, 개선)   (최신, 최고)
```

**3. 적용 범위 확대**:

```
단일 가수(Any-to-One) → 임의 간(Any-to-Any) → 제로샷(Zero-shot)
(고정)                (유연)              (범용)
```

#### 9.3 주요 혁신 사항

**LDM-SVC의 혁신 (2024)**:[4]

DiffSVC의 주요 문제였던 "음성 누출"을 해결:
- 잠재 공간에서의 확산 (원본 스펙트로그램이 아님)
- 분류기 없는 지도(Classifier-Free Guidance)로 음성 특성 강화
- 메모리 효율성 향상

**HQ-SVC의 혁신 (2025)**:[8][9]

DiffSVC의 "저자원 시나리오" 미해결 문제 해결:
- 소비자급 GPU (3GB VRAM)에서 훈련 가능
- 작은 데이터셋(몇 시간)으로도 고품질 결과
- 음성 초해상도까지 동일 모델로 지원

**Everyone-Can-Sing의 혁신 (2025)**:[13]

DiffSVC의 "제한된 응용" 확장:
- 음성 샘플만으로 제로샷 가성 변환 (ASR 불필요)
- 리릭, 악보, 음성 샘플로 전체 음성 합성
- 음성→가성 변환으로 응용 범위 극대화

#### 9.4 현재 최첨단 성능 (2025년)

**SVC 챌린지 2025 결과**:[6][3]

1. **음성 신원 변환(Task 1)**: 상위 시스템들이 원본 녹음과 거의 동등한 수준(MOS ≈ 4.4~4.6)
2. **음성 스타일 변환(Task 2)**: 여전히 도전적 (MOS ≈ 3.5~3.9)
   - 어려운 스타일: 음성음(breathy), 글리산도(glissando), 비브라토(vibrato)[3][6]
3. **상위 시스템**: Vevo 1.5 (LLM 미세조정) > Serenade (흐름 매칭) > NU-SVC (확산)[6][3]

#### 9.5 개방형 문제들 (Open Challenges)

**아직 해결되지 않은 문제들**:

1. **음성 스타일 모델링**: 복잡한 음성 표현을 완벽하게 모델링하지 못함[3][6]
2. **교차 언어 정확성**: 다국어 음성 변환 시 발음 오류 여전히 존재[14]
3. **실시간 처리**: 딥러닝 기반 방법들의 계산 지연이 실시간 처리에 부적합
4. **소량의 참조 음성**: 참조 음성이 매우 제한적일 때(수 초)의 성능 저하[9][8]
5. **음악성 보존**: 리듬 정확도, 음정 안정성을 동시에 보존하기 어려움

***

### 10. 결론

**DiffSVC**는 2021년 가성 음성 변환 분야에 **확산 확률 모델을 처음 도입**함으로써 패러다임 전환을 일으킨 획기적인 논문이다. 안정적이고 효율적인 ELBO 기반 훈련, 멀티모달 조건부 입력(PPG + F0 + Loudness), 그리고 양방향 잔차 합성곱 구조를 통해 기존 방식들을 능가하는 자연스러움(3.97)과 음성 유사도(4.67)를 달성했다.[2][1]

그러나 **단일 가수 훈련, 제한된 데이터, 느린 추론 속도**라는 한계를 가지고 있었다. 이후 2022-2025년 연구들은 이러한 한계를 해결하기 위해:[1]

1. **속도 최적화**: 흐름 매칭, 일관성 증류로 100 단계 → 1-4 단계 단축
2. **일반화 성능 향상**: SSL 특성, 특성 분리, 다단계 훈련으로 제로샷/교차도메인 성능 획기적 개선
3. **표현력 확대**: 단순 음성 특성에서 음성 스타일, 음성 기법까지 모델링
4. **실용성 강화**: 저자원 시나리오, 다국어, 음성 초해상도 지원[17][16][14][8][9][13][6][3]

DiffSVC는 단순히 한 분야의 논문을 넘어, **음성 생성 전반(TTS, VC, SVS)을 확산 모델 기반으로 전환**하는 촉매가 되었으며, 향후 음성 기술의 발전은 DiffSVC가 정초한 기반 위에서 이루어지고 있다.

***

### 참고문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/a8c595e4-dd58-42f6-b5f9-bd85d98dface/2105.13871v1.pdf)
[2](https://ieeexplore.ieee.org/document/9688219/)
[3](https://arxiv.org/abs/2509.15629)
[4](https://arxiv.org/abs/2406.05325)
[5](https://ojs.aaai.org/index.php/AAAI/article/view/29932)
[6](https://arxiv.org/html/2509.15629v1)
[7](https://arxiv.org/html/2312.10741v1)
[8](https://www.semanticscholar.org/paper/7d3ed0ec93bf76d18d8936b8135fa2566a22e241)
[9](https://arxiv.org/html/2511.08496v1)
[10](http://arxiv.org/pdf/2407.07728.pdf)
[11](https://ieeexplore.ieee.org/document/10800358/)
[12](http://arxiv.org/pdf/2406.05325.pdf)
[13](https://ieeexplore.ieee.org/document/10889751/)
[14](http://arxiv.org/pdf/2501.05586.pdf)
[15](https://arxiv.org/html/2401.01792v1)
[16](https://www.semanticscholar.org/paper/1da1739839847db9c38222845ec358e2ec58f033)
[17](https://arxiv.org/pdf/2406.05692.pdf)
[18](https://www.siam.org/publications/siam-news/articles/generalization-of-diffusion-models-principles-theory-and-implications/)
[19](https://www.ijcai.org/proceedings/2022/0577.pdf)
[20](https://arxiv.org/pdf/2109.13821.pdf)
[21](https://iopscience.iop.org/article/10.1149/MA2025-031223mtgabs)
[22](https://nbseh.org/index.php/journals/article/view/41)
[23](https://arxiv.org/abs/2402.12660)
[24](http://arxiv.org/pdf/2409.08583.pdf)
[25](https://arxiv.org/pdf/2105.13871.pdf)
[26](http://arxiv.org/pdf/2405.04627.pdf)
[27](https://arxiv.org/pdf/2402.12660.pdf)
[28](https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf)
[29](https://www.isca-archive.org/odyssey_2020/sisman20_odyssey.pdf)
[30](https://arxiv.org/pdf/2509.15629.pdf)
[31](https://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf)
[32](http://www.apsipa.org/proceedings/2019/pdfs/73.pdf)
[33](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/diffsvc/)
[34](https://arxiv.org/html/2409.08583v2)
[35](https://www.isca-archive.org/interspeech_2025/chen25d_interspeech.pdf)
[36](https://arxiv.org/abs/2404.04904)
[37](https://aclanthology.org/2024.acl-long.589)
[38](https://www.semanticscholar.org/paper/1b05f6397db294dcfe5f328f81d3f5facf9f0e6b)
[39](http://arxiv.org/pdf/2501.13870.pdf)
[40](https://arxiv.org/pdf/2411.09943.pdf)
[41](http://arxiv.org/pdf/2310.06546.pdf)
[42](https://openreview.net/forum?id=j8WHjM9aMm)
[43](https://patents.google.com/patent/US20180012613A1/en)
[44](https://arxiv.org/pdf/2511.08496.pdf)
[45](https://arxiv.org/pdf/2307.00393.pdf)
[46](https://www.isca-archive.org/interspeech_2025/liu25h_interspeech.pdf)
[47](https://www.nature.com/articles/s42005-024-01837-w)
[48](http://poster-openaccess.com/files/ICIC2025/2517.pdf)
[49](https://github.com/jungwoo-ha/WeeklyArxivTalk/issues/88)
