# Pseudo-Labeling for Massively Multilingual Speech Recognition

## 1. 핵심 주장 및 주요 기여  
이 논문은 60개 언어를 동시에 처리하는 거대 다국어 자동음성인식(ASR) 모델에 “Pseudo-Labeling” 기법을 적용하여,  
- **저자원 언어의 인식 성능을 획기적으로 향상**  
- **대규모 비라벨 음성 데이터(VoxPopuli) 활용을 위한 간단하면서도 효과적인 레시피**  
를 제안한다.

주요 기여는 다음과 같다.  
1. 다국어 모델을 특정 언어로 **사전 단일언어 미세조정(fine-tuning)** 후에 해당 언어로부터 유사 레이블(PLs)을 생성하는 방법 제안  
2. PLs를 활용해  
   -  모델을 처음부터 재학습하거나  
   -  CV(Common Voice) 데이터로 미세조정  
   하는 두 가지 전략 비교 및 최적화  
3. 대규모 다국어 모델이 **LibriSpeech** 같은 새로운 도메인으로 일반화되는 것을 실험적으로 입증  

## 2. 문제 정의  
- **다국어 ASR의 저자원 언어 성능 한계**: Common Voice에 포함된 60개 언어 중 10여 개 언어는 1시간 미만의 학습 데이터만 보유.  
- **비라벨 대규모 음성 데이터 활용 난제**: VoxPopuli와 같은 대규모 unlabeled 데이터는 도메인·언어 편향 문제로 바로 적용 시 PL의 품질 저하 및 학습 붕괴(collapse) 발생.

## 3. 제안 방법  
### 3.1 모델 구조  
- **컨볼루션 + 36-layer Transformer 인코더**  
- **CTC(head)**: 8065개 문자(한자 포함)  
- **LID(head)**: 60개 언어 분류 (학습 시에만 사용)  

### 3.2 Pseudo-Labeling 레시피  
1. **다국어 감독 학습**: CV 전체(60개 언어)로 기본 모델 θ 학습  
2. **단일언어 미세조정**: CV 특정 언어로 θ → θ′ (10k 업데이트)  
3. **PL 생성**: θ′로 해당 언어의 VoxPopuli 데이터를 greedy CTC 디코딩하여 PL 확보  
4. **최종 학습**  
   -  전략 A: θ에 PL 추가로 이어 학습(fine-tune)  
   -  전략 B: CV+PL 전부로 모델을 스크래치 재학습 후 CV 미세조정  

#### 3.2.1 수식  
모델 학습 손실:  

$$
\ell = \ell_{CTC} + \gamma\,\ell_{LID}
$$  

– γ=1일 때 최적.  

디코딩 시 외부 언어모델 LM 활용:  

$$
\hat{y} = \arg\max_y \bigl[\log p_\theta(y|x) + \alpha \log p_{LM}(y) + \beta |y|\bigr]
$$  

### 3.3 장기 발화 대비 크로핑 워밍업  
- VP 평균 30초 vs. CV 5초 → 미라벨 상태로 장기 발화 무응답 문제  
- **초기 10k 업데이트 동안 10초 단위 크로핑** 후 PL 생성 → 모델이 긴 발화 적응  

## 4. 성능 향상  
- VP PL 활용 시 19개 VP 언어 평균 CER 24.8% → 10.6% (단일언어 U-fine-tuning)  
- 대규모 스크래치 학습 후 CV 미세조정으로 테스트 CER 10.6% (대조군 24.8%)  
- LibriSpeech WER(4-gram LM)  
  -  CV만 학습: clean 37.6%→ CV+VP fine-tune: clean 9.0%, other 16.8%  
- PL을 통한 **도메인 일반화** 및 **저자원 언어 획기적 개선** 확인  

## 5. 한계 및 고려사항  
- VP에 없는 언어(Catalan, Kabyle 등)는 PL 부재로 학습량 불균형 발생  
- VP 데이터 도메인(통역) 잡음, 노이즈로 인한 PL 품질 저하  
- **언어 식별 정보 필요**: PL 생성 시 말하는 언어 사전 지정 필요  

## 6. 일반화 성능 향상 관점  
- **장기 발화 적응**: 크로핑 워밍업 덕분에 긴 발화 처리 능력↑  
- **언어 불확실성 감소**: 다국어 pretrained 모델을 단일언어로 미세조정 후 PL 생성  
- **도메인 이탈 보완**: PL로 학습된 대규모 모델이 다른 음성 도메인(LibriSpeech)에서도 강건성 확보  

## 7. 향후 연구 영향 및 고려점  
이 논문은 **다국어 ASR**과 **준지도 학습**의 결합 가능성을 제시하며, 후속 연구에서 다음을 고려할 것이다.  
- **언어 미지정(Unlabeled) 상황**에서 PL 생성 자동화  
- **반복적 self-training**으로 PL 품질 및 성능 지속 개선  
- **노이즈 로버스트 PL 생성**: PL 필터링, confidence 기반 예제 선택  
- **대형 언어모델 통합**: 생성 단계에서 LM 조건부 PL 생성  

앞으로 대규모 다국어 음성 데이터 활용과 ASR 모델의 도메인·언어 일반화 성능을 크게 끌어올리는 연구들이 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/ad21a027-fb07-4610-8cd4-9653f7251a56/2111.00161v3.pdf)
