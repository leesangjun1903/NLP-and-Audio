# GETMusic: Generating Music Tracks with a Unified Representation and Diffusion Framework

### 1. 핵심 주장과 주요 기여 요약

GETMusic은 심볼 음악 생성의 실무적 문제를 해결하는 연구입니다. 기존 음악 생성 모델들은 특정 악기 조합(예: 멜로디→반주 생성)에만 최적화되어 있어, 사용자의 다양한 작곡 요구를 충족하지 못합니다. 이 논문은 두 가지 핵심 혁신으로 해결책을 제시합니다:

**첫째, GETScore 표현**은 음악을 2D 구조로 재설계합니다. 트랙이 수직으로 스택되고 시간이 수평으로 진행하는 방식으로, 시퀀스 기반 방법의 "트랙 인터리빙 문제"를 해결합니다. 기존 REMI 시퀀스는 다른 악기의 음표들이 시간 순서대로 섞여있어 특정 악기 생성이 어렵지만, GETScore는 각 악기를 명확히 분리합니다.

**둘째, GETDiff 모델**은 이산 확산 모델 기반 비자기회귀 생성을 제안합니다. 이를 통해 665개의 모든 가능한 소스-타겟 악기 조합을 단일 모델로 처리할 수 있으며, 추론 시 학습되지 않은 마스킹 패턴도 제로샷으로 생성 가능합니다.

**주요 기여**:
- 임의의 소스-타겟 악기 조합에서 재학습 없이 작동하는 통합 프레임워크
- 동시음 내의 음표 간 조화를 보존하는 컴팩트한 표현
- 기존 특정 작업 전용 모델들을 능가하는 성능

***

### 2. 문제 정의, 제안 방법, 모델 구조, 성능 향상 및 한계

#### 2.1 해결하고자 하는 문제

기존 음악 생성 연구는 두 가지 방식으로 분류되며, 각각 한계를 가집니다:

**시퀀스 기반 방식** (REMI, MusicBERT): 음악을 토큰 시퀀스로 표현합니다. 각 음표는 onset, pitch, duration, instrument 등 여러 속성을 나타내는 여러 토큰으로 구성되고, 시간 순서대로 배열됩니다. 문제는 다른 악기의 음표들이 인터리빙되어, 자기회귀 모델이 언제 목표 악기의 토큰을 출력할지 암묵적으로 결정해야 한다는 점입니다. 또한 긴 시퀀스에서 악기 간 의존성이 약화됩니다.

**이미지 기반 방식** (pianoroll): 음악을 2D 이미지로 표현하여 악기를 분리합니다. 그러나 악기의 전체 음역대를 포함해야 하므로 매우 크고 희소한 이미지가 되어, 고해상도 생성이 어렵습니다.

이 문제들은 **유연한 소스-타겟 조합 지원**을 원하는 실무 요구를 충족하지 못합니다.

#### 2.2 제안하는 방법

**GETScore 표현**

GETScore는 두 가지 설계 원칙으로 구성됩니다:

**(1) 트랙 배열**: 트랙을 수직으로 스택하고, 시간을 수평축으로 사용합니다. 수평축은 16분음표 길이의 타임 유닛으로 분할됩니다.

**(2) 음표 토큰화**: 각 음표는 두 개의 토큰으로만 표현됩니다:
- **피치 토큰**: 음표의 높이 (악기별 어휘)
- **듀레이션 토큰**: 음표의 길이 (공유 어휘)

동시에 울리는 여러 음표는 **복합 토큰**으로 병합됩니다. 연구팀의 데이터 분석에 따르면, 97% 이상의 동시 음표 그룹이 같은 듀레이션을 가지므로, 듀레이션 병합 손실은 미미합니다 (0.5%의 경우만 1타임 유닛을 초과하는 차이).

드럼은 음표 개념이 없으므로, 드럼 액션(킥, 스네어 등)을 피치 토큰으로, 특수 토큰을 듀레이션으로 처리합니다.

**GETDiff 모델**

모델은 이산 확산 과정 (Discrete Diffusion Process)을 따릅니다:

**(1) 전방 프로세스** (Forward Process):

토큰은 특수 토큰 [MASK]로 전환되는 마르코프 과정을 따릅니다. 전이 행렬 $Q_t \in \mathbb{R}^{K \times K}$는 다음과 같이 정의됩니다:

```math
Q_t = \begin{pmatrix} \alpha_t & 0 & \cdots & 0 & \gamma_t \\ 0 & \alpha_t & \cdots & 0 & \gamma_t \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \cdots & 1 & 0 \\ \gamma_t & \gamma_t & \cdots & 0 & 1 \end{pmatrix}
```

여기서 $\alpha_t$는 토큰이 현재 상태 유지 확률, $\gamma_t = 1 - \alpha_t$는 [MASK]로 전환 확률입니다.

주변 분포 (marginal)와 후방 분포 (posterior)는:

$$q(x_t|x_0) = v^{\top}(x_t)Q_t v(x_0), \quad \text{where} \quad Q_t = Q_t \cdots Q_1$$

$$q(x_{t-1}|x_t, x_0) = \frac{v^{\top}(x_t)Q_t v(x_{t-1}) \cdot v^{\top}(x_{t-1})Q_{t-1}v(x_0)}{v^{\top}(x_t)Q_t v(x_0)}$$

**(2) 역방 프로세스** (Denoising Process):

모델은 변분 하한 (VLB)을 최적화합니다:

$$L_{\text{vlb}} = \mathbb{E}_q[-\log p_\theta(x_0|x_1)] + \sum_{t=2}^{T} D_{\text{KL}}[q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t)] + D_{\text{KL}}[q(x_T|x_0) \| p(x_T)]$$

x0-매개변수화를 통해 보조 손실을 추가합니다:

$$L_\lambda = L_{\text{vlb}} + \lambda \mathbb{E}_q \left[\sum_{t=2}^{T} -\log p_\theta(x_0|x_t)\right]$$

여기서 $\lambda = 0.001$입니다.

**(3) 훈련 메커니즘**:

각 훈련 스텝에서, m개 트랙을 소스로, n개 트랙을 타겟으로 무작위 선택합니다 ($m \geq 0, n > 0, m+n \leq I$). 소스 트랙은 지상 진실(ground truth)로 유지되고, 타겟 트랙만 마스킹됩니다. 미포함 트랙은 [EMPTY] 토큰으로 채워져 간섭을 방지합니다.

**(4) 추론 메커니즘**:

사용자가 소스와 타겟을 지정하면, 초기 $x_T$는 소스의 지상 진실 + 타겟의 [MASK] + 미포함 트랙의 [EMPTY]로 구성됩니다. 각 디노이징 스텝 $t-1$에서 토큰을 생성한 후, **소스 토큰을 지상 진실로 복원**하고 미포함 트랙을 다시 비웁니다. 이는 조건 일관성을 보장합니다.

#### 2.3 모델 구조

GETDiff는 세 가지 주요 컴포넌트로 구성됩니다:

| 컴포넌트 | 설명 |
|---------|------|
| **임베딩 모듈** | 2I행 × L 열의 GETScore를 d=96 차원 임베딩으로 변환 |
| **Roformer 레이어** | 12개의 Roformer 레이어 (상대 위치 정보 포함, 모델 차원=768) |
| **디코딩 모듈** | 어휘 크기 K에 대한 분류 헤드, Gumbel-Softmax로 토큰 생성 |

**조건 플래그** (Condition Flags): 소스 트랙의 토큰이 신뢰할 수 있음을 모델에 알리는 학습 가능한 플래그를 임베딩에 추가합니다. 이는 디노이징 중 소스 토큰의 예측 오차를 방지합니다.

**모델 규모**: 약 86M 파라미터, 8개 V100 GPU에서 50 에포크(약 70시간) 훈련

#### 2.4 성능 향상

**실험 설정**:
- 데이터: Musescore에서 수집한 1,569,469개 MIDI 파일 → 137,812개 GETScore (약 2,800시간)
- 비교 대상: PopMAG (특정 작업), Museformer (무조건부 생성)
- 평가 지표: 
  - **Chord Accuracy (CA)**: 생성된 악기와 지상 진실의 코드 일치도
  - **KL Divergence**: 피치, 듀레이션, IOI (Inter-Onset Interval) 분포의 차이
  - **Human Rating (HR)**: 10명 평가자의 1-5점 평가

| 작업 | 모델 | CA(%) ↑ | KL_Pitch ↓ | KL_Dur ↓ | KL_IOI ↓ | HR ↑ |
|-----|------|--------|-----------|----------|----------|------|
| **반주 생성** | PopMAG | 61.17 | 10.98 | 7.00 | 6.92 | 2.88 |
| | GETMusic | **65.48** | 10.05 | **4.21** | **4.22** | **3.35** |
| **멜로디 생성** | PopMAG | 73.70 | 10.64 | 3.97 | 4.03 | 3.14 |
| | GETMusic | **81.88** | 9.82 | **3.67** | **3.49** | **3.52** |
| **무조건부** | Museformer | - | 8.19 | 3.34 | 5.71 | 3.05 |
| | GETMusic | - | **7.99** | 3.38 | **5.33** | **3.18** |

**주요 성과**:
- 반주 생성에서 CA 4.31% 개선, KL_Dur 40% 감소
- 멜로디 생성에서 CA 8.18% 개선, KL_IOI 13% 감소
- 비자기회귀 생성으로 **추론 시간 4.8초** (PopMAG AR: 17.13초, 23.32초 대비 5배 이상 빠름)

#### 2.5 한계

1. **길이 제한**: 최대 512 타임 유닛 (약 128 비트), 장곡 생성에 제약

2. **복합 토큰 단순화**: 
   - 0.5%의 동시 음표에서 1타임 유닛 초과 듀레이션 차이 존재
   - 듀레이션 다양성 손실 가능

3. **악기 기반 어휘 분리**: 
   - 악기별 별도 피치 어휘 (드럼 4,369개 vs 멜로디 128개) → 모델 복잡도 증가
   - 새로운 악기 추가 시 어휘 재구축 필요

4. **조건 플래그 의존성**: 제거 시 AG 작업에서 CA 20.32% 급락 (65.48% → 45.16%)

5. **구조적 생성**: 곡의 verse-chorus 구조 등 고수준 악곡 구조 모델링 미흡

6. **제로샷 생성의 평가 어려움**: 유연성이 높지만, 임의 마스킹 위치 생성의 음악성 평가 기준 부재

***

### 3. 모델의 일반화 성능 향상 가능성 분석

#### 3.1 GETMusic의 일반화 강점

**1. 소스-타겟 조합의 완벽한 커버리지**

6개 악기에서 총 $3^6 - 2^6 = 729 - 64 = 665$개의 유효 조합을 단일 모델로 처리합니다. 이는:
- 학습 중 동적으로 다양한 조합을 학습
- 모델이 악기 간 일반적인 음악적 원칙을 학습하도록 강제

**2. 비자기회귀 특성으로 인한 유연성**

자기회귀 모델과 달리, GETDiff는 :
- 고정된 생성 순서가 없음
- 동일한 모델로 **제로샷 마스킹 생성** 지원 (학습되지 않은 마스킹 패턴)
- 예: 두 곡 사이의 음표를 [MASK]로 설정하여 조화로운 다리 생성

**3. 아키텍처 유연성**

Roformer의 상대 위치 정보 포함으로 길이 외삽 (length extrapolation)에 유리하여, 훈련보다 긴 음악 생성 가능 가능성 존재.

#### 3.2 일반화 한계 및 개선 방향

**현재 한계**:

1. **고정 악기 수 제한**: 
   - 아키텍처가 I=6으로 고정됨
   - 새로운 악기 추가는 모델 재학습 필요

2. **도메인 외 일반화 부족**:
   - Musescore 팝 음악 중심 데이터
   - 클래식, 재즈 등 다른 장르 성능 미평가
   - 악기별 음악 스타일 다양성 미흡

3. **악기 간 음악적 의존성 제한된 모델링**:
   - 동시음 내 의존성은 복합 토큰으로 보존
   - 비동시음 악기 간 장기 의존성은 상대적으로 약함
   - 예: 드럼 리듬이 피아노 리듬에 미치는 영향 불충분

**개선 가능성**:

| 방향 | 전략 | 기대 효과 |
|-----|------|---------|
| **계층적 구조** | Whole-Song 스타일 cascaded 확산 통합 | 장곡, verse-chorus 구조 생성 |
| **다중 모달리티** | 텍스트/이미지 조건 추가 (MusicLDM 방식) | 장르, 분위기 제어 일반화 |
| **계산 효율성** | Mamba 아키텍처로 대체 (SMDIM) | 더 긴 시퀀스, 더 많은 악기 처리 |
| **감정/스타일 제어** | 분류기 기반 가이드 (AGE-DM) | 감정적 일관성 있는 생성 |
| **크로스 도메인** | 다양한 장르 대규모 사전학습 | 새로운 악기/스타일에 빠른 적응 |

***

### 4. 2020년 이후 관련 최신 연구 비교 분석

#### 4.1 주요 연구 동향

**Phase 1 (2020-2022): 기초 확립**
- **기존 방식**: 자기회귀 트랜스포머 (PopMAG, MusicBERT), GAN (MuseGAN)
- **한계**: 특정 작업 전용, 악기 수 제한

**Phase 2 (2023): 확산 모델 도입**
- **Discrete Diffusion Probabilistic Models** [Mittal et al., 2023]
  - 첫 이산 심볼 음악 확산 모델
  - D3PM 직접 적용, 간단하지만 제어성 제한
  
- **GETMusic** [Lv et al., 2023]
  - **차별점**: 2D 표현 + 통합 프레임워크
  - **성과**: 665 조합 지원, 비자기회귀

**Phase 3 (2024-2025): 확산 모델 고도화**

| 연구 | 주요 혁신 | 강점 | 한계 |
|-----|---------|------|------|
| **SCG** (2024) | 미분 불가능 규칙 가이드 | 플러그&플레이, 제어성 높음 | 단일 규칙 중심 |
| **Whole-Song** (2024 ICLR) | 계층적 cascaded 확산 | 250+ 측정 장곡, 구조 모델링 | 복잡도 증가 |
| **SMDIM** (2024) | Mamba + 확산 | 선형 복잡도, 확장성 | Mamba 초기 단계 |
| **Multi-Track MusicLDM** (2024) | 잠재 확산 + 다중 모달 | 텍스트 조건, 효율성 | 악기별 세밀한 제어 약함 |
| **SYMPLEX** (2024) | Simplex 확산 + 어휘 사전 | 어휘 사전 제어, 빠른 샘플링 | 제한된 표현 크기 |

#### 4.2 표현 방식 비교

| 표현 | 방식 | 악기 분리 | 길이 효율성 | 장점 | 단점 |
|-----|------|---------|-----------|------|------|
| **REMI** (시퀀스) | Track 토큰 + 타임스텝 | 불충분 | 낮음 | 단순 | 악기 인터리빙 |
| **Pianoroll** (이미지) | 2D 그리드 | 우수 | 매우 낮음 | 시각적 | 희소, 악기별 큰 이미지 |
| **GETScore** | 2D 정렬 (트랙×타임) | 우수 | **높음** | **컴팩트** | 고정 악기 수 |
| **계층적** (Whole-Song) | 다수준 언어 | 우수 | 중간 | 구조 모델링 | 복잡도 높음 |
| **그래프** | 음표 간 관계 | 우수 | 중간 | 유연한 제어 | 계산 비용 높음 |

#### 4.3 생성 패러다임 비교

| 패러다임 | 모델 | 추론 속도 | 제어성 | 다양성 | 조화성 |
|--------|------|---------|--------|--------|--------|
| **자기회귀** | PopMAG, MMT | 느림 (17-23s) | 높음 | 중간 | 높음 |
| **비자기회귀** | GETMusic | **빠름 (4.8s)** | 중간 | **높음** | **높음** |
| **확산 (이산)** | GETMusic, SCG | 중간 | 중간 | 높음 | 높음 |
| **확산 (계층)** | Whole-Song | 느림 | **높음** | **높음** | **높음** |
| **잠재 확산** | MusicLDM | 빠름 | 중간 | 중간 | 중간 |

#### 4.4 평가 지표 진화

**2020-2023**: 기본 메트릭 (KL Divergence, Chord Accuracy)

**2024 이후**: 
- **세밀한 음악성 평가**: 
  - Fine-Grained Guidance (FGG) 기반 음표-레벨 정확도
  - 계층적 평가 (구조 vs 음표 수준)
- **인간 평가 강화**: 
  - 청취 테스트 Turing 스타일 (GETMusic: 평가자 간 κ > 0.6)
  - 감정, 장르별 세분화 평가
- **일반화 평가**: 
  - 크로스 도메인 성능 (클래식, 재즈 등)
  - 제로샷 전이 학습

#### 4.5 핵심 기술 축소판

| 기술 | GETMusic | Whole-Song | SMDIM | MusicLDM |
|-----|---------|-----------|--------|-----------|
| **표현** | 2D GETScore | 계층적 언어 | 시퀀스 | 잠재 벡터 |
| **모델** | Roformer | Cascaded Diffusion | Mamba+Diff | Latent Diffusion |
| **악기 수** | 6 (고정) | 가변 | 가변 | 가변 |
| **길이** | 512 타임유닛 | 250+ 측정 | 큼 | 가변 |
| **조건** | 악기 마스크 | 계층+구조 | 시퀀스 | 텍스트/음악 |
| **추론 속도** | **4.8s** | 느림 | 빠름 | 중간 |
| **일반화** | 665 조합 | 장곡 구조 | 길이 확장성 | 다중 모달 |

***

### 5. 해당 논문의 앞으로의 영향과 연구 시 고려할 점

#### 5.1 학술적 영향

**1. 표현 설계의 새로운 패러다임**

GETScore의 성공은 음악 AI 커뮤니티에 중요한 교훈을 제시합니다:
- 단순한 인코딩 선택이 모델 성능에 미치는 영향이 큼
- 도메인 특성 (악기 분리, 동시음 의존성)에 맞춘 표현 설계의 중요성
- Whole-Song의 계층적 언어, SYMPLEX의 어휘 사전 등 후속 연구에 영향

**2. 통합 프레임워크의 가능성 확대**

665개 조합을 단일 모델로 처리하는 설계는:
- 멀티태스크 학습의 음악 분야 응용 확대
- Few-shot/Zero-shot 음악 생성의 가능성 제시
- 위키뮤직, 대규모 음악 생성 플랫폼의 기초 마련

**3. 비자기회귀 생성의 재평가**

자기회귀 방식이 표준이던 음악 생성에서:
- 확산 모델의 병렬 생성 이점 입증 (5배 빠른 추론)
- 음악 생성 = 자기회귀라는 고정관념 깨뜨림
- SMDIM, DiffRhythm 등 비자기회귀 연구 활성화

#### 5.2 실무/산업 응용

**1. 음악 작곡 보조 도구**
- 사용자가 특정 악기만 생성 (예: 보컬 고정, 반주 생성)
- 다양한 악기 조합 자동 배치

**2. 실시간 대화형 음악 생성**
- 빠른 추론 속도 (4.8s)로 사용자 피드백 반영 가능
- 웹/모바일 애플리케이션 실현 가능

**3. 음악 편집 및 복원**
- 마스크 및 디노이징으로 특정 부분 재생성
- 손상된 악기 트랙 자동 복원

#### 5.3 앞으로 연구 시 고려할 핵심 사항

##### (1) 아키텍처 확장성

**과제**: 고정 6악기 제한 극복

**고려사항**:
- **가변 길이 처리**: Transformer의 position embedding 일반화 필요
- **악기 증가**: 피치 어휘 폭발 방지 (현재 드럼 4,369개) → 공유 음표 표현 탐색
- **계층적 구조**: 악기 그룹화 (현악 family, 건반 family) → 어휘 효율성

**벤치마크**: 
- 10+ 악기 정상 작동
- 새 악기 추가 시 전체 재학습 불필요한 설계

##### (2) 장기 구조 모델링

**과제**: 512 타임유닛 → 전곡 생성 (2,000+ 타임유닛)

**고려사항**:
- **Whole-Song 통합**: 계층적 cascaded 확산
  - Level 1: 전체 곡 형태 (verse, chorus, bridge)
  - Level 2: 섹션 내 악기 흐름
  - Level 3: 음표 레벨 디테일
  
- **위치 정보 외삽**: 
  - Roformer의 상대 위치가 외삽에 유리하나, 명시적 length generalization 모듈 필요
  - 예: ALiBi (Attention with Linear Biases) 고려

- **메모리 효율**: 
  - 시퀀스 길이 4배 증가 → 메모리 16배 증가
  - Sparse Attention, Linformer 등 효율적 주의 메커니즘 필요

##### (3) 도메인 외 일반화

**과제**: Musescore 팝 음악 데이터 의존성

**고려사항**:
- **다중 장르 데이터**: 클래식, 재즈, 전자음악 포함 대규모 데이터셋
  - 현재: Musescore (주로 팝) → Classical Archives, JAZZML 추가
  - 문제: 장르별 악기 구성 차이 (재즈는 드럼 리듭 복잡도 높음)

- **장르 조건 추가**:
  - 명시적 장르 토큰 또는 스타일 임베딩
  - 계층적 모델에서 상위 레벨에서 장르 결정

- **크로스 도메인 검증**:
  - 팝 데이터 학습 → 클래식 평가 (역도 수행)
  - Domain gap 정량화 및 적응 방법 연구

##### (4) 음악성 평가의 고도화

**과제**: 정량 지표 (KL Divergence)의 한계
- 분포 일치가 높아도 음악성 낮을 수 있음
- 인간 평가자 수 적음 (10명)

**고려사항**:
- **음악 정보 검색 (MIR) 메트릭**:
  - 멜로디 연속성 (음정 변화 매끄러움)
  - 리듬 일관성 (비트 안정성)
  - 조화 합의성 (코드 진행 자연성)
  - 도구: Essentia, librosa 활용

- **대규모 인간 평가**: 
  - 최소 30-50명 평가자
  - 다양한 음악 배경 (작곡가, 평신도)
  - 블라인드 테스트 엄격 설계

- **컨텐츠 기반 메트릭**:
  - 음표 밀도 일관성
  - 악기 간 음량 균형
  - 코드-멜로디 일치도

##### (5) 조건부 생성의 정교화

**과제**: 현재는 악기 마스킹만 지원

**고려사항**:
- **다중 조건 결합**:
  - 악기 + 감정 (AGE-DM 방식)
  - 악기 + 장르 + 동적 (빠르기, 악센트)
  - 악기 + 화성 진행 (명시적 코드 시퀀스)

- **강도 있는 제어**:
  - 조건 강도 조절 (classifier-free guidance 활용)
  - 예: "매우 강한 조건" vs "약한 조건" → 창의성 vs 정확성 트레이드오프

- **인터랙티브 조정**:
  - 생성 중 조건 수정 (예: 중간에 악기 추가)
  - Loop 기반 생성으로 사용자 피드백 반영

##### (6) 계산 효율성

**과제**: 현재 70시간 훈련 (8×V100)

**고려사항**:
- **Mamba 아키텍처 통합** (SMDIM 벤치마크):
  - 선형 복잡도로 시퀀스 길이 4배 증가 가능
  - 훈련 시간 50% 감소 기대

- **양자화/증류**:
  - INT8 양자화로 메모리 4배 절감
  - 학생 모델 증류 (지식 전이) 고려

- **분산 훈련 최적화**:
  - 대규모 배치 학습 (pipeline parallelism)
  - 혼합 정밀도 (mixed precision) 자동 적용

##### (7) 다중 모달리티 통합

**과제**: 현재 MIDI/음악 정보만 사용

**고려사항**:
- **텍스트 조건** (MusicLDM, XMusic):
  - 작곡 설명: "Up-tempo jazz with strong drums"
  - 악기별 지시: "violins play legato, piano staccato"

- **이미지/비디오 조건**:
  - 비디오의 시각적 내용에 맞춘 음악 생성
  - Art2Mus 스타일의 미술작품 → 음악

- **성능 특성** (Performance):
  - 악기별 주법 (pizzicato, tremolo 등)
  - 다이나믹 곡선 (crescendo, diminuendo)

- **가사 연결** (Lyric-aware):
  - 가사의 음절과 음표 길이 동기화
  - 감정적 톤 일치

##### (8) 윤리 및 저작권

**고려사항**:
- **훈련 데이터 투명성**: Musescore 저작권 곡 사용 명시
- **생성 곡의 법적 상태**: 학습 모델이 저작권 곡 복제하지 않는지 검증 필요
- **공정한 크레딧**: AI 보조 작곡의 작곡가 크레딧 기준 정립

***

### 결론

GETMusic은 음악 생성의 실무적 문제를 우아하게 해결하는 연구입니다. **2D 표현의 단순성**과 **이산 확산의 유연성**을 결합하여, 665개의 소스-타겟 조합을 단일 모델로 처리하는 것은 당대 (2023년) 획기적이었습니다.

하지만 더 나은 음악 생성의 미래를 위해서는:

1. **구조적 일관성**: 계층적 모델로 장곡 생성
2. **지능형 제어**: 다중 조건, 강도 조절 가능
3. **효율성**: Mamba 같은 신기술로 확장성 확보
4. **일반화**: 도메인, 악기, 스타일 다양화

이러한 방향들은 이미 2024-2025년 최신 연구에서 개별적으로 실현되고 있으며, **GETScore의 명확한 표현 설계**와 **비자기회귀 확산의 효율성**을 기반으로 하는 **하이브리드 프레임워크**의 등장이 향후 주목할 가치가 있습니다.

<span style="display:none">[^1_1][^1_10][^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_2][^1_20][^1_21][^1_22][^1_23][^1_24][^1_25][^1_26][^1_27][^1_28][^1_29][^1_3][^1_30][^1_31][^1_32][^1_33][^1_34][^1_35][^1_36][^1_37][^1_38][^1_39][^1_4][^1_40][^1_41][^1_42][^1_43][^1_44][^1_45][^1_46][^1_47][^1_48][^1_49][^1_5][^1_50][^1_51][^1_52][^1_53][^1_54][^1_55][^1_56][^1_57][^1_58][^1_59][^1_6][^1_60][^1_61][^1_62][^1_63][^1_64][^1_65][^1_66][^1_67][^1_68][^1_69][^1_7][^1_8][^1_9]</span>

<div align="center">⁂</div>

[^1_1]: 2305.10841v2.pdf

[^1_2]: https://arxiv.org/abs/2402.14285

[^1_3]: https://dl.acm.org/doi/10.1145/3696409.3700289

[^1_4]: https://arxiv.org/abs/2405.12666

[^1_5]: https://www.semanticscholar.org/paper/a00f60c3ef8ed3d7bee6708b8a73ea4fdf1016f7

[^1_6]: https://arxiv.org/abs/2405.09901

[^1_7]: https://arxiv.org/abs/2305.09489

[^1_8]: https://arxiv.org/abs/2310.14040

[^1_9]: https://ieeexplore.ieee.org/document/10734713/

[^1_10]: https://arxiv.org/abs/2408.01950

[^1_11]: https://arxiv.org/abs/2408.02711

[^1_12]: https://arxiv.org/pdf/2410.08435.pdf

[^1_13]: https://arxiv.org/pdf/2402.14285v4.pdf

[^1_14]: https://arxiv.org/html/2503.17654

[^1_15]: https://arxiv.org/pdf/2310.14040.pdf

[^1_16]: https://arxiv.org/pdf/2305.10841.pdf

[^1_17]: https://arxiv.org/html/2501.08809v1

[^1_18]: https://arxiv.org/pdf/2305.09489.pdf

[^1_19]: http://arxiv.org/pdf/2310.14044.pdf

[^1_20]: https://icml.cc/virtual/2024/poster/33440

[^1_21]: https://salu133445.github.io/mmt/

[^1_22]: https://ceur-ws.org/Vol-3519/paper3.pdf

[^1_23]: https://iclr.cc/virtual/2024/poster/17633

[^1_24]: https://arxiv.org/html/2511.07268

[^1_25]: https://www.ijcai.org/proceedings/2023/0643.pdf

[^1_26]: https://www.semanticscholar.org/paper/Symbolic-Music-Generation-with-Diffusion-Models-Mittal-Engel/93d00ea9c87268f867b4addb8043be35d6996d18

[^1_27]: https://www.ijcai.org/proceedings/2025/1127.pdf

[^1_28]: https://ieeexplore.ieee.org/document/10630499/

[^1_29]: https://liner.com/review/symbolic-music-generation-with-nondifferentiable-rule-guided-diffusion

[^1_30]: https://journals.sagepub.com/doi/10.1177/14727978251337904

[^1_31]: https://arxiv.org/html/2409.02845v1

[^1_32]: http://arxiv.org/pdf/2405.09901.pdf

[^1_33]: https://www.sciencedirect.com/org/science/article/pii/S1552628324001236

[^1_34]: https://dl.acm.org/doi/10.1007/s11227-022-04914-5

[^1_35]: https://www.arxiv.org/pdf/2507.20128.pdf

[^1_36]: https://arxiv.org/pdf/1709.06298.pdf

[^1_37]: https://arxiv.org/abs/2310.14044

[^1_38]: https://arxiv.org/abs/2402.04609

[^1_39]: https://ieeexplore.ieee.org/document/10840310/

[^1_40]: https://ieeexplore.ieee.org/document/10697156/

[^1_41]: https://www.mdpi.com/1424-8220/24/11/3266

[^1_42]: https://www.semanticscholar.org/paper/cd4fa63a5cd68feb58b1993a38e24c8c5a9178a1

[^1_43]: https://dl.acm.org/doi/10.1145/3637528.3671828

[^1_44]: https://ieeexplore.ieee.org/document/10509843/

[^1_45]: https://ieeexplore.ieee.org/document/10637422/

[^1_46]: https://ieeexplore.ieee.org/document/10571357/

[^1_47]: https://www.mdpi.com/2072-4292/16/2/420

[^1_48]: https://arxiv.org/html/2409.02845v2

[^1_49]: https://www.ijfmr.com/papers/2024/3/20239.pdf

[^1_50]: https://arxiv.org/html/2503.01183v1

[^1_51]: http://arxiv.org/pdf/2501.09972.pdf

[^1_52]: https://arxiv.org/pdf/2405.02801.pdf

[^1_53]: http://arxiv.org/pdf/2410.04906.pdf

[^1_54]: http://arxiv.org/pdf/2503.19611.pdf

[^1_55]: https://arxiv.org/html/2504.00837v2

[^1_56]: https://arxiv.org/html/2507.08333v1

[^1_57]: https://pmc.ncbi.nlm.nih.gov/articles/PMC9470348/

[^1_58]: https://openreview.net/forum?id=h922Qhkmx1

[^1_59]: https://www.ijcai.org/proceedings/2023/0648.pdf

[^1_60]: http://article.sapub.org/10.5923.j.ijpbs.20110101.05.html

[^1_61]: https://www.ijcai.org/proceedings/2025/1129.pdf

[^1_62]: http://www.jmlr.org/papers/volume26/25-0171/25-0171.pdf

[^1_63]: https://archives.ismir.net/ismir2021/paper/000093.pdf

[^1_64]: https://onlinelibrary.wiley.com/doi/10.1002/ett.70324?af=R

[^1_65]: https://kimjy99.github.io/논문리뷰/symbolic-music-diffusion/

[^1_66]: https://pmc.ncbi.nlm.nih.gov/articles/PMC5770452/

[^1_67]: https://arxiv.org/html/2511.15038v1

[^1_68]: https://liner.com/ko/review/scorebased-continuoustime-discrete-diffusion-models

[^1_69]: https://ieeexplore.ieee.org/document/10830574/
