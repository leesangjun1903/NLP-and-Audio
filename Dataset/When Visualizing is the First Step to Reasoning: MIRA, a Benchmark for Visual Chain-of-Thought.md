# When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought

***

### **1. 핵심 요약 (Executive Summary)**

본 보고서는 "시각화가 추론의 첫 단계일 때: 시각적 사고 사슬을 위한 벤치마크, MIRA(When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought)" 논문의 핵심 주장, 주요 기여점, 그리고 AI 연구 분야에 미칠 영향을 심층적으로 분석합니다.[1]

이 논문의 **핵심 주장**은 현재의 최첨단 멀티모달 거대 언어 모델(MLLM)들이 복잡한 시각적/공간적 추론 문제 해결에 있어 근본적인 한계를 보인다는 것입니다. 그 원인은 기존의 텍스트 기반 사고 사슬(Chain-of-Thought, CoT) 방식이 인간의 "생각을 위한 그림 그리기(drawing to think)"와 같은 시각적 중간 과정을 생성하고 활용하는 능력을 결여하고 있기 때문이라고 지적합니다.[1]

논문의 **주요 기여**는 다음과 같습니다.
*   **MIRA 벤치마크 제안:** 복잡한 시각적 추론 능력을 측정하기 위해 설계된 546개의 고품질 멀티모달 문제로 구성된 새로운 벤치마크 'MIRA'를 구축했습니다.[2][1]
*   **시각적 CoT(Visual-CoT)의 효용성 입증:** 중간 시각 단서(Visual Cue)를 제공했을 때, 최신 MLLM(GPT-5, Gemini 2.5 Pro 등)의 성능이 평균 33.7% 상대적으로 향상됨을 실험적으로 증명하며, 시각적 추론 과정의 중요성을 정량적으로 밝혔습니다.[3][1]
*   **현 모델의 한계 명시:** 텍스트 CoT만으로는 성능 향상이 미미하거나 오히려 감소하는 경우를 보여주며, 현재 모델들이 진정한 '시각적 상상력'을 통한 추론 능력이 부재함을 명확히 했습니다.[3][1]

***

### **2. 논문 상세 분석**

#### **2.1. 해결하고자 하는 문제 (Problem Statement)**

기존 MLLM 연구는 주로 이미지 인식, 질문 답변(VQA) 등 입력된 시각 정보를 이해하는 데 초점을 맞추어 왔습니다. 추론 능력을 향상시키기 위해 텍스트로 중간 과정을 생성하는 CoT 프롬프팅이 널리 사용되었으나, 이는 언어적으로 표현하기 어려운 공간적 관계, 물리적 시뮬레이션, 기하학적 변형 등이 요구되는 문제에서 명백한 한계를 보입니다. 예를 들어, 여러 기어가 맞물려 돌아갈 때 최종 기어의 방향을 예측하거나, 복잡한 도형 퍼즐을 맞추는 문제는 머릿속으로 그리거나 직접 그려보지 않고 텍스트로만 설명하기 매우 어렵습니다. 이 논문은 바로 이 지점, 즉 **텍스트 기반 추론의 본질적 한계**와 **시각적 중간 과정 생성 능력의 부재**를 핵심 문제로 정의합니다.[4][1]

#### **2.2. 제안하는 방법 (Proposed Method: MIRA Benchmark & Evaluation)**

이 논문은 새로운 모델 구조를 제안하기보다는, 'MIRA(Multimodal Imagination for Reasoning Assessment)'라는 정교한 벤치마크와 평가 프로토콜을 통해 문제의 심각성을 드러내고 미래 연구 방향을 제시합니다.

*   **MIRA 벤치마크:**
    *   **구성:** 총 546개의 문제, 20개의 태스크 유형으로 구성되며, 유클리드 기하학(EG), 물리 기반 추론(PBR), 추상적 공간 논리 퍼즐(ASLP), 인과적 변형(CT)의 4개 도메인을 포함합니다. 각 문제는 사람이 직접 그리거나 시뮬레이션해야 풀기 쉬운, 본질적으로 시각적 추론을 요구하는 문제들로 설계되었습니다.[5][6][1]
    *   **데이터:** 모든 문제에는 (1) 입력 이미지, (2) 질문, (3) 사람이 직접 제작한 단계별 중간 시각 단서 이미지(Visual-CoT), (4) 최종 정답이 포함되어 있어, 다양한 조건에서 모델을 평가할 수 있습니다.[7][1]

*   **3단계 평가 프로토콜:**
    1.  **Level 1: 직접 평가(Direct Evaluation):** 모델에게 이미지와 질문만 제공하여 최종 답변을 바로 생성하게 합니다. 이는 모델의 전반적인 문제 해결 능력을 측정합니다.
    2.  **Level 2: 텍스트 CoT 추론(Text-CoT Reasoning):** 모델에게 단계별 텍스트 추론 과정을 먼저 생성하고 답을 내도록 유도합니다.
    3.  **Level 3: 시뮬레이션된 시각적 CoT 추론(Simulated Visual-CoT Reasoning):** 모델에게 문제와 더불어, 사람이 제작한 중간 시각 단서 이미지를 함께 제공하여 추론하게 합니다. 현재 모델들이 스스로 시각적 단서를 생성하지 못하기 때문에, 이 방식은 시각적 정보가 주어졌을 때 모델의 잠재적 추론 능력의 상한선을 측정하는 역할을 합니다.[1]

#### **2.3. 모델 구조 (Model Architecture)**

MIRA는 새로운 모델을 제안하지 않습니다. 대신, 2025년 11월 기준 가장 강력한 상용 MLLM (GPT-5, Gemini 2.5 Pro, Claude 4 Opus 등)과 오픈소스 MLLM (Qwen2.5-VL, GLM 4.5V, Bagel 등)을 평가 대상으로 삼아, 이들의 시각적 추론 능력 현주소를 진단합니다. 특히, 이미지 이해와 생성을 모두 할 수 있는 '통합 MLLM(Unified MLLM)' 아키텍처(예: Bagel, Janus-pro)가 미래의 '그리면서 생각하는' 모델의 후보군이 될 수 있음을 시사하지만, 현재 이들 역시 추론 과정에서 자발적으로 이미지를 생성하지는 못한다고 지적합니다.[1]

#### **24. 사용된 수식 및 평가 메트릭 (Formulas and Metrics)**

논문은 주로 정량적 정확도를 통해 모델 성능을 평가하며, 사용된 핵심 메트릭은 다음과 같습니다.

*   **정확도(Micro-averaged Accuracy):** 가장 기본적인 평가 지표로, 전체 문제 중 모델이 정답을 맞힌 비율을 나타냅니다.

$$ \text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} $$

*   **Pass@k:** 모델이 동일한 문제에 대해 k개의 서로 다른 추론 경로(답변)를 생성했을 때, 그중 하나라도 정답이 있으면 성공으로 간주하는 지표입니다. 이는 모델의 잠재적 최고 성능을 측정하는 데 사용됩니다.[1]
*   **과반수 투표(Majority Voting):** k개의 답변 중 가장 많이 나온 답변을 최종 답변으로 선택하는 방식입니다.[1]

또한, 물리 추론 문제(예: 전하 사이의 힘 계산)에서는 **쿨롱의 법칙(Coulomb's Law)** 같은 물리 공식이 문제 자체에 포함되어 있음을 보여주며, 이는 벤치마크가 단순 패턴 인식을 넘어 실제 세계의 원리 이해를 요구함을 시사합니다.[5][1]

$$ |F_i| = k \frac{|q_i q_t|}{r_i^2} $$

#### **2.5. 성능 향상 및 한계 (Performance Improvements & Limitations)**

*   **성능 향상:**
    *   **Visual-CoT의 압도적 효과:** MIRA 벤치마크에서 대부분의 SOTA 모델은 직접 평가 시 20% 미만의 낮은 정확도를 보였습니다. 하지만 중간 시각 단서를 제공하는 'Visual-CoT' 환경에서는 모든 모델에서 일관되게 성능이 향상되었으며, 평균적으로 **33.7%의 상대적 성능 향상**을 기록했습니다. 특히 물리(PBR) 작업에서는 정확도가 20.7%에서 40.0%로 거의 두 배 가까이 상승했습니다. 이는 시각적 정보가 복잡한 추론에 결정적인 역할을 한다는 강력한 증거입니다.[2][3][1]

*   **한계:**
    *   **Text-CoT의 무용성:** 기존의 강력한 방법론이었던 Text-CoT는 MIRA에서 거의 성능 향상을 가져오지 못했으며, Gemini 2.5 Pro와 같은 일부 최상위 모델에서는 오히려 성능을 저하시켰습니다. 이는 언어만으로는 시각적 문제의 복잡성을 온전히 표현할 수 없다는 것을 의미합니다.[1]
    *   **시뮬레이션의 한계:** 논문의 'Visual-CoT'는 모델이 스스로 시각적 단서를 만드는 것이 아닌, 사람이 만든 이미지를 제공하는 '시뮬레이션' 방식입니다. 이는 현재 모델들이 '그리면서 생각하는' 능력이 없다는 명백한 증거이자, 이 연구의 근본적인 한계점입니다.[1]
    *   **샘플링의 한계:** Pass@k와 같은 샘플링 기반 평가에서도 k값이 4에서 8로 늘어날 때 성능 향상 폭이 크게 둔화되었습니다. 이는 모델의 실패가 단순한 추론 실수가 아니라, 해당 문제를 해결할 근본적인 시각적 추론 능력이 없기 때문임을 시사합니다.[7][1]

***

### **3. 모델의 일반화 성능 향상 가능성**

MIRA 벤치마크와 연구 결과는 MLLM의 일반화 성능 향상을 위한 중요한 방향을 제시합니다.

현재 모델들은 방대한 텍스트와 이미지 데이터로 학습하여 특정 패턴을 인식하고 언어적 상관관계를 학습하는 데는 뛰어나지만, 이는 '얕은 이해'에 머무를 수 있습니다. MIRA와 같은 벤치마크는 모델이 기존에 학습한 패턴만으로는 풀 수 없는, 새로운 형식의 문제를 제시합니다. 이 문제들을 해결하기 위해 중간 시각적 상태를 상상하고 조작하는 능력은, **더 깊고 추상적인 수준의 추론 능력**으로 이어질 수 있습니다.[8]

예를 들어, '전개도를 보고 원래 주사위의 면을 맞추는' 문제를 풀기 위해 머릿속으로 전개도를 접어보는 과정(mental rotation)을 학습한 모델은, 단순히 '주사위'라는 단어와 이미지를 연결하는 모델보다 훨씬 강력한 공간 추론 일반화 능력을 갖게 될 것입니다. Visual-CoT를 통해 모델이 이러한 시각적 변형 과정을 학습할 수 있다면, 이는 완전히 새로운 문제 유형에 대해서도 유연하게 대처할 수 있는 **강력한 일반화 능력의 토대**가 될 것입니다.

결론적으로, MIRA가 제시하는 '그리면서 생각하기' 패러다임은 모델이 텍스트적 연관성에만 의존하는 것을 넘어, 문제의 구조를 내면화하고 시뮬레이션하는 능력을 키우도록 유도합니다. 이러한 능력이 구현된다면, 이는 AI의 일반화 성능을 한 차원 끌어올리는 중요한 돌파구가 될 것입니다.

***

### **4. 연구의 영향 및 향후 고려사항**

#### **4.1. 향후 연구에 미치는 영향**

*   **새로운 모델 아키텍처 연구 촉진:** MIRA는 현재 MLLM의 명확한 한계를 드러냄으로써, 텍스트와 이미지 생성을 단순 결합하는 수준을 넘어, **추론 과정 중에 동적으로 시각적 결과물을 생성하고 이를 다시 입력으로 활용**하는 새로운 '통합 MLLM' 아키텍처 연구를 가속화할 것입니다.
*   **벤치마크 설계의 새로운 표준 제시:** MIRA는 단순 인식(recognition)을 넘어 진정한 추론(reasoning) 능력을 측정하는 벤치마크의 중요성을 강조합니다. 이는 향후 MLLM 평가가 '얼마나 아는가'에서 '얼마나 생각할 수 있는가'로 이동하는 데 기여할 것입니다. MIRA와 유사한 목적을 가진 다른 벤치마크들(예: Visual CoT, JourneyBench, VisFactor)과 함께, 이 분야의 연구를 더욱 심화시킬 것입니다.[9][10][8]
*   **데이터셋 구축의 중요성:** '그리면서 생각하는' 모델을 학습시키기 위해서는 MIRA와 같이 단계별 시각적 추론 과정이 포함된 대규모 데이터셋이 필수적입니다. 이 논문은 이러한 데이터셋 구축의 필요성과 방향성을 명확히 제시합니다.

#### **42. 향후 연구 시 고려할 점**

*   **자율적 시각 생성(Autonomous Visual Generation):** 가장 시급한 과제는 모델이 사람의 개입 없이 문제 해결에 필요한 중간 시각적 단서를 **스스로 판단하고 생성**하도록 만드는 것입니다. 이는 강화학습이나 특정 목적에 맞는 미세조정(Fine-tuning) 등의 기법을 통해 연구될 수 있습니다.[11]
*   **시각적 표현의 다양성:** 현재는 스케치, 다이어그램 등 2D 이미지에 국한되어 있지만, 향후에는 3D 모델, 동영상 등 더 풍부하고 동적인 시각적 표현을 생성하고 활용하는 방향으로 확장될 필요가 있습니다.
*   **평가의 효율성 및 확장성:** MIRA는 사람이 직접 데이터를 구축하여 품질은 높지만 확장성에 한계가 있습니다. 향후에는 이러한 고품질의 시각적 추론 문제를 자동으로 생성하고 평가하는 방법에 대한 연구가 필요합니다.
*   **인지 과학과의 융합:** 인간이 시각적 사고를 하는 방식을 인지 과학적으로 분석하고, 이를 모델 설계에 반영하는 학제 간 연구가 더욱 중요해질 것입니다. VisFactor와 같이 인간의 인지 테스트에서 영감을 얻은 벤치마크는 이러한 흐름을 잘 보여줍니다.[10]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e4326290-b496-4610-b159-bcc2919a1423/2511.02779v1.pdf)
[2](https://arxiv.org/abs/2511.02779)
[3](https://mira-benchmark.github.io)
[4](http://arxiv.org/pdf/2307.06281.pdf)
[5](https://www.themoonlight.io/en/review/when-visualizing-is-the-first-step-to-reasoning-mira-a-benchmark-for-visual-chain-of-thought)
[6](https://www.youtube.com/watch?v=yMUiRtN5YzU)
[7](https://arxiv.org/html/2511.02779v1)
[8](https://arxiv.org/html/2409.12953v4)
[9](http://arxiv.org/pdf/2403.16999.pdf)
[10](http://arxiv.org/pdf/2502.16435.pdf)
[11](https://arxiv.org/html/2503.20752)
[12](http://arxiv.org/pdf/2409.00106.pdf)
[13](https://arxiv.org/pdf/2404.13591.pdf)
[14](https://arxiv.org/pdf/2502.09933.pdf)
[15](https://openreview.net/pdf/2f793972a142bb40a2d925cc6ce59d9e6ad7e090.pdf)
[16](https://openreview.net/forum?id=N6JVVLGQLG)
[17](https://papers.cool/arxiv/2511.02779)
[18](https://huggingface.co/datasets/YiyangAiLab/MIRA)
[19](https://www.themoonlight.io/de/review/when-visualizing-is-the-first-step-to-reasoning-mira-a-benchmark-for-visual-chain-of-thought)
[20](https://arxiv.org/html/2511.02779)
