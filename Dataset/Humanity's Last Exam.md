# Humanity's Last Exam

### 1. 핵심 주장과 주요 기여

**Humanity's Last Exam (HLE)**는 Center for AI Safety와 Scale AI가 공동으로 개발한 획기적인 벤치마크로, 현재 대규모 언어모델(LLM)의 급속한 발전에 따른 **기존 벤치마크 포화 문제**를 직접적으로 해결하기 위해 설계되었습니다.[1]

#### 핵심 문제점
기존 벤치마크(예: MMLU)에서 최신 LLM들이 90% 이상의 정확도를 달성하면서, 모델 성능의 미세한 개선을 정밀하게 측정할 수 없게 되었습니다. HLE는 이러한 **벤치마크 포화**에 대한 직접적인 응답입니다.[1]

#### 주요 기여
HLE는 **2,500개의 극도로 도전적인 문제**로 구성되며, 100개 이상의 학과 분야에 걸쳐 있습니다. 특히:[1]

- 수학, 인문학, 자연과학 등 다양한 분야의 고급 문제 포함
- 세계적 수준의 수학 문제 강조로 **깊이 있는 추론 능력** 평가
- 전 세계 900명 이상의 주제 전문가가 참여한 글로벌 협력 프로젝트
- 약 500개 기관, 50개국을 아우르는 광범위한 네트워크[1]

***

### 2. 논문의 문제 정의 및 해결 방법

#### 2.1 해결하고자 하는 문제

**벤치마크 포화 현상(Benchmark Saturation)**
- GPT-4O, Grok 2, Claude 3.5 등 최첨단 모델들이 MMLU에서 90% 이상 달성
- 모델 간 성능 차이를 정밀하게 구별 불가능
- AI 능력의 정확한 측정 및 평가 불가능

**캘리브레이션 문제(Calibration Error)**
- 모델들이 틀린 답을 높은 자신감으로 제시
- 확신도와 실제 성능 간의 심각한 불일치[1]

#### 2.2 제안된 방법론

**다단계 엄격한 검증 프로세스**

##### LLM 난이도 검증
먼저 모든 질문을 최첨단 LLM들로 사전 검증합니다:[1]

- 정확-매칭 질문(exact-match): 모든 모델이 틀린 문제만 포함
- 객관식 질문(multiple-choice): 평균적으로 임의 추측보다 나쁜 성능 보이는 문제만 포함
- 70,000회 이상의 모델 실행으로 약 13,000개 질문이 1차 필터링

##### 인간 전문가 검토 (2단계)
1. **1차 검토**: 각 분야 대학원 수준 이상의 학위 소유자가 1-3회 검토
   - 질문 정확성, 명확성, 학문성 수준 평가
   - 기여자에게 상세한 피드백 제공

2. **2차 검토**: 1차에서 우수한 검토자와 조직자가 최종 승인
   - 최고 50개 질문에 $5,000, 다음 500개에 $500의 상금제도 운영
   - 피드백 결과 약 15.4%의 전문가 의견 불일치율 (생명과학 분야 18%)[1]

#### 2.3 평가 설정 및 수식

**표준화된 평가 프롬프트**
모든 모델은 다음 형식으로 응답하도록 구조화:

$$\text{Explanation: } \{explanation\}$$

$$\text{Answer: } \{answer\}$$

```math
\text{Confidence: } \{0\sim100\%\}
```

**캘리브레이션 오류 측정**
$$\text{RMS Calibration Error} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(\text{confidence}_i - \text{accuracy}_i)^2}$$

이 지표는 모델이 표현한 자신감이 실제 성능과 얼마나 일치하는지 측정합니다.[1]

***

### 3. 모델 구조 및 성능 평가

#### 3.1 벤치마크 구성

**다중 포맷 질문**
- **정확-매칭 질문**: 76% (정확한 문자열 출력 필요)
- **객관식 질문**: 24% (5개 이상 선택지 중 선택)
- **멀티모달**: 약 14%의 질문이 텍스트-이미지 포함[1]

**주제 분포**
- 경제학, 생태학, 인공지능, 음악학, 철학, 신경과학, 법학, 미술사 등 100+개 과목
- STEM과 인문학의 균형 있는 구성

#### 3.2 최첨단 모델의 성능 결과

| 모델 | 정확도 (%) ↑ | 캘리브레이션 오류 (%) ↓ |
|------|------------|-------------------|
| O3-MINI (HIGH) | 13.4 | 80 |
| DeepSeek-R1* | 8.5 | 73 |
| O1 | 8.0 | 83 |
| Gemini 2.0 Flash Thinking | 6.6 | 82 |
| Gemini 1.5 Pro | 4.6 | 88 |
| Claude 3.5 Sonnet | 4.1 | 84 |
| Grok 2 | 3.0 | 87 |
| GPT-4O | 2.7 | 89 |

*DeepSeek-R1과 O3-MINI는 텍스트 전용 질문에 대해서만 평가[1]

**핵심 발견**:
- 모든 모델이 극도로 낮은 정확도 (2.7%-13.4%)
- 높은 캘리브레이션 오류(80% 이상) - 모델들이 틀린 답을 높은 자신감으로 제시
- **할루시네이션/혼동(hallucination)의 명확한 증거**

#### 3.3 추론 모델의 토큰 효율성 분석

추론 모델(reasoning model)들은 성능 향상을 위해 상당한 추가 계산량 필요:

$$\text{평균 완성 토큰 수} = \text{추론 토큰} + \text{출력 토큰}$$

- Gemini 2.0 Flash Thinking: 정확도 6.6%, 평균 ~3,000-7,000 토큰
- O1: 정확도 8.0%, 평균 ~2,000-8,000 토큰  
- DeepSeek-R1: 정확도 8.5%, 평균 ~2,000-8,000 토큰[1]

**결론**: 향후 모델 개발은 정확도뿐 아니라 **계산 효율성**을 동시에 달성해야 함

***

### 4. 일반화 성능 향상 가능성 중점 분석

#### 4.1 현재 제한 사항

**구조화된 과제에 국한된 측정**
HLE는 **폐쇄형 학문적 질문**에만 초점을 맞춰 있습니다. 실제 연구 능력이나 창의적 문제 해결을 측정하지 않습니다.[1]

#### 4.2 일반화 성능 개선의 잠재적 경로

##### A. 도메인 특화 미세 조정(Domain-Specific Fine-tuning)
특정 분야(예: 수학, 생명과학)의 고급 데이터로 모델 재학습하여 해당 분야의 성능 향상

##### B. 계층적 추론 구조(Hierarchical Reasoning)
복잡한 문제를 하위 문제로 분해하고 단계적으로 해결하는 능력 강화

##### C. 멀티 모드 학습(Multi-Modal Learning) 
최신 AI 모델들은 텍스트와 이미지를 모두 처리하는 능력이 필수적이며, 이는 약 14%의 질문에서 직접 테스트됩니다.[1]

##### D. 불확실성 정량화(Uncertainty Quantification)
베이지안 방법론이나 온도 스케일링(Temperature Scaling)을 통해 모델의 확신도를 보정[1]

#### 4.3 앞으로의 성능 궤적 예측

논문에서 지적하듯이, **역사적 패턴상 벤치마크는 빠르게 포화**됩니다.[1]

기대치:
- 2025년 말까지 모델들이 HLE에서 **50% 이상 정확도 달성 가능**
- 하지만 이는 **폐쇄형 학문 문제**에만 국한되며, 
- **자율적 연구 능력** 또는 **인공지능 일반(AGI)**을 의미하지 않음[1]

***

### 5. 한계점 및 고려사항

#### 5.1 데이터셋 품질 이슈

**전문가 의견 불일치율**
- 일반 분야: 15.4%의 전문가 의견 불일치
- 생명과학/의학: 약 18% (일부 논쟁: 약 30% 추정)[1]

**검증 과정의 한계**
- 5분 이상 걸리는 문제는 검토자가 완전히 검증하지 않음
- 최첨단 연구 경험에 기반한 문제는 외부 검증 어려움

#### 5.2 모델 평가의 한계

**검색 가능한 질문 제거의 불완전성**
- 인터넷 검색 가능한 질문을 완전히 제거할 수 없음
- 모델이 학습 데이터에서 본 내용을 재생산할 가능성

**단일 프롬프팅 방식의 제약**
- 모든 모델에 동일한 프롬프트 구조 강제
- 모델 특화 프롬프팅의 이점 미반영

***

### 6. 향후 연구에 미치는 영향 및 고려사항

#### 6.1 벤치마크 설계의 새로운 기준 수립

**HLE의 영향력**
- 포화된 벤치마크에서 벗어나 **지속적으로 유효한 평가 시스템 필요**
- 다른 벤치마크 개발자들에게 **엄격한 다단계 검증 프로세스**의 중요성 강조
- 전문가 참여 기반의 글로벌 협력 모델 제시[1]

#### 6.2 HLE-ROLLING: 동적 벤차마킹

커뮤니티 피드백을 반영한 **지속적 업데이트 계획**:
- 오류 보고 시스템 운영 중
- 새로운 질문 포함으로 정기적 갱신 예정
- 모델이 성능 천장에 도달할 때 평활한 전환 제공[1]

#### 6.3 최신 연구 동향 기반 고려사항

##### A. 스케일링 법칙의 한계 인식
최근 AI 업계는 단순한 모델 크기 증가(scaling)의 한계를 인식하고 있습니다.[2]

- 2025년 이후 모델 개발은 **추론 특화 모델**(o1, o3, DeepSeek-R1, Gemini Thinking)로 전환
- 계산 효율성과 추론 깊이의 균형이 새로운 평가 기준

##### B. 캘리브레이션의 핵심 중요성 강조
HLE의 높은 캘리브레이션 오류(80% 이상)는 모델 신뢰성 문제를 드러냅니다:[3][4]

$$\text{Temperature Scaling: } p_{\text{calibrated}} = \sigma\left(\frac{\text{logit}}{T}\right)$$

여기서 $T$는 온도 매개변수로, 확률 분포를 부드럽게 조정하는 후처리 기법

##### C. 다중 복잡도 평가의 필요성
최근 연구(DeepRD)에서 보여지듯, 모델 성능은 **문제 복잡도에 따라 비선형적으로 저하**됩니다.[5]

향후 벤치마크는 다양한 복잡도 수준을 포함하여 모델의 실제 **일반화 경계**(generalization boundary)를 파악해야 합니다.

##### D. 컨텍스트 로트(Context Rot) 문제
입력 길이 증가에 따른 성능 저하를 고려한 평가 필요:

$$\text{Performance Degradation} = f(\text{context length}, \text{distraction level})$$[6]

#### 6.4 미래 연구자를 위한 구체적 권고사항

**1. 벤치마크 개발 시**
- 다중 언어 및 문화적 다양성 포함
- 시간에 따른 변화 능력 평가 메커니즘 (예: TiEBe)
- 오픈-엔디드 문제와 폐쇄형 문제의 균형

**2. 모델 평가 시**
- RMS 캘리브레이션 오류 의무적 보고
- 스케일링 법칙의 한계 고려
- 계산 토큰 효율성 평가

**3. 모델 개발 시**
- 단순 스케일링보다 추론 깊이 우선
- 불확실성 표현 능력 강화
- 도메인별 성능 이질성 분석

***

### 결론

**Humanity's Last Exam**은 단순한 벤치마크를 넘어 **AI 평가 방법론의 새로운 표준**을 제시합니다. 현재의 극도로 낮은 모델 성능(2.7%-13.4%)과 높은 캘리브레이션 오류는 최첨단 모델들 사이의 성능 격차를 정밀하게 측정할 수 있게 해줍니다.[7][1]

특히 이 논문이 보여주는 **전문가 기반 글로벌 협력**, **다단계 엄격한 검증**, 그리고 **동적 업데이트 시스템**(HLE-ROLLING)은 향후 벤치마크 개발의 모범사례가 될 것입니다. 

다만 연구자들은 HLE가 폐쇄형 학문 문제에만 초점을 맞추고 있다는 점, 그리고 현재의 스케일링 법칙 한계를 고려하여 **다차원적 평가 시스템 구축**에 집중해야 합니다. 향후 AI 개발은 단순한 성능 향상을 넘어 **효율성, 신뢰성, 그리고 일반화 능력**의 통합적 평가를 요구할 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/9296fc59-c321-4022-bcbf-e09df0829c4a/2501.14249v9.pdf)
[2](https://contents.premium.naver.com/banya/banyacompany/contents/250228110248877sq)
[3](https://irukai.tistory.com/12)
[4](https://velog.io/@kbin6961/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-On-Calibration-of-Modern-Neural-Networks)
[5](https://discuss.pytorch.kr/t/2025-10-27-11-02-ai-ml/8104)
[6](https://library.gongbuhow.com/docs/general/research/context-rot/)
[7](https://scale.com/leaderboard/humanitys_last_exam)
[8](https://arxiv.org/pdf/2306.05179.pdf)
[9](https://arxiv.org/pdf/2304.06364.pdf)
[10](https://www.tandfonline.com/doi/full/10.1080/01425692.2025.2454316)
[11](https://arxiv.org/pdf/2501.07482.pdf)
[12](https://arxiv.org/abs/2410.15037)
[13](https://dl.acm.org/doi/pdf/10.1145/3580305.3599827)
[14](http://arxiv.org/pdf/2402.16694.pdf)
[15](https://arxiv.org/pdf/2403.10378.pdf)
[16](https://www.futurehouse.org/research-announcements/hle-exam)
[17](https://www.emergentmind.com/topics/humanity-s-last-exam-hle-benchmark)
[18](https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Large-Language-Models-A-Survey-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0)
[19](https://agi.safe.ai)
