# Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality

### 1. 핵심 주장 및 주요 기여

**Winoground** 논문은 **시각-언어 모델(Vision-Language Models, VLMs)의 합성성(Compositionality) 능력**을 체계적으로 평가하는 새로운 벤치마크를 제시합니다. 이 연구의 핵심 기여는 세 가지입니다.[1]

첫째, **동일한 단어로 구성되되 다른 순서의 캡션 쌍**을 이용하여 참된 합성성 능력을 측정합니다. 예를 들어 "갈색 개가 하얀 소파 위에 있다"와 "흰 개가 갈색 소파 위에 있다"는 단어 집합은 동일하지만 시각적으로 완전히 다른 이미지를 지칭합니다. 이는 단어 자체가 아닌 **단어 순서의 중요성**을 측정할 수 있게 합니다.[1]

둘째, **수작업으로 정밀하게 큐레이션된 1,600개 이미지-텍스트 쌍**으로 구성된 벤치마크를 제공합니다. 전문 주석자들은 70가지의 세밀한 언어학적 태그(객체 스왑, 관계 스왑, 실용성 등)와 시각적 태그(기호성, 실용성, 시리즈)를 부여하여 모델 성능의 상세한 분석을 가능하게 합니다.[1]

셋째, **12개의 최첨단 VL 변환기와 RNN 기반 모델을 광범위하게 평가**한 결과, 놀랍게도 거의 모든 모델이 무작위 기준(25%)보다 겨우 약간 나은 성능만 보였습니다. 이는 현재의 VLMs가 인간 수준의 합성성 이해에 훨씬 못 미친다는 증거입니다.[1]

***

### 2. 해결 문제 및 제안 방법

#### 2.1 문제 정의

Winoground가 해결하고자 하는 근본 문제는 **합성 추론 능력(Compositional Reasoning)의 부재**입니다. 기존 VLMs는:[1]

- 단어 순서에 대한 민감성이 매우 낮음
- 텍스트와 이미지 간의 의미적 대응을 정확히 파악하지 못함
- 복합적인 시각-언어 구조를 이해하지 못함

#### 2.2 평가 메트릭 (수식)

Winoground는 세 가지 평가 메트릭을 제안합니다:

**텍스트 스코어 (Text Score):**

$$f(C_0, I_0, C_1, I_1) = \begin{cases} 1 & \text{if } s(C_0, I_0) \geq s(C_1, I_0) \text{ and } s(C_1, I_1) \geq s(C_0, I_1) \\ 0 & \text{otherwise} \end{cases}$$

여기서 $s$는 모델의 이미지-캡션 쌍에 대한 점수입니다.[1]

**이미지 스코어 (Image Score):**

$$g(C_0, I_0, C_1, I_1) = \begin{cases} 1 & \text{if } s(C_0, I_0) \geq s(C_0, I_1) \text{ and } s(C_1, I_1) \geq s(C_1, I_0) \\ 0 & \text{otherwise} \end{cases}$$

**그룹 스코어 (Group Score):**

$$h(C_0, I_0, C_1, I_1) = \begin{cases} 1 & \text{if } f(C_0, I_0, C_1, I_1) = 1 \text{ and } g(C_0, I_0, C_1, I_1) = 1 \\ 0 & \text{otherwise} \end{cases}$$

그룹 스코어는 모든 4가지 이미지-캡션 조합이 올바르게 순위 매겨져야만 1을 반환합니다.[1]

#### 2.3 데이터셋 구성 및 태그

데이터셋은 **400개의 예제(800개의 고유 이미지와 캡션)**로 구성되며, 총 1,600개의 이미지-텍스트 쌍을 생성합니다. 언어학적 태그는 세 가지 주요 범주로 분류됩니다:[1]

| 태그 범주 | 세부 내용 | 예시 |
|---------|---------|------|
| 객체 스왑 (141개) | 명사구 재배열 | "컵이 풀 속에" vs "풀이 컵 속에" |
| 관계 스왑 (233개) | 동사, 형용사, 전치사 재배열 | "개가 앉고 사람이 서 있다" vs "사람이 앉고 개가 서 있다" |
| 양쪽 스왑 (26개) | 객체와 관계 모두 변경 | "화재 트럭" vs "트럭 화재" |

***

### 3. 모델 아키텍처 및 성능

#### 3.1 평가된 모델 아키텍처

논문은 **10개의 멀티모달 변환기**와 **2개의 RNN 기반 모델**을 평가합니다:[1]

**단일 스트림 모델** (merged attention):
- VinVL, UNITER, ViLLA, VisualBERT, ViLT

**이중 스트림 모델** (separate encoders):
- CLIP, FLAVA, LXMERT, ViLBERT, UniT

이들 모델의 주요 차이점은:

1. **이미지 임베딩**: Faster R-CNN 기반 영역 특성 vs Vision Transformer (ViT)
2. **어텐션 메커니즘**: 병합 어텐션 vs 크로스 모달 어텐션
3. **사전학습 데이터**: 0.03M ~ 400M 이미지-캡션 쌍

#### 3.2 성능 결과

**전체 성능 결과:**

| 모델 | 텍스트 스코어 | 이미지 스코어 | 그룹 스코어 |
|------|-------------|-------------|-----------|
| MTurk 인간 | 89.50 | 88.50 | 85.50 |
| 무작위 기준 | 25.00 | 25.00 | 16.67 |
| VinVL (최고성능) | 37.75 | 17.75 | 14.50 |
| UNITER (large) | 38.00 | 14.00 | 10.50 |
| ViLLA (large) | 37.00 | 13.25 | 11.00 |
| ViLT | 34.75 | 14.00 | 9.25 |
| CLIP | 30.75 | 10.50 | 8.00 |

**핵심 발견:**[1]

- 텍스트 스코어에서 인간과의 격차: **약 50%**
- 이미지 스코어에서 인간과의 격차: **약 70%**
- 그룹 스코어에서 모든 모델이 무작위 기준 이하 (16.67%)

#### 3.3 구조적 특성별 성능

**이미지 스코어 vs 텍스트 스코어 격차:**
논문은 흥미로운 현상을 발견합니다. 모든 모델이 **이미지 검색보다 캡션 검색에서 훨씬 우수한 성능**을 보입니다. 이는 텍스트 인코더가 시각 인코더보다 더 강력하거나, 텍스트 모달이 다양한 편향을 가지고 있음을 시사합니다.[1]

***

### 4. 성능 향상 및 한계 분석

#### 4.1 성능 향상 가능성

**사전학습 데이터 규모와의 상관관계:**
논문은 **강한 양의 상관관계**를 발견합니다:[1]

$$\text{Pearson Correlation} = 0.84 \text{ (텍스트 스코어)}, 0.76 \text{ (이미지 스코어)}$$

CLIP과 FLAVA를 제외한 모델들에서, 사전학습 데이터 규모가 증가할수록 Winoground 성능이 선형적으로 향상됩니다.

**언어학적 복잡도의 영향:**
- 단어 길이와 성능 간의 **음의 상관관계**: 더 긴 캡션일수록 모델 성능 저하
- 인간은 반대 패턴 (긴 캡션에서 더 높은 정확도)
- 이는 모델의 **약한 언어 인코딩 능력**을 시사합니다.[1]

#### 4.2 실패 분석

**시각 인코더의 약점:**
ViLT 모델의 워드-리전 어텐션 히트맵 분석 결과, 모델은 형용사를 무시하는 경향을 보입니다. 예를 들어, "갈색 개"와 "흰 개" 모두에서 동일한 개 지역을 강조합니다.[1]

**주요 실패 원인:**

1. **시리즈 태그에서의 완전 실패**: 같은 사진 시리즈의 유사한 이미지들을 구별하지 못함 (그룹 스코어 0)
2. **실용성(Pragmatics) 이해 부족**: 관용적 표현이나 전치사 부착 선호도 파악 불가
3. **합성 구조 미이해**: "현재를 시청하기" vs "시계 제시" 같은 구조적 차이 인식 부족

#### 4.3 모델 일반화 성능 향상의 한계

**단일 벡터 표현의 한계:**[2]
최근 연구에서 밝혀진 바에 따르면, 단일 벡터 표현은 복잡한 멀티모달 데이터를 충분히 표현하기에 부족합니다. 생성형 모델(GPT-4를 사용한 프롬프팅)이 Winoground에서 **최대 10% 성능 향상**을 달성했습니다.[2]

**구조적 한계:**
- 변환기는 단어 **공존 통계에 의존**하며 진정한 단어 순서 민감성이 부족
- 병합 어텐션과 크로스 모달 어텐션 모두 동일한 수준의 성능 저하를 보임
- 모델 아키텍처 유형 (단일 스트림 vs 이중 스트림)은 성능에 유의미한 영향을 미치지 않음[1]

***

### 5. 모델 일반화 성능 향상을 위한 최신 연구 (2023-2025)

#### 5.1 자연언어 추론 기반 접근

**CECE (Caption Expansion with Contradictions and Entailments) 방법**은 최신 기법으로서:[3]
- 자연언어 추론(NLI)을 활용하여 전제에서 함축과 모순을 생성
- Winoground에서 **그룹 스코어 +19.2%** (최전 사전 작업 대비)
- EqBen에서 **+12.9%** 성능 향상[3]

이 방법은 **미세 조정 없이도** 최첨단 성능을 달성합니다.

#### 5.2 합성 데이터를 활용한 향상

**SyViC (Synthetic Visual Concepts) 데이터셋:**[4]
- 백만 규모의 합성 데이터로 시각적 개념 이해 향상
- VL-Checklist에서 **4.3%** 성능 향상
- ARO 벤치마크에서 **9.9%** 개선
- Winoground와 같은 합성 벤치마크 성능도 향상

#### 5.3 이미지 설명의 정교성 증대

**ImageInWords (IIW) 프레임워크**:[5]
- 인간 기반 루프를 통한 정교한 이미지 설명 큐레이션
- Winoground에서 **최대 6%** 성능 향상
- 모델의 합성성 추론 능력 개선

#### 5.4 토큰 수준 신뢰도 분석

**Token-Level Confidence (TLC) 방법**:[6]
- 단어별 신뢰도 집계를 통한 이미지-캡션 일관성 평가
- Winoground 그룹 스코어 **2배 이상** 향상
- 객체 환각 감소 (MS COCO에서 상대 30% 감소)

#### 5.5 LLM 프롬프팅의 역할

**생성형 모델 활용:**[2]
- GPT-4를 사용한 이미지 설명과 단계별 추론
- 임베딩 기반 방법보다 우수한 성능
- 최적화된 설명으로 **최대 10% 추가 향상**

#### 5.6 입력 순서 민감성 분석

**최신 발견 (2024):** 멀티모달 대규모 언어 모델(MLLMs)에서 발견된 새로운 문제:[7]
- 멀티모달 입력의 순서 변경 시 성능 급격한 변동 (무작위 추측 수준까지)
- **Position-Invariant Accuracy (PIA)** 메트릭 제안
- 비디오-캡션 매칭에서 **14.7%** 성능 향상 가능
- 이는 Winoground의 합성성 문제와 깊은 연관

#### 5.7 강화 학습 기반 향상

**최신 연구 (2024-2025):**[8]
- 감독 미세 조정(SFT) 대비 강화 학습(RL) 훈련이 합성 일반화에서 더 우수
- 시각-텍스트 정렬 강화와 정확한 시각 기반이 핵심
- "시각 먼저 설명하기" 전략이 합성성 향상에 효과적

***

### 6. 이 논문이 미치는 영향 및 향후 연구 방향

#### 6.1 학문적 영향

Winoground는 **합성성 평가 벤치마크**로서 다음과 같은 중대한 영향을 미쳤습니다:

1. **새로운 평가 패러다임 수립**: Winograd Schema Challenge를 시각-언어 영역에 적용하여 **최소 쌍 원리(minimal pair principle)**의 중요성을 입증[1]

2. **광범위한 후속 연구 촉발**: 2022년 이후 Winoground에서의 성능 향상을 목표로 하는 다양한 연구가 진행되었으며, 위에서 언급한 여러 최신 방법들이 이에 해당합니다.

3. **모델 평가의 표준화**: VLM의 진정한 합성성 능력을 측정하는 도구로 널리 인정받고 있습니다.

#### 6.2 향후 연구 고려사항

**1. 아키텍처 개선 방향:**
- 단일 벡터 표현 대신 **구조화된 표현** 학습 필요
- 단어 순서와 의존성에 더 민감한 어텐션 메커니즘 개발
- 크로스 모달 정렬을 위한 명시적 구문 분석 포함

**2. 사전학습 전략:**
- 기존의 대규모 일반 데이터셋이 아닌 **합성성 중심의 사전학습 목적 함수** 설계
- 어려운 음수(hard negatives) 샘플링을 통한 합성성 강조
- 대조 학습 대신 순위 손실 함수 사용

**3. 데이터 관점:**
- Winoground와 같은 **최소 쌍 기반 벤치마크** 확대 필요
- 다언어 확장 (현재 논문은 영어만 포함)[1]
- 다양한 시각적 현상 (객체, 속성, 관계, 공간)을 균형있게 포함

**4. 평가 메트릭 개선:**
- Position-Invariant Accuracy와 같은 **순서 불변적 평가**[7]
- 부분 신용 메커니즘 도입 (현재는 완전 일치/불일치만 계산)
- 다중 대안 평가 설정

**5. 실용적 응용:**
- 이미지 검색 시스템에서의 합성성 향상
- 텍스트-이미지 생성 모델의 정확도 개선
- 시각적 질문 답변(VQA)에서의 정밀한 관계 이해

**6. 교차 모달 상호작용 분석:**
- 시각 모달과 언어 모달 간의 **비대칭성** 분석[1]
- 각 모달의 편향 메커니즘 규명
- 모달 간 정렬 강화 방법 개발

**7. 일반화 한계 극복:**
- 사전학습 데이터 규모의 한계 초월 (장기적 목표)
- 소규모 미세 조정으로도 강한 합성성 습득 가능성[9]
- 전이 학습 메커니즘 강화

***

### 결론

Winoground는 **시각-언어 모델의 합성성 이해 능력의 심각한 부족**을 체계적으로 규명한 중요한 연구입니다. 이후 3년간의 연구를 통해 자연언어 추론, 합성 데이터, 강화 학습 등 다양한 접근법으로 약 **19-30% 수준의 성능 향상**이 달성되었지만, 여전히 인간 수준(85.50% 그룹 스코어)에 도달하기 위해서는 근본적인 아키텍처 혁신과 새로운 학습 패러다임의 개발이 필요합니다. 특히 최신 연구에서 강조되는 **시각-텍스트 정렬**, **입력 순서 불변성**, **구조적 이해** 같은 요소들이 향후 VLM 발전의 핵심 열쇠가 될 것으로 예상됩니다.[8][3]

***

### 참고문헌 (논문의 주요 인용)

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/57c75baa-bcd8-4a4a-a5a6-d948e906480e/2204.03162v2.pdf)
[2](https://arxiv.org/abs/2401.11337)
[3](https://arxiv.org/abs/2410.22315)
[4](https://ieeexplore.ieee.org/document/10377020/)
[5](https://arxiv.org/abs/2405.02793)
[6](https://ieeexplore.ieee.org/document/10484304/)
[7](https://arxiv.org/abs/2410.16983)
[8](https://openreview.net/forum?id=J76cCYTJub)
[9](https://aclanthology.org/2024.findings-acl.99.pdf)
[10](https://link.springer.com/10.1007/978-3-031-72890-7_11)
[11](https://arxiv.org/abs/2306.01879)
[12](https://www.semanticscholar.org/paper/9868f6f8d686b8e50c2099f9cd72d5bc61dbd5da)
[13](https://www.semanticscholar.org/paper/1b4e27ea87900e31147b002d17cb4f79834f0290)
[14](https://arxiv.org/abs/2510.09586)
[15](https://arxiv.org/html/2401.11337v1)
[16](https://arxiv.org/html/2410.22315)
[17](https://aclanthology.org/2023.findings-emnlp.818.pdf)
[18](https://arxiv.org/abs/2305.16328)
[19](https://arxiv.org/html/2412.08125v2)
[20](http://arxiv.org/pdf/2410.05210.pdf)
[21](https://arxiv.org/pdf/2401.01974.pdf)
[22](https://aclanthology.org/2023.findings-emnlp.755.pdf)
[23](https://openaccess.thecvf.com/content/CVPR2022/papers/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.pdf)
[24](https://proceedings.mlr.press/v239/wu23a.html)
[25](https://arxiv.org/abs/2204.03162)
[26](https://arxiv.org/html/2410.16983v1)
[27](https://www.semanticscholar.org/paper/Winoground:-Probing-Vision-and-Language-Models-for-Thrush-Jiang/c435ecd0321dcec1f25e458bf930311f9e1d04b6)
[28](https://aclanthology.org/2024.emnlp-main.996/)
[29](https://openreview.net/forum?id=G3aXjVAJjU)
