# GPQA: A Graduate-Level Google-Proof Q&A Benchmark

### 요약 및 핵심 내용

**GPQA 논문의 핵심 주장**[1]

GPQA는 인간 전문가를 초월하는 AI 시스템의 감시 능력(scalable oversight) 개발을 위한 구체적 토대를 제공합니다. 특히 과학적 지식 창출과 같이 매우 어려운 질문에서 AI의 출력을 검증해야 할 상황에 대비하기 위해, 인간 평가자가 정답을 직접 판단할 수 없는 상황에서도 작동 가능한 감시 방법론이 필수임을 주장합니다.[1]

**주요 기여**[1]

논문은 448개의 고품질 대학원 수준 객관식 질문(생물학, 물리학, 화학)으로 구성된 GPQA를 제시하며, 3가지 구성으로 데이터를 분류합니다: GPQA Extended (546개), GPQA Main (448개), GPQA Diamond (198개).  61명의 박사 수준 계약업체를 고용하여 4단계 검증 파이프라인을 구현했으며, 평균 시간당 $95의 높은 인센티브로 품질을 보장했습니다.  핵심 성과로 비전문가(다른 분야 박사) 정확도 34%, GPT-4 정확도 39%, 전문가 정확도 65%(사후 동의 분석 시 74%)를 달성했습니다.[1]

***

### 해결 문제 및 방법론

**근본적 문제**[1]

기존 RLHF 기반 평가 방식은 인간 평가자가 정답을 판단할 수 있을 때만 작동합니다. AI가 인간 능력을 초월하면 평가자가 출력의 정확성을 검증할 수 없게 되며, 할루시네이션과 거짓말 경향이 심화될 위험이 있습니다.  기존 벤치마크(TriviaQA, SQuAD)는 비전문가가 인터넷 검색으로 쉽게 해결 가능하며, 현재 LLM이 대부분을 포화시켰고, 통계적 파워가 부족하며, 객관성이 부재합니다.[1]

**방법론: 4단계 검증 파이프라인**[1]

1단계 질문 작성에서는 박사급 전문가가 명확성, 객관성, 어려움, Search 저항성을 만족하는 질문을 작성합니다.  인센티브 구조는 기본 $10 + 정답 보너스($20×2) + 어려움 보너스($15×3) + 추가 보너스($30) = 최대 $115입니다.[1]

2단계 제1 전문가 검증에서는 같은 도메인 전문가가 객관성과 피드백을 제공합니다. 3단계 질문 개정에서는 작성자가 피드백을 반영하여 개정합니다. 4단계 제2 전문가 및 비전문가 검증에서는 다른 전문가 1명과 다른 도메인의 박사 3명이 평가합니다.[1]

***

### 모델 구조 및 성능

**평가 체계**[1]

폐쇄형 설정에서 모델은 인터넷 접근 없이 답변합니다: 영-샷, 수-샷, 영-샷 CoT, 수-샷 CoT 4가지 프롬프팅 방법을 사용합니다.  개방형 설정에서는 Self-Ask 기반 검색 도구를 제공하며, GPT-4의 37.2% 기권율을 Fallback 전략으로 처리합니다.[1]

**기본 모델 성능**[1]

| 모델 | 폐쇄형(최고) | 개방형 | 비고 |
|------|-----------|-------|------|
| **Llama-2-70B** | 30.4% | - | Few-Shot CoT |
| **GPT-3.5-turbo** | 28.2% | - | Few-Shot CoT |
| **GPT-4** | 38.7% | 41.0% | Few-Shot CoT + Search |
| **비전문가(인간)** | 33.9% | - | 인터넷 30분 이상 |
| **전문가(인간)** | 65.4% | - | Extended 세트 |

개방형 설정에서 검색 추가 효과는 미미하며(39.7% → 41.0%), 높은 기권율이 효과를 제한합니다.[1]

**도메인별 성능**[1]

$$\text{전문성 격차} = \text{전문가 정확도} - \text{비전문가 정확도}$$

| 도메인 | 질문 수 | 전문가 | 비전문가 | 격차 |
|-------|-------|-------|--------|------|
| 생물학 | 105 | 66.7% | 43.2% | 23.5% |
| 물리학 | 227 | 57.3% | 32.5% | 24.8% |
| 화학 | 214 | 72.0% | 31.4% | **40.6%** |

화학(특히 유기화학)이 가장 어려우며, GPT-4는 생물학(58.1%)에서 가장 강합니다.[1]

***

### 일반화 성능 향상 가능성

**현재 상황 분석**[2][3]

2023년 GPQA 공개 당시 GPT-4는 39% 정확도를 달성했습니다.  2024년 3월 Claude 3 Opus는 약 60%에 도달했으며,  최신 모델들(OpenAI o1)은 2024년 후반 약 83%, Aristotle-X1은 2025년 중반 92.4%를 달성했습니다.  성능 개선 속도는 연 약 35.6% 포인트입니다.[4][2]

**긍정적 일반화 지표**[2][1]

전문가-비전문가 격차가 40-60%p 존재하여 여전히 구별력을 유지합니다.  화학 > 물리학 > 생물학 어려움 순서가 일관성 있게 유지되며, 유기화학의 난해성이 지속됩니다.  포스트혹 합의 메커니즘으로 전문가 동의율이 80%+ 도달하여 추론 과정 이해 가능성을 시사합니다.[1]

**부정적 신호와 포화 우려**[3][2]

최고 모델이 92.4%에 도달하여 GPQA-Diamond의 구별력이 감소하고 있습니다.  비전문가 성능은 21.9%로 거의 변함없어 천장에 접근했음을 시사합니다. 벤치마크 포화 문제로 인해 향후 벤치마크 재설계 필요성이 제기되고 있습니다.[2]

**검색 도구 한계**[1]

개방형 설정에서 검색 추가의 효과가 1-3% 미만으로, 단순 인터넷 접근만으로는 부족하며 더 깊은 이해가 필요함을 시사합니다.  이는 향후 감시 방법론이 추론 과정 평가 중심으로 진화해야 함을 의미합니다.[1]

***

### 모델 구조의 한계

**규모 및 선정 한계**[1]

448개 문제는 모델 훈련에 부적절하며, 50-60% 이상의 큰 정확도 차이만 검출 가능합니다.  비전문가 선정이 다른 도메인 박사 수준으로 제한되어, 실제 비전문가(일반인, 학부생)가 아닙니다.  따라서 성능 수치는 상한선이며, 실제 격차는 더 클 수 있습니다.[1]

**편향 및 객관성 문제**[1]

Upwork 기반 채용으로 인한 지역, 언어, 배경 편향이 있으며, 특정 문화권의 과학 교육 전통이 반영됩니다.  포스트혹 동의 분류에서 "나쁜 질문" 비율이 28.3%로 상당하여 객관성 평가 자체의 주관성이 문제입니다.[1]

**구조적 한계**[1]

GPQA는 여전히 인간 전문가가 정답을 알 수 있는 문제이므로, 향후 AI의 "진정한 초인적 능력"은 다른 성격이 됩니다.  모든 질문이 텍스트 기반이라 유기화학의 분자 구조 시각화 등 다중 모달 정보가 부족합니다.  과학의 동적 특성이 미반영되어 2023년 기준 "확정된 지식"만 포함됩니다.[1]

***

### 영향 및 최신 연구 동향

**벤치마크 설계 원칙 확립**[1]

Irving & Askell (2019)의 9대 감시 기준 중 7가지를 충족합니다: 참 답변 기지(전문가 검증), 거짓 답변 그럴듯함(비전문가 구별 불가), 전문가 우월성, 검증 어려움, 검증 가능 사실, 표면 특징 회피, 현실성.[1]

**후속 연구 촉발**[5][6]

SuperGPQA (2025): 285개 대학원 분야로 확대하여 산업, 농업, 법, 의학 등 포함합니다.[5]

Dynamic-KGQA (2025): 지식 그래프 기반 동적 생성으로 암기 위험을 완화합니다.[7]

CHASE (2024): LLM 기반 자동 생성으로 40-60% 정확도의 어려운 문제를 창출합니다.[6]

Scalable Oversight via Partitioned Supervision (2025): 여러 좁은 전문가의 "틀린 답" 지적으로 감시하는 방법론입니다.[8]

***

### 향후 연구 방향

**단기 (2025-2026)**[5][2][1]

GPQA-Diamond의 역할 재정의가 필요합니다.  SuperGPQA 활용으로 광역 평가를 시작하고, 동적 벤치마크 도입을 검토해야 합니다.  실제 관심 대상의 다양한 비전문가 평가가 필수입니다.[5][2][1]

**중기 (2026-2028)**[9]

프로세스 중심 평가로 전환하여 정답이 아닌 추론 과정을 평가합니다.  실제 감시 환경에 가까운 비전문가 평가를 시작하고, 상호작용형 감시 방법론(Debate, Market-Making, Recursive Reward Modeling)을 보편화합니다.[9][1]

**장기 (2028+)**

미답 질문(Open Question) 기반 벤치마크를 개발하여 진정한 초인적 능력을 검증합니다. AI와 인간의 협력적 문제해결 평가를 수행하고, 장기적 신지식 생성 능력을 검증합니다.

***

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c2f87d0d-1958-418d-8fb0-6288eb079355/2311.12022v1.pdf)
[2](https://intuitionlabs.ai/articles/gpqa-diamond-ai-benchmark)
[3](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance)
[4](https://openreview.net/forum?id=Ti67584b98)
[5](https://arxiv.org/abs/2502.14739)
[6](https://openreview.net/forum?id=2VhFZPYqjE)
[7](https://arxiv.org/pdf/2503.05049.pdf)
[8](https://www.arxiv.org/abs/2510.22500)
[9](https://arxiv.org/html/2504.18838v1)
[10](http://arxiv.org/pdf/2312.09781.pdf)
[11](https://arxiv.org/pdf/2410.22368.pdf)
[12](https://arxiv.org/pdf/2309.02784.pdf)
[13](https://aclanthology.org/2023.emnlp-main.298.pdf)
[14](https://arxiv.org/pdf/2202.12359.pdf)
[15](https://arxiv.org/pdf/2304.06364.pdf)
[16](https://www.edps.europa.eu/data-protection/technology-monitoring/techsonar/scalable-oversight)
[17](https://alignmentproject.aisi.gov.uk/research-area/benchmark-design-and-evaluation)
[18](https://papers.nips.cc/paper_files/paper/2024/file/4e6f22305275966513990f53cec908e0-Paper-Datasets_and_Benchmarks_Track.pdf)
[19](https://epoch.ai/data-insights/self-reported-gpqa)
