# MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark

### 1. 핵심 주장과 주요 기여 (요약)

MMLU-Pro는 빠르게 진화하는 대규모 언어 모델(LLM)의 능력을 평가하기 위한 기존 MMLU 벤치마크의 한계를 직접 해결하는 논문이다. 이 연구의 핵심 주장은 다음과 같다:[1]

**기존 MMLU의 문제점**: 원본 MMLU는 성능 포화(saturation) 문제를 보이고 있다. GPT-4가 2023년 3월 86.4%의 정확도를 달성한 이후, 최신 모델들(GPT-4-Turbo, Gemini-1.5-Pro, Claude, LLaMA-3-400B)은 모두 86-87% 사이에 군집되어 있어 모델 간 차별성을 구분하기 어려워졌다.[1]

**MMLU-Pro의 핵심 혁신**: 

1) **10개의 선택지 도입**: MMLU의 4개 선택지에서 10개로 확대하여 임의 추측의 확률을 크게 감소시킨다.[1]

2) **추론 중심의 난이도 상향**: 단순 지식 기반의 질문에서 복잡한 추론을 요구하는 질문으로 초점을 전환한다.[1]

3) **고질의 데이터셋**: 전문가 검토 2단계와 LLM 기반 재검증을 통해 모호하고 오류가 있는 질문을 제거한다.[1]

**주요 성과**: MMLU-Pro는 GPT-4o의 정확도를 88.7%에서 72.6%로 16-33% 감소시키고, 모델 간 변별성을 향상시키며, 프롬프트 변동에 대한 민감도를 4-5%에서 2%로 감소시킨다.[1]

***

### 2. 해결하고자 하는 문제 및 제안 방법

#### 2.1 문제 정의

원본 MMLU의 주요 한계는 세 가지로 요약된다:[1]

1. **선택지 부족**: MMLU의 4개 선택지는 모델이 진정한 이해 없이 지름길(shortcut)을 이용하여 답변할 가능성을 제시한다.

2. **지식 중심의 문제**: 대부분의 질문이 추론보다 지식을 요구하여, 대다수 모델이 Chain-of-Thought(CoT) 없이 직접 답변으로 더 나은 성능을 보인다.

3. **데이터 노이즈**: 원본 MMLU의 일부 질문은 답변 불가능하거나 잘못 주석 처리되어 있어 평가의 천장(ceiling)을 낮춘다.

#### 2.2 데이터셋 구성 파이프라인

MMLU-Pro의 구성은 다음과 같은 단계를 거친다:[1]

**초기 필터링 (Initial Filtering)**

8개 모델(Llama-2-7B, Llama-2-13B, Mistral-7B, Gemma-7B, Yi-6B 등)을 사용하여 각 질문에 대해 평가한다. 4개 이상의 모델이 정확히 답변한 질문은 '너무 쉬운' 것으로 간주되어 제외된다. 이 과정을 통해 원본 MMLU의 13,937개 질문 중 5,886개(42.23%)가 필터링된다.[1]

**질문 수집 및 통합**

다양한 출처에서 질문을 통합한다:[1]
- 원본 MMLU 질문 (56.6%)
- STEM 웹사이트 (33.9%)
- TheoremQA (4.97%)
- SciBench (4.5%)

STEM 웹사이트와 TheoremQA의 질문은 GPT-4-Turbo를 사용하여 객관식 형식으로 변환되며, 해당 답변의 정확성을 수동으로 검증한다.[1]

**선택지 확대 (Option Augmentation)**

GPT-4-Turbo를 사용하여 4개 선택지에서 10개로 확대한다. 새로운 6개 선택지는 단순히 양적 추가가 아니라, 정확한 답변을 위해 신중한 추론이 필요한 그럴듯한 방해선택지(plausible distractors)로 설계된다.[1]

**전문가 검토 (Expert Review)**

2단계로 진행된다:[1]

- **1단계**: 각 답변의 정확성을 검증하고, 객관식 형식에 부적절한 질문을 제거한다.
- **2단계**: Gemini-1.5-Pro를 사용하여 모든 선택지를 재평가하고 거짓 음성(false negative)을 식별한 후, 인간 전문가가 이를 검토한다.

표1에서 보면, 식별된 오류 분포는:[1]
- 잘못된 답변: 1,344개
- 거짓 음성 선택지: 2,266개
- 불량 질문: 1,263개

최종적으로 MMLU-Pro는 14개 학문 분야에 걸쳐 12,032개의 질문으로 구성되며, 83%의 질문이 정확히 10개 선택지를 가지고 있고, 17%는 10개 미만, 평균 9.47개 선택지를 보유한다.[1]

#### 2.3 평가 방법론

**5-Shot Chain-of-Thought 프롬프팅**

모델의 성능은 5-shot CoT 접근법을 사용하여 측정된다. 이는 각 학문 분야별로 5개의 대표적 시연 사례를 선택하여 구성된다.[1]

**답변 추출 메커니즘**

정규표현식 패턴 `answer is [A-J]`을 사용하여 모델의 생성 내용에서 답변을 추출한다. 이 정규표현식이 실패하면 보조 정규표현식 `.a[A-J]`를 시도하며, 둘 다 실패하면 답변 선택지 중 무작위 선택이 이루어진다.[1]

***

### 3. 모델 구조 및 성능 분석

#### 3.1 평가 모델

총 50개 이상의 모델이 평가되었으며, 주요 모델들의 성능은 다음과 같다:[1]

**폐쇄형 모델 (Closed-source Models)**

| 모델 | 전체 정확도 | 수학 | 물리학 | 공학 |
|------|---------|------|--------|------|
| GPT-4o | 72.6% | 76.1% | 74.7% | 55.0% |
| Gemini-1.5-Pro | 69.0% | 72.8% | 70.4% | 48.7% |
| Claude-3-Opus | 68.5% | 69.6% | 69.7% | 48.4% |
| GPT-4-Turbo | 63.7% | 62.8% | 61.0% | 35.9% |

**오픈소스 모델 (Open-source Models)**

| 모델 | 전체 정확도 |
|------|---------|
| Llama-3-70B-Instruct | 56.2% |
| DeepSeek-V2-Chat | 54.8% |
| Qwen1.5-72B-Chat | 52.6% |
| Llama-3-70B | 52.8% |

#### 3.2 학문 분야별 성능 특성

**계산/추론 집약적 분야 (수학, 물리학)**

이들 분야에서 모델 간 성능 격차가 가장 크다. GPT-4o의 76.1%(수학), 74.7%(물리학)에서 Mistral-7B-v0.1의 23.5%(수학), 24.8%(물리학)까지 나타난다. 이는 MMLU-Pro의 모델 차별성 증대를 잘 보여준다.[1]

**지식 집약적 분야 (역사, 심리학)**

일반적으로 더 높은 성능 기준선을 보인다. 흥미롭게도 DeepSeek-V2-Chat은 이 분야에서 동료 모델 대비 상대적으로 약한 성능을 보여, 지식 검색보다 추론 능력이 더 우수함을 시사한다.[1]

**저성능 분야 (공학, 법학)**

공학은 STEM 웹사이트의 새로운 질문들로 인해 복잡한 공식 도출과 다단계 계산을 요구하며, 법학은 복잡하고 상세한 질문 및 증가된 선택지 수로 인해 더 깊은 법적 추론을 필요로 한다.[1]

***

### 4. 일반화 성능 향상 및 모델 학습 동역학

#### 4.1 Chain-of-Thought의 효과

표3에서 보이듯이, CoT와 직접 답변(Direct Answering, DA) 간의 성능 차이는 MMLU-Pro에서 훨씬 크다:[1]

**MMLU 대비 MMLU-Pro에서의 CoT 성능 향상**

| 모델 | MMLU (CoT-DA) | MMLU-Pro (CoT-DA) |
|------|---|---|
| GPT-4o | +1.5% | +19.1% |
| GPT-4-Turbo | -0.2% | +15.3% |
| Phi-3-medium | +1.4% | +8.2% |
| Llama-3-8B | -3.9% | +3.9% |

이 차이는 MMLU-Pro가 더 깊은 추론 능력을 평가하기 위해 특별히 설계되었음을 입증한다. 원본 MMLU에서 CoT는 성능을 해칠 수도 있지만, MMLU-Pro에서는 지속적으로 성능을 향상시킨다.[1]

#### 4.2 프롬프트 견고성

MMLU-Pro는 프롬프트 변형에 대한 현저히 높은 견고성을 보인다:[1]

**프롬프트 변동성 비교**

24개의 서로 다르지만 합리적인 프롬프트 스타일을 사용한 평가 결과:
- MMLU: 일반적으로 4-5% 변동, 최대 10.98%
- MMLU-Pro: 일반적으로 ~2% 변동, 최대 3.74%

이는 MMLU-Pro가 벤치마크의 견고성을 상당히 향상시켰음을 의미한다.[1]

#### 4.3 일반화 성능 분석

최신 연구에 따르면, 모델 크기 증대가 반드시 더 나은 일반화를 의미하지는 않는다. 합성 다중 경로 추론 환경에서의 연구에 따르면, 과도한 매개변수화는 과도한 암기로 인해 추론 성능을 저해할 수 있다. 이는 MMLU-Pro의 핵심 발견 중 하나 - 더 도전적인 질문이 진정한 추론 능력의 발달을 강제한다는 주장 - 을 강화한다.[2][1]

***

### 5. 오류 분석 및 한계

#### 5.1 GPT-4o 오류 분석

현재 최고 성능 모델인 GPT-4o의 120개 오류 사례에 대한 상세한 분석:[1]

**오류 원인 분포**

$$\text{추론 오류}: 39\% \mid \text{지식 부족}: 35\% \mid \text{계산 오류}: 12\% \mid \text{기타}: 14\%$$

**상세 분석**

- **추론 오류 (39%)**: 모델이 올바른 정보를 회수하더라도 논리적 처리 단계에서 어려움을 겪는다. 이는 모델이 훈련 데이터의 패턴 인식보다 진정한 이해에 의존하지 않음을 시사한다.[1]

- **지식 부족 (35%)**: 특정 도메인 지식의 결핍이 원인. 예를 들어, 재무 계산, 광학 원리 적용 등에서 오류가 발생한다.[1]

- **계산 오류 (12%)**: 정확한 공식은 가지고 있지만 다단계 계산에서 실수가 발생한다.[1]

#### 5.2 벤치마크의 한계

다음과 같은 제약 사항이 존재한다:[1]

1. **객관식 형식의 한계**: 객관식 형식은 개방형 답변만큼 깊이 있는 이해와 창의적인 반응 생성을 효과적으로 평가할 수 없다.

2. **언어 모델 중심**: MMLU-Pro는 언어 모델에만 초점을 맞추며 시각, 청각, 텍스트 데이터를 합성해야 하는 다중 모달 모델을 평가하지 않는다.

3. **벤치마크 누수**: 충분한 시간이 지나면 학습된 모델이 공개 데이터셋을 통해 벤치마크에 오염될 수 있다.

***

### 6. 앞으로의 연구에 미치는 영향 및 고려 사항

#### 6.1 벤치마크 진화의 패러다임

**동적 벤치마크 업데이트의 필요성**

MMLU-Pro의 성공은 벤치마크가 정적이 아닌 동적 구조여야 함을 시사한다. 최신 연구에 따르면, 자동 데이터셋 업데이트 메커니즘이 필요하다. 모델이 기존 벤치마크를 정복하면 새로운 어려운 질문으로 벤치마크를 갱신하는 방식이 효율적이다.[3]

**MMLU-Pro 이후의 진화**

2024년 10월 발표된 MMLU-Pro+는 한 단계 더 나아간다. 이는 다중 정답을 갖는 질문을 통합하여 지름길 학습(shortcut learning)을 평가하고 더 높은 차원의 추론을 테스트한다.[4]

#### 6.2 추론 능력과 일반화

**도메인 간 일반화**

최신 연구에 따르면, 수학 데이터로 학습한 모델이 코딩, 과학 등 다른 분야에서도 성능 향상을 보인다. 이는 MMLU-Pro의 추론 중심 설계가 더 나은 전이 학습(transfer learning)을 가능하게 함을 시사한다.[5]

**학습 동역학과 일반화**

최근 ICML 2025에서 발표된 연구에 따르면, 모델의 사전 암기 훈련 정확도(pre-memorization train accuracy)가 테스트 성능을 효과적으로 예측할 수 있다.[6]

$$R^2 \geq 0.9$$

이는 MMLU-Pro 같은 도전적 벤치마크에서 모델이 순수 패턴 인식이 아닌 진정한 추론을 학습하는지 평가할 수 있음을 의미한다.

#### 6.3 다중 모달 벤치마킹의 방향

MMLU-Pro의 단일 모달 제한을 극복하기 위해, MMEvalPro 같은 다중 모달 벤치마크가 출현했다. 이는 시각, 청각, 텍스트를 통합하면서도 MMLU-Pro의 견고성을 유지한다. 최고 성능 다중 모달 모델이 인간 성능보다 31.73% 뒤쳐지는 반면, 기존 벤치마크에서는 평균 8.03%만 뒤쳐진다.[7]

#### 6.4 다언어 평가

EU20-MMLU 같은 다언어 벤치마크의 개발도 진행 중이다. 이는 MMLU-Pro의 원칙을 다른 언어권에 확대하는 중요한 방향이다.[8]

#### 6.5 실무적 고려사항

**조직 차원의 평가**

2025년 LLM 평가 현황에 따르면, 일반 벤치마크("MMLU 통과율 X%")만으로는 실무 응용에 적합하지 않다. 조직은 다음 7가지 차원에서 포괄적 평가 프레임워크를 구축해야 한다:[9]

1. 정확성 및 지식
2. 안전성 및 해악 방지
3. 공정성 및 편향
4. 견고성
5. 보정 및 불확실성
6. 효율성
7. 정렬 및 도움 유용성

**벤치마크 과적합의 위험**

MMLU 같은 특정 벤치마크에 과도하게 최적화되는 것을 방지해야 한다. 최신 다중 평가 프레임워크(MixEval)에 따르면, 벤치마크 혼합 접근법이 Chatbot Arena와 0.96의 상관도를 보여, 더 신뢰할 수 있는 평가를 제공한다.[10]

***

### 결론

MMLU-Pro는 단순한 벤치마크 개선을 넘어 **LLM 평가의 패러다임 전환**을 나타낸다. 지식 기반에서 추론 기반으로, 정적 평가에서 동적 평가로, 단일 모달에서 다중 모달로의 진화를 보여준다. 향후 연구에서는 (1) 자동 벤치마크 업데이트 메커니즘, (2) 진정한 추론 학습 vs. 패턴 암기의 구분, (3) 도메인 간 일반화 향상, (4) 다중 모달 및 다언어 평가 확대가 중요한 과제로 남아 있다.[4][7][5][2]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/db95db5e-1673-48b7-bb46-bd974d8c4344/2406.01574v6.pdf)
[2](https://openreview.net/forum?id=JNTaZD7Iam)
[3](https://arxiv.org/pdf/2402.11894.pdf)
[4](http://arxiv.org/pdf/2409.02257v2.pdf)
[5](https://www.microsoft.com/en-us/research/blog/new-methods-boost-reasoning-in-small-and-large-language-models/)
[6](https://proceedings.mlr.press/v267/kang25f.html)
[7](http://arxiv.org/pdf/2407.00468.pdf)
[8](http://arxiv.org/pdf/2410.08928.pdf)
[9](https://responsibleailabs.ai/knowledge-hub/articles/llm-evaluation-benchmarks-2025)
[10](http://arxiv.org/pdf/2406.06565.pdf)
[11](https://arxiv.org/pdf/2406.04127.pdf)
[12](https://arxiv.org/pdf/2410.22368.pdf)
[13](https://arxiv.org/pdf/2406.01574.pdf)
[14](https://github.com/TIGER-AI-Lab/MMLU-Pro)
[15](https://www.deepchecks.com/evaluate-state-of-the-art-llm-models/)
[16](https://discuss.pytorch.kr/t/mmlu-pro-llm-mmlu/4421)
[17](https://www.linkedin.com/pulse/latest-evaluation-benchmarks-large-language-models-jin-sdfse)
[18](https://metaschool.so/articles/mmlu-benchmark/)
